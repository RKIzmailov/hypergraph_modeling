{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Hypergraph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import dhg\n",
    "from dhg import Graph, Hypergraph\n",
    "from dhg.data import Cooking200, News20\n",
    "from dhg.models import GCN, HGNN, HGNNP, HNHN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.utils import split_by_ratio\n",
    "\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ast import literal_eval\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\.dhg\n",
      "d:\\Rustem\\2_Education\\9_UWindsor_CSS\\COMP_8720-Topics_in_AI\\project\\hyper_modeling\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(dhg.CACHE_ROOT)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=dhg.data.Cooking200()\n",
    "# for key in d.content:\n",
    "#     d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooking200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = Cooking200(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 27.05090s, Loss: 3.00726\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.11877s, Loss: 2.87365\n",
      "Epoch: 2, Time: 0.97288s, Loss: 2.63956\n",
      "update best: 0.09500\n",
      "Epoch: 3, Time: 0.97195s, Loss: 2.46788\n",
      "Epoch: 4, Time: 0.95451s, Loss: 2.36260\n",
      "Epoch: 5, Time: 0.97938s, Loss: 2.28780\n",
      "update best: 0.13000\n",
      "Epoch: 6, Time: 1.09381s, Loss: 2.21204\n",
      "update best: 0.13500\n",
      "Epoch: 7, Time: 0.93865s, Loss: 2.13388\n",
      "Epoch: 8, Time: 1.13131s, Loss: 2.06262\n",
      "update best: 0.14000\n",
      "Epoch: 9, Time: 1.40688s, Loss: 1.99684\n",
      "Epoch: 10, Time: 1.38883s, Loss: 1.91773\n",
      "Epoch: 11, Time: 1.27973s, Loss: 1.84279\n",
      "Epoch: 12, Time: 1.31780s, Loss: 1.76692\n",
      "Epoch: 13, Time: 1.21678s, Loss: 1.69607\n",
      "Epoch: 14, Time: 1.50236s, Loss: 1.62226\n",
      "Epoch: 15, Time: 1.37457s, Loss: 1.55436\n",
      "Epoch: 16, Time: 1.23136s, Loss: 1.48475\n",
      "Epoch: 17, Time: 1.41042s, Loss: 1.41734\n",
      "Epoch: 18, Time: 1.25632s, Loss: 1.34714\n",
      "Epoch: 19, Time: 1.30601s, Loss: 1.27656\n",
      "update best: 0.16500\n",
      "Epoch: 20, Time: 1.20952s, Loss: 1.20234\n",
      "Epoch: 21, Time: 1.19049s, Loss: 1.14656\n",
      "Epoch: 22, Time: 1.15300s, Loss: 1.08389\n",
      "Epoch: 23, Time: 1.26429s, Loss: 1.02032\n",
      "Epoch: 24, Time: 1.30182s, Loss: 0.95723\n",
      "Epoch: 25, Time: 1.42021s, Loss: 0.90450\n",
      "Epoch: 26, Time: 1.22268s, Loss: 0.84604\n",
      "Epoch: 27, Time: 1.20182s, Loss: 0.79651\n",
      "Epoch: 28, Time: 1.27569s, Loss: 0.74074\n",
      "Epoch: 29, Time: 1.19896s, Loss: 0.69620\n",
      "Epoch: 30, Time: 1.54059s, Loss: 0.66053\n",
      "Epoch: 31, Time: 1.35103s, Loss: 0.61037\n",
      "Epoch: 32, Time: 1.32461s, Loss: 0.56589\n",
      "update best: 0.17000\n",
      "Epoch: 33, Time: 1.33072s, Loss: 0.53298\n",
      "update best: 0.19000\n",
      "Epoch: 34, Time: 1.29055s, Loss: 0.49662\n",
      "update best: 0.21000\n",
      "Epoch: 35, Time: 1.22395s, Loss: 0.46070\n",
      "update best: 0.22500\n",
      "Epoch: 36, Time: 1.18261s, Loss: 0.43163\n",
      "Epoch: 37, Time: 1.18048s, Loss: 0.40783\n",
      "Epoch: 38, Time: 1.32629s, Loss: 0.37449\n",
      "Epoch: 39, Time: 1.18640s, Loss: 0.34452\n",
      "Epoch: 40, Time: 1.17261s, Loss: 0.32959\n",
      "Epoch: 41, Time: 1.35494s, Loss: 0.30877\n",
      "Epoch: 42, Time: 1.29722s, Loss: 0.29094\n",
      "update best: 0.23000\n",
      "Epoch: 43, Time: 1.23423s, Loss: 0.27251\n",
      "update best: 0.23500\n",
      "Epoch: 44, Time: 1.29604s, Loss: 0.25644\n",
      "Epoch: 45, Time: 1.26459s, Loss: 0.24776\n",
      "update best: 0.24500\n",
      "Epoch: 46, Time: 1.37385s, Loss: 0.23207\n",
      "Epoch: 47, Time: 1.25992s, Loss: 0.21182\n",
      "Epoch: 48, Time: 1.31817s, Loss: 0.20342\n",
      "Epoch: 49, Time: 1.46455s, Loss: 0.19555\n",
      "update best: 0.27500\n",
      "Epoch: 50, Time: 1.30664s, Loss: 0.18163\n",
      "update best: 0.28000\n",
      "Epoch: 51, Time: 1.44705s, Loss: 0.17652\n",
      "Epoch: 52, Time: 1.16958s, Loss: 0.16214\n",
      "Epoch: 53, Time: 1.26941s, Loss: 0.15734\n",
      "Epoch: 54, Time: 1.34628s, Loss: 0.14466\n",
      "Epoch: 55, Time: 1.22824s, Loss: 0.14203\n",
      "update best: 0.28500\n",
      "Epoch: 56, Time: 1.34575s, Loss: 0.13349\n",
      "update best: 0.33000\n",
      "Epoch: 57, Time: 1.28025s, Loss: 0.12913\n",
      "update best: 0.34500\n",
      "Epoch: 58, Time: 1.30383s, Loss: 0.12390\n",
      "Epoch: 59, Time: 1.24166s, Loss: 0.11869\n",
      "Epoch: 60, Time: 1.29664s, Loss: 0.11507\n",
      "Epoch: 61, Time: 1.29878s, Loss: 0.11152\n",
      "Epoch: 62, Time: 1.50177s, Loss: 0.10387\n",
      "Epoch: 63, Time: 1.30076s, Loss: 0.10022\n",
      "Epoch: 64, Time: 1.24141s, Loss: 0.09575\n",
      "update best: 0.36500\n",
      "Epoch: 65, Time: 1.22373s, Loss: 0.09613\n",
      "Epoch: 66, Time: 1.36058s, Loss: 0.09143\n",
      "Epoch: 67, Time: 1.39538s, Loss: 0.09173\n",
      "update best: 0.38500\n",
      "Epoch: 68, Time: 1.21041s, Loss: 0.08172\n",
      "Epoch: 69, Time: 1.31102s, Loss: 0.08590\n",
      "Epoch: 70, Time: 1.16226s, Loss: 0.08126\n",
      "update best: 0.39500\n",
      "Epoch: 71, Time: 1.38064s, Loss: 0.07688\n",
      "Epoch: 72, Time: 1.21638s, Loss: 0.07967\n",
      "Epoch: 73, Time: 1.16902s, Loss: 0.07509\n",
      "update best: 0.42500\n",
      "Epoch: 74, Time: 1.25318s, Loss: 0.07688\n",
      "Epoch: 75, Time: 1.19744s, Loss: 0.07053\n",
      "Epoch: 76, Time: 1.23429s, Loss: 0.06861\n",
      "Epoch: 77, Time: 1.18709s, Loss: 0.06849\n",
      "update best: 0.43500\n",
      "Epoch: 78, Time: 1.27665s, Loss: 0.06825\n",
      "Epoch: 79, Time: 1.34091s, Loss: 0.06182\n",
      "Epoch: 80, Time: 1.33787s, Loss: 0.06590\n",
      "Epoch: 81, Time: 1.26965s, Loss: 0.06319\n",
      "Epoch: 82, Time: 1.19577s, Loss: 0.06313\n",
      "Epoch: 83, Time: 1.32820s, Loss: 0.06400\n",
      "Epoch: 84, Time: 1.30356s, Loss: 0.06643\n",
      "Epoch: 85, Time: 1.32382s, Loss: 0.05816\n",
      "Epoch: 86, Time: 1.27220s, Loss: 0.05907\n",
      "Epoch: 87, Time: 1.24621s, Loss: 0.05757\n",
      "Epoch: 88, Time: 1.26966s, Loss: 0.05826\n",
      "Epoch: 89, Time: 1.16190s, Loss: 0.05518\n",
      "update best: 0.45000\n",
      "Epoch: 90, Time: 1.21202s, Loss: 0.05523\n",
      "Epoch: 91, Time: 1.17484s, Loss: 0.05426\n",
      "Epoch: 92, Time: 1.21003s, Loss: 0.05314\n",
      "Epoch: 93, Time: 1.17332s, Loss: 0.05175\n",
      "Epoch: 94, Time: 1.15069s, Loss: 0.05127\n",
      "update best: 0.45500\n",
      "Epoch: 95, Time: 1.19286s, Loss: 0.05020\n",
      "Epoch: 96, Time: 1.24532s, Loss: 0.05462\n",
      "Epoch: 97, Time: 1.27569s, Loss: 0.04893\n",
      "Epoch: 98, Time: 1.21614s, Loss: 0.05175\n",
      "Epoch: 99, Time: 1.39048s, Loss: 0.04687\n",
      "Epoch: 100, Time: 1.24326s, Loss: 0.04922\n",
      "Epoch: 101, Time: 1.24431s, Loss: 0.04435\n",
      "Epoch: 102, Time: 1.19226s, Loss: 0.04717\n",
      "Epoch: 103, Time: 1.35605s, Loss: 0.04634\n",
      "Epoch: 104, Time: 1.18235s, Loss: 0.04488\n",
      "Epoch: 105, Time: 1.17856s, Loss: 0.04129\n",
      "Epoch: 106, Time: 1.23227s, Loss: 0.04684\n",
      "Epoch: 107, Time: 1.26220s, Loss: 0.04706\n",
      "Epoch: 108, Time: 1.23621s, Loss: 0.04364\n",
      "Epoch: 109, Time: 1.18581s, Loss: 0.04682\n",
      "Epoch: 110, Time: 1.22108s, Loss: 0.04590\n",
      "Epoch: 111, Time: 1.33051s, Loss: 0.04269\n",
      "Epoch: 112, Time: 1.14877s, Loss: 0.04778\n",
      "Epoch: 113, Time: 1.16577s, Loss: 0.04367\n",
      "update best: 0.46000\n",
      "Epoch: 114, Time: 1.27170s, Loss: 0.04239\n",
      "Epoch: 115, Time: 1.19230s, Loss: 0.04223\n",
      "Epoch: 116, Time: 1.30985s, Loss: 0.04184\n",
      "Epoch: 117, Time: 1.34204s, Loss: 0.04116\n",
      "Epoch: 118, Time: 1.19596s, Loss: 0.04068\n",
      "Epoch: 119, Time: 1.23354s, Loss: 0.03786\n",
      "Epoch: 120, Time: 1.42493s, Loss: 0.04072\n",
      "update best: 0.47500\n",
      "Epoch: 121, Time: 1.24169s, Loss: 0.03664\n",
      "Epoch: 122, Time: 1.24111s, Loss: 0.03947\n",
      "Epoch: 123, Time: 1.23722s, Loss: 0.03868\n",
      "Epoch: 124, Time: 1.31976s, Loss: 0.03620\n",
      "Epoch: 125, Time: 1.19306s, Loss: 0.03710\n",
      "Epoch: 126, Time: 1.18464s, Loss: 0.03816\n",
      "Epoch: 127, Time: 1.23385s, Loss: 0.03773\n",
      "Epoch: 128, Time: 1.15674s, Loss: 0.03621\n",
      "Epoch: 129, Time: 1.22596s, Loss: 0.03730\n",
      "Epoch: 130, Time: 1.14205s, Loss: 0.03664\n",
      "Epoch: 131, Time: 1.15340s, Loss: 0.03373\n",
      "Epoch: 132, Time: 1.32747s, Loss: 0.03674\n",
      "Epoch: 133, Time: 1.22853s, Loss: 0.04360\n",
      "Epoch: 134, Time: 1.34267s, Loss: 0.03768\n",
      "Epoch: 135, Time: 1.41897s, Loss: 0.03557\n",
      "Epoch: 136, Time: 1.31278s, Loss: 0.03471\n",
      "Epoch: 137, Time: 1.20829s, Loss: 0.03593\n",
      "Epoch: 138, Time: 1.20432s, Loss: 0.03424\n",
      "Epoch: 139, Time: 1.29845s, Loss: 0.03539\n",
      "Epoch: 140, Time: 1.38204s, Loss: 0.03442\n",
      "Epoch: 141, Time: 1.27594s, Loss: 0.03629\n",
      "Epoch: 142, Time: 1.30157s, Loss: 0.03312\n",
      "Epoch: 143, Time: 1.18935s, Loss: 0.03841\n",
      "Epoch: 144, Time: 1.25200s, Loss: 0.04076\n",
      "Epoch: 145, Time: 1.23712s, Loss: 0.06876\n",
      "Epoch: 146, Time: 1.13729s, Loss: 0.03650\n",
      "Epoch: 147, Time: 1.23742s, Loss: 0.06332\n",
      "Epoch: 148, Time: 1.35226s, Loss: 0.04392\n",
      "Epoch: 149, Time: 1.48591s, Loss: 0.06565\n",
      "Epoch: 150, Time: 1.18588s, Loss: 0.04007\n",
      "Epoch: 151, Time: 1.20833s, Loss: 0.07563\n",
      "Epoch: 152, Time: 1.35463s, Loss: 0.03581\n",
      "Epoch: 153, Time: 1.26155s, Loss: 0.09672\n",
      "Epoch: 154, Time: 1.18569s, Loss: 0.05547\n",
      "Epoch: 155, Time: 1.18879s, Loss: 0.04690\n",
      "Epoch: 156, Time: 1.28119s, Loss: 0.05561\n",
      "Epoch: 157, Time: 1.25364s, Loss: 0.05781\n",
      "Epoch: 158, Time: 1.22963s, Loss: 0.04668\n",
      "Epoch: 159, Time: 1.18674s, Loss: 0.04285\n",
      "Epoch: 160, Time: 1.33488s, Loss: 0.04534\n",
      "Epoch: 161, Time: 1.34731s, Loss: 0.04604\n",
      "Epoch: 162, Time: 1.34935s, Loss: 0.03965\n",
      "Epoch: 163, Time: 1.26145s, Loss: 0.03661\n",
      "Epoch: 164, Time: 1.27685s, Loss: 0.03958\n",
      "Epoch: 165, Time: 1.24223s, Loss: 0.03301\n",
      "Epoch: 166, Time: 1.29492s, Loss: 0.03590\n",
      "Epoch: 167, Time: 1.16915s, Loss: 0.02961\n",
      "Epoch: 168, Time: 1.18475s, Loss: 0.03390\n",
      "Epoch: 169, Time: 1.22329s, Loss: 0.03066\n",
      "Epoch: 170, Time: 1.19592s, Loss: 0.03081\n",
      "Epoch: 171, Time: 1.24858s, Loss: 0.02582\n",
      "Epoch: 172, Time: 1.29168s, Loss: 0.02749\n",
      "Epoch: 173, Time: 1.19019s, Loss: 0.02687\n",
      "Epoch: 174, Time: 1.21539s, Loss: 0.02652\n",
      "Epoch: 175, Time: 1.30223s, Loss: 0.02704\n",
      "Epoch: 176, Time: 1.52252s, Loss: 0.02565\n",
      "Epoch: 177, Time: 1.42731s, Loss: 0.03043\n",
      "Epoch: 178, Time: 1.26753s, Loss: 0.02406\n",
      "Epoch: 179, Time: 2.60946s, Loss: 0.03481\n",
      "Epoch: 180, Time: 2.52241s, Loss: 0.02621\n",
      "Epoch: 181, Time: 2.60322s, Loss: 0.04958\n",
      "Epoch: 182, Time: 2.56016s, Loss: 0.03141\n",
      "Epoch: 183, Time: 2.58308s, Loss: 0.04558\n",
      "Epoch: 184, Time: 2.49239s, Loss: 0.05380\n",
      "Epoch: 185, Time: 2.60713s, Loss: 0.04330\n",
      "Epoch: 186, Time: 2.59470s, Loss: 0.03930\n",
      "Epoch: 187, Time: 2.72259s, Loss: 0.04647\n",
      "Epoch: 188, Time: 2.51598s, Loss: 0.04012\n",
      "Epoch: 189, Time: 2.54825s, Loss: 0.03249\n",
      "Epoch: 190, Time: 2.59999s, Loss: 0.03584\n",
      "Epoch: 191, Time: 2.76925s, Loss: 0.03525\n",
      "Epoch: 192, Time: 2.05392s, Loss: 0.03017\n",
      "Epoch: 193, Time: 1.00734s, Loss: 0.03143\n",
      "Epoch: 194, Time: 1.02639s, Loss: 0.03074\n",
      "Epoch: 195, Time: 1.19079s, Loss: 0.02683\n",
      "Epoch: 196, Time: 0.99288s, Loss: 0.02414\n",
      "Epoch: 197, Time: 1.00549s, Loss: 0.02461\n",
      "Epoch: 198, Time: 1.13103s, Loss: 0.02479\n",
      "Epoch: 199, Time: 1.02974s, Loss: 0.02296\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 120\n",
      "{'accuracy': 0.45623305439949036, 'f1_score': 0.3653164109596893, 'f1_score -> average@micro': 0.4562330429815793}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 1.27188s, Loss: 3.00386\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.54415s, Loss: 2.70985\n",
      "Epoch: 2, Time: 1.50128s, Loss: 2.37330\n",
      "Epoch: 3, Time: 1.81600s, Loss: 2.18877\n",
      "Epoch: 4, Time: 1.50025s, Loss: 2.05079\n",
      "Epoch: 5, Time: 1.49115s, Loss: 1.92147\n",
      "Epoch: 6, Time: 1.39025s, Loss: 1.80867\n",
      "update best: 0.09000\n",
      "Epoch: 7, Time: 1.39048s, Loss: 1.68405\n",
      "update best: 0.09500\n",
      "Epoch: 8, Time: 1.40198s, Loss: 1.55780\n",
      "Epoch: 9, Time: 1.32872s, Loss: 1.45725\n",
      "Epoch: 10, Time: 1.33453s, Loss: 1.36144\n",
      "Epoch: 11, Time: 1.98168s, Loss: 1.23599\n",
      "Epoch: 12, Time: 1.50997s, Loss: 1.14826\n",
      "Epoch: 13, Time: 1.64627s, Loss: 1.03714\n",
      "Epoch: 14, Time: 1.72242s, Loss: 0.94961\n",
      "update best: 0.10000\n",
      "Epoch: 15, Time: 1.53512s, Loss: 0.87812\n",
      "update best: 0.10500\n",
      "Epoch: 16, Time: 1.35259s, Loss: 0.79208\n",
      "Epoch: 17, Time: 1.35914s, Loss: 0.72543\n",
      "Epoch: 18, Time: 1.38262s, Loss: 0.65884\n",
      "Epoch: 19, Time: 1.37241s, Loss: 0.59730\n",
      "Epoch: 20, Time: 1.41043s, Loss: 0.55186\n",
      "Epoch: 21, Time: 1.45364s, Loss: 0.48377\n",
      "Epoch: 22, Time: 1.33518s, Loss: 0.44027\n",
      "Epoch: 23, Time: 1.42071s, Loss: 0.39254\n",
      "Epoch: 24, Time: 1.34800s, Loss: 0.35922\n",
      "Epoch: 25, Time: 1.33080s, Loss: 0.31876\n",
      "Epoch: 26, Time: 1.29542s, Loss: 0.29382\n",
      "Epoch: 27, Time: 1.43415s, Loss: 0.25786\n",
      "Epoch: 28, Time: 1.37351s, Loss: 0.23379\n",
      "Epoch: 29, Time: 1.43001s, Loss: 0.21166\n",
      "update best: 0.11000\n",
      "Epoch: 30, Time: 1.27605s, Loss: 0.18889\n",
      "Epoch: 31, Time: 1.30442s, Loss: 0.17234\n",
      "Epoch: 32, Time: 1.25549s, Loss: 0.15468\n",
      "update best: 0.12000\n",
      "Epoch: 33, Time: 1.28189s, Loss: 0.14374\n",
      "update best: 0.12500\n",
      "Epoch: 34, Time: 1.25804s, Loss: 0.13073\n",
      "Epoch: 35, Time: 1.24336s, Loss: 0.11533\n",
      "update best: 0.13500\n",
      "Epoch: 36, Time: 1.28270s, Loss: 0.10756\n",
      "update best: 0.14000\n",
      "Epoch: 37, Time: 1.22811s, Loss: 0.09825\n",
      "update best: 0.15000\n",
      "Epoch: 38, Time: 1.37117s, Loss: 0.09213\n",
      "update best: 0.15500\n",
      "Epoch: 39, Time: 1.32696s, Loss: 0.08591\n",
      "Epoch: 40, Time: 1.31704s, Loss: 0.08078\n",
      "Epoch: 41, Time: 1.43949s, Loss: 0.07544\n",
      "Epoch: 42, Time: 1.37561s, Loss: 0.07192\n",
      "Epoch: 43, Time: 1.38291s, Loss: 0.06496\n",
      "Epoch: 44, Time: 1.30007s, Loss: 0.06504\n",
      "Epoch: 45, Time: 1.34068s, Loss: 0.06036\n",
      "Epoch: 46, Time: 1.35244s, Loss: 0.05478\n",
      "Epoch: 47, Time: 1.37770s, Loss: 0.05395\n",
      "Epoch: 48, Time: 1.43613s, Loss: 0.05261\n",
      "Epoch: 49, Time: 1.39924s, Loss: 0.05150\n",
      "Epoch: 50, Time: 1.44943s, Loss: 0.04703\n",
      "Epoch: 51, Time: 1.46634s, Loss: 0.04725\n",
      "Epoch: 52, Time: 1.47481s, Loss: 0.04718\n",
      "Epoch: 53, Time: 1.44230s, Loss: 0.04429\n",
      "update best: 0.16500\n",
      "Epoch: 54, Time: 1.46572s, Loss: 0.04436\n",
      "update best: 0.19500\n",
      "Epoch: 55, Time: 1.43431s, Loss: 0.04274\n",
      "update best: 0.21000\n",
      "Epoch: 56, Time: 1.49898s, Loss: 0.04083\n",
      "Epoch: 57, Time: 1.40041s, Loss: 0.04188\n",
      "update best: 0.23500\n",
      "Epoch: 58, Time: 1.43422s, Loss: 0.04199\n",
      "update best: 0.25000\n",
      "Epoch: 59, Time: 1.37231s, Loss: 0.04150\n",
      "update best: 0.28000\n",
      "Epoch: 60, Time: 1.53890s, Loss: 0.03965\n",
      "update best: 0.28500\n",
      "Epoch: 61, Time: 1.41634s, Loss: 0.03752\n",
      "Epoch: 62, Time: 1.51327s, Loss: 0.03809\n",
      "Epoch: 63, Time: 1.36452s, Loss: 0.03837\n",
      "Epoch: 64, Time: 1.44806s, Loss: 0.03919\n",
      "Epoch: 65, Time: 1.47120s, Loss: 0.03748\n",
      "Epoch: 66, Time: 1.32771s, Loss: 0.03714\n",
      "Epoch: 67, Time: 1.32778s, Loss: 0.03501\n",
      "Epoch: 68, Time: 1.31287s, Loss: 0.03392\n",
      "update best: 0.31000\n",
      "Epoch: 69, Time: 1.32246s, Loss: 0.03251\n",
      "update best: 0.34500\n",
      "Epoch: 70, Time: 1.40177s, Loss: 0.03559\n",
      "update best: 0.37000\n",
      "Epoch: 71, Time: 1.32568s, Loss: 0.03304\n",
      "Epoch: 72, Time: 1.42692s, Loss: 0.03425\n",
      "Epoch: 73, Time: 1.72085s, Loss: 0.03320\n",
      "update best: 0.37500\n",
      "Epoch: 74, Time: 1.43077s, Loss: 0.03200\n",
      "Epoch: 75, Time: 1.35294s, Loss: 0.03212\n",
      "update best: 0.39000\n",
      "Epoch: 76, Time: 1.30638s, Loss: 0.03018\n",
      "Epoch: 77, Time: 1.30985s, Loss: 0.03008\n",
      "update best: 0.39500\n",
      "Epoch: 78, Time: 1.31392s, Loss: 0.03139\n",
      "Epoch: 79, Time: 1.43192s, Loss: 0.02849\n",
      "update best: 0.40000\n",
      "Epoch: 80, Time: 1.40814s, Loss: 0.03067\n",
      "update best: 0.41000\n",
      "Epoch: 81, Time: 1.31110s, Loss: 0.02883\n",
      "update best: 0.45500\n",
      "Epoch: 82, Time: 1.43553s, Loss: 0.02801\n",
      "update best: 0.46000\n",
      "Epoch: 83, Time: 1.42795s, Loss: 0.02717\n",
      "update best: 0.47500\n",
      "Epoch: 84, Time: 1.36155s, Loss: 0.02615\n",
      "Epoch: 85, Time: 1.50270s, Loss: 0.02605\n",
      "Epoch: 86, Time: 1.58583s, Loss: 0.02592\n",
      "Epoch: 87, Time: 1.50066s, Loss: 0.02628\n",
      "Epoch: 88, Time: 1.85431s, Loss: 0.02556\n",
      "Epoch: 89, Time: 1.68659s, Loss: 0.02421\n",
      "Epoch: 90, Time: 1.32606s, Loss: 0.02532\n",
      "Epoch: 91, Time: 1.39409s, Loss: 0.02611\n",
      "Epoch: 92, Time: 1.45525s, Loss: 0.02378\n",
      "Epoch: 93, Time: 1.37677s, Loss: 0.02510\n",
      "Epoch: 94, Time: 1.31949s, Loss: 0.02376\n",
      "Epoch: 95, Time: 1.29086s, Loss: 0.02301\n",
      "Epoch: 96, Time: 1.28152s, Loss: 0.02325\n",
      "Epoch: 97, Time: 1.22093s, Loss: 0.02275\n",
      "Epoch: 98, Time: 1.31907s, Loss: 0.02393\n",
      "update best: 0.48500\n",
      "Epoch: 99, Time: 1.37080s, Loss: 0.02081\n",
      "Epoch: 100, Time: 1.40924s, Loss: 0.02302\n",
      "Epoch: 101, Time: 1.69402s, Loss: 0.02299\n",
      "update best: 0.49500\n",
      "Epoch: 102, Time: 1.32635s, Loss: 0.02197\n",
      "Epoch: 103, Time: 1.33762s, Loss: 0.02165\n",
      "Epoch: 104, Time: 1.32808s, Loss: 0.02216\n",
      "Epoch: 105, Time: 1.32325s, Loss: 0.02217\n",
      "Epoch: 106, Time: 1.39723s, Loss: 0.02029\n",
      "Epoch: 107, Time: 1.37769s, Loss: 0.02081\n",
      "Epoch: 108, Time: 1.46459s, Loss: 0.02086\n",
      "Epoch: 109, Time: 1.31666s, Loss: 0.01997\n",
      "Epoch: 110, Time: 1.33238s, Loss: 0.02177\n",
      "Epoch: 111, Time: 1.56508s, Loss: 0.02051\n",
      "Epoch: 112, Time: 1.42414s, Loss: 0.02007\n",
      "Epoch: 113, Time: 1.27276s, Loss: 0.02031\n",
      "Epoch: 114, Time: 1.27076s, Loss: 0.01989\n",
      "Epoch: 115, Time: 1.33571s, Loss: 0.01988\n",
      "Epoch: 116, Time: 1.46237s, Loss: 0.02025\n",
      "Epoch: 117, Time: 1.29031s, Loss: 0.01742\n",
      "Epoch: 118, Time: 1.35665s, Loss: 0.01813\n",
      "update best: 0.50000\n",
      "Epoch: 119, Time: 1.33894s, Loss: 0.01844\n",
      "Epoch: 120, Time: 1.48070s, Loss: 0.01895\n",
      "Epoch: 121, Time: 1.43016s, Loss: 0.01908\n",
      "Epoch: 122, Time: 1.44818s, Loss: 0.01838\n",
      "Epoch: 123, Time: 1.33738s, Loss: 0.01817\n",
      "Epoch: 124, Time: 1.56268s, Loss: 0.01969\n",
      "Epoch: 125, Time: 1.36915s, Loss: 0.01744\n",
      "Epoch: 126, Time: 1.29028s, Loss: 0.01882\n",
      "Epoch: 127, Time: 1.34672s, Loss: 0.01783\n",
      "Epoch: 128, Time: 1.47206s, Loss: 0.01902\n",
      "Epoch: 129, Time: 1.49847s, Loss: 0.01799\n",
      "Epoch: 130, Time: 1.34965s, Loss: 0.01803\n",
      "Epoch: 131, Time: 1.35305s, Loss: 0.01799\n",
      "Epoch: 132, Time: 1.27873s, Loss: 0.01825\n",
      "Epoch: 133, Time: 1.29053s, Loss: 0.01852\n",
      "Epoch: 134, Time: 1.37004s, Loss: 0.01676\n",
      "Epoch: 135, Time: 1.32739s, Loss: 0.01684\n",
      "Epoch: 136, Time: 1.29580s, Loss: 0.01663\n",
      "Epoch: 137, Time: 1.31958s, Loss: 0.01641\n",
      "Epoch: 138, Time: 1.31170s, Loss: 0.01648\n",
      "Epoch: 139, Time: 1.30674s, Loss: 0.01749\n",
      "Epoch: 140, Time: 1.28187s, Loss: 0.01731\n",
      "Epoch: 141, Time: 1.27024s, Loss: 0.01744\n",
      "Epoch: 142, Time: 1.33814s, Loss: 0.01645\n",
      "Epoch: 143, Time: 1.42886s, Loss: 0.01751\n",
      "Epoch: 144, Time: 1.47848s, Loss: 0.01728\n",
      "Epoch: 145, Time: 1.38144s, Loss: 0.01801\n",
      "Epoch: 146, Time: 1.46823s, Loss: 0.01696\n",
      "Epoch: 147, Time: 1.45598s, Loss: 0.02160\n",
      "Epoch: 148, Time: 1.72187s, Loss: 0.01769\n",
      "Epoch: 149, Time: 1.70991s, Loss: 0.02529\n",
      "Epoch: 150, Time: 1.31159s, Loss: 0.02321\n",
      "Epoch: 151, Time: 1.46281s, Loss: 0.03171\n",
      "Epoch: 152, Time: 1.41087s, Loss: 0.01670\n",
      "Epoch: 153, Time: 1.51190s, Loss: 0.04494\n",
      "Epoch: 154, Time: 1.42654s, Loss: 0.01860\n",
      "Epoch: 155, Time: 1.53499s, Loss: 0.02177\n",
      "Epoch: 156, Time: 1.52821s, Loss: 0.03615\n",
      "Epoch: 157, Time: 1.38554s, Loss: 0.01814\n",
      "Epoch: 158, Time: 1.37736s, Loss: 0.01865\n",
      "Epoch: 159, Time: 1.54945s, Loss: 0.02191\n",
      "Epoch: 160, Time: 1.44442s, Loss: 0.02447\n",
      "Epoch: 161, Time: 1.44328s, Loss: 0.02430\n",
      "Epoch: 162, Time: 1.43570s, Loss: 0.01935\n",
      "Epoch: 163, Time: 1.39545s, Loss: 0.01849\n",
      "Epoch: 164, Time: 1.39675s, Loss: 0.01774\n",
      "Epoch: 165, Time: 1.39179s, Loss: 0.01589\n",
      "Epoch: 166, Time: 1.36605s, Loss: 0.01702\n",
      "Epoch: 167, Time: 1.40766s, Loss: 0.01585\n",
      "Epoch: 168, Time: 1.46108s, Loss: 0.01639\n",
      "Epoch: 169, Time: 1.38070s, Loss: 0.01560\n",
      "Epoch: 170, Time: 1.45014s, Loss: 0.01578\n",
      "Epoch: 171, Time: 1.57744s, Loss: 0.01494\n",
      "Epoch: 172, Time: 1.47759s, Loss: 0.01431\n",
      "Epoch: 173, Time: 1.40481s, Loss: 0.01409\n",
      "Epoch: 174, Time: 1.49832s, Loss: 0.01369\n",
      "Epoch: 175, Time: 1.50229s, Loss: 0.01306\n",
      "Epoch: 176, Time: 1.45149s, Loss: 0.01334\n",
      "Epoch: 177, Time: 1.46486s, Loss: 0.01242\n",
      "Epoch: 178, Time: 1.63365s, Loss: 0.01266\n",
      "Epoch: 179, Time: 2.42034s, Loss: 0.01165\n",
      "Epoch: 180, Time: 1.51814s, Loss: 0.01231\n",
      "Epoch: 181, Time: 1.64823s, Loss: 0.01221\n",
      "Epoch: 182, Time: 1.37344s, Loss: 0.01222\n",
      "Epoch: 183, Time: 1.35596s, Loss: 0.01264\n",
      "Epoch: 184, Time: 1.41736s, Loss: 0.01289\n",
      "Epoch: 185, Time: 1.38307s, Loss: 0.01289\n",
      "Epoch: 186, Time: 1.40860s, Loss: 0.01329\n",
      "Epoch: 187, Time: 1.40430s, Loss: 0.01247\n",
      "Epoch: 188, Time: 1.48603s, Loss: 0.01202\n",
      "Epoch: 189, Time: 1.38514s, Loss: 0.01232\n",
      "Epoch: 190, Time: 1.34340s, Loss: 0.01294\n",
      "Epoch: 191, Time: 1.43782s, Loss: 0.01216\n",
      "Epoch: 192, Time: 1.48778s, Loss: 0.01211\n",
      "Epoch: 193, Time: 1.38422s, Loss: 0.01375\n",
      "Epoch: 194, Time: 1.39332s, Loss: 0.01281\n",
      "Epoch: 195, Time: 1.32660s, Loss: 0.01381\n",
      "Epoch: 196, Time: 1.30333s, Loss: 0.01335\n",
      "Epoch: 197, Time: 1.49123s, Loss: 0.01330\n",
      "Epoch: 198, Time: 1.79635s, Loss: 0.01353\n",
      "Epoch: 199, Time: 1.50184s, Loss: 0.01235\n",
      "\n",
      "train finished!\n",
      "best val: 0.50000\n",
      "test...\n",
      "final result: epoch: 118\n",
      "{'accuracy': 0.5289161801338196, 'f1_score': 0.40503309054521114, 'f1_score -> average@micro': 0.5289161787805227}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.10855s, Loss: 3.00464\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.08103s, Loss: 2.85373\n",
      "Epoch: 2, Time: 0.09561s, Loss: 2.39547\n",
      "Epoch: 3, Time: 0.08431s, Loss: 2.16992\n",
      "Epoch: 4, Time: 0.08556s, Loss: 1.98238\n",
      "Epoch: 5, Time: 0.08011s, Loss: 1.82536\n",
      "update best: 0.07500\n",
      "Epoch: 6, Time: 0.09422s, Loss: 1.69112\n",
      "update best: 0.08000\n",
      "Epoch: 7, Time: 0.08904s, Loss: 1.55686\n",
      "Epoch: 8, Time: 0.09059s, Loss: 1.43632\n",
      "update best: 0.11000\n",
      "Epoch: 9, Time: 0.07604s, Loss: 1.31332\n",
      "update best: 0.22000\n",
      "Epoch: 10, Time: 0.08363s, Loss: 1.20795\n",
      "Epoch: 11, Time: 0.07904s, Loss: 1.09272\n",
      "Epoch: 12, Time: 0.07904s, Loss: 1.00381\n",
      "Epoch: 13, Time: 0.14408s, Loss: 0.90282\n",
      "Epoch: 14, Time: 0.08704s, Loss: 0.81816\n",
      "Epoch: 15, Time: 0.07804s, Loss: 0.74913\n",
      "Epoch: 16, Time: 0.08156s, Loss: 0.66262\n",
      "Epoch: 17, Time: 0.08205s, Loss: 0.60239\n",
      "Epoch: 18, Time: 0.11778s, Loss: 0.53853\n",
      "Epoch: 19, Time: 0.09161s, Loss: 0.48864\n",
      "Epoch: 20, Time: 0.09068s, Loss: 0.42724\n",
      "Epoch: 21, Time: 0.08355s, Loss: 0.38623\n",
      "Epoch: 22, Time: 0.10086s, Loss: 0.34136\n",
      "Epoch: 23, Time: 0.09659s, Loss: 0.30420\n",
      "Epoch: 24, Time: 0.09107s, Loss: 0.27242\n",
      "Epoch: 25, Time: 0.08208s, Loss: 0.23802\n",
      "Epoch: 26, Time: 0.09509s, Loss: 0.21636\n",
      "Epoch: 27, Time: 0.08206s, Loss: 0.19498\n",
      "Epoch: 28, Time: 0.08175s, Loss: 0.17500\n",
      "Epoch: 29, Time: 0.08208s, Loss: 0.15270\n",
      "Epoch: 30, Time: 0.07806s, Loss: 0.13763\n",
      "Epoch: 31, Time: 0.09466s, Loss: 0.12347\n",
      "Epoch: 32, Time: 0.09861s, Loss: 0.11316\n",
      "Epoch: 33, Time: 0.09258s, Loss: 0.10082\n",
      "Epoch: 34, Time: 0.09119s, Loss: 0.09246\n",
      "Epoch: 35, Time: 0.08960s, Loss: 0.08394\n",
      "Epoch: 36, Time: 0.08559s, Loss: 0.07804\n",
      "Epoch: 37, Time: 0.08110s, Loss: 0.07072\n",
      "Epoch: 38, Time: 0.09057s, Loss: 0.06743\n",
      "Epoch: 39, Time: 0.10018s, Loss: 0.06343\n",
      "Epoch: 40, Time: 0.09457s, Loss: 0.05752\n",
      "Epoch: 41, Time: 0.09508s, Loss: 0.05332\n",
      "Epoch: 42, Time: 0.08177s, Loss: 0.05070\n",
      "Epoch: 43, Time: 0.08888s, Loss: 0.04732\n",
      "Epoch: 44, Time: 0.09961s, Loss: 0.04493\n",
      "Epoch: 45, Time: 0.08239s, Loss: 0.04645\n",
      "Epoch: 46, Time: 0.09278s, Loss: 0.04098\n",
      "Epoch: 47, Time: 0.10937s, Loss: 0.04156\n",
      "Epoch: 48, Time: 0.09224s, Loss: 0.03786\n",
      "update best: 0.23500\n",
      "Epoch: 49, Time: 0.08105s, Loss: 0.03783\n",
      "update best: 0.25500\n",
      "Epoch: 50, Time: 0.09759s, Loss: 0.03815\n",
      "update best: 0.26500\n",
      "Epoch: 51, Time: 0.11816s, Loss: 0.03539\n",
      "Epoch: 52, Time: 0.12713s, Loss: 0.03394\n",
      "Epoch: 53, Time: 0.11458s, Loss: 0.03498\n",
      "Epoch: 54, Time: 0.11960s, Loss: 0.03471\n",
      "Epoch: 55, Time: 0.12266s, Loss: 0.03187\n",
      "Epoch: 56, Time: 0.11107s, Loss: 0.03311\n",
      "Epoch: 57, Time: 0.10058s, Loss: 0.03236\n",
      "Epoch: 58, Time: 0.12010s, Loss: 0.03206\n",
      "Epoch: 59, Time: 0.11112s, Loss: 0.03009\n",
      "Epoch: 60, Time: 0.11309s, Loss: 0.03027\n",
      "Epoch: 61, Time: 0.10956s, Loss: 0.03062\n",
      "Epoch: 62, Time: 0.11258s, Loss: 0.03146\n",
      "Epoch: 63, Time: 0.10657s, Loss: 0.02908\n",
      "Epoch: 64, Time: 0.12109s, Loss: 0.03267\n",
      "update best: 0.27000\n",
      "Epoch: 65, Time: 0.10493s, Loss: 0.02796\n",
      "update best: 0.28500\n",
      "Epoch: 66, Time: 0.11657s, Loss: 0.02921\n",
      "update best: 0.31000\n",
      "Epoch: 67, Time: 0.10761s, Loss: 0.02787\n",
      "Epoch: 68, Time: 0.11518s, Loss: 0.02633\n",
      "update best: 0.31500\n",
      "Epoch: 69, Time: 0.11161s, Loss: 0.02667\n",
      "update best: 0.32500\n",
      "Epoch: 70, Time: 0.10609s, Loss: 0.02667\n",
      "update best: 0.33000\n",
      "Epoch: 71, Time: 0.11265s, Loss: 0.02442\n",
      "update best: 0.33500\n",
      "Epoch: 72, Time: 0.10656s, Loss: 0.02611\n",
      "update best: 0.35000\n",
      "Epoch: 73, Time: 0.11760s, Loss: 0.02556\n",
      "Epoch: 74, Time: 0.11409s, Loss: 0.02474\n",
      "Epoch: 75, Time: 0.11714s, Loss: 0.02414\n",
      "Epoch: 76, Time: 0.10258s, Loss: 0.02310\n",
      "Epoch: 77, Time: 0.12112s, Loss: 0.02235\n",
      "Epoch: 78, Time: 0.11957s, Loss: 0.02200\n",
      "Epoch: 79, Time: 0.11412s, Loss: 0.02231\n",
      "Epoch: 80, Time: 0.10962s, Loss: 0.02164\n",
      "update best: 0.35500\n",
      "Epoch: 81, Time: 0.12378s, Loss: 0.02111\n",
      "update best: 0.41000\n",
      "Epoch: 82, Time: 0.11511s, Loss: 0.02094\n",
      "update best: 0.44500\n",
      "Epoch: 83, Time: 0.10360s, Loss: 0.02243\n",
      "Epoch: 84, Time: 0.11059s, Loss: 0.02188\n",
      "Epoch: 85, Time: 0.11810s, Loss: 0.02204\n",
      "Epoch: 86, Time: 0.12082s, Loss: 0.01969\n",
      "Epoch: 87, Time: 0.11518s, Loss: 0.02077\n",
      "Epoch: 88, Time: 0.11263s, Loss: 0.01974\n",
      "Epoch: 89, Time: 0.10610s, Loss: 0.01937\n",
      "Epoch: 90, Time: 0.11911s, Loss: 0.02079\n",
      "Epoch: 91, Time: 0.12612s, Loss: 0.01972\n",
      "Epoch: 92, Time: 0.12459s, Loss: 0.01940\n",
      "Epoch: 93, Time: 0.13528s, Loss: 0.01956\n",
      "Epoch: 94, Time: 0.21866s, Loss: 0.01888\n",
      "Epoch: 95, Time: 0.11109s, Loss: 0.01911\n",
      "Epoch: 96, Time: 0.12051s, Loss: 0.01821\n",
      "Epoch: 97, Time: 0.11910s, Loss: 0.01749\n",
      "Epoch: 98, Time: 0.11008s, Loss: 0.01748\n",
      "Epoch: 99, Time: 0.11158s, Loss: 0.01779\n",
      "Epoch: 100, Time: 0.11512s, Loss: 0.01833\n",
      "Epoch: 101, Time: 0.11421s, Loss: 0.01708\n",
      "Epoch: 102, Time: 0.12238s, Loss: 0.01692\n",
      "Epoch: 103, Time: 0.11256s, Loss: 0.01745\n",
      "update best: 0.45000\n",
      "Epoch: 104, Time: 0.10949s, Loss: 0.01689\n",
      "Epoch: 105, Time: 0.11056s, Loss: 0.01643\n",
      "Epoch: 106, Time: 0.10858s, Loss: 0.01698\n",
      "Epoch: 107, Time: 0.12488s, Loss: 0.01411\n",
      "update best: 0.45500\n",
      "Epoch: 108, Time: 0.11812s, Loss: 0.01557\n",
      "Epoch: 109, Time: 0.11909s, Loss: 0.01596\n",
      "update best: 0.46000\n",
      "Epoch: 110, Time: 0.10959s, Loss: 0.01623\n",
      "Epoch: 111, Time: 0.11502s, Loss: 0.01557\n",
      "update best: 0.47500\n",
      "Epoch: 112, Time: 0.10019s, Loss: 0.01667\n",
      "Epoch: 113, Time: 0.11613s, Loss: 0.01662\n",
      "Epoch: 114, Time: 0.10405s, Loss: 0.01519\n",
      "Epoch: 115, Time: 0.12112s, Loss: 0.01566\n",
      "Epoch: 116, Time: 0.11459s, Loss: 0.01502\n",
      "Epoch: 117, Time: 0.11607s, Loss: 0.01577\n",
      "Epoch: 118, Time: 0.11427s, Loss: 0.01539\n",
      "Epoch: 119, Time: 0.11919s, Loss: 0.01461\n",
      "Epoch: 120, Time: 0.10885s, Loss: 0.01566\n",
      "Epoch: 121, Time: 0.12457s, Loss: 0.01513\n",
      "Epoch: 122, Time: 0.12417s, Loss: 0.01506\n",
      "Epoch: 123, Time: 0.12285s, Loss: 0.01606\n",
      "Epoch: 124, Time: 0.12110s, Loss: 0.01423\n",
      "Epoch: 125, Time: 0.11711s, Loss: 0.01454\n",
      "Epoch: 126, Time: 0.12613s, Loss: 0.01424\n",
      "Epoch: 127, Time: 0.10858s, Loss: 0.01419\n",
      "Epoch: 128, Time: 0.10856s, Loss: 0.01394\n",
      "Epoch: 129, Time: 0.10873s, Loss: 0.01380\n",
      "Epoch: 130, Time: 0.10656s, Loss: 0.01443\n",
      "Epoch: 131, Time: 0.11498s, Loss: 0.01307\n",
      "Epoch: 132, Time: 0.11861s, Loss: 0.01310\n",
      "Epoch: 133, Time: 0.12256s, Loss: 0.01282\n",
      "Epoch: 134, Time: 0.11357s, Loss: 0.01314\n",
      "Epoch: 135, Time: 0.11219s, Loss: 0.01436\n",
      "Epoch: 136, Time: 0.10663s, Loss: 0.01356\n",
      "Epoch: 137, Time: 0.11052s, Loss: 0.01348\n",
      "Epoch: 138, Time: 0.11165s, Loss: 0.01287\n",
      "Epoch: 139, Time: 0.11514s, Loss: 0.01285\n",
      "Epoch: 140, Time: 0.16839s, Loss: 0.01282\n",
      "Epoch: 141, Time: 0.12262s, Loss: 0.01282\n",
      "Epoch: 142, Time: 0.11057s, Loss: 0.01368\n",
      "Epoch: 143, Time: 0.11360s, Loss: 0.01278\n",
      "Epoch: 144, Time: 0.10558s, Loss: 0.01363\n",
      "Epoch: 145, Time: 0.11208s, Loss: 0.01316\n",
      "Epoch: 146, Time: 0.11662s, Loss: 0.01272\n",
      "Epoch: 147, Time: 0.11758s, Loss: 0.01261\n",
      "Epoch: 148, Time: 0.12404s, Loss: 0.01321\n",
      "Epoch: 149, Time: 0.10555s, Loss: 0.01308\n",
      "Epoch: 150, Time: 0.12204s, Loss: 0.01287\n",
      "Epoch: 151, Time: 0.11496s, Loss: 0.01174\n",
      "Epoch: 152, Time: 0.10958s, Loss: 0.01301\n",
      "Epoch: 153, Time: 0.11310s, Loss: 0.01285\n",
      "Epoch: 154, Time: 0.11858s, Loss: 0.01269\n",
      "Epoch: 155, Time: 0.12313s, Loss: 0.01245\n",
      "Epoch: 156, Time: 0.10827s, Loss: 0.01330\n",
      "Epoch: 157, Time: 0.12212s, Loss: 0.01280\n",
      "Epoch: 158, Time: 0.10757s, Loss: 0.01255\n",
      "Epoch: 159, Time: 0.10765s, Loss: 0.01220\n",
      "Epoch: 160, Time: 0.12216s, Loss: 0.01134\n",
      "Epoch: 161, Time: 0.12055s, Loss: 0.01127\n",
      "Epoch: 162, Time: 0.10458s, Loss: 0.01153\n",
      "Epoch: 163, Time: 0.10329s, Loss: 0.01129\n",
      "Epoch: 164, Time: 0.10956s, Loss: 0.01120\n",
      "Epoch: 165, Time: 0.10456s, Loss: 0.01126\n",
      "Epoch: 166, Time: 0.10960s, Loss: 0.01091\n",
      "Epoch: 167, Time: 0.10909s, Loss: 0.01097\n",
      "Epoch: 168, Time: 0.12124s, Loss: 0.01180\n",
      "Epoch: 169, Time: 0.12729s, Loss: 0.01197\n",
      "Epoch: 170, Time: 0.11857s, Loss: 0.01069\n",
      "Epoch: 171, Time: 0.10259s, Loss: 0.01087\n",
      "Epoch: 172, Time: 0.11161s, Loss: 0.01150\n",
      "Epoch: 173, Time: 0.10921s, Loss: 0.01196\n",
      "Epoch: 174, Time: 0.10658s, Loss: 0.01160\n",
      "Epoch: 175, Time: 0.11360s, Loss: 0.01139\n",
      "Epoch: 176, Time: 0.11757s, Loss: 0.01102\n",
      "Epoch: 177, Time: 0.10807s, Loss: 0.01557\n",
      "Epoch: 178, Time: 0.11157s, Loss: 0.03755\n",
      "Epoch: 179, Time: 0.10435s, Loss: 0.01259\n",
      "Epoch: 180, Time: 0.11604s, Loss: 0.04858\n",
      "Epoch: 181, Time: 0.10258s, Loss: 0.04517\n",
      "Epoch: 182, Time: 0.12508s, Loss: 0.01797\n",
      "Epoch: 183, Time: 0.11606s, Loss: 0.02016\n",
      "Epoch: 184, Time: 0.12010s, Loss: 0.02098\n",
      "Epoch: 185, Time: 0.10121s, Loss: 0.02448\n",
      "Epoch: 186, Time: 0.11561s, Loss: 0.02332\n",
      "Epoch: 187, Time: 0.11158s, Loss: 0.02146\n",
      "Epoch: 188, Time: 0.11559s, Loss: 0.01876\n",
      "Epoch: 189, Time: 0.10807s, Loss: 0.01704\n",
      "Epoch: 190, Time: 0.11109s, Loss: 0.01545\n",
      "Epoch: 191, Time: 0.11456s, Loss: 0.01565\n",
      "Epoch: 192, Time: 0.10509s, Loss: 0.01590\n",
      "Epoch: 193, Time: 0.11213s, Loss: 0.01600\n",
      "Epoch: 194, Time: 0.10910s, Loss: 0.01321\n",
      "Epoch: 195, Time: 0.11056s, Loss: 0.01114\n",
      "Epoch: 196, Time: 0.10511s, Loss: 0.01090\n",
      "Epoch: 197, Time: 0.10709s, Loss: 0.01140\n",
      "Epoch: 198, Time: 0.10055s, Loss: 0.01134\n",
      "Epoch: 199, Time: 0.10959s, Loss: 0.01008\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 111\n",
      "{'accuracy': 0.4370983839035034, 'f1_score': 0.3630498297451491, 'f1_score -> average@micro': 0.4370983864058261}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-citation Citaseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is cocitation_citeseer dataset:\n",
       "  ->  num_classes\n",
       "  ->  num_vertices\n",
       "  ->  num_edges\n",
       "  ->  dim_features\n",
       "  ->  features\n",
       "  ->  edge_list\n",
       "  ->  labels\n",
       "  ->  train_mask\n",
       "  ->  val_mask\n",
       "  ->  test_mask\n",
       "Please try `data['name']` to get the specified data."
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = dhg.data.CocitationCiteseer(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['num_vertices'])\n",
    "data['num_edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.07557s, Loss: 1.84384\n",
      "update best: 0.18053\n",
      "Epoch: 1, Time: 0.01852s, Loss: 1.32729\n",
      "Epoch: 2, Time: 0.02073s, Loss: 0.89455\n",
      "Epoch: 3, Time: 0.01821s, Loss: 0.58217\n",
      "Epoch: 4, Time: 0.02025s, Loss: 0.33297\n",
      "Epoch: 5, Time: 0.01954s, Loss: 0.21457\n",
      "Epoch: 6, Time: 0.01855s, Loss: 0.16569\n",
      "update best: 0.18399\n",
      "Epoch: 7, Time: 0.01419s, Loss: 0.11529\n",
      "update best: 0.20258\n",
      "Epoch: 8, Time: 0.02618s, Loss: 0.11493\n",
      "update best: 0.21928\n",
      "Epoch: 9, Time: 0.02598s, Loss: 0.08893\n",
      "update best: 0.22716\n",
      "Epoch: 10, Time: 0.01781s, Loss: 0.04256\n",
      "update best: 0.23503\n",
      "Epoch: 11, Time: 0.01740s, Loss: 0.05914\n",
      "update best: 0.24071\n",
      "Epoch: 12, Time: 0.01607s, Loss: 0.03737\n",
      "update best: 0.24354\n",
      "Epoch: 13, Time: 0.01743s, Loss: 0.03301\n",
      "update best: 0.25457\n",
      "Epoch: 14, Time: 0.01613s, Loss: 0.02690\n",
      "update best: 0.25772\n",
      "Epoch: 15, Time: 0.01786s, Loss: 0.01817\n",
      "update best: 0.25835\n",
      "Epoch: 16, Time: 0.01224s, Loss: 0.04195\n",
      "Epoch: 17, Time: 0.01985s, Loss: 0.04121\n",
      "Epoch: 18, Time: 0.02273s, Loss: 0.01221\n",
      "Epoch: 19, Time: 0.02539s, Loss: 0.01795\n",
      "Epoch: 20, Time: 0.02135s, Loss: 0.03318\n",
      "Epoch: 21, Time: 0.02106s, Loss: 0.01417\n",
      "Epoch: 22, Time: 0.02165s, Loss: 0.01202\n",
      "Epoch: 23, Time: 0.02191s, Loss: 0.00621\n",
      "Epoch: 24, Time: 0.02259s, Loss: 0.01527\n",
      "Epoch: 25, Time: 0.01803s, Loss: 0.00521\n",
      "Epoch: 26, Time: 0.02032s, Loss: 0.01303\n",
      "Epoch: 27, Time: 0.02088s, Loss: 0.01279\n",
      "Epoch: 28, Time: 0.01999s, Loss: 0.00548\n",
      "Epoch: 29, Time: 0.01940s, Loss: 0.00536\n",
      "Epoch: 30, Time: 0.01582s, Loss: 0.01232\n",
      "Epoch: 31, Time: 0.02379s, Loss: 0.00742\n",
      "Epoch: 32, Time: 0.01838s, Loss: 0.00472\n",
      "Epoch: 33, Time: 0.01057s, Loss: 0.01984\n",
      "Epoch: 34, Time: 0.02498s, Loss: 0.00515\n",
      "Epoch: 35, Time: 0.01750s, Loss: 0.00302\n",
      "Epoch: 36, Time: 0.01646s, Loss: 0.01405\n",
      "Epoch: 37, Time: 0.01622s, Loss: 0.00204\n",
      "Epoch: 38, Time: 0.01946s, Loss: 0.00648\n",
      "Epoch: 39, Time: 0.01720s, Loss: 0.00954\n",
      "Epoch: 40, Time: 0.01785s, Loss: 0.00736\n",
      "Epoch: 41, Time: 0.01665s, Loss: 0.00238\n",
      "Epoch: 42, Time: 0.01664s, Loss: 0.00368\n",
      "Epoch: 43, Time: 0.01792s, Loss: 0.00203\n",
      "Epoch: 44, Time: 0.01528s, Loss: 0.00152\n",
      "Epoch: 45, Time: 0.01737s, Loss: 0.00106\n",
      "Epoch: 46, Time: 0.01749s, Loss: 0.00479\n",
      "Epoch: 47, Time: 0.01904s, Loss: 0.00352\n",
      "Epoch: 48, Time: 0.01904s, Loss: 0.01470\n",
      "Epoch: 49, Time: 0.02189s, Loss: 0.00244\n",
      "Epoch: 50, Time: 0.02238s, Loss: 0.00861\n",
      "Epoch: 51, Time: 0.01767s, Loss: 0.00358\n",
      "Epoch: 52, Time: 0.02101s, Loss: 0.00322\n",
      "Epoch: 53, Time: 0.01907s, Loss: 0.00118\n",
      "Epoch: 54, Time: 0.01659s, Loss: 0.00263\n",
      "Epoch: 55, Time: 0.02012s, Loss: 0.01326\n",
      "Epoch: 56, Time: 0.01604s, Loss: 0.00693\n",
      "Epoch: 57, Time: 0.01645s, Loss: 0.00257\n",
      "Epoch: 58, Time: 0.01814s, Loss: 0.00101\n",
      "Epoch: 59, Time: 0.01666s, Loss: 0.00911\n",
      "Epoch: 60, Time: 0.01661s, Loss: 0.00936\n",
      "Epoch: 61, Time: 0.01737s, Loss: 0.00123\n",
      "Epoch: 62, Time: 0.01507s, Loss: 0.01041\n",
      "Epoch: 63, Time: 0.01793s, Loss: 0.01212\n",
      "Epoch: 64, Time: 0.02018s, Loss: 0.00214\n",
      "Epoch: 65, Time: 0.02066s, Loss: 0.00262\n",
      "Epoch: 66, Time: 0.01813s, Loss: 0.00584\n",
      "Epoch: 67, Time: 0.01768s, Loss: 0.00087\n",
      "Epoch: 68, Time: 0.01970s, Loss: 0.01147\n",
      "Epoch: 69, Time: 0.02049s, Loss: 0.01437\n",
      "Epoch: 70, Time: 0.02059s, Loss: 0.00154\n",
      "Epoch: 71, Time: 0.01939s, Loss: 0.00321\n",
      "Epoch: 72, Time: 0.02094s, Loss: 0.00186\n",
      "Epoch: 73, Time: 0.01794s, Loss: 0.00224\n",
      "Epoch: 74, Time: 0.01856s, Loss: 0.00588\n",
      "Epoch: 75, Time: 0.01907s, Loss: 0.02405\n",
      "Epoch: 76, Time: 0.01663s, Loss: 0.00334\n",
      "Epoch: 77, Time: 0.02108s, Loss: 0.00402\n",
      "update best: 0.26402\n",
      "Epoch: 78, Time: 0.01662s, Loss: 0.00632\n",
      "update best: 0.27190\n",
      "Epoch: 79, Time: 0.01623s, Loss: 0.00302\n",
      "update best: 0.27347\n",
      "Epoch: 80, Time: 0.01536s, Loss: 0.00129\n",
      "update best: 0.27599\n",
      "Epoch: 81, Time: 0.02290s, Loss: 0.00393\n",
      "Epoch: 82, Time: 0.01831s, Loss: 0.00084\n",
      "Epoch: 83, Time: 0.01609s, Loss: 0.00363\n",
      "Epoch: 84, Time: 0.01558s, Loss: 0.00176\n",
      "Epoch: 85, Time: 0.01565s, Loss: 0.01176\n",
      "Epoch: 86, Time: 0.01718s, Loss: 0.00149\n",
      "Epoch: 87, Time: 0.00983s, Loss: 0.00122\n",
      "Epoch: 88, Time: 0.01315s, Loss: 0.00072\n",
      "Epoch: 89, Time: 0.01795s, Loss: 0.00815\n",
      "Epoch: 90, Time: 0.01974s, Loss: 0.00086\n",
      "Epoch: 91, Time: 0.01664s, Loss: 0.00212\n",
      "Epoch: 92, Time: 0.01660s, Loss: 0.00074\n",
      "Epoch: 93, Time: 0.01622s, Loss: 0.00070\n",
      "Epoch: 94, Time: 0.01654s, Loss: 0.00234\n",
      "Epoch: 95, Time: 0.01416s, Loss: 0.00231\n",
      "Epoch: 96, Time: 0.01375s, Loss: 0.01695\n",
      "Epoch: 97, Time: 0.01603s, Loss: 0.00428\n",
      "Epoch: 98, Time: 0.01818s, Loss: 0.00508\n",
      "Epoch: 99, Time: 0.01530s, Loss: 0.00125\n",
      "Epoch: 100, Time: 0.01592s, Loss: 0.00322\n",
      "Epoch: 101, Time: 0.01391s, Loss: 0.00112\n",
      "Epoch: 102, Time: 0.01667s, Loss: 0.00270\n",
      "Epoch: 103, Time: 0.01702s, Loss: 0.00323\n",
      "Epoch: 104, Time: 0.01760s, Loss: 0.00200\n",
      "Epoch: 105, Time: 0.02148s, Loss: 0.00109\n",
      "Epoch: 106, Time: 0.01639s, Loss: 0.01693\n",
      "Epoch: 107, Time: 0.01756s, Loss: 0.00453\n",
      "Epoch: 108, Time: 0.01256s, Loss: 0.00070\n",
      "Epoch: 109, Time: 0.01725s, Loss: 0.00992\n",
      "Epoch: 110, Time: 0.01615s, Loss: 0.01284\n",
      "Epoch: 111, Time: 0.02071s, Loss: 0.00394\n",
      "Epoch: 112, Time: 0.01622s, Loss: 0.00095\n",
      "Epoch: 113, Time: 0.01667s, Loss: 0.00139\n",
      "Epoch: 114, Time: 0.01630s, Loss: 0.00704\n",
      "Epoch: 115, Time: 0.02093s, Loss: 0.01456\n",
      "Epoch: 116, Time: 0.01951s, Loss: 0.00099\n",
      "Epoch: 117, Time: 0.01817s, Loss: 0.00307\n",
      "Epoch: 118, Time: 0.01674s, Loss: 0.01291\n",
      "Epoch: 119, Time: 0.01944s, Loss: 0.00089\n",
      "Epoch: 120, Time: 0.01007s, Loss: 0.00867\n",
      "Epoch: 121, Time: 0.01729s, Loss: 0.00143\n",
      "Epoch: 122, Time: 0.01749s, Loss: 0.00413\n",
      "Epoch: 123, Time: 0.01876s, Loss: 0.00115\n",
      "Epoch: 124, Time: 0.02038s, Loss: 0.00473\n",
      "Epoch: 125, Time: 0.01647s, Loss: 0.00423\n",
      "Epoch: 126, Time: 0.01669s, Loss: 0.00284\n",
      "Epoch: 127, Time: 0.01748s, Loss: 0.00151\n",
      "Epoch: 128, Time: 0.01738s, Loss: 0.00751\n",
      "Epoch: 129, Time: 0.01639s, Loss: 0.00503\n",
      "Epoch: 130, Time: 0.02013s, Loss: 0.00140\n",
      "Epoch: 131, Time: 0.01960s, Loss: 0.00309\n",
      "Epoch: 132, Time: 0.01598s, Loss: 0.00381\n",
      "update best: 0.27977\n",
      "Epoch: 133, Time: 0.01861s, Loss: 0.00176\n",
      "update best: 0.28607\n",
      "Epoch: 134, Time: 0.01128s, Loss: 0.00439\n",
      "update best: 0.29332\n",
      "Epoch: 135, Time: 0.01695s, Loss: 0.00276\n",
      "Epoch: 136, Time: 0.01538s, Loss: 0.00186\n",
      "Epoch: 137, Time: 0.01881s, Loss: 0.00234\n",
      "Epoch: 138, Time: 0.01441s, Loss: 0.00368\n",
      "Epoch: 139, Time: 0.01693s, Loss: 0.00487\n",
      "Epoch: 140, Time: 0.02137s, Loss: 0.00286\n",
      "Epoch: 141, Time: 0.01160s, Loss: 0.00101\n",
      "Epoch: 142, Time: 0.01736s, Loss: 0.00221\n",
      "Epoch: 143, Time: 0.01846s, Loss: 0.00689\n",
      "Epoch: 144, Time: 0.01861s, Loss: 0.00191\n",
      "Epoch: 145, Time: 0.01987s, Loss: 0.00288\n",
      "Epoch: 146, Time: 0.01812s, Loss: 0.00674\n",
      "Epoch: 147, Time: 0.01984s, Loss: 0.00450\n",
      "Epoch: 148, Time: 0.01314s, Loss: 0.00107\n",
      "Epoch: 149, Time: 0.01819s, Loss: 0.00232\n",
      "Epoch: 150, Time: 0.01849s, Loss: 0.00186\n",
      "Epoch: 151, Time: 0.01651s, Loss: 0.00094\n",
      "Epoch: 152, Time: 0.01596s, Loss: 0.01419\n",
      "Epoch: 153, Time: 0.01539s, Loss: 0.00092\n",
      "Epoch: 154, Time: 0.01435s, Loss: 0.00103\n",
      "Epoch: 155, Time: 0.01859s, Loss: 0.00089\n",
      "Epoch: 156, Time: 0.01928s, Loss: 0.00860\n",
      "Epoch: 157, Time: 0.02187s, Loss: 0.00186\n",
      "Epoch: 158, Time: 0.01796s, Loss: 0.04007\n",
      "Epoch: 159, Time: 0.02077s, Loss: 0.01148\n",
      "Epoch: 160, Time: 0.01883s, Loss: 0.01324\n",
      "Epoch: 161, Time: 0.01697s, Loss: 0.00500\n",
      "Epoch: 162, Time: 0.01291s, Loss: 0.02147\n",
      "Epoch: 163, Time: 0.01828s, Loss: 0.00268\n",
      "Epoch: 164, Time: 0.01877s, Loss: 0.00114\n",
      "Epoch: 165, Time: 0.01660s, Loss: 0.00103\n",
      "Epoch: 166, Time: 0.01687s, Loss: 0.00143\n",
      "Epoch: 167, Time: 0.01458s, Loss: 0.00802\n",
      "Epoch: 168, Time: 0.01996s, Loss: 0.00196\n",
      "Epoch: 169, Time: 0.02152s, Loss: 0.00107\n",
      "Epoch: 170, Time: 0.02028s, Loss: 0.00500\n",
      "Epoch: 171, Time: 0.01687s, Loss: 0.00304\n",
      "Epoch: 172, Time: 0.02011s, Loss: 0.00600\n",
      "Epoch: 173, Time: 0.02363s, Loss: 0.00318\n",
      "Epoch: 174, Time: 0.02392s, Loss: 0.00424\n",
      "Epoch: 175, Time: 0.02470s, Loss: 0.00479\n",
      "Epoch: 176, Time: 0.01839s, Loss: 0.00095\n",
      "Epoch: 177, Time: 0.01289s, Loss: 0.00309\n",
      "Epoch: 178, Time: 0.01559s, Loss: 0.01341\n",
      "Epoch: 179, Time: 0.01824s, Loss: 0.00103\n",
      "Epoch: 180, Time: 0.02663s, Loss: 0.00252\n",
      "Epoch: 181, Time: 0.01736s, Loss: 0.00145\n",
      "Epoch: 182, Time: 0.01917s, Loss: 0.00156\n",
      "Epoch: 183, Time: 0.02012s, Loss: 0.00154\n",
      "Epoch: 184, Time: 0.02048s, Loss: 0.00170\n",
      "Epoch: 185, Time: 0.01641s, Loss: 0.00569\n",
      "Epoch: 186, Time: 0.01850s, Loss: 0.00530\n",
      "Epoch: 187, Time: 0.01862s, Loss: 0.00718\n",
      "Epoch: 188, Time: 0.01619s, Loss: 0.00135\n",
      "Epoch: 189, Time: 0.02075s, Loss: 0.00858\n",
      "Epoch: 190, Time: 0.01812s, Loss: 0.00070\n",
      "Epoch: 191, Time: 0.01697s, Loss: 0.00161\n",
      "Epoch: 192, Time: 0.01435s, Loss: 0.00187\n",
      "Epoch: 193, Time: 0.02099s, Loss: 0.01581\n",
      "Epoch: 194, Time: 0.01537s, Loss: 0.00251\n",
      "Epoch: 195, Time: 0.02052s, Loss: 0.00978\n",
      "Epoch: 196, Time: 0.01819s, Loss: 0.00208\n",
      "Epoch: 197, Time: 0.01932s, Loss: 0.00614\n",
      "Epoch: 198, Time: 0.01474s, Loss: 0.01333\n",
      "Epoch: 199, Time: 0.02028s, Loss: 0.00244\n",
      "\n",
      "train finished!\n",
      "best val: 0.29332\n",
      "test...\n",
      "final result: epoch: 134\n",
      "{'accuracy': 0.29332074522972107, 'f1_score': 0.24036976724855488, 'f1_score -> average@micro': 0.2933207309388784}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.02666s, Loss: 1.81649\n",
      "update best: 0.21424\n",
      "Epoch: 1, Time: 0.02883s, Loss: 1.43765\n",
      "update best: 0.21865\n",
      "Epoch: 2, Time: 0.02005s, Loss: 1.22796\n",
      "Epoch: 3, Time: 0.01828s, Loss: 1.15794\n",
      "Epoch: 4, Time: 0.01936s, Loss: 1.12441\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.02366s, Loss: 1.10783\n",
      "update best: 0.22275\n",
      "Epoch: 6, Time: 0.01793s, Loss: 1.09250\n",
      "update best: 0.22369\n",
      "Epoch: 7, Time: 0.02164s, Loss: 1.08502\n",
      "update best: 0.22495\n",
      "Epoch: 8, Time: 0.02327s, Loss: 1.07616\n",
      "update best: 0.22527\n",
      "Epoch: 9, Time: 0.02244s, Loss: 1.07437\n",
      "Epoch: 10, Time: 0.01950s, Loss: 1.07368\n",
      "update best: 0.22590\n",
      "Epoch: 11, Time: 0.01640s, Loss: 1.06851\n",
      "Epoch: 12, Time: 0.01686s, Loss: 1.06937\n",
      "Epoch: 13, Time: 0.01804s, Loss: 1.07050\n",
      "Epoch: 14, Time: 0.01849s, Loss: 1.06823\n",
      "update best: 0.22684\n",
      "Epoch: 15, Time: 0.02724s, Loss: 1.06642\n",
      "update best: 0.22810\n",
      "Epoch: 16, Time: 0.04121s, Loss: 1.06693\n",
      "update best: 0.22842\n",
      "Epoch: 17, Time: 0.01856s, Loss: 1.06795\n",
      "update best: 0.22873\n",
      "Epoch: 18, Time: 0.01636s, Loss: 1.06691\n",
      "update best: 0.23031\n",
      "Epoch: 19, Time: 0.01900s, Loss: 1.06681\n",
      "update best: 0.23125\n",
      "Epoch: 20, Time: 0.01937s, Loss: 1.06605\n",
      "update best: 0.23346\n",
      "Epoch: 21, Time: 0.02448s, Loss: 1.06690\n",
      "Epoch: 22, Time: 0.01854s, Loss: 1.06663\n",
      "update best: 0.23377\n",
      "Epoch: 23, Time: 0.02143s, Loss: 1.06689\n",
      "Epoch: 24, Time: 0.02080s, Loss: 1.06694\n",
      "Epoch: 25, Time: 0.01814s, Loss: 1.06539\n",
      "update best: 0.23440\n",
      "Epoch: 26, Time: 0.02194s, Loss: 1.07002\n",
      "Epoch: 27, Time: 0.02151s, Loss: 1.06522\n",
      "Epoch: 28, Time: 0.02571s, Loss: 1.06616\n",
      "update best: 0.23693\n",
      "Epoch: 29, Time: 0.02481s, Loss: 1.06519\n",
      "update best: 0.23913\n",
      "Epoch: 30, Time: 0.01903s, Loss: 1.06896\n",
      "update best: 0.23976\n",
      "Epoch: 31, Time: 0.01825s, Loss: 1.06508\n",
      "update best: 0.24228\n",
      "Epoch: 32, Time: 0.01837s, Loss: 1.06613\n",
      "update best: 0.24354\n",
      "Epoch: 33, Time: 0.02131s, Loss: 1.06547\n",
      "Epoch: 34, Time: 0.01762s, Loss: 1.06507\n",
      "Epoch: 35, Time: 0.02063s, Loss: 1.06493\n",
      "Epoch: 36, Time: 0.02128s, Loss: 1.06490\n",
      "Epoch: 37, Time: 0.01768s, Loss: 1.06516\n",
      "Epoch: 38, Time: 0.01795s, Loss: 1.06532\n",
      "Epoch: 39, Time: 0.01812s, Loss: 1.06980\n",
      "Epoch: 40, Time: 0.01674s, Loss: 1.06490\n",
      "Epoch: 41, Time: 0.01593s, Loss: 1.06702\n",
      "Epoch: 42, Time: 0.01489s, Loss: 1.06530\n",
      "Epoch: 43, Time: 0.01657s, Loss: 1.06540\n",
      "Epoch: 44, Time: 0.01704s, Loss: 1.06647\n",
      "update best: 0.24417\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06484\n",
      "update best: 0.24543\n",
      "Epoch: 46, Time: 0.01660s, Loss: 1.06728\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.01782s, Loss: 1.06481\n",
      "Epoch: 48, Time: 0.01757s, Loss: 1.06632\n",
      "Epoch: 49, Time: 0.01485s, Loss: 1.06521\n",
      "Epoch: 50, Time: 0.01675s, Loss: 1.06575\n",
      "Epoch: 51, Time: 0.01786s, Loss: 1.06525\n",
      "Epoch: 52, Time: 0.02277s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.02166s, Loss: 1.06507\n",
      "Epoch: 54, Time: 0.01743s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01731s, Loss: 1.06621\n",
      "Epoch: 56, Time: 0.01820s, Loss: 1.06738\n",
      "Epoch: 57, Time: 0.01791s, Loss: 1.06510\n",
      "Epoch: 58, Time: 0.02247s, Loss: 1.06537\n",
      "Epoch: 59, Time: 0.02010s, Loss: 1.06517\n",
      "Epoch: 60, Time: 0.01746s, Loss: 1.06534\n",
      "Epoch: 61, Time: 0.01992s, Loss: 1.06506\n",
      "Epoch: 62, Time: 0.01787s, Loss: 1.06508\n",
      "update best: 0.24764\n",
      "Epoch: 63, Time: 0.02259s, Loss: 1.06570\n",
      "update best: 0.24953\n",
      "Epoch: 64, Time: 0.01247s, Loss: 1.06505\n",
      "update best: 0.25142\n",
      "Epoch: 65, Time: 0.01587s, Loss: 1.06507\n",
      "Epoch: 66, Time: 0.01652s, Loss: 1.06519\n",
      "update best: 0.25268\n",
      "Epoch: 67, Time: 0.01768s, Loss: 1.06502\n",
      "update best: 0.25299\n",
      "Epoch: 68, Time: 0.01546s, Loss: 1.06523\n",
      "update best: 0.25331\n",
      "Epoch: 69, Time: 0.01801s, Loss: 1.07305\n",
      "update best: 0.25551\n",
      "Epoch: 70, Time: 0.01831s, Loss: 1.06549\n",
      "update best: 0.25583\n",
      "Epoch: 71, Time: 0.01908s, Loss: 1.06533\n",
      "Epoch: 72, Time: 0.02021s, Loss: 1.06593\n",
      "Epoch: 73, Time: 0.01627s, Loss: 1.06515\n",
      "Epoch: 74, Time: 0.01677s, Loss: 1.06505\n",
      "Epoch: 75, Time: 0.01870s, Loss: 1.06534\n",
      "update best: 0.25677\n",
      "Epoch: 76, Time: 0.01737s, Loss: 1.06526\n",
      "update best: 0.25992\n",
      "Epoch: 77, Time: 0.01953s, Loss: 1.06741\n",
      "update best: 0.26654\n",
      "Epoch: 78, Time: 0.02144s, Loss: 1.06646\n",
      "update best: 0.26906\n",
      "Epoch: 79, Time: 0.02197s, Loss: 1.06549\n",
      "update best: 0.27095\n",
      "Epoch: 80, Time: 0.02261s, Loss: 1.06540\n",
      "update best: 0.27190\n",
      "Epoch: 81, Time: 0.02229s, Loss: 1.06554\n",
      "Epoch: 82, Time: 0.02355s, Loss: 1.06607\n",
      "update best: 0.27473\n",
      "Epoch: 83, Time: 0.01814s, Loss: 1.06520\n",
      "Epoch: 84, Time: 0.01883s, Loss: 1.06574\n",
      "update best: 0.27505\n",
      "Epoch: 85, Time: 0.01675s, Loss: 1.06542\n",
      "update best: 0.27662\n",
      "Epoch: 86, Time: 0.01655s, Loss: 1.06497\n",
      "update best: 0.27788\n",
      "Epoch: 87, Time: 0.01687s, Loss: 1.06536\n",
      "update best: 0.27820\n",
      "Epoch: 88, Time: 0.01637s, Loss: 1.06712\n",
      "update best: 0.27851\n",
      "Epoch: 89, Time: 0.01623s, Loss: 1.06507\n",
      "Epoch: 90, Time: 0.01593s, Loss: 1.06662\n",
      "Epoch: 91, Time: 0.01839s, Loss: 1.06555\n",
      "Epoch: 92, Time: 0.01371s, Loss: 1.06724\n",
      "Epoch: 93, Time: 0.01482s, Loss: 1.06553\n",
      "update best: 0.28040\n",
      "Epoch: 94, Time: 0.01767s, Loss: 1.06508\n",
      "Epoch: 95, Time: 0.01703s, Loss: 1.06521\n",
      "update best: 0.28166\n",
      "Epoch: 96, Time: 0.01775s, Loss: 1.06502\n",
      "update best: 0.28261\n",
      "Epoch: 97, Time: 0.01963s, Loss: 1.06545\n",
      "update best: 0.28387\n",
      "Epoch: 98, Time: 0.02233s, Loss: 1.06513\n",
      "update best: 0.28481\n",
      "Epoch: 99, Time: 0.01748s, Loss: 1.06547\n",
      "Epoch: 100, Time: 0.01556s, Loss: 1.06497\n",
      "Epoch: 101, Time: 0.01438s, Loss: 1.06506\n",
      "update best: 0.28544\n",
      "Epoch: 102, Time: 0.01760s, Loss: 1.06503\n",
      "update best: 0.28639\n",
      "Epoch: 103, Time: 0.01655s, Loss: 1.06542\n",
      "update best: 0.28702\n",
      "Epoch: 104, Time: 0.01242s, Loss: 1.06585\n",
      "Epoch: 105, Time: 0.01357s, Loss: 1.06535\n",
      "Epoch: 106, Time: 0.01611s, Loss: 1.06541\n",
      "Epoch: 107, Time: 0.01720s, Loss: 1.06517\n",
      "Epoch: 108, Time: 0.01844s, Loss: 1.06508\n",
      "Epoch: 109, Time: 0.01629s, Loss: 1.06523\n",
      "Epoch: 110, Time: 0.01776s, Loss: 1.06669\n",
      "Epoch: 111, Time: 0.02685s, Loss: 1.06504\n",
      "Epoch: 112, Time: 0.01608s, Loss: 1.06830\n",
      "Epoch: 113, Time: 0.02013s, Loss: 1.06510\n",
      "Epoch: 114, Time: 0.01823s, Loss: 1.06535\n",
      "Epoch: 115, Time: 0.01609s, Loss: 1.06590\n",
      "Epoch: 116, Time: 0.01215s, Loss: 1.06531\n",
      "Epoch: 117, Time: 0.01296s, Loss: 1.06569\n",
      "Epoch: 118, Time: 0.01771s, Loss: 1.06568\n",
      "Epoch: 119, Time: 0.02355s, Loss: 1.06620\n",
      "Epoch: 120, Time: 0.01563s, Loss: 1.06539\n",
      "Epoch: 121, Time: 0.01465s, Loss: 1.06579\n",
      "Epoch: 122, Time: 0.01447s, Loss: 1.06534\n",
      "Epoch: 123, Time: 0.01752s, Loss: 1.06539\n",
      "Epoch: 124, Time: 0.01862s, Loss: 1.06516\n",
      "Epoch: 125, Time: 0.01149s, Loss: 1.06508\n",
      "update best: 0.28733\n",
      "Epoch: 126, Time: 0.01664s, Loss: 1.06511\n",
      "update best: 0.28922\n",
      "Epoch: 127, Time: 0.01757s, Loss: 1.06536\n",
      "update best: 0.29017\n",
      "Epoch: 128, Time: 0.01645s, Loss: 1.06550\n",
      "update best: 0.29080\n",
      "Epoch: 129, Time: 0.01800s, Loss: 1.06510\n",
      "update best: 0.29238\n",
      "Epoch: 130, Time: 0.01650s, Loss: 1.06519\n",
      "Epoch: 131, Time: 0.02048s, Loss: 1.06574\n",
      "Epoch: 132, Time: 0.01997s, Loss: 1.06540\n",
      "Epoch: 133, Time: 0.02081s, Loss: 1.06570\n",
      "Epoch: 134, Time: 0.01810s, Loss: 1.06556\n",
      "Epoch: 135, Time: 0.01953s, Loss: 1.06589\n",
      "update best: 0.29364\n",
      "Epoch: 136, Time: 0.01920s, Loss: 1.06544\n",
      "Epoch: 137, Time: 0.02303s, Loss: 1.06539\n",
      "Epoch: 138, Time: 0.01508s, Loss: 1.06505\n",
      "Epoch: 139, Time: 0.01842s, Loss: 1.06553\n",
      "Epoch: 140, Time: 0.02009s, Loss: 1.06553\n",
      "Epoch: 141, Time: 0.02001s, Loss: 1.06507\n",
      "Epoch: 142, Time: 0.02050s, Loss: 1.06608\n",
      "Epoch: 143, Time: 0.02168s, Loss: 1.06573\n",
      "Epoch: 144, Time: 0.01856s, Loss: 1.06533\n",
      "Epoch: 145, Time: 0.01972s, Loss: 1.06542\n",
      "Epoch: 146, Time: 0.01655s, Loss: 1.06536\n",
      "Epoch: 147, Time: 0.01834s, Loss: 1.06544\n",
      "Epoch: 148, Time: 0.01313s, Loss: 1.06630\n",
      "Epoch: 149, Time: 0.01821s, Loss: 1.06515\n",
      "Epoch: 150, Time: 0.01853s, Loss: 1.06549\n",
      "Epoch: 151, Time: 0.01937s, Loss: 1.06519\n",
      "Epoch: 152, Time: 0.01679s, Loss: 1.06495\n",
      "Epoch: 153, Time: 0.01791s, Loss: 1.06549\n",
      "Epoch: 154, Time: 0.01480s, Loss: 1.06536\n",
      "Epoch: 155, Time: 0.01520s, Loss: 1.06516\n",
      "Epoch: 156, Time: 0.01627s, Loss: 1.06595\n",
      "Epoch: 157, Time: 0.01555s, Loss: 1.06553\n",
      "Epoch: 158, Time: 0.02305s, Loss: 1.06554\n",
      "Epoch: 159, Time: 0.01351s, Loss: 1.06529\n",
      "Epoch: 160, Time: 0.01414s, Loss: 1.06528\n",
      "Epoch: 161, Time: 0.01765s, Loss: 1.06544\n",
      "Epoch: 162, Time: 0.01710s, Loss: 1.06587\n",
      "Epoch: 163, Time: 0.02061s, Loss: 1.06593\n",
      "Epoch: 164, Time: 0.02033s, Loss: 1.06537\n",
      "Epoch: 165, Time: 0.01972s, Loss: 1.06553\n",
      "Epoch: 166, Time: 0.01385s, Loss: 1.06530\n",
      "Epoch: 167, Time: 0.01608s, Loss: 1.06550\n",
      "Epoch: 168, Time: 0.01672s, Loss: 1.06533\n",
      "Epoch: 169, Time: 0.01599s, Loss: 1.06530\n",
      "Epoch: 170, Time: 0.01611s, Loss: 1.06539\n",
      "Epoch: 171, Time: 0.01605s, Loss: 1.06524\n",
      "Epoch: 172, Time: 0.01934s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01833s, Loss: 1.06556\n",
      "Epoch: 174, Time: 0.01524s, Loss: 1.06522\n",
      "update best: 0.29490\n",
      "Epoch: 175, Time: 0.01827s, Loss: 1.06555\n",
      "update best: 0.29521\n",
      "Epoch: 176, Time: 0.01649s, Loss: 1.06508\n",
      "Epoch: 177, Time: 0.01646s, Loss: 1.06528\n",
      "update best: 0.29647\n",
      "Epoch: 178, Time: 0.02095s, Loss: 1.06525\n",
      "update best: 0.29805\n",
      "Epoch: 179, Time: 0.01807s, Loss: 1.06549\n",
      "Epoch: 180, Time: 0.01872s, Loss: 1.06556\n",
      "Epoch: 181, Time: 0.01824s, Loss: 1.06508\n",
      "Epoch: 182, Time: 0.01404s, Loss: 1.06543\n",
      "Epoch: 183, Time: 0.01711s, Loss: 1.06532\n",
      "Epoch: 184, Time: 0.01564s, Loss: 1.06961\n",
      "Epoch: 185, Time: 0.01837s, Loss: 1.06526\n",
      "Epoch: 186, Time: 0.01663s, Loss: 1.06531\n",
      "Epoch: 187, Time: 0.00980s, Loss: 1.06604\n",
      "Epoch: 188, Time: 0.01678s, Loss: 1.06544\n",
      "Epoch: 189, Time: 0.01867s, Loss: 1.06573\n",
      "Epoch: 190, Time: 0.01688s, Loss: 1.06559\n",
      "Epoch: 191, Time: 0.01976s, Loss: 1.06660\n",
      "Epoch: 192, Time: 0.01817s, Loss: 1.06627\n",
      "Epoch: 193, Time: 0.02570s, Loss: 1.06583\n",
      "Epoch: 194, Time: 0.01776s, Loss: 1.06552\n",
      "Epoch: 195, Time: 0.01549s, Loss: 1.06540\n",
      "Epoch: 196, Time: 0.01602s, Loss: 1.06679\n",
      "Epoch: 197, Time: 0.01553s, Loss: 1.06629\n",
      "Epoch: 198, Time: 0.01786s, Loss: 1.06716\n",
      "Epoch: 199, Time: 0.01856s, Loss: 1.06667\n",
      "\n",
      "train finished!\n",
      "best val: 0.29805\n",
      "test...\n",
      "final result: epoch: 178\n",
      "{'accuracy': 0.29804661870002747, 'f1_score': 0.21974182550479207, 'f1_score -> average@micro': 0.2980466288594833}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.04270s, Loss: 1.82284\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02071s, Loss: 1.51886\n",
      "Epoch: 2, Time: 0.02244s, Loss: 1.30142\n",
      "Epoch: 3, Time: 0.01422s, Loss: 1.19327\n",
      "Epoch: 4, Time: 0.01827s, Loss: 1.14901\n",
      "update best: 0.21456\n",
      "Epoch: 5, Time: 0.01704s, Loss: 1.11603\n",
      "update best: 0.21582\n",
      "Epoch: 6, Time: 0.01643s, Loss: 1.09219\n",
      "update best: 0.21676\n",
      "Epoch: 7, Time: 0.01922s, Loss: 1.08688\n",
      "Epoch: 8, Time: 0.02138s, Loss: 1.08081\n",
      "update best: 0.21802\n",
      "Epoch: 9, Time: 0.02001s, Loss: 1.07927\n",
      "update best: 0.21991\n",
      "Epoch: 10, Time: 0.02502s, Loss: 1.07121\n",
      "update best: 0.22401\n",
      "Epoch: 11, Time: 0.01725s, Loss: 1.07070\n",
      "update best: 0.22842\n",
      "Epoch: 12, Time: 0.01808s, Loss: 1.06910\n",
      "update best: 0.23220\n",
      "Epoch: 13, Time: 0.01767s, Loss: 1.06739\n",
      "update best: 0.23756\n",
      "Epoch: 14, Time: 0.01468s, Loss: 1.06867\n",
      "update best: 0.24228\n",
      "Epoch: 15, Time: 0.02318s, Loss: 1.06965\n",
      "update best: 0.24890\n",
      "Epoch: 16, Time: 0.01846s, Loss: 1.06565\n",
      "update best: 0.25488\n",
      "Epoch: 17, Time: 0.02093s, Loss: 1.06624\n",
      "update best: 0.26024\n",
      "Epoch: 18, Time: 0.01909s, Loss: 1.06569\n",
      "update best: 0.26591\n",
      "Epoch: 19, Time: 0.01976s, Loss: 1.06635\n",
      "update best: 0.26938\n",
      "Epoch: 20, Time: 0.02251s, Loss: 1.06535\n",
      "update best: 0.27347\n",
      "Epoch: 21, Time: 0.01153s, Loss: 1.06856\n",
      "update best: 0.27788\n",
      "Epoch: 22, Time: 0.01665s, Loss: 1.07035\n",
      "update best: 0.28135\n",
      "Epoch: 23, Time: 0.02305s, Loss: 1.06538\n",
      "update best: 0.28670\n",
      "Epoch: 24, Time: 0.02006s, Loss: 1.06527\n",
      "update best: 0.28859\n",
      "Epoch: 25, Time: 0.02235s, Loss: 1.06552\n",
      "update best: 0.29049\n",
      "Epoch: 26, Time: 0.01630s, Loss: 1.06530\n",
      "update best: 0.29080\n",
      "Epoch: 27, Time: 0.02152s, Loss: 1.06492\n",
      "update best: 0.29112\n",
      "Epoch: 28, Time: 0.01894s, Loss: 1.06538\n",
      "update best: 0.29301\n",
      "Epoch: 29, Time: 0.02661s, Loss: 1.06589\n",
      "update best: 0.29553\n",
      "Epoch: 30, Time: 0.02075s, Loss: 1.06627\n",
      "Epoch: 31, Time: 0.02027s, Loss: 1.06568\n",
      "Epoch: 32, Time: 0.01624s, Loss: 1.06576\n",
      "update best: 0.29584\n",
      "Epoch: 33, Time: 0.01920s, Loss: 1.06508\n",
      "update best: 0.29647\n",
      "Epoch: 34, Time: 0.01991s, Loss: 1.06536\n",
      "update best: 0.29679\n",
      "Epoch: 35, Time: 0.01840s, Loss: 1.06501\n",
      "update best: 0.29710\n",
      "Epoch: 36, Time: 0.01724s, Loss: 1.06524\n",
      "Epoch: 37, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 38, Time: 0.02036s, Loss: 1.06499\n",
      "Epoch: 39, Time: 0.01830s, Loss: 1.06507\n",
      "Epoch: 40, Time: 0.01822s, Loss: 1.06540\n",
      "update best: 0.29805\n",
      "Epoch: 41, Time: 0.01940s, Loss: 1.06509\n",
      "Epoch: 42, Time: 0.01928s, Loss: 1.06522\n",
      "update best: 0.29836\n",
      "Epoch: 43, Time: 0.01889s, Loss: 1.06523\n",
      "Epoch: 44, Time: 0.02267s, Loss: 1.06530\n",
      "update best: 0.29931\n",
      "Epoch: 45, Time: 0.02758s, Loss: 1.06553\n",
      "Epoch: 46, Time: 0.02273s, Loss: 1.06594\n",
      "update best: 0.30183\n",
      "Epoch: 47, Time: 0.02789s, Loss: 1.06519\n",
      "update best: 0.30372\n",
      "Epoch: 48, Time: 0.01263s, Loss: 1.06524\n",
      "update best: 0.30466\n",
      "Epoch: 49, Time: 0.01633s, Loss: 1.06493\n",
      "update best: 0.30592\n",
      "Epoch: 50, Time: 0.02007s, Loss: 1.06548\n",
      "Epoch: 51, Time: 0.01813s, Loss: 1.06498\n",
      "Epoch: 52, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.01938s, Loss: 1.06489\n",
      "update best: 0.30718\n",
      "Epoch: 54, Time: 0.01763s, Loss: 1.06500\n",
      "update best: 0.30876\n",
      "Epoch: 55, Time: 0.01670s, Loss: 1.06570\n",
      "update best: 0.31128\n",
      "Epoch: 56, Time: 0.01785s, Loss: 1.06502\n",
      "Epoch: 57, Time: 0.01928s, Loss: 1.06531\n",
      "Epoch: 58, Time: 0.02257s, Loss: 1.06599\n",
      "update best: 0.31159\n",
      "Epoch: 59, Time: 0.02586s, Loss: 1.06529\n",
      "Epoch: 60, Time: 0.02166s, Loss: 1.06604\n",
      "Epoch: 61, Time: 0.01866s, Loss: 1.06585\n",
      "Epoch: 62, Time: 0.02232s, Loss: 1.06490\n",
      "Epoch: 63, Time: 0.01852s, Loss: 1.06505\n",
      "Epoch: 64, Time: 0.02313s, Loss: 1.06505\n",
      "Epoch: 65, Time: 0.01801s, Loss: 1.06512\n",
      "Epoch: 66, Time: 0.02064s, Loss: 1.06598\n",
      "Epoch: 67, Time: 0.02058s, Loss: 1.06529\n",
      "Epoch: 68, Time: 0.01511s, Loss: 1.06910\n",
      "update best: 0.31191\n",
      "Epoch: 69, Time: 0.02213s, Loss: 1.06611\n",
      "update best: 0.31317\n",
      "Epoch: 70, Time: 0.02055s, Loss: 1.06589\n",
      "update best: 0.31411\n",
      "Epoch: 71, Time: 0.01592s, Loss: 1.06512\n",
      "Epoch: 72, Time: 0.02295s, Loss: 1.06520\n",
      "Epoch: 73, Time: 0.02415s, Loss: 1.06497\n",
      "Epoch: 74, Time: 0.01714s, Loss: 1.06511\n",
      "Epoch: 75, Time: 0.01442s, Loss: 1.06539\n",
      "Epoch: 76, Time: 0.02115s, Loss: 1.06501\n",
      "Epoch: 77, Time: 0.01995s, Loss: 1.06772\n",
      "Epoch: 78, Time: 0.01706s, Loss: 1.06505\n",
      "Epoch: 79, Time: 0.01946s, Loss: 1.06511\n",
      "Epoch: 80, Time: 0.01968s, Loss: 1.06532\n",
      "Epoch: 81, Time: 0.02007s, Loss: 1.06583\n",
      "Epoch: 82, Time: 0.01847s, Loss: 1.06540\n",
      "Epoch: 83, Time: 0.01677s, Loss: 1.06522\n",
      "Epoch: 84, Time: 0.01916s, Loss: 1.06552\n",
      "Epoch: 85, Time: 0.01701s, Loss: 1.06588\n",
      "Epoch: 86, Time: 0.01758s, Loss: 1.06530\n",
      "Epoch: 87, Time: 0.01681s, Loss: 1.06513\n",
      "Epoch: 88, Time: 0.01884s, Loss: 1.06695\n",
      "Epoch: 89, Time: 0.01809s, Loss: 1.06502\n",
      "Epoch: 90, Time: 0.01902s, Loss: 1.06542\n",
      "Epoch: 91, Time: 0.01726s, Loss: 1.06501\n",
      "Epoch: 92, Time: 0.01917s, Loss: 1.07111\n",
      "Epoch: 93, Time: 0.02157s, Loss: 1.06501\n",
      "Epoch: 94, Time: 0.01932s, Loss: 1.06579\n",
      "Epoch: 95, Time: 0.01742s, Loss: 1.06544\n",
      "Epoch: 96, Time: 0.01689s, Loss: 1.06592\n",
      "Epoch: 97, Time: 0.02184s, Loss: 1.06546\n",
      "Epoch: 98, Time: 0.01851s, Loss: 1.06559\n",
      "Epoch: 99, Time: 0.01974s, Loss: 1.06655\n",
      "Epoch: 100, Time: 0.02153s, Loss: 1.06613\n",
      "Epoch: 101, Time: 0.02100s, Loss: 1.06529\n",
      "Epoch: 102, Time: 0.02183s, Loss: 1.06513\n",
      "update best: 0.31537\n",
      "Epoch: 103, Time: 0.01861s, Loss: 1.06602\n",
      "update best: 0.31664\n",
      "Epoch: 104, Time: 0.02366s, Loss: 1.06506\n",
      "Epoch: 105, Time: 0.02303s, Loss: 1.06543\n",
      "Epoch: 106, Time: 0.01787s, Loss: 1.06549\n",
      "Epoch: 107, Time: 0.02056s, Loss: 1.06499\n",
      "Epoch: 108, Time: 0.02198s, Loss: 1.06506\n",
      "Epoch: 109, Time: 0.02249s, Loss: 1.06522\n",
      "Epoch: 110, Time: 0.02321s, Loss: 1.06508\n",
      "Epoch: 111, Time: 0.02333s, Loss: 1.06503\n",
      "Epoch: 112, Time: 0.02006s, Loss: 1.06550\n",
      "Epoch: 113, Time: 0.01862s, Loss: 1.06498\n",
      "Epoch: 114, Time: 0.01844s, Loss: 1.06520\n",
      "Epoch: 115, Time: 0.01950s, Loss: 1.06557\n",
      "Epoch: 116, Time: 0.01550s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.01858s, Loss: 1.06584\n",
      "Epoch: 118, Time: 0.01633s, Loss: 1.06530\n",
      "Epoch: 119, Time: 0.02066s, Loss: 1.06533\n",
      "Epoch: 120, Time: 0.01966s, Loss: 1.06518\n",
      "Epoch: 121, Time: 0.01675s, Loss: 1.06587\n",
      "Epoch: 122, Time: 0.02211s, Loss: 1.06538\n",
      "Epoch: 123, Time: 0.01883s, Loss: 1.06525\n",
      "Epoch: 124, Time: 0.01462s, Loss: 1.06792\n",
      "Epoch: 125, Time: 0.01905s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.02171s, Loss: 1.06519\n",
      "Epoch: 127, Time: 0.01829s, Loss: 1.06529\n",
      "Epoch: 128, Time: 0.01958s, Loss: 1.06525\n",
      "Epoch: 129, Time: 0.01915s, Loss: 1.06577\n",
      "Epoch: 130, Time: 0.02259s, Loss: 1.06504\n",
      "Epoch: 131, Time: 0.01623s, Loss: 1.06562\n",
      "Epoch: 132, Time: 0.01713s, Loss: 1.06536\n",
      "Epoch: 133, Time: 0.01912s, Loss: 1.06614\n",
      "Epoch: 134, Time: 0.01647s, Loss: 1.06519\n",
      "Epoch: 135, Time: 0.01784s, Loss: 1.06531\n",
      "Epoch: 136, Time: 0.01811s, Loss: 1.06508\n",
      "Epoch: 137, Time: 0.01808s, Loss: 1.06545\n",
      "Epoch: 138, Time: 0.01983s, Loss: 1.06556\n",
      "Epoch: 139, Time: 0.01699s, Loss: 1.06502\n",
      "Epoch: 140, Time: 0.01636s, Loss: 1.06519\n",
      "Epoch: 141, Time: 0.01850s, Loss: 1.06617\n",
      "Epoch: 142, Time: 0.01634s, Loss: 1.06517\n",
      "Epoch: 143, Time: 0.01739s, Loss: 1.06538\n",
      "Epoch: 144, Time: 0.01833s, Loss: 1.06525\n",
      "Epoch: 145, Time: 0.01783s, Loss: 1.06514\n",
      "Epoch: 146, Time: 0.01796s, Loss: 1.06501\n",
      "Epoch: 147, Time: 0.01856s, Loss: 1.06551\n",
      "Epoch: 148, Time: 0.02472s, Loss: 1.06531\n",
      "Epoch: 149, Time: 0.01940s, Loss: 1.06562\n",
      "Epoch: 150, Time: 0.01549s, Loss: 1.06684\n",
      "Epoch: 151, Time: 0.01839s, Loss: 1.06553\n",
      "Epoch: 152, Time: 0.02012s, Loss: 1.06618\n",
      "Epoch: 153, Time: 0.02237s, Loss: 1.06527\n",
      "Epoch: 154, Time: 0.01830s, Loss: 1.07220\n",
      "Epoch: 155, Time: 0.02025s, Loss: 1.06542\n",
      "Epoch: 156, Time: 0.02134s, Loss: 1.06580\n",
      "Epoch: 157, Time: 0.01560s, Loss: 1.06539\n",
      "Epoch: 158, Time: 0.01388s, Loss: 1.06653\n",
      "Epoch: 159, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 160, Time: 0.01732s, Loss: 1.06567\n",
      "Epoch: 161, Time: 0.01975s, Loss: 1.06511\n",
      "Epoch: 162, Time: 0.01835s, Loss: 1.06567\n",
      "Epoch: 163, Time: 0.01690s, Loss: 1.06527\n",
      "Epoch: 164, Time: 0.01715s, Loss: 1.06890\n",
      "Epoch: 165, Time: 0.02312s, Loss: 1.06546\n",
      "Epoch: 166, Time: 0.01980s, Loss: 1.06614\n",
      "Epoch: 167, Time: 0.02019s, Loss: 1.06497\n",
      "Epoch: 168, Time: 0.01731s, Loss: 1.06529\n",
      "Epoch: 169, Time: 0.01955s, Loss: 1.06562\n",
      "Epoch: 170, Time: 0.01743s, Loss: 1.06597\n",
      "Epoch: 171, Time: 0.01865s, Loss: 1.06532\n",
      "Epoch: 172, Time: 0.01997s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01628s, Loss: 1.06550\n",
      "Epoch: 174, Time: 0.01769s, Loss: 1.06514\n",
      "Epoch: 175, Time: 0.01837s, Loss: 1.06553\n",
      "Epoch: 176, Time: 0.02044s, Loss: 1.06574\n",
      "Epoch: 177, Time: 0.02251s, Loss: 1.06569\n",
      "Epoch: 178, Time: 0.01868s, Loss: 1.06546\n",
      "Epoch: 179, Time: 0.02551s, Loss: 1.06544\n",
      "Epoch: 180, Time: 0.01607s, Loss: 1.06540\n",
      "Epoch: 181, Time: 0.01928s, Loss: 1.06541\n",
      "Epoch: 182, Time: 0.02239s, Loss: 1.06517\n",
      "Epoch: 183, Time: 0.02016s, Loss: 1.06519\n",
      "Epoch: 184, Time: 0.02079s, Loss: 1.06513\n",
      "Epoch: 185, Time: 0.02155s, Loss: 1.06497\n",
      "Epoch: 186, Time: 0.01768s, Loss: 1.06551\n",
      "Epoch: 187, Time: 0.02082s, Loss: 1.06646\n",
      "Epoch: 188, Time: 0.01630s, Loss: 1.06507\n",
      "Epoch: 189, Time: 0.02140s, Loss: 1.06555\n",
      "Epoch: 190, Time: 0.01823s, Loss: 1.06513\n",
      "Epoch: 191, Time: 0.02309s, Loss: 1.06557\n",
      "Epoch: 192, Time: 0.01628s, Loss: 1.06538\n",
      "Epoch: 193, Time: 0.01901s, Loss: 1.06559\n",
      "Epoch: 194, Time: 0.02195s, Loss: 1.06503\n",
      "Epoch: 195, Time: 0.01769s, Loss: 1.06567\n",
      "Epoch: 196, Time: 0.01739s, Loss: 1.06650\n",
      "Epoch: 197, Time: 0.01997s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01885s, Loss: 1.06514\n",
      "Epoch: 199, Time: 0.01803s, Loss: 1.06532\n",
      "\n",
      "train finished!\n",
      "best val: 0.31664\n",
      "test...\n",
      "final result: epoch: 103\n",
      "{'accuracy': 0.3166351616382599, 'f1_score': 0.24460102376042017, 'f1_score -> average@micro': 0.3166351606805293}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Experiment (No Top-k) ===\n",
      "Using original hypergraph (no Top-k preprocessing)\n",
      "Epoch: 0, Time: 0.06937s, Loss: 1.84667\n",
      "update best: 0.21014\n",
      "Epoch: 1, Time: 0.03478s, Loss: 1.56151\n",
      "Epoch: 2, Time: 0.02969s, Loss: 1.32516\n",
      "update best: 0.21361\n",
      "Epoch: 3, Time: 0.04779s, Loss: 1.21602\n",
      "update best: 0.21960\n",
      "Epoch: 4, Time: 0.02403s, Loss: 1.15841\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.01888s, Loss: 1.12637\n",
      "Epoch: 6, Time: 0.01813s, Loss: 1.11725\n",
      "Epoch: 7, Time: 0.01915s, Loss: 1.10174\n",
      "Epoch: 8, Time: 0.01913s, Loss: 1.10152\n",
      "Epoch: 9, Time: 0.01982s, Loss: 1.08360\n",
      "update best: 0.22054\n",
      "Epoch: 10, Time: 0.01888s, Loss: 1.07704\n",
      "update best: 0.22149\n",
      "Epoch: 11, Time: 0.01946s, Loss: 1.07767\n",
      "Epoch: 12, Time: 0.01809s, Loss: 1.07627\n",
      "update best: 0.22180\n",
      "Epoch: 13, Time: 0.01632s, Loss: 1.07444\n",
      "update best: 0.22243\n",
      "Epoch: 14, Time: 0.02069s, Loss: 1.07337\n",
      "update best: 0.22369\n",
      "Epoch: 15, Time: 0.01838s, Loss: 1.07144\n",
      "update best: 0.22464\n",
      "Epoch: 16, Time: 0.02056s, Loss: 1.06839\n",
      "update best: 0.22527\n",
      "Epoch: 17, Time: 0.01934s, Loss: 1.07026\n",
      "update best: 0.22590\n",
      "Epoch: 18, Time: 0.01997s, Loss: 1.06749\n",
      "update best: 0.22810\n",
      "Epoch: 19, Time: 0.02518s, Loss: 1.06799\n",
      "update best: 0.22873\n",
      "Epoch: 20, Time: 0.02006s, Loss: 1.06585\n",
      "update best: 0.23031\n",
      "Epoch: 21, Time: 0.03864s, Loss: 1.06540\n",
      "update best: 0.23094\n",
      "Epoch: 22, Time: 0.02472s, Loss: 1.06563\n",
      "update best: 0.23283\n",
      "Epoch: 23, Time: 0.01746s, Loss: 1.06588\n",
      "update best: 0.23440\n",
      "Epoch: 24, Time: 0.01816s, Loss: 1.06663\n",
      "update best: 0.23503\n",
      "Epoch: 25, Time: 0.01646s, Loss: 1.06579\n",
      "update best: 0.23661\n",
      "Epoch: 26, Time: 0.01418s, Loss: 1.06555\n",
      "update best: 0.23850\n",
      "Epoch: 27, Time: 0.01940s, Loss: 1.06543\n",
      "update best: 0.23913\n",
      "Epoch: 28, Time: 0.01814s, Loss: 1.06576\n",
      "update best: 0.24228\n",
      "Epoch: 29, Time: 0.01654s, Loss: 1.06630\n",
      "update best: 0.24354\n",
      "Epoch: 30, Time: 0.02108s, Loss: 1.06971\n",
      "update best: 0.24480\n",
      "Epoch: 31, Time: 0.02196s, Loss: 1.06689\n",
      "update best: 0.24606\n",
      "Epoch: 32, Time: 0.02126s, Loss: 1.06523\n",
      "update best: 0.24732\n",
      "Epoch: 33, Time: 0.02229s, Loss: 1.06573\n",
      "update best: 0.24921\n",
      "Epoch: 34, Time: 0.02110s, Loss: 1.06611\n",
      "update best: 0.25205\n",
      "Epoch: 35, Time: 0.01491s, Loss: 1.06583\n",
      "update best: 0.25268\n",
      "Epoch: 36, Time: 0.02071s, Loss: 1.06536\n",
      "update best: 0.25299\n",
      "Epoch: 37, Time: 0.02373s, Loss: 1.06547\n",
      "update best: 0.25520\n",
      "Epoch: 38, Time: 0.01887s, Loss: 1.06559\n",
      "update best: 0.25772\n",
      "Epoch: 39, Time: 0.01726s, Loss: 1.06651\n",
      "update best: 0.26024\n",
      "Epoch: 40, Time: 0.01665s, Loss: 1.06545\n",
      "update best: 0.26213\n",
      "Epoch: 41, Time: 0.01568s, Loss: 1.06932\n",
      "update best: 0.26686\n",
      "Epoch: 42, Time: 0.01882s, Loss: 1.06518\n",
      "update best: 0.27284\n",
      "Epoch: 43, Time: 0.01847s, Loss: 1.06632\n",
      "update best: 0.27694\n",
      "Epoch: 44, Time: 0.02051s, Loss: 1.06535\n",
      "update best: 0.28261\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06487\n",
      "update best: 0.28922\n",
      "Epoch: 46, Time: 0.01278s, Loss: 1.06737\n",
      "update best: 0.29395\n",
      "Epoch: 47, Time: 0.02161s, Loss: 1.06573\n",
      "update best: 0.30025\n",
      "Epoch: 48, Time: 0.01658s, Loss: 1.06771\n",
      "update best: 0.30529\n",
      "Epoch: 49, Time: 0.02203s, Loss: 1.06531\n",
      "update best: 0.30844\n",
      "Epoch: 50, Time: 0.02295s, Loss: 1.06524\n",
      "Epoch: 51, Time: 0.01611s, Loss: 1.06735\n",
      "Epoch: 52, Time: 0.01487s, Loss: 1.06611\n",
      "Epoch: 53, Time: 0.01798s, Loss: 1.06505\n",
      "Epoch: 54, Time: 0.01751s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01700s, Loss: 1.06523\n",
      "Epoch: 56, Time: 0.01782s, Loss: 1.06524\n",
      "update best: 0.30907\n",
      "Epoch: 57, Time: 0.01779s, Loss: 1.06639\n",
      "Epoch: 58, Time: 0.01718s, Loss: 1.06497\n",
      "Epoch: 59, Time: 0.01743s, Loss: 1.06528\n",
      "Epoch: 60, Time: 0.01958s, Loss: 1.06625\n",
      "Epoch: 61, Time: 0.02069s, Loss: 1.06578\n",
      "Epoch: 62, Time: 0.02205s, Loss: 1.06531\n",
      "Epoch: 63, Time: 0.01974s, Loss: 1.06577\n",
      "Epoch: 64, Time: 0.01765s, Loss: 1.06954\n",
      "Epoch: 65, Time: 0.01933s, Loss: 1.06588\n",
      "Epoch: 66, Time: 0.02240s, Loss: 1.06586\n",
      "Epoch: 67, Time: 0.01643s, Loss: 1.06498\n",
      "Epoch: 68, Time: 0.02143s, Loss: 1.06496\n",
      "Epoch: 69, Time: 0.01870s, Loss: 1.06564\n",
      "Epoch: 70, Time: 0.01784s, Loss: 1.06518\n",
      "Epoch: 71, Time: 0.01618s, Loss: 1.06531\n",
      "Epoch: 72, Time: 0.01860s, Loss: 1.06525\n",
      "Epoch: 73, Time: 0.01958s, Loss: 1.06627\n",
      "Epoch: 74, Time: 0.01948s, Loss: 1.06626\n",
      "Epoch: 75, Time: 0.01534s, Loss: 1.06529\n",
      "Epoch: 76, Time: 0.02089s, Loss: 1.06523\n",
      "Epoch: 77, Time: 0.01645s, Loss: 1.06529\n",
      "Epoch: 78, Time: 0.01748s, Loss: 1.06489\n",
      "Epoch: 79, Time: 0.01631s, Loss: 1.06540\n",
      "Epoch: 80, Time: 0.01887s, Loss: 1.06661\n",
      "Epoch: 81, Time: 0.01971s, Loss: 1.06601\n",
      "Epoch: 82, Time: 0.01765s, Loss: 1.06509\n",
      "Epoch: 83, Time: 0.01417s, Loss: 1.06528\n",
      "Epoch: 84, Time: 0.02164s, Loss: 1.06500\n",
      "Epoch: 85, Time: 0.01546s, Loss: 1.06513\n",
      "Epoch: 86, Time: 0.01848s, Loss: 1.06620\n",
      "Epoch: 87, Time: 0.01827s, Loss: 1.06537\n",
      "Epoch: 88, Time: 0.02056s, Loss: 1.06561\n",
      "Epoch: 89, Time: 0.01637s, Loss: 1.06596\n",
      "Epoch: 90, Time: 0.01818s, Loss: 1.06500\n",
      "Epoch: 91, Time: 0.01835s, Loss: 1.06544\n",
      "Epoch: 92, Time: 0.01780s, Loss: 1.06622\n",
      "Epoch: 93, Time: 0.01545s, Loss: 1.06540\n",
      "Epoch: 94, Time: 0.01683s, Loss: 1.06533\n",
      "Epoch: 95, Time: 0.00961s, Loss: 1.06524\n",
      "Epoch: 96, Time: 0.01486s, Loss: 1.06558\n",
      "Epoch: 97, Time: 0.01445s, Loss: 1.06523\n",
      "Epoch: 98, Time: 0.01343s, Loss: 1.06561\n",
      "Epoch: 99, Time: 0.01764s, Loss: 1.06520\n",
      "Epoch: 100, Time: 0.01711s, Loss: 1.06568\n",
      "Epoch: 101, Time: 0.01642s, Loss: 1.06556\n",
      "Epoch: 102, Time: 0.01880s, Loss: 1.06547\n",
      "Epoch: 103, Time: 0.02141s, Loss: 1.06525\n",
      "Epoch: 104, Time: 0.02041s, Loss: 1.06551\n",
      "Epoch: 105, Time: 0.01838s, Loss: 1.06518\n",
      "Epoch: 106, Time: 0.02069s, Loss: 1.06562\n",
      "Epoch: 107, Time: 0.01834s, Loss: 1.06548\n",
      "Epoch: 108, Time: 0.01653s, Loss: 1.06662\n",
      "Epoch: 109, Time: 0.01662s, Loss: 1.06501\n",
      "Epoch: 110, Time: 0.01663s, Loss: 1.06511\n",
      "Epoch: 111, Time: 0.01668s, Loss: 1.06533\n",
      "Epoch: 112, Time: 0.01661s, Loss: 1.06556\n",
      "Epoch: 113, Time: 0.01515s, Loss: 1.06608\n",
      "Epoch: 114, Time: 0.01924s, Loss: 1.06579\n",
      "Epoch: 115, Time: 0.01623s, Loss: 1.06643\n",
      "Epoch: 116, Time: 0.01687s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.02302s, Loss: 1.06563\n",
      "Epoch: 118, Time: 0.02006s, Loss: 1.06565\n",
      "Epoch: 119, Time: 0.01810s, Loss: 1.06541\n",
      "Epoch: 120, Time: 0.01875s, Loss: 1.06568\n",
      "Epoch: 121, Time: 0.01556s, Loss: 1.06552\n",
      "Epoch: 122, Time: 0.01580s, Loss: 1.06582\n",
      "Epoch: 123, Time: 0.01397s, Loss: 1.06587\n",
      "Epoch: 124, Time: 0.01464s, Loss: 1.06540\n",
      "Epoch: 125, Time: 0.01721s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.01737s, Loss: 1.06720\n",
      "Epoch: 127, Time: 0.01820s, Loss: 1.06625\n",
      "Epoch: 128, Time: 0.01723s, Loss: 1.06561\n",
      "Epoch: 129, Time: 0.01836s, Loss: 1.06507\n",
      "Epoch: 130, Time: 0.02318s, Loss: 1.06653\n",
      "Epoch: 131, Time: 0.01722s, Loss: 1.06598\n",
      "Epoch: 132, Time: 0.01641s, Loss: 1.06506\n",
      "Epoch: 133, Time: 0.01652s, Loss: 1.06573\n",
      "Epoch: 134, Time: 0.02001s, Loss: 1.07398\n",
      "Epoch: 135, Time: 0.02194s, Loss: 1.06520\n",
      "Epoch: 136, Time: 0.01737s, Loss: 1.06595\n",
      "Epoch: 137, Time: 0.01596s, Loss: 1.06582\n",
      "Epoch: 138, Time: 0.01830s, Loss: 1.06600\n",
      "Epoch: 139, Time: 0.01419s, Loss: 1.06568\n",
      "Epoch: 140, Time: 0.01439s, Loss: 1.06647\n",
      "Epoch: 141, Time: 0.01596s, Loss: 1.06581\n",
      "Epoch: 142, Time: 0.01908s, Loss: 1.06571\n",
      "Epoch: 143, Time: 0.01774s, Loss: 1.06544\n",
      "Epoch: 144, Time: 0.01544s, Loss: 1.06689\n",
      "Epoch: 145, Time: 0.01647s, Loss: 1.06594\n",
      "Epoch: 146, Time: 0.01264s, Loss: 1.06531\n",
      "Epoch: 147, Time: 0.01781s, Loss: 1.06523\n",
      "Epoch: 148, Time: 0.01835s, Loss: 1.06504\n",
      "Epoch: 149, Time: 0.01573s, Loss: 1.06545\n",
      "Epoch: 150, Time: 0.02005s, Loss: 1.06600\n",
      "Epoch: 151, Time: 0.01521s, Loss: 1.06673\n",
      "Epoch: 152, Time: 0.01710s, Loss: 1.06540\n",
      "Epoch: 153, Time: 0.01482s, Loss: 1.06659\n",
      "Epoch: 154, Time: 0.01641s, Loss: 1.06537\n",
      "Epoch: 155, Time: 0.01722s, Loss: 1.06526\n",
      "Epoch: 156, Time: 0.01513s, Loss: 1.06598\n",
      "Epoch: 157, Time: 0.01639s, Loss: 1.06505\n",
      "Epoch: 158, Time: 0.01848s, Loss: 1.06601\n",
      "Epoch: 159, Time: 0.01890s, Loss: 1.07954\n",
      "Epoch: 160, Time: 0.01546s, Loss: 1.06505\n",
      "Epoch: 161, Time: 0.01483s, Loss: 1.06558\n",
      "Epoch: 162, Time: 0.01780s, Loss: 1.06533\n",
      "Epoch: 163, Time: 0.01427s, Loss: 1.06565\n",
      "Epoch: 164, Time: 0.01694s, Loss: 1.06739\n",
      "Epoch: 165, Time: 0.01820s, Loss: 1.06558\n",
      "Epoch: 166, Time: 0.01369s, Loss: 1.06535\n",
      "update best: 0.31065\n",
      "Epoch: 167, Time: 0.01588s, Loss: 1.06600\n",
      "update best: 0.31222\n",
      "Epoch: 168, Time: 0.01453s, Loss: 1.06576\n",
      "update best: 0.31411\n",
      "Epoch: 169, Time: 0.01234s, Loss: 1.06575\n",
      "Epoch: 170, Time: 0.01634s, Loss: 1.06672\n",
      "Epoch: 171, Time: 0.01523s, Loss: 1.07067\n",
      "Epoch: 172, Time: 0.01926s, Loss: 1.06599\n",
      "Epoch: 173, Time: 0.01206s, Loss: 1.06577\n",
      "Epoch: 174, Time: 0.01843s, Loss: 1.06545\n",
      "Epoch: 175, Time: 0.01554s, Loss: 1.06565\n",
      "Epoch: 176, Time: 0.01660s, Loss: 1.06719\n",
      "Epoch: 177, Time: 0.01723s, Loss: 1.06799\n",
      "Epoch: 178, Time: 0.01610s, Loss: 1.06552\n",
      "Epoch: 179, Time: 0.01629s, Loss: 1.06543\n",
      "Epoch: 180, Time: 0.01191s, Loss: 1.06553\n",
      "Epoch: 181, Time: 0.01449s, Loss: 1.06599\n",
      "Epoch: 182, Time: 0.01931s, Loss: 1.06560\n",
      "Epoch: 183, Time: 0.01864s, Loss: 1.06541\n",
      "Epoch: 184, Time: 0.01652s, Loss: 1.06559\n",
      "Epoch: 185, Time: 0.02020s, Loss: 1.06522\n",
      "Epoch: 186, Time: 0.01820s, Loss: 1.06582\n",
      "Epoch: 187, Time: 0.01541s, Loss: 1.06633\n",
      "Epoch: 188, Time: 0.01390s, Loss: 1.06574\n",
      "Epoch: 189, Time: 0.01462s, Loss: 1.06550\n",
      "Epoch: 190, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 191, Time: 0.01547s, Loss: 1.06542\n",
      "Epoch: 192, Time: 0.01745s, Loss: 1.06509\n",
      "Epoch: 193, Time: 0.01468s, Loss: 1.06807\n",
      "Epoch: 194, Time: 0.01670s, Loss: 1.06519\n",
      "Epoch: 195, Time: 0.02036s, Loss: 1.06596\n",
      "Epoch: 196, Time: 0.01871s, Loss: 1.06728\n",
      "Epoch: 197, Time: 0.01775s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01911s, Loss: 1.06590\n",
      "Epoch: 199, Time: 0.01631s, Loss: 1.06588\n",
      "\n",
      "train finished!\n",
      "best val: 0.31411\n",
      "test...\n",
      "final result: epoch: 168\n",
      "{'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "\n",
      "=== Experiment with Top-k Densest Subgraphs ===\n",
      "Using Top-3 Densest Subgraphs preprocessing\n",
      "Epoch: 0, Time: 0.06559s, Loss: 1.82510\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02902s, Loss: 1.62440\n",
      "Epoch: 2, Time: 0.02847s, Loss: 1.39076\n",
      "Epoch: 3, Time: 0.02935s, Loss: 1.26225\n",
      "Epoch: 4, Time: 0.03275s, Loss: 1.21692\n",
      "Epoch: 5, Time: 0.03256s, Loss: 1.19965\n",
      "Epoch: 6, Time: 0.03542s, Loss: 1.19788\n",
      "Epoch: 7, Time: 0.02822s, Loss: 1.16639\n",
      "Epoch: 8, Time: 0.02576s, Loss: 1.16581\n",
      "Epoch: 9, Time: 0.02931s, Loss: 1.16196\n",
      "Epoch: 10, Time: 0.02472s, Loss: 1.16319\n",
      "Epoch: 11, Time: 0.03068s, Loss: 1.15742\n",
      "Epoch: 12, Time: 0.02637s, Loss: 1.15874\n",
      "Epoch: 13, Time: 0.02626s, Loss: 1.15653\n",
      "Epoch: 14, Time: 0.02875s, Loss: 1.15744\n",
      "Epoch: 15, Time: 0.03449s, Loss: 1.15601\n",
      "Epoch: 16, Time: 0.03268s, Loss: 1.15622\n",
      "Epoch: 17, Time: 0.03114s, Loss: 1.15699\n",
      "Epoch: 18, Time: 0.02857s, Loss: 1.15729\n",
      "Epoch: 19, Time: 0.02153s, Loss: 1.15600\n",
      "Epoch: 20, Time: 0.03093s, Loss: 1.15623\n",
      "Epoch: 21, Time: 0.03050s, Loss: 1.15606\n",
      "Epoch: 22, Time: 0.02683s, Loss: 1.15718\n",
      "update best: 0.21456\n",
      "Epoch: 23, Time: 0.03116s, Loss: 1.15620\n",
      "update best: 0.21550\n",
      "Epoch: 24, Time: 0.02556s, Loss: 1.15575\n",
      "update best: 0.21834\n",
      "Epoch: 25, Time: 0.03636s, Loss: 1.15600\n",
      "update best: 0.21960\n",
      "Epoch: 26, Time: 0.03473s, Loss: 1.15822\n",
      "update best: 0.21991\n",
      "Epoch: 27, Time: 0.02267s, Loss: 1.15585\n",
      "update best: 0.22117\n",
      "Epoch: 28, Time: 0.02434s, Loss: 1.15610\n",
      "update best: 0.22369\n",
      "Epoch: 29, Time: 0.03203s, Loss: 1.15606\n",
      "update best: 0.22464\n",
      "Epoch: 30, Time: 0.03443s, Loss: 1.15708\n",
      "update best: 0.22716\n",
      "Epoch: 31, Time: 0.02783s, Loss: 1.15568\n",
      "Epoch: 32, Time: 0.02653s, Loss: 1.15782\n",
      "update best: 0.22779\n",
      "Epoch: 33, Time: 0.02463s, Loss: 1.15597\n",
      "update best: 0.22999\n",
      "Epoch: 34, Time: 0.03037s, Loss: 1.15607\n",
      "Epoch: 35, Time: 0.02218s, Loss: 1.16056\n",
      "update best: 0.23125\n",
      "Epoch: 36, Time: 0.02994s, Loss: 1.15738\n",
      "update best: 0.23251\n",
      "Epoch: 37, Time: 0.03148s, Loss: 1.15582\n",
      "update best: 0.23566\n",
      "Epoch: 38, Time: 0.02190s, Loss: 1.15774\n",
      "update best: 0.23693\n",
      "Epoch: 39, Time: 0.03193s, Loss: 1.15581\n",
      "update best: 0.23787\n",
      "Epoch: 40, Time: 0.03539s, Loss: 1.15770\n",
      "update best: 0.23913\n",
      "Epoch: 41, Time: 0.03075s, Loss: 1.15707\n",
      "update best: 0.24008\n",
      "Epoch: 42, Time: 0.02908s, Loss: 1.15580\n",
      "update best: 0.24134\n",
      "Epoch: 43, Time: 0.02855s, Loss: 1.16010\n",
      "update best: 0.24417\n",
      "Epoch: 44, Time: 0.02485s, Loss: 1.15595\n",
      "update best: 0.24480\n",
      "Epoch: 45, Time: 0.02687s, Loss: 1.15583\n",
      "update best: 0.24606\n",
      "Epoch: 46, Time: 0.02626s, Loss: 1.15602\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.02447s, Loss: 1.15593\n",
      "update best: 0.24764\n",
      "Epoch: 48, Time: 0.02797s, Loss: 1.15584\n",
      "Epoch: 49, Time: 0.02457s, Loss: 1.15605\n",
      "update best: 0.24858\n",
      "Epoch: 50, Time: 0.03008s, Loss: 1.15597\n",
      "update best: 0.24890\n",
      "Epoch: 51, Time: 0.02840s, Loss: 1.15596\n",
      "update best: 0.24921\n",
      "Epoch: 52, Time: 0.02966s, Loss: 1.15576\n",
      "update best: 0.25047\n",
      "Epoch: 53, Time: 0.02614s, Loss: 1.15571\n",
      "Epoch: 54, Time: 0.02849s, Loss: 1.15593\n",
      "Epoch: 55, Time: 0.03094s, Loss: 1.15567\n",
      "Epoch: 56, Time: 0.02563s, Loss: 1.15609\n",
      "update best: 0.25110\n",
      "Epoch: 57, Time: 0.02449s, Loss: 1.15612\n",
      "update best: 0.25236\n",
      "Epoch: 58, Time: 0.02537s, Loss: 1.15575\n",
      "update best: 0.25299\n",
      "Epoch: 59, Time: 0.02872s, Loss: 1.15582\n",
      "update best: 0.25425\n",
      "Epoch: 60, Time: 0.03122s, Loss: 1.15583\n",
      "update best: 0.25457\n",
      "Epoch: 61, Time: 0.03164s, Loss: 1.15581\n",
      "Epoch: 62, Time: 0.03158s, Loss: 1.15587\n",
      "Epoch: 63, Time: 0.02415s, Loss: 1.15588\n",
      "update best: 0.25520\n",
      "Epoch: 64, Time: 0.03063s, Loss: 1.15595\n",
      "update best: 0.25551\n",
      "Epoch: 65, Time: 0.02690s, Loss: 1.15598\n",
      "Epoch: 66, Time: 0.02762s, Loss: 1.15590\n",
      "Epoch: 67, Time: 0.02945s, Loss: 1.15575\n",
      "update best: 0.25677\n",
      "Epoch: 68, Time: 0.03464s, Loss: 1.15585\n",
      "update best: 0.25803\n",
      "Epoch: 69, Time: 0.03152s, Loss: 1.15595\n",
      "update best: 0.25835\n",
      "Epoch: 70, Time: 0.03722s, Loss: 1.15586\n",
      "update best: 0.25866\n",
      "Epoch: 71, Time: 0.02826s, Loss: 1.15621\n",
      "Epoch: 72, Time: 0.03272s, Loss: 1.15589\n",
      "Epoch: 73, Time: 0.03118s, Loss: 1.15693\n",
      "Epoch: 74, Time: 0.02265s, Loss: 1.15596\n",
      "update best: 0.25961\n",
      "Epoch: 75, Time: 0.03024s, Loss: 1.15583\n",
      "Epoch: 76, Time: 0.02835s, Loss: 1.15777\n",
      "Epoch: 77, Time: 0.03109s, Loss: 1.15602\n",
      "Epoch: 78, Time: 0.02700s, Loss: 1.15594\n",
      "Epoch: 79, Time: 0.03067s, Loss: 1.15586\n",
      "update best: 0.25992\n",
      "Epoch: 80, Time: 0.02818s, Loss: 1.15567\n",
      "Epoch: 81, Time: 0.03168s, Loss: 1.15586\n",
      "Epoch: 82, Time: 0.03077s, Loss: 1.15606\n",
      "update best: 0.26024\n",
      "Epoch: 83, Time: 0.02648s, Loss: 1.15578\n",
      "update best: 0.26055\n",
      "Epoch: 84, Time: 0.03257s, Loss: 1.15599\n",
      "update best: 0.26087\n",
      "Epoch: 85, Time: 0.03028s, Loss: 1.15595\n",
      "Epoch: 86, Time: 0.03238s, Loss: 1.15612\n",
      "Epoch: 87, Time: 0.02833s, Loss: 1.15617\n",
      "Epoch: 88, Time: 0.02255s, Loss: 1.15612\n",
      "Epoch: 89, Time: 0.03143s, Loss: 1.15581\n",
      "Epoch: 90, Time: 0.03042s, Loss: 1.15598\n",
      "update best: 0.26150\n",
      "Epoch: 91, Time: 0.03111s, Loss: 1.15649\n",
      "Epoch: 92, Time: 0.02915s, Loss: 1.15632\n",
      "Epoch: 93, Time: 0.03287s, Loss: 1.15573\n",
      "update best: 0.26181\n",
      "Epoch: 94, Time: 0.03230s, Loss: 1.15726\n",
      "update best: 0.26213\n",
      "Epoch: 95, Time: 0.02853s, Loss: 1.15721\n",
      "Epoch: 96, Time: 0.03244s, Loss: 1.15582\n",
      "Epoch: 97, Time: 0.03126s, Loss: 1.15621\n",
      "update best: 0.26244\n",
      "Epoch: 98, Time: 0.02367s, Loss: 1.15600\n",
      "update best: 0.26307\n",
      "Epoch: 99, Time: 0.03417s, Loss: 1.15601\n",
      "Epoch: 100, Time: 0.02501s, Loss: 1.15614\n",
      "Epoch: 101, Time: 0.03208s, Loss: 1.15612\n",
      "Epoch: 102, Time: 0.02693s, Loss: 1.15597\n",
      "Epoch: 103, Time: 0.02612s, Loss: 1.15600\n",
      "Epoch: 104, Time: 0.02501s, Loss: 1.15746\n",
      "update best: 0.26402\n",
      "Epoch: 105, Time: 0.02798s, Loss: 1.15580\n",
      "update best: 0.26560\n",
      "Epoch: 106, Time: 0.02560s, Loss: 1.15584\n",
      "update best: 0.26654\n",
      "Epoch: 107, Time: 0.02655s, Loss: 1.15595\n",
      "update best: 0.26717\n",
      "Epoch: 108, Time: 0.03282s, Loss: 1.15584\n",
      "Epoch: 109, Time: 0.02493s, Loss: 1.15637\n",
      "update best: 0.26875\n",
      "Epoch: 110, Time: 0.02819s, Loss: 1.15628\n",
      "Epoch: 111, Time: 0.03212s, Loss: 1.15633\n",
      "Epoch: 112, Time: 0.02784s, Loss: 1.15634\n",
      "update best: 0.26969\n",
      "Epoch: 113, Time: 0.03127s, Loss: 1.15574\n",
      "Epoch: 114, Time: 0.02956s, Loss: 1.15815\n",
      "Epoch: 115, Time: 0.03216s, Loss: 1.15664\n",
      "Epoch: 116, Time: 0.02278s, Loss: 1.15611\n",
      "Epoch: 117, Time: 0.03280s, Loss: 1.15593\n",
      "Epoch: 118, Time: 0.02437s, Loss: 1.15600\n",
      "Epoch: 119, Time: 0.02233s, Loss: 1.15648\n",
      "Epoch: 120, Time: 0.03344s, Loss: 1.15758\n",
      "Epoch: 121, Time: 0.03560s, Loss: 1.15583\n",
      "Epoch: 122, Time: 0.03149s, Loss: 1.15573\n",
      "Epoch: 123, Time: 0.03710s, Loss: 1.15598\n",
      "Epoch: 124, Time: 0.02458s, Loss: 1.15600\n",
      "Epoch: 125, Time: 0.02412s, Loss: 1.15620\n",
      "Epoch: 126, Time: 0.03206s, Loss: 1.15640\n",
      "Epoch: 127, Time: 0.02860s, Loss: 1.15602\n",
      "Epoch: 128, Time: 0.02843s, Loss: 1.15700\n",
      "Epoch: 129, Time: 0.02828s, Loss: 1.15622\n",
      "Epoch: 130, Time: 0.02049s, Loss: 1.15669\n",
      "Epoch: 131, Time: 0.02248s, Loss: 1.15658\n",
      "Epoch: 132, Time: 0.02196s, Loss: 1.15610\n",
      "Epoch: 133, Time: 0.02219s, Loss: 1.15620\n",
      "Epoch: 134, Time: 0.02394s, Loss: 1.15636\n",
      "Epoch: 135, Time: 0.03258s, Loss: 1.15674\n",
      "Epoch: 136, Time: 0.02634s, Loss: 1.15605\n",
      "Epoch: 137, Time: 0.03094s, Loss: 1.15939\n",
      "update best: 0.27064\n",
      "Epoch: 138, Time: 0.02247s, Loss: 1.15665\n",
      "update best: 0.27127\n",
      "Epoch: 139, Time: 0.02647s, Loss: 1.15624\n",
      "update best: 0.27190\n",
      "Epoch: 140, Time: 0.02678s, Loss: 1.15860\n",
      "update best: 0.27505\n",
      "Epoch: 141, Time: 0.03109s, Loss: 1.15604\n",
      "Epoch: 142, Time: 0.02169s, Loss: 1.15715\n",
      "Epoch: 143, Time: 0.03042s, Loss: 1.15603\n",
      "Epoch: 144, Time: 0.02342s, Loss: 1.15832\n",
      "Epoch: 145, Time: 0.03094s, Loss: 1.15732\n",
      "Epoch: 146, Time: 0.03019s, Loss: 1.15621\n",
      "Epoch: 147, Time: 0.02492s, Loss: 1.15594\n",
      "Epoch: 148, Time: 0.02848s, Loss: 1.15635\n",
      "Epoch: 149, Time: 0.03001s, Loss: 1.15624\n",
      "Epoch: 150, Time: 0.03320s, Loss: 1.15611\n",
      "Epoch: 151, Time: 0.03006s, Loss: 1.15612\n",
      "Epoch: 152, Time: 0.03538s, Loss: 1.15713\n",
      "Epoch: 153, Time: 0.03224s, Loss: 1.15649\n",
      "Epoch: 154, Time: 0.03126s, Loss: 1.15601\n",
      "Epoch: 155, Time: 0.03383s, Loss: 1.15616\n",
      "Epoch: 156, Time: 0.02578s, Loss: 1.15601\n",
      "Epoch: 157, Time: 0.02708s, Loss: 1.15609\n",
      "Epoch: 158, Time: 0.03211s, Loss: 1.15645\n",
      "Epoch: 159, Time: 0.02848s, Loss: 1.15666\n",
      "Epoch: 160, Time: 0.02643s, Loss: 1.15719\n",
      "Epoch: 161, Time: 0.03038s, Loss: 1.15599\n",
      "Epoch: 162, Time: 0.03179s, Loss: 1.15762\n",
      "Epoch: 163, Time: 0.03095s, Loss: 1.15674\n",
      "Epoch: 164, Time: 0.02292s, Loss: 1.15618\n",
      "Epoch: 165, Time: 0.02855s, Loss: 1.15583\n",
      "Epoch: 166, Time: 0.02436s, Loss: 1.15596\n",
      "Epoch: 167, Time: 0.03066s, Loss: 1.15606\n",
      "Epoch: 168, Time: 0.03028s, Loss: 1.15703\n",
      "Epoch: 169, Time: 0.03328s, Loss: 1.15598\n",
      "Epoch: 170, Time: 0.03524s, Loss: 1.15600\n",
      "Epoch: 171, Time: 0.02893s, Loss: 1.15637\n",
      "Epoch: 172, Time: 0.02216s, Loss: 1.15700\n",
      "Epoch: 173, Time: 0.03289s, Loss: 1.15589\n",
      "Epoch: 174, Time: 0.02688s, Loss: 1.15718\n",
      "Epoch: 175, Time: 0.02044s, Loss: 1.15571\n",
      "Epoch: 176, Time: 0.02914s, Loss: 1.15585\n",
      "Epoch: 177, Time: 0.02653s, Loss: 1.15718\n",
      "Epoch: 178, Time: 0.02829s, Loss: 1.15691\n",
      "Epoch: 179, Time: 0.03039s, Loss: 1.15713\n",
      "Epoch: 180, Time: 0.03574s, Loss: 1.15608\n",
      "Epoch: 181, Time: 0.02998s, Loss: 1.15629\n",
      "Epoch: 182, Time: 0.03262s, Loss: 1.15587\n",
      "Epoch: 183, Time: 0.02428s, Loss: 1.15661\n",
      "Epoch: 184, Time: 0.03045s, Loss: 1.15654\n",
      "Epoch: 185, Time: 0.02416s, Loss: 1.15611\n",
      "Epoch: 186, Time: 0.03054s, Loss: 1.15641\n",
      "Epoch: 187, Time: 0.02843s, Loss: 1.15752\n",
      "Epoch: 188, Time: 0.02668s, Loss: 1.15607\n",
      "Epoch: 189, Time: 0.03046s, Loss: 1.15627\n",
      "Epoch: 190, Time: 0.02530s, Loss: 1.15581\n",
      "Epoch: 191, Time: 0.03288s, Loss: 1.15662\n",
      "Epoch: 192, Time: 0.03075s, Loss: 1.15663\n",
      "Epoch: 193, Time: 0.03123s, Loss: 1.15609\n",
      "Epoch: 194, Time: 0.02127s, Loss: 1.15605\n",
      "Epoch: 195, Time: 0.02872s, Loss: 1.15676\n",
      "Epoch: 196, Time: 0.02444s, Loss: 1.15607\n",
      "Epoch: 197, Time: 0.02503s, Loss: 1.15975\n",
      "Epoch: 198, Time: 0.03029s, Loss: 1.15604\n",
      "Epoch: 199, Time: 0.03272s, Loss: 1.15623\n",
      "\n",
      "train finished!\n",
      "best val: 0.27505\n",
      "test...\n",
      "final result: epoch: 140\n",
      "{'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n",
      "\n",
      "=== Comparison of Results ===\n",
      "Baseline performance: {'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "Top-k performance: {'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import os\n",
    "# from copy import deepcopy\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import dhg\n",
    "# from dhg import Graph, Hypergraph\n",
    "# from dhg.data import Cooking200, News20\n",
    "# from dhg.models import GCN, HGNN, HGNNP, HNHN\n",
    "# from dhg.random import set_seed\n",
    "# from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "# from dhg.utils import split_by_ratio\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def get_top_k_densest_subgraphs(hg: Hypergraph, k: int = 3) -> list:\n",
    "    \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "    nodes = set(range(hg.num_v))\n",
    "    subgraphs = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if len(nodes) < 3:  # min_size\n",
    "            break\n",
    "            \n",
    "        current_nodes = set(nodes)\n",
    "        best_subset = None\n",
    "        best_density = -1\n",
    "        \n",
    "        while len(current_nodes) >= 3:\n",
    "            edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "            density = edge_count / len(current_nodes)\n",
    "            \n",
    "            if density > best_density:\n",
    "                best_density = density\n",
    "                best_subset = set(current_nodes)\n",
    "            \n",
    "            # Remove node with lowest degree\n",
    "            degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "            node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "            current_nodes.remove(node_to_remove)\n",
    "        \n",
    "        if best_subset:\n",
    "            subgraphs.append((best_subset, best_density))\n",
    "            nodes -= best_subset\n",
    "    \n",
    "    return subgraphs\n",
    "\n",
    "def preprocess_hypergraph_with_topk(hg: Hypergraph, k: int = 3) -> Hypergraph:\n",
    "    \"\"\"Preprocess hypergraph by focusing on top-k densest subgraphs\"\"\"\n",
    "    subgraphs = get_top_k_densest_subgraphs(hg, k)\n",
    "    if not subgraphs:\n",
    "        return hg\n",
    "    \n",
    "    # Combine all nodes from top-k subgraphs\n",
    "    important_nodes = set()\n",
    "    for subset, _ in subgraphs:\n",
    "        important_nodes.update(subset)\n",
    "    \n",
    "    # Create new hypergraph with only important edges\n",
    "    new_edges = []\n",
    "    for e in hg.e[0]:\n",
    "        if set(e).issubset(important_nodes):\n",
    "            new_edges.append(e)\n",
    "    \n",
    "    return Hypergraph(hg.num_v, new_edges)\n",
    "\n",
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res\n",
    "\n",
    "def run_experiment(use_topk=False, k=3):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    \n",
    "    if use_topk:\n",
    "        print(f\"Using Top-{k} Densest Subgraphs preprocessing\")\n",
    "        G = preprocess_hypergraph_with_topk(G, k)\n",
    "    else:\n",
    "        print(\"Using original hypergraph (no Top-k preprocessing)\")\n",
    "    \n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    test_mask = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch, best_val = 0, 0\n",
    "    for epoch in range(200):\n",
    "        # train\n",
    "        train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(net, X, G, lbl, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_epoch = epoch\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "# Run experiments\n",
    "print(\"=== Baseline Experiment (No Top-k) ===\")\n",
    "baseline_results = run_experiment(use_topk=False)\n",
    "\n",
    "print(\"\\n=== Experiment with Top-k Densest Subgraphs ===\")\n",
    "topk_results = run_experiment(use_topk=True, k=3)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Comparison of Results ===\")\n",
    "print(f\"Baseline performance: {baseline_results}\")\n",
    "print(f\"Top-k performance: {topk_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.11)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/utkarshx27/movies-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.13M/5.13M [00:00<00:00, 23.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n",
      "Path to dataset files: C:\\Users\\rustem_izmailov\\.cache\\kagglehub\\datasets\\utkarshx27\\movies-dataset\\versions\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"utkarshx27/movies-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(movie_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4803 entries, 0 to 4802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 4803 non-null   int64  \n",
      " 1   budget                4803 non-null   int64  \n",
      " 2   genres                4775 non-null   object \n",
      " 3   homepage              1712 non-null   object \n",
      " 4   id                    4803 non-null   int64  \n",
      " 5   keywords              4391 non-null   object \n",
      " 6   original_language     4803 non-null   object \n",
      " 7   original_title        4803 non-null   object \n",
      " 8   overview              4800 non-null   object \n",
      " 9   popularity            4803 non-null   float64\n",
      " 10  production_companies  4803 non-null   object \n",
      " 11  production_countries  4803 non-null   object \n",
      " 12  release_date          4802 non-null   object \n",
      " 13  revenue               4803 non-null   int64  \n",
      " 14  runtime               4801 non-null   float64\n",
      " 15  spoken_languages      4803 non-null   object \n",
      " 16  status                4803 non-null   object \n",
      " 17  tagline               3959 non-null   object \n",
      " 18  title                 4803 non-null   object \n",
      " 19  vote_average          4803 non-null   float64\n",
      " 20  vote_count            4803 non-null   int64  \n",
      " 21  cast                  4760 non-null   object \n",
      " 22  crew                  4803 non-null   object \n",
      " 23  director              4773 non-null   object \n",
      "dtypes: float64(3), int64(5), object(16)\n",
      "memory usage: 900.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Action Adventure Fantasy Science Fiction\n",
       "1                    Adventure Fantasy Action\n",
       "2                      Action Adventure Crime\n",
       "3                 Action Crime Drama Thriller\n",
       "4            Action Adventure Science Fiction\n",
       "Name: genres, dtype: object"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genres.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphExperiment:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.results = []\n",
    "        \n",
    "    def run_experiments(self, n_samples=500):\n",
    "        \"\"\"Запускает все варианты экспериментов\"\"\"\n",
    "        # 1. Базовый гиперграф (без плотных подграфов)\n",
    "        print(\"\\n=== Experiment 1: Base Hypergraph ===\")\n",
    "        base_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=False\n",
    "        )\n",
    "        base_results = self._train_and_evaluate(base_data)\n",
    "        self.results.append((\"Base\", base_results))\n",
    "        \n",
    "        # 2. С топ-k плотными подграфами\n",
    "        print(\"\\n=== Experiment 2: With Densest Subgraphs ===\")\n",
    "        dense_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=True,\n",
    "            k=5\n",
    "        )\n",
    "        dense_results = self._train_and_evaluate(dense_data)\n",
    "        self.results.append((\"With Densest Subgraphs\", dense_results))\n",
    "        \n",
    "        # 3. Только жанровые гиперребра\n",
    "        print(\"\\n=== Experiment 3: Genre-Only Hypergraph ===\")\n",
    "        genre_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            hyperedge_types=[\"genre\"]\n",
    "        )\n",
    "        genre_results = self._train_and_evaluate(genre_data)\n",
    "        self.results.append((\"Genre-Only\", genre_results))\n",
    "        \n",
    "        # Визуализация результатов\n",
    "        self._visualize_results()\n",
    "\n",
    "    def _train_and_evaluate(self, data) -> Dict[str, float]:\n",
    "        \"\"\"Обучает модели и возвращает результаты\"\"\"\n",
    "        # Get all the data we need\n",
    "        X = data[\"features\"]\n",
    "        lbl = data[\"labels\"]\n",
    "        edge_list = data[\"edge_list\"]\n",
    "        num_vertices = data[\"num_vertices\"]\n",
    "        train_mask = data[\"train_mask\"]\n",
    "        val_mask = data[\"val_mask\"]\n",
    "        test_mask = data[\"test_mask\"]\n",
    "        num_classes = data[\"num_classes\"]\n",
    "        \n",
    "        # Check if edge_list is empty\n",
    "        if not edge_list:\n",
    "            print(\"Warning: Empty edge list! Creating a fallback edge list based on nearest neighbors.\")\n",
    "            # Create a fallback edge list using k-nearest neighbors\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X.numpy())\n",
    "            _, indices = knn.kneighbors(X.numpy())\n",
    "            edge_list = [list(idx) for idx in indices]\n",
    "        \n",
    "        # Now use the potentially updated edge_list\n",
    "        hg = Hypergraph(num_vertices, edge_list)\n",
    "        masks = {\n",
    "            \"train\": train_mask,\n",
    "            \"val\": val_mask,\n",
    "            \"test\": test_mask\n",
    "        }\n",
    "        \n",
    "        models = {\n",
    "            \"HGNN\": HGNN(X.shape[1], 64, num_classes),\n",
    "            \"HGNNP\": HGNNP(X.shape[1], 64, num_classes, use_bn=True),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model = model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            \n",
    "            best_val = 0\n",
    "            for epoch in range(100):\n",
    "                train(model, X, hg, lbl, masks[\"train\"], optimizer, epoch)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    val_res = infer(model, X, hg, lbl, masks[\"val\"])\n",
    "                    # Extract the accuracy value from the dictionary\n",
    "                    val_accuracy = val_res.get('accuracy', 0)  # Assuming accuracy is the key\n",
    "                    if val_accuracy > best_val:\n",
    "                        best_val = val_accuracy\n",
    "            \n",
    "            test_res = infer(model, X, hg, lbl, masks[\"test\"], test=True)\n",
    "            results[name] = {\n",
    "                \"val_accuracy\": best_val,\n",
    "                \"test_accuracy\": test_res.get('accuracy', 0)  # Also extract accuracy\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _visualize_results(self):\n",
    "        \"\"\"Визуализирует сравнение экспериментов\"\"\"\n",
    "        df = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df.plot(\n",
    "            x=\"Experiment\",\n",
    "            y=[\"HGNN Val\", \"HGNN Test\", \"HGNNP Val\", \"HGNNP Test\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"hypergraph_comparison.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Results Summary ===\")\n",
    "        print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieHypergraphDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        n_samples: Optional[int] = None,\n",
    "        use_densest_subgraphs: bool = False,\n",
    "        k: int = 3,\n",
    "        hyperedge_types: List[str] = [\"genre\", \"director\", \"numerical\"]\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.n_samples = n_samples\n",
    "        self.use_densest_subgraphs = use_densest_subgraphs\n",
    "        self.k = k\n",
    "        self.hyperedge_types = hyperedge_types\n",
    "        self._content = self._build_dataset()\n",
    "\n",
    "    def _build_dataset(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build the complete dataset dictionary\"\"\"\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(self.data_root)\n",
    "        if self.n_samples:\n",
    "            df = df.sample(min(self.n_samples, len(df)))\n",
    "\n",
    "        df = self._preprocess_data(df)\n",
    "        \n",
    "        # Create features and labels\n",
    "        features = self._create_features(df)\n",
    "        labels = (df['revenue'] > df['budget']).astype(int).values\n",
    "        \n",
    "        # Create splits\n",
    "        train_mask, val_mask, test_mask = self._create_splits(labels)\n",
    "        \n",
    "        # Create hyperedges\n",
    "        edge_list = self._create_hyperedges(df)\n",
    "        \n",
    "        # Add densest subgraphs if enabled\n",
    "        if self.use_densest_subgraphs and len(edge_list) > 0:\n",
    "            temp_hg = Hypergraph(len(df), edge_list)\n",
    "            top_k_subgraphs = self._get_top_k_densest_subgraphs(temp_hg, k=self.k)\n",
    "            edge_list.extend([list(subset) for subset, _ in top_k_subgraphs])\n",
    "        \n",
    "        return {\n",
    "            \"num_classes\": 2,\n",
    "            \"num_vertices\": len(df),\n",
    "            \"num_edges\": len(edge_list),\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"labels\": torch.LongTensor(labels),\n",
    "            \"edge_list\": edge_list,\n",
    "            \"train_mask\": train_mask,\n",
    "            \"val_mask\": val_mask,\n",
    "            \"test_mask\": test_mask,\n",
    "        }\n",
    "\n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
    "        # Convert stringified lists to actual lists\n",
    "        for col in ['genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: literal_eval(x) \n",
    "                    if pd.notna(x) and isinstance(x, str) and x.startswith('[') \n",
    "                    else []\n",
    "                )\n",
    "        \n",
    "        # Handle genres specially if they're space-separated strings\n",
    "        if 'genres' in df.columns:\n",
    "            df['genres'] = df['genres'].apply(\n",
    "                lambda x: [{'name': g.strip()} for g in x.split()] \n",
    "                if pd.notna(x) and isinstance(x, str) and not x.startswith('[')\n",
    "                else x\n",
    "            )\n",
    "        \n",
    "        # Fill missing values\n",
    "        text_cols = ['overview', 'tagline', 'director']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "        \n",
    "        num_cols = ['runtime', 'budget', 'revenue', 'popularity', 'vote_average', 'vote_count']\n",
    "        for col in num_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _create_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create feature matrix combining numerical and text features\"\"\"\n",
    "        # Numerical features\n",
    "        num_features = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
    "        X_num = StandardScaler().fit_transform(df[num_features].values) if num_features else np.zeros((len(df), 0))\n",
    "        \n",
    "        # Text features from overview\n",
    "        tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "        X_text = tfidf.fit_transform(df['overview']).toarray() if 'overview' in df.columns else np.zeros((len(df), 0))\n",
    "        \n",
    "        return np.concatenate([X_num, X_text], axis=1)\n",
    "\n",
    "    def _create_splits(self, labels: np.ndarray) -> tuple:\n",
    "        \"\"\"Create train/val/test splits with stratification\"\"\"\n",
    "        indices = np.arange(len(labels))\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=0.3, stratify=labels)\n",
    "        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=labels[test_idx])\n",
    "        \n",
    "        train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        \n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "        \n",
    "        return train_mask, val_mask, test_mask\n",
    "\n",
    "    def _create_hyperedges(self, df: pd.DataFrame) -> list:\n",
    "        \"\"\"Create hyperedges based on specified types\"\"\"\n",
    "        edge_list = []\n",
    "        \n",
    "        if \"genre\" in self.hyperedge_types and 'genres' in df.columns:\n",
    "            genre_to_movies = {}\n",
    "            for idx, genres in enumerate(df['genres']):\n",
    "                if isinstance(genres, list):\n",
    "                    for genre in genres:\n",
    "                        name = genre['name'] if isinstance(genre, dict) else genre\n",
    "                        if name not in genre_to_movies:\n",
    "                            genre_to_movies[name] = []\n",
    "                        genre_to_movies[name].append(idx)\n",
    "            edge_list.extend(list(genre_to_movies.values()))\n",
    "        \n",
    "        if \"director\" in self.hyperedge_types and 'director' in df.columns:\n",
    "            director_to_movies = {}\n",
    "            for idx, director in enumerate(df['director']):\n",
    "                if pd.notna(director):\n",
    "                    if director not in director_to_movies:\n",
    "                        director_to_movies[director] = []\n",
    "                    director_to_movies[director].append(idx)\n",
    "            edge_list.extend(list(director_to_movies.values()))\n",
    "        \n",
    "        if \"numerical\" in self.hyperedge_types:\n",
    "            numerical_features = ['budget', 'popularity', 'runtime', 'vote_average']\n",
    "            X_num = df[numerical_features].values\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X_num)\n",
    "            _, indices = knn.kneighbors(X_num)\n",
    "            edge_list.extend([list(idx) for idx in indices])\n",
    "        \n",
    "        return edge_list\n",
    "\n",
    "    def _get_top_k_densest_subgraphs(self, hg: Hypergraph, k: int = 3) -> list:\n",
    "        \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "        nodes = set(range(hg.num_v))\n",
    "        subgraphs = []\n",
    "        \n",
    "        for _ in range(k):\n",
    "            if len(nodes) < 3:  # min_size\n",
    "                break\n",
    "                \n",
    "            current_nodes = set(nodes)\n",
    "            best_subset = None\n",
    "            best_density = -1\n",
    "            \n",
    "            while len(current_nodes) >= 3:\n",
    "                edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "                density = edge_count / len(current_nodes)\n",
    "                \n",
    "                if density > best_density:\n",
    "                    best_density = density\n",
    "                    best_subset = set(current_nodes)\n",
    "                \n",
    "                # Remove node with lowest degree\n",
    "                degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "                node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "                current_nodes.remove(node_to_remove)\n",
    "            \n",
    "            if best_subset:\n",
    "                subgraphs.append((best_subset, best_density))\n",
    "                nodes -= best_subset\n",
    "        \n",
    "        return subgraphs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._content[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    \n",
    "    # Handle both cases: when evaluator returns dict or float\n",
    "    if isinstance(res, dict):\n",
    "        return res\n",
    "    else:\n",
    "        # If single float returned, assume it's accuracy\n",
    "        return {'accuracy': res, 'f1_score': res}  # Using same value for both for compatibility\n",
    "\n",
    "def evaluate_model(model, hypergraph_type, X, hg, labels, masks):\n",
    "    results = {}\n",
    "    for name, mask in masks.items():\n",
    "        res = infer(model, X, hg, labels, mask, test=(name == \"test\"))\n",
    "        results[f\"{hypergraph_type}_{name}\"] = res\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1: Base Hypergraph ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69112\n",
      "Epoch: 1, Loss: 0.66235\n",
      "Epoch: 2, Loss: 0.63524\n",
      "Epoch: 3, Loss: 0.61238\n",
      "Epoch: 4, Loss: 0.58599\n",
      "Epoch: 5, Loss: 0.55757\n",
      "Epoch: 6, Loss: 0.54222\n",
      "Epoch: 7, Loss: 0.51972\n",
      "Epoch: 8, Loss: 0.51287\n",
      "Epoch: 9, Loss: 0.50268\n",
      "Epoch: 10, Loss: 0.48778\n",
      "Epoch: 11, Loss: 0.47216\n",
      "Epoch: 12, Loss: 0.46113\n",
      "Epoch: 13, Loss: 0.46434\n",
      "Epoch: 14, Loss: 0.44284\n",
      "Epoch: 15, Loss: 0.44244\n",
      "Epoch: 16, Loss: 0.42341\n",
      "Epoch: 17, Loss: 0.41219\n",
      "Epoch: 18, Loss: 0.41091\n",
      "Epoch: 19, Loss: 0.39324\n",
      "Epoch: 20, Loss: 0.39751\n",
      "Epoch: 21, Loss: 0.39395\n",
      "Epoch: 22, Loss: 0.38533\n",
      "Epoch: 23, Loss: 0.38006\n",
      "Epoch: 24, Loss: 0.37904\n",
      "Epoch: 25, Loss: 0.36347\n",
      "Epoch: 26, Loss: 0.36947\n",
      "Epoch: 27, Loss: 0.35982\n",
      "Epoch: 28, Loss: 0.35105\n",
      "Epoch: 29, Loss: 0.36140\n",
      "Epoch: 30, Loss: 0.34777\n",
      "Epoch: 31, Loss: 0.34356\n",
      "Epoch: 32, Loss: 0.32708\n",
      "Epoch: 33, Loss: 0.33521\n",
      "Epoch: 34, Loss: 0.32863\n",
      "Epoch: 35, Loss: 0.32771\n",
      "Epoch: 36, Loss: 0.32651\n",
      "Epoch: 37, Loss: 0.31124\n",
      "Epoch: 38, Loss: 0.31586\n",
      "Epoch: 39, Loss: 0.30079\n",
      "Epoch: 40, Loss: 0.30773\n",
      "Epoch: 41, Loss: 0.30536\n",
      "Epoch: 42, Loss: 0.28589\n",
      "Epoch: 43, Loss: 0.28450\n",
      "Epoch: 44, Loss: 0.27833\n",
      "Epoch: 45, Loss: 0.27004\n",
      "Epoch: 46, Loss: 0.27938\n",
      "Epoch: 47, Loss: 0.26778\n",
      "Epoch: 48, Loss: 0.26300\n",
      "Epoch: 49, Loss: 0.26580\n",
      "Epoch: 50, Loss: 0.26309\n",
      "Epoch: 51, Loss: 0.25481\n",
      "Epoch: 52, Loss: 0.24850\n",
      "Epoch: 53, Loss: 0.24785\n",
      "Epoch: 54, Loss: 0.24702\n",
      "Epoch: 55, Loss: 0.24889\n",
      "Epoch: 56, Loss: 0.24020\n",
      "Epoch: 57, Loss: 0.23333\n",
      "Epoch: 58, Loss: 0.23529\n",
      "Epoch: 59, Loss: 0.22903\n",
      "Epoch: 60, Loss: 0.23053\n",
      "Epoch: 61, Loss: 0.23186\n",
      "Epoch: 62, Loss: 0.23015\n",
      "Epoch: 63, Loss: 0.21550\n",
      "Epoch: 64, Loss: 0.22244\n",
      "Epoch: 65, Loss: 0.22521\n",
      "Epoch: 66, Loss: 0.21740\n",
      "Epoch: 67, Loss: 0.22419\n",
      "Epoch: 68, Loss: 0.20827\n",
      "Epoch: 69, Loss: 0.21423\n",
      "Epoch: 70, Loss: 0.22197\n",
      "Epoch: 71, Loss: 0.21410\n",
      "Epoch: 72, Loss: 0.21475\n",
      "Epoch: 73, Loss: 0.21584\n",
      "Epoch: 74, Loss: 0.20199\n",
      "Epoch: 75, Loss: 0.18763\n",
      "Epoch: 76, Loss: 0.20546\n",
      "Epoch: 77, Loss: 0.20649\n",
      "Epoch: 78, Loss: 0.19500\n",
      "Epoch: 79, Loss: 0.21450\n",
      "Epoch: 80, Loss: 0.19957\n",
      "Epoch: 81, Loss: 0.18565\n",
      "Epoch: 82, Loss: 0.19002\n",
      "Epoch: 83, Loss: 0.18186\n",
      "Epoch: 84, Loss: 0.18751\n",
      "Epoch: 85, Loss: 0.18447\n",
      "Epoch: 86, Loss: 0.19713\n",
      "Epoch: 87, Loss: 0.18764\n",
      "Epoch: 88, Loss: 0.17839\n",
      "Epoch: 89, Loss: 0.17223\n",
      "Epoch: 90, Loss: 0.18634\n",
      "Epoch: 91, Loss: 0.17689\n",
      "Epoch: 92, Loss: 0.17932\n",
      "Epoch: 93, Loss: 0.18745\n",
      "Epoch: 94, Loss: 0.18826\n",
      "Epoch: 95, Loss: 0.18304\n",
      "Epoch: 96, Loss: 0.18902\n",
      "Epoch: 97, Loss: 0.17815\n",
      "Epoch: 98, Loss: 0.17528\n",
      "Epoch: 99, Loss: 0.16895\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.71439\n",
      "Epoch: 1, Loss: 0.50134\n",
      "Epoch: 2, Loss: 0.46156\n",
      "Epoch: 3, Loss: 0.42171\n",
      "Epoch: 4, Loss: 0.39933\n",
      "Epoch: 5, Loss: 0.39274\n",
      "Epoch: 6, Loss: 0.36736\n",
      "Epoch: 7, Loss: 0.37034\n",
      "Epoch: 8, Loss: 0.34763\n",
      "Epoch: 9, Loss: 0.34698\n",
      "Epoch: 10, Loss: 0.33346\n",
      "Epoch: 11, Loss: 0.32664\n",
      "Epoch: 12, Loss: 0.29846\n",
      "Epoch: 13, Loss: 0.29236\n",
      "Epoch: 14, Loss: 0.28090\n",
      "Epoch: 15, Loss: 0.25492\n",
      "Epoch: 16, Loss: 0.26112\n",
      "Epoch: 17, Loss: 0.24196\n",
      "Epoch: 18, Loss: 0.23292\n",
      "Epoch: 19, Loss: 0.22771\n",
      "Epoch: 20, Loss: 0.19561\n",
      "Epoch: 21, Loss: 0.21212\n",
      "Epoch: 22, Loss: 0.20680\n",
      "Epoch: 23, Loss: 0.18827\n",
      "Epoch: 24, Loss: 0.16805\n",
      "Epoch: 25, Loss: 0.16853\n",
      "Epoch: 26, Loss: 0.16643\n",
      "Epoch: 27, Loss: 0.16278\n",
      "Epoch: 28, Loss: 0.15411\n",
      "Epoch: 29, Loss: 0.15930\n",
      "Epoch: 30, Loss: 0.16741\n",
      "Epoch: 31, Loss: 0.16194\n",
      "Epoch: 32, Loss: 0.13064\n",
      "Epoch: 33, Loss: 0.13016\n",
      "Epoch: 34, Loss: 0.14786\n",
      "Epoch: 35, Loss: 0.14524\n",
      "Epoch: 36, Loss: 0.12316\n",
      "Epoch: 37, Loss: 0.14543\n",
      "Epoch: 38, Loss: 0.12147\n",
      "Epoch: 39, Loss: 0.16950\n",
      "Epoch: 40, Loss: 0.11847\n",
      "Epoch: 41, Loss: 0.11799\n",
      "Epoch: 42, Loss: 0.11690\n",
      "Epoch: 43, Loss: 0.11369\n",
      "Epoch: 44, Loss: 0.13399\n",
      "Epoch: 45, Loss: 0.17725\n",
      "Epoch: 46, Loss: 0.11549\n",
      "Epoch: 47, Loss: 0.11986\n",
      "Epoch: 48, Loss: 0.14459\n",
      "Epoch: 49, Loss: 0.11853\n",
      "Epoch: 50, Loss: 0.10937\n",
      "Epoch: 51, Loss: 0.09687\n",
      "Epoch: 52, Loss: 0.09043\n",
      "Epoch: 53, Loss: 0.08571\n",
      "Epoch: 54, Loss: 0.09585\n",
      "Epoch: 55, Loss: 0.10056\n",
      "Epoch: 56, Loss: 0.11762\n",
      "Epoch: 57, Loss: 0.12707\n",
      "Epoch: 58, Loss: 0.11308\n",
      "Epoch: 59, Loss: 0.10480\n",
      "Epoch: 60, Loss: 0.08599\n",
      "Epoch: 61, Loss: 0.09238\n",
      "Epoch: 62, Loss: 0.11835\n",
      "Epoch: 63, Loss: 0.11595\n",
      "Epoch: 64, Loss: 0.08560\n",
      "Epoch: 65, Loss: 0.10817\n",
      "Epoch: 66, Loss: 0.09819\n",
      "Epoch: 67, Loss: 0.09705\n",
      "Epoch: 68, Loss: 0.10032\n",
      "Epoch: 69, Loss: 0.09681\n",
      "Epoch: 70, Loss: 0.08505\n",
      "Epoch: 71, Loss: 0.08533\n",
      "Epoch: 72, Loss: 0.07607\n",
      "Epoch: 73, Loss: 0.10608\n",
      "Epoch: 74, Loss: 0.10103\n",
      "Epoch: 75, Loss: 0.09741\n",
      "Epoch: 76, Loss: 0.11355\n",
      "Epoch: 77, Loss: 0.05822\n",
      "Epoch: 78, Loss: 0.07850\n",
      "Epoch: 79, Loss: 0.07217\n",
      "Epoch: 80, Loss: 0.12965\n",
      "Epoch: 81, Loss: 0.09480\n",
      "Epoch: 82, Loss: 0.07810\n",
      "Epoch: 83, Loss: 0.11602\n",
      "Epoch: 84, Loss: 0.08532\n",
      "Epoch: 85, Loss: 0.07457\n",
      "Epoch: 86, Loss: 0.08246\n",
      "Epoch: 87, Loss: 0.10572\n",
      "Epoch: 88, Loss: 0.10062\n",
      "Epoch: 89, Loss: 0.10755\n",
      "Epoch: 90, Loss: 0.11014\n",
      "Epoch: 91, Loss: 0.07433\n",
      "Epoch: 92, Loss: 0.05931\n",
      "Epoch: 93, Loss: 0.09400\n",
      "Epoch: 94, Loss: 0.06431\n",
      "Epoch: 95, Loss: 0.07451\n",
      "Epoch: 96, Loss: 0.09156\n",
      "Epoch: 97, Loss: 0.11226\n",
      "Epoch: 98, Loss: 0.08628\n",
      "Epoch: 99, Loss: 0.09141\n",
      "\n",
      "=== Experiment 2: With Densest Subgraphs ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.70098\n",
      "Epoch: 1, Loss: 0.68442\n",
      "Epoch: 2, Loss: 0.66562\n",
      "Epoch: 3, Loss: 0.64448\n",
      "Epoch: 4, Loss: 0.63101\n",
      "Epoch: 5, Loss: 0.61375\n",
      "Epoch: 6, Loss: 0.59395\n",
      "Epoch: 7, Loss: 0.58731\n",
      "Epoch: 8, Loss: 0.56950\n",
      "Epoch: 9, Loss: 0.55366\n",
      "Epoch: 10, Loss: 0.54611\n",
      "Epoch: 11, Loss: 0.54058\n",
      "Epoch: 12, Loss: 0.53113\n",
      "Epoch: 13, Loss: 0.51910\n",
      "Epoch: 14, Loss: 0.50516\n",
      "Epoch: 15, Loss: 0.49548\n",
      "Epoch: 16, Loss: 0.48825\n",
      "Epoch: 17, Loss: 0.48405\n",
      "Epoch: 18, Loss: 0.46224\n",
      "Epoch: 19, Loss: 0.45427\n",
      "Epoch: 20, Loss: 0.44806\n",
      "Epoch: 21, Loss: 0.43104\n",
      "Epoch: 22, Loss: 0.41930\n",
      "Epoch: 23, Loss: 0.42311\n",
      "Epoch: 24, Loss: 0.41357\n",
      "Epoch: 25, Loss: 0.41387\n",
      "Epoch: 26, Loss: 0.40375\n",
      "Epoch: 27, Loss: 0.39132\n",
      "Epoch: 28, Loss: 0.39232\n",
      "Epoch: 29, Loss: 0.37448\n",
      "Epoch: 30, Loss: 0.36941\n",
      "Epoch: 31, Loss: 0.37298\n",
      "Epoch: 32, Loss: 0.38386\n",
      "Epoch: 33, Loss: 0.36346\n",
      "Epoch: 34, Loss: 0.34633\n",
      "Epoch: 35, Loss: 0.34583\n",
      "Epoch: 36, Loss: 0.36542\n",
      "Epoch: 37, Loss: 0.33314\n",
      "Epoch: 38, Loss: 0.33390\n",
      "Epoch: 39, Loss: 0.33430\n",
      "Epoch: 40, Loss: 0.33661\n",
      "Epoch: 41, Loss: 0.32401\n",
      "Epoch: 42, Loss: 0.32010\n",
      "Epoch: 43, Loss: 0.30861\n",
      "Epoch: 44, Loss: 0.30920\n",
      "Epoch: 45, Loss: 0.30979\n",
      "Epoch: 46, Loss: 0.29466\n",
      "Epoch: 47, Loss: 0.30079\n",
      "Epoch: 48, Loss: 0.29541\n",
      "Epoch: 49, Loss: 0.27930\n",
      "Epoch: 50, Loss: 0.29162\n",
      "Epoch: 51, Loss: 0.28829\n",
      "Epoch: 52, Loss: 0.28475\n",
      "Epoch: 53, Loss: 0.29321\n",
      "Epoch: 54, Loss: 0.28885\n",
      "Epoch: 55, Loss: 0.27108\n",
      "Epoch: 56, Loss: 0.26965\n",
      "Epoch: 57, Loss: 0.26909\n",
      "Epoch: 58, Loss: 0.26609\n",
      "Epoch: 59, Loss: 0.27320\n",
      "Epoch: 60, Loss: 0.25993\n",
      "Epoch: 61, Loss: 0.25909\n",
      "Epoch: 62, Loss: 0.25850\n",
      "Epoch: 63, Loss: 0.25882\n",
      "Epoch: 64, Loss: 0.25320\n",
      "Epoch: 65, Loss: 0.26443\n",
      "Epoch: 66, Loss: 0.24812\n",
      "Epoch: 67, Loss: 0.25541\n",
      "Epoch: 68, Loss: 0.24493\n",
      "Epoch: 69, Loss: 0.24819\n",
      "Epoch: 70, Loss: 0.24111\n",
      "Epoch: 71, Loss: 0.23328\n",
      "Epoch: 72, Loss: 0.23087\n",
      "Epoch: 73, Loss: 0.23054\n",
      "Epoch: 74, Loss: 0.24066\n",
      "Epoch: 75, Loss: 0.23992\n",
      "Epoch: 76, Loss: 0.22572\n",
      "Epoch: 77, Loss: 0.23533\n",
      "Epoch: 78, Loss: 0.23344\n",
      "Epoch: 79, Loss: 0.23139\n",
      "Epoch: 80, Loss: 0.22689\n",
      "Epoch: 81, Loss: 0.21941\n",
      "Epoch: 82, Loss: 0.24063\n",
      "Epoch: 83, Loss: 0.22031\n",
      "Epoch: 84, Loss: 0.23702\n",
      "Epoch: 85, Loss: 0.22490\n",
      "Epoch: 86, Loss: 0.23510\n",
      "Epoch: 87, Loss: 0.22282\n",
      "Epoch: 88, Loss: 0.21741\n",
      "Epoch: 89, Loss: 0.21853\n",
      "Epoch: 90, Loss: 0.22042\n",
      "Epoch: 91, Loss: 0.22002\n",
      "Epoch: 92, Loss: 0.21965\n",
      "Epoch: 93, Loss: 0.23101\n",
      "Epoch: 94, Loss: 0.20475\n",
      "Epoch: 95, Loss: 0.21237\n",
      "Epoch: 96, Loss: 0.20967\n",
      "Epoch: 97, Loss: 0.20830\n",
      "Epoch: 98, Loss: 0.22000\n",
      "Epoch: 99, Loss: 0.21401\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.67926\n",
      "Epoch: 1, Loss: 0.55527\n",
      "Epoch: 2, Loss: 0.48991\n",
      "Epoch: 3, Loss: 0.45247\n",
      "Epoch: 4, Loss: 0.41720\n",
      "Epoch: 5, Loss: 0.39734\n",
      "Epoch: 6, Loss: 0.37120\n",
      "Epoch: 7, Loss: 0.35698\n",
      "Epoch: 8, Loss: 0.33355\n",
      "Epoch: 9, Loss: 0.32114\n",
      "Epoch: 10, Loss: 0.31621\n",
      "Epoch: 11, Loss: 0.30518\n",
      "Epoch: 12, Loss: 0.27959\n",
      "Epoch: 13, Loss: 0.27445\n",
      "Epoch: 14, Loss: 0.27825\n",
      "Epoch: 15, Loss: 0.25090\n",
      "Epoch: 16, Loss: 0.23109\n",
      "Epoch: 17, Loss: 0.21623\n",
      "Epoch: 18, Loss: 0.20700\n",
      "Epoch: 19, Loss: 0.21811\n",
      "Epoch: 20, Loss: 0.18114\n",
      "Epoch: 21, Loss: 0.17970\n",
      "Epoch: 22, Loss: 0.17206\n",
      "Epoch: 23, Loss: 0.15900\n",
      "Epoch: 24, Loss: 0.17905\n",
      "Epoch: 25, Loss: 0.15046\n",
      "Epoch: 26, Loss: 0.15609\n",
      "Epoch: 27, Loss: 0.14623\n",
      "Epoch: 28, Loss: 0.15026\n",
      "Epoch: 29, Loss: 0.13024\n",
      "Epoch: 30, Loss: 0.12600\n",
      "Epoch: 31, Loss: 0.16254\n",
      "Epoch: 32, Loss: 0.11654\n",
      "Epoch: 33, Loss: 0.12166\n",
      "Epoch: 34, Loss: 0.12351\n",
      "Epoch: 35, Loss: 0.11249\n",
      "Epoch: 36, Loss: 0.12519\n",
      "Epoch: 37, Loss: 0.11029\n",
      "Epoch: 38, Loss: 0.11783\n",
      "Epoch: 39, Loss: 0.12998\n",
      "Epoch: 40, Loss: 0.10030\n",
      "Epoch: 41, Loss: 0.11867\n",
      "Epoch: 42, Loss: 0.12751\n",
      "Epoch: 43, Loss: 0.09317\n",
      "Epoch: 44, Loss: 0.10447\n",
      "Epoch: 45, Loss: 0.12894\n",
      "Epoch: 46, Loss: 0.12175\n",
      "Epoch: 47, Loss: 0.12763\n",
      "Epoch: 48, Loss: 0.08175\n",
      "Epoch: 49, Loss: 0.15820\n",
      "Epoch: 50, Loss: 0.11484\n",
      "Epoch: 51, Loss: 0.07874\n",
      "Epoch: 52, Loss: 0.10033\n",
      "Epoch: 53, Loss: 0.11208\n",
      "Epoch: 54, Loss: 0.09062\n",
      "Epoch: 55, Loss: 0.09552\n",
      "Epoch: 56, Loss: 0.10380\n",
      "Epoch: 57, Loss: 0.06948\n",
      "Epoch: 58, Loss: 0.14637\n",
      "Epoch: 59, Loss: 0.12512\n",
      "Epoch: 60, Loss: 0.14089\n",
      "Epoch: 61, Loss: 0.08854\n",
      "Epoch: 62, Loss: 0.09923\n",
      "Epoch: 63, Loss: 0.19918\n",
      "Epoch: 64, Loss: 0.12783\n",
      "Epoch: 65, Loss: 0.08840\n",
      "Epoch: 66, Loss: 0.09919\n",
      "Epoch: 67, Loss: 0.12697\n",
      "Epoch: 68, Loss: 0.08200\n",
      "Epoch: 69, Loss: 0.11156\n",
      "Epoch: 70, Loss: 0.12195\n",
      "Epoch: 71, Loss: 0.08604\n",
      "Epoch: 72, Loss: 0.08971\n",
      "Epoch: 73, Loss: 0.10345\n",
      "Epoch: 74, Loss: 0.10967\n",
      "Epoch: 75, Loss: 0.12446\n",
      "Epoch: 76, Loss: 0.09859\n",
      "Epoch: 77, Loss: 0.10332\n",
      "Epoch: 78, Loss: 0.11215\n",
      "Epoch: 79, Loss: 0.09618\n",
      "Epoch: 80, Loss: 0.07323\n",
      "Epoch: 81, Loss: 0.07337\n",
      "Epoch: 82, Loss: 0.10492\n",
      "Epoch: 83, Loss: 0.10415\n",
      "Epoch: 84, Loss: 0.10159\n",
      "Epoch: 85, Loss: 0.07303\n",
      "Epoch: 86, Loss: 0.11258\n",
      "Epoch: 87, Loss: 0.07080\n",
      "Epoch: 88, Loss: 0.09407\n",
      "Epoch: 89, Loss: 0.07904\n",
      "Epoch: 90, Loss: 0.06914\n",
      "Epoch: 91, Loss: 0.07663\n",
      "Epoch: 92, Loss: 0.10186\n",
      "Epoch: 93, Loss: 0.07867\n",
      "Epoch: 94, Loss: 0.09193\n",
      "Epoch: 95, Loss: 0.06598\n",
      "Epoch: 96, Loss: 0.09085\n",
      "Epoch: 97, Loss: 0.07941\n",
      "Epoch: 98, Loss: 0.07819\n",
      "Epoch: 99, Loss: 0.10675\n",
      "\n",
      "=== Experiment 3: Genre-Only Hypergraph ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Empty edge list! Creating a fallback edge list based on nearest neighbors.\n",
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.70138\n",
      "Epoch: 1, Loss: 0.66480\n",
      "Epoch: 2, Loss: 0.63156\n",
      "Epoch: 3, Loss: 0.60616\n",
      "Epoch: 4, Loss: 0.57182\n",
      "Epoch: 5, Loss: 0.55546\n",
      "Epoch: 6, Loss: 0.54255\n",
      "Epoch: 7, Loss: 0.52450\n",
      "Epoch: 8, Loss: 0.51574\n",
      "Epoch: 9, Loss: 0.51963\n",
      "Epoch: 10, Loss: 0.50523\n",
      "Epoch: 11, Loss: 0.51114\n",
      "Epoch: 12, Loss: 0.49880\n",
      "Epoch: 13, Loss: 0.49176\n",
      "Epoch: 14, Loss: 0.48970\n",
      "Epoch: 15, Loss: 0.48529\n",
      "Epoch: 16, Loss: 0.48184\n",
      "Epoch: 17, Loss: 0.48338\n",
      "Epoch: 18, Loss: 0.47569\n",
      "Epoch: 19, Loss: 0.48052\n",
      "Epoch: 20, Loss: 0.47274\n",
      "Epoch: 21, Loss: 0.47735\n",
      "Epoch: 22, Loss: 0.46514\n",
      "Epoch: 23, Loss: 0.46843\n",
      "Epoch: 24, Loss: 0.46633\n",
      "Epoch: 25, Loss: 0.46006\n",
      "Epoch: 26, Loss: 0.45575\n",
      "Epoch: 27, Loss: 0.45750\n",
      "Epoch: 28, Loss: 0.45252\n",
      "Epoch: 29, Loss: 0.45226\n",
      "Epoch: 30, Loss: 0.44903\n",
      "Epoch: 31, Loss: 0.44452\n",
      "Epoch: 32, Loss: 0.43981\n",
      "Epoch: 33, Loss: 0.44432\n",
      "Epoch: 34, Loss: 0.44382\n",
      "Epoch: 35, Loss: 0.43623\n",
      "Epoch: 36, Loss: 0.44027\n",
      "Epoch: 37, Loss: 0.42530\n",
      "Epoch: 38, Loss: 0.43454\n",
      "Epoch: 39, Loss: 0.43194\n",
      "Epoch: 40, Loss: 0.42478\n",
      "Epoch: 41, Loss: 0.42526\n",
      "Epoch: 42, Loss: 0.42452\n",
      "Epoch: 43, Loss: 0.42281\n",
      "Epoch: 44, Loss: 0.42435\n",
      "Epoch: 45, Loss: 0.41643\n",
      "Epoch: 46, Loss: 0.41574\n",
      "Epoch: 47, Loss: 0.40872\n",
      "Epoch: 48, Loss: 0.40995\n",
      "Epoch: 49, Loss: 0.40203\n",
      "Epoch: 50, Loss: 0.40402\n",
      "Epoch: 51, Loss: 0.40495\n",
      "Epoch: 52, Loss: 0.40311\n",
      "Epoch: 53, Loss: 0.39690\n",
      "Epoch: 54, Loss: 0.39836\n",
      "Epoch: 55, Loss: 0.38830\n",
      "Epoch: 56, Loss: 0.38942\n",
      "Epoch: 57, Loss: 0.38868\n",
      "Epoch: 58, Loss: 0.38952\n",
      "Epoch: 59, Loss: 0.39228\n",
      "Epoch: 60, Loss: 0.38147\n",
      "Epoch: 61, Loss: 0.38281\n",
      "Epoch: 62, Loss: 0.38082\n",
      "Epoch: 63, Loss: 0.37484\n",
      "Epoch: 64, Loss: 0.38077\n",
      "Epoch: 65, Loss: 0.37330\n",
      "Epoch: 66, Loss: 0.37149\n",
      "Epoch: 67, Loss: 0.37673\n",
      "Epoch: 68, Loss: 0.37420\n",
      "Epoch: 69, Loss: 0.36166\n",
      "Epoch: 70, Loss: 0.36348\n",
      "Epoch: 71, Loss: 0.36434\n",
      "Epoch: 72, Loss: 0.36705\n",
      "Epoch: 73, Loss: 0.35824\n",
      "Epoch: 74, Loss: 0.36979\n",
      "Epoch: 75, Loss: 0.36311\n",
      "Epoch: 76, Loss: 0.36563\n",
      "Epoch: 77, Loss: 0.35412\n",
      "Epoch: 78, Loss: 0.36486\n",
      "Epoch: 79, Loss: 0.36141\n",
      "Epoch: 80, Loss: 0.36109\n",
      "Epoch: 81, Loss: 0.35232\n",
      "Epoch: 82, Loss: 0.35927\n",
      "Epoch: 83, Loss: 0.35272\n",
      "Epoch: 84, Loss: 0.36990\n",
      "Epoch: 85, Loss: 0.34601\n",
      "Epoch: 86, Loss: 0.34689\n",
      "Epoch: 87, Loss: 0.34937\n",
      "Epoch: 88, Loss: 0.35893\n",
      "Epoch: 89, Loss: 0.34106\n",
      "Epoch: 90, Loss: 0.34650\n",
      "Epoch: 91, Loss: 0.33958\n",
      "Epoch: 92, Loss: 0.34742\n",
      "Epoch: 93, Loss: 0.34122\n",
      "Epoch: 94, Loss: 0.33961\n",
      "Epoch: 95, Loss: 0.34151\n",
      "Epoch: 96, Loss: 0.34131\n",
      "Epoch: 97, Loss: 0.33838\n",
      "Epoch: 98, Loss: 0.33819\n",
      "Epoch: 99, Loss: 0.33567\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 1.02300\n",
      "Epoch: 1, Loss: 0.51834\n",
      "Epoch: 2, Loss: 0.50384\n",
      "Epoch: 3, Loss: 0.49336\n",
      "Epoch: 4, Loss: 0.48810\n",
      "Epoch: 5, Loss: 0.44812\n",
      "Epoch: 6, Loss: 0.44510\n",
      "Epoch: 7, Loss: 0.43595\n",
      "Epoch: 8, Loss: 0.42415\n",
      "Epoch: 9, Loss: 0.43290\n",
      "Epoch: 10, Loss: 0.41529\n",
      "Epoch: 11, Loss: 0.40927\n",
      "Epoch: 12, Loss: 0.40684\n",
      "Epoch: 13, Loss: 0.39126\n",
      "Epoch: 14, Loss: 0.38150\n",
      "Epoch: 15, Loss: 0.37802\n",
      "Epoch: 16, Loss: 0.36841\n",
      "Epoch: 17, Loss: 0.36774\n",
      "Epoch: 18, Loss: 0.36139\n",
      "Epoch: 19, Loss: 0.34672\n",
      "Epoch: 20, Loss: 0.33821\n",
      "Epoch: 21, Loss: 0.33468\n",
      "Epoch: 22, Loss: 0.33374\n",
      "Epoch: 23, Loss: 0.32653\n",
      "Epoch: 24, Loss: 0.32664\n",
      "Epoch: 25, Loss: 0.34678\n",
      "Epoch: 26, Loss: 0.31866\n",
      "Epoch: 27, Loss: 0.32151\n",
      "Epoch: 28, Loss: 0.34144\n",
      "Epoch: 29, Loss: 0.30334\n",
      "Epoch: 30, Loss: 0.30319\n",
      "Epoch: 31, Loss: 0.30316\n",
      "Epoch: 32, Loss: 0.30520\n",
      "Epoch: 33, Loss: 0.30260\n",
      "Epoch: 34, Loss: 0.29772\n",
      "Epoch: 35, Loss: 0.31746\n",
      "Epoch: 36, Loss: 0.29626\n",
      "Epoch: 37, Loss: 0.29648\n",
      "Epoch: 38, Loss: 0.29647\n",
      "Epoch: 39, Loss: 0.27539\n",
      "Epoch: 40, Loss: 0.29154\n",
      "Epoch: 41, Loss: 0.27678\n",
      "Epoch: 42, Loss: 0.26601\n",
      "Epoch: 43, Loss: 0.27550\n",
      "Epoch: 44, Loss: 0.28142\n",
      "Epoch: 45, Loss: 0.26628\n",
      "Epoch: 46, Loss: 0.26608\n",
      "Epoch: 47, Loss: 0.29446\n",
      "Epoch: 48, Loss: 0.25962\n",
      "Epoch: 49, Loss: 0.25997\n",
      "Epoch: 50, Loss: 0.26889\n",
      "Epoch: 51, Loss: 0.25900\n",
      "Epoch: 52, Loss: 0.26651\n",
      "Epoch: 53, Loss: 0.26480\n",
      "Epoch: 54, Loss: 0.24569\n",
      "Epoch: 55, Loss: 0.26534\n",
      "Epoch: 56, Loss: 0.28447\n",
      "Epoch: 57, Loss: 0.25421\n",
      "Epoch: 58, Loss: 0.25272\n",
      "Epoch: 59, Loss: 0.27569\n",
      "Epoch: 60, Loss: 0.27010\n",
      "Epoch: 61, Loss: 0.23951\n",
      "Epoch: 62, Loss: 0.25542\n",
      "Epoch: 63, Loss: 0.25599\n",
      "Epoch: 64, Loss: 0.27725\n",
      "Epoch: 65, Loss: 0.25983\n",
      "Epoch: 66, Loss: 0.26178\n",
      "Epoch: 67, Loss: 0.24587\n",
      "Epoch: 68, Loss: 0.24066\n",
      "Epoch: 69, Loss: 0.24411\n",
      "Epoch: 70, Loss: 0.26634\n",
      "Epoch: 71, Loss: 0.24873\n",
      "Epoch: 72, Loss: 0.24058\n",
      "Epoch: 73, Loss: 0.25517\n",
      "Epoch: 74, Loss: 0.23976\n",
      "Epoch: 75, Loss: 0.23087\n",
      "Epoch: 76, Loss: 0.25456\n",
      "Epoch: 77, Loss: 0.23108\n",
      "Epoch: 78, Loss: 0.25663\n",
      "Epoch: 79, Loss: 0.25272\n",
      "Epoch: 80, Loss: 0.23342\n",
      "Epoch: 81, Loss: 0.22806\n",
      "Epoch: 82, Loss: 0.23387\n",
      "Epoch: 83, Loss: 0.22601\n",
      "Epoch: 84, Loss: 0.22345\n",
      "Epoch: 85, Loss: 0.24371\n",
      "Epoch: 86, Loss: 0.24363\n",
      "Epoch: 87, Loss: 0.24260\n",
      "Epoch: 88, Loss: 0.22087\n",
      "Epoch: 89, Loss: 0.25153\n",
      "Epoch: 90, Loss: 0.23184\n",
      "Epoch: 91, Loss: 0.23053\n",
      "Epoch: 92, Loss: 0.26327\n",
      "Epoch: 93, Loss: 0.26475\n",
      "Epoch: 94, Loss: 0.24434\n",
      "Epoch: 95, Loss: 0.22884\n",
      "Epoch: 96, Loss: 0.24375\n",
      "Epoch: 97, Loss: 0.22414\n",
      "Epoch: 98, Loss: 0.22942\n",
      "Epoch: 99, Loss: 0.22183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results Summary ===\n",
      "            Experiment  HGNN Val  HGNN Test  HGNNP Val  HGNNP Test\n",
      "                  Base  0.700000   0.666667   0.600000    0.666667\n",
      "With Densest Subgraphs  0.700000   0.766667   0.700000    0.700000\n",
      "            Genre-Only  0.766667   0.700000   0.766667    0.666667\n"
     ]
    }
   ],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "experiment = HypergraphExperiment(\n",
    "    data_path=movie_path\n",
    ")\n",
    "experiment.run_experiments(n_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
