{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Hypergraph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import dhg\n",
    "from dhg import Graph, Hypergraph\n",
    "from dhg.data import Cooking200, CocitationCiteseer, HouseCommittees\n",
    "from dhg.models import GCN, HGNN, HGNNP, UniGCN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.utils import split_by_ratio\n",
    "\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\.dhg\n",
      "d:\\Rustem\\2_Education\\9_UWindsor_CSS\\COMP_8720-Topics_in_AI\\project\\hyper_modeling\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(dhg.CACHE_ROOT)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=dhg.data.Cooking200()\n",
    "# for key in d.content:\n",
    "#     d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooking200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is house_committees dataset:\n",
       "  ->  num_classes\n",
       "  ->  num_vertices\n",
       "  ->  num_edges\n",
       "  ->  edge_list\n",
       "  ->  labels\n",
       "  ->  train_mask\n",
       "  ->  test_mask\n",
       "  ->  val_mask\n",
       "Please try `data['name']` to get the specified data."
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = Cooking200(data_path)\n",
    "data = dhg.data.CocitationCiteseer(data_path)\n",
    "data = dhg.data.HouseCommittees(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes : 3\n",
      "num_vertices : 1290\n",
      "num_edges : 341\n"
     ]
    }
   ],
   "source": [
    "for key in data.content[:3]:\n",
    "    print(key,\":\", data[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 27.05090s, Loss: 3.00726\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.11877s, Loss: 2.87365\n",
      "Epoch: 2, Time: 0.97288s, Loss: 2.63956\n",
      "update best: 0.09500\n",
      "Epoch: 3, Time: 0.97195s, Loss: 2.46788\n",
      "Epoch: 4, Time: 0.95451s, Loss: 2.36260\n",
      "Epoch: 5, Time: 0.97938s, Loss: 2.28780\n",
      "update best: 0.13000\n",
      "Epoch: 6, Time: 1.09381s, Loss: 2.21204\n",
      "update best: 0.13500\n",
      "Epoch: 7, Time: 0.93865s, Loss: 2.13388\n",
      "Epoch: 8, Time: 1.13131s, Loss: 2.06262\n",
      "update best: 0.14000\n",
      "Epoch: 9, Time: 1.40688s, Loss: 1.99684\n",
      "Epoch: 10, Time: 1.38883s, Loss: 1.91773\n",
      "Epoch: 11, Time: 1.27973s, Loss: 1.84279\n",
      "Epoch: 12, Time: 1.31780s, Loss: 1.76692\n",
      "Epoch: 13, Time: 1.21678s, Loss: 1.69607\n",
      "Epoch: 14, Time: 1.50236s, Loss: 1.62226\n",
      "Epoch: 15, Time: 1.37457s, Loss: 1.55436\n",
      "Epoch: 16, Time: 1.23136s, Loss: 1.48475\n",
      "Epoch: 17, Time: 1.41042s, Loss: 1.41734\n",
      "Epoch: 18, Time: 1.25632s, Loss: 1.34714\n",
      "Epoch: 19, Time: 1.30601s, Loss: 1.27656\n",
      "update best: 0.16500\n",
      "Epoch: 20, Time: 1.20952s, Loss: 1.20234\n",
      "Epoch: 21, Time: 1.19049s, Loss: 1.14656\n",
      "Epoch: 22, Time: 1.15300s, Loss: 1.08389\n",
      "Epoch: 23, Time: 1.26429s, Loss: 1.02032\n",
      "Epoch: 24, Time: 1.30182s, Loss: 0.95723\n",
      "Epoch: 25, Time: 1.42021s, Loss: 0.90450\n",
      "Epoch: 26, Time: 1.22268s, Loss: 0.84604\n",
      "Epoch: 27, Time: 1.20182s, Loss: 0.79651\n",
      "Epoch: 28, Time: 1.27569s, Loss: 0.74074\n",
      "Epoch: 29, Time: 1.19896s, Loss: 0.69620\n",
      "Epoch: 30, Time: 1.54059s, Loss: 0.66053\n",
      "Epoch: 31, Time: 1.35103s, Loss: 0.61037\n",
      "Epoch: 32, Time: 1.32461s, Loss: 0.56589\n",
      "update best: 0.17000\n",
      "Epoch: 33, Time: 1.33072s, Loss: 0.53298\n",
      "update best: 0.19000\n",
      "Epoch: 34, Time: 1.29055s, Loss: 0.49662\n",
      "update best: 0.21000\n",
      "Epoch: 35, Time: 1.22395s, Loss: 0.46070\n",
      "update best: 0.22500\n",
      "Epoch: 36, Time: 1.18261s, Loss: 0.43163\n",
      "Epoch: 37, Time: 1.18048s, Loss: 0.40783\n",
      "Epoch: 38, Time: 1.32629s, Loss: 0.37449\n",
      "Epoch: 39, Time: 1.18640s, Loss: 0.34452\n",
      "Epoch: 40, Time: 1.17261s, Loss: 0.32959\n",
      "Epoch: 41, Time: 1.35494s, Loss: 0.30877\n",
      "Epoch: 42, Time: 1.29722s, Loss: 0.29094\n",
      "update best: 0.23000\n",
      "Epoch: 43, Time: 1.23423s, Loss: 0.27251\n",
      "update best: 0.23500\n",
      "Epoch: 44, Time: 1.29604s, Loss: 0.25644\n",
      "Epoch: 45, Time: 1.26459s, Loss: 0.24776\n",
      "update best: 0.24500\n",
      "Epoch: 46, Time: 1.37385s, Loss: 0.23207\n",
      "Epoch: 47, Time: 1.25992s, Loss: 0.21182\n",
      "Epoch: 48, Time: 1.31817s, Loss: 0.20342\n",
      "Epoch: 49, Time: 1.46455s, Loss: 0.19555\n",
      "update best: 0.27500\n",
      "Epoch: 50, Time: 1.30664s, Loss: 0.18163\n",
      "update best: 0.28000\n",
      "Epoch: 51, Time: 1.44705s, Loss: 0.17652\n",
      "Epoch: 52, Time: 1.16958s, Loss: 0.16214\n",
      "Epoch: 53, Time: 1.26941s, Loss: 0.15734\n",
      "Epoch: 54, Time: 1.34628s, Loss: 0.14466\n",
      "Epoch: 55, Time: 1.22824s, Loss: 0.14203\n",
      "update best: 0.28500\n",
      "Epoch: 56, Time: 1.34575s, Loss: 0.13349\n",
      "update best: 0.33000\n",
      "Epoch: 57, Time: 1.28025s, Loss: 0.12913\n",
      "update best: 0.34500\n",
      "Epoch: 58, Time: 1.30383s, Loss: 0.12390\n",
      "Epoch: 59, Time: 1.24166s, Loss: 0.11869\n",
      "Epoch: 60, Time: 1.29664s, Loss: 0.11507\n",
      "Epoch: 61, Time: 1.29878s, Loss: 0.11152\n",
      "Epoch: 62, Time: 1.50177s, Loss: 0.10387\n",
      "Epoch: 63, Time: 1.30076s, Loss: 0.10022\n",
      "Epoch: 64, Time: 1.24141s, Loss: 0.09575\n",
      "update best: 0.36500\n",
      "Epoch: 65, Time: 1.22373s, Loss: 0.09613\n",
      "Epoch: 66, Time: 1.36058s, Loss: 0.09143\n",
      "Epoch: 67, Time: 1.39538s, Loss: 0.09173\n",
      "update best: 0.38500\n",
      "Epoch: 68, Time: 1.21041s, Loss: 0.08172\n",
      "Epoch: 69, Time: 1.31102s, Loss: 0.08590\n",
      "Epoch: 70, Time: 1.16226s, Loss: 0.08126\n",
      "update best: 0.39500\n",
      "Epoch: 71, Time: 1.38064s, Loss: 0.07688\n",
      "Epoch: 72, Time: 1.21638s, Loss: 0.07967\n",
      "Epoch: 73, Time: 1.16902s, Loss: 0.07509\n",
      "update best: 0.42500\n",
      "Epoch: 74, Time: 1.25318s, Loss: 0.07688\n",
      "Epoch: 75, Time: 1.19744s, Loss: 0.07053\n",
      "Epoch: 76, Time: 1.23429s, Loss: 0.06861\n",
      "Epoch: 77, Time: 1.18709s, Loss: 0.06849\n",
      "update best: 0.43500\n",
      "Epoch: 78, Time: 1.27665s, Loss: 0.06825\n",
      "Epoch: 79, Time: 1.34091s, Loss: 0.06182\n",
      "Epoch: 80, Time: 1.33787s, Loss: 0.06590\n",
      "Epoch: 81, Time: 1.26965s, Loss: 0.06319\n",
      "Epoch: 82, Time: 1.19577s, Loss: 0.06313\n",
      "Epoch: 83, Time: 1.32820s, Loss: 0.06400\n",
      "Epoch: 84, Time: 1.30356s, Loss: 0.06643\n",
      "Epoch: 85, Time: 1.32382s, Loss: 0.05816\n",
      "Epoch: 86, Time: 1.27220s, Loss: 0.05907\n",
      "Epoch: 87, Time: 1.24621s, Loss: 0.05757\n",
      "Epoch: 88, Time: 1.26966s, Loss: 0.05826\n",
      "Epoch: 89, Time: 1.16190s, Loss: 0.05518\n",
      "update best: 0.45000\n",
      "Epoch: 90, Time: 1.21202s, Loss: 0.05523\n",
      "Epoch: 91, Time: 1.17484s, Loss: 0.05426\n",
      "Epoch: 92, Time: 1.21003s, Loss: 0.05314\n",
      "Epoch: 93, Time: 1.17332s, Loss: 0.05175\n",
      "Epoch: 94, Time: 1.15069s, Loss: 0.05127\n",
      "update best: 0.45500\n",
      "Epoch: 95, Time: 1.19286s, Loss: 0.05020\n",
      "Epoch: 96, Time: 1.24532s, Loss: 0.05462\n",
      "Epoch: 97, Time: 1.27569s, Loss: 0.04893\n",
      "Epoch: 98, Time: 1.21614s, Loss: 0.05175\n",
      "Epoch: 99, Time: 1.39048s, Loss: 0.04687\n",
      "Epoch: 100, Time: 1.24326s, Loss: 0.04922\n",
      "Epoch: 101, Time: 1.24431s, Loss: 0.04435\n",
      "Epoch: 102, Time: 1.19226s, Loss: 0.04717\n",
      "Epoch: 103, Time: 1.35605s, Loss: 0.04634\n",
      "Epoch: 104, Time: 1.18235s, Loss: 0.04488\n",
      "Epoch: 105, Time: 1.17856s, Loss: 0.04129\n",
      "Epoch: 106, Time: 1.23227s, Loss: 0.04684\n",
      "Epoch: 107, Time: 1.26220s, Loss: 0.04706\n",
      "Epoch: 108, Time: 1.23621s, Loss: 0.04364\n",
      "Epoch: 109, Time: 1.18581s, Loss: 0.04682\n",
      "Epoch: 110, Time: 1.22108s, Loss: 0.04590\n",
      "Epoch: 111, Time: 1.33051s, Loss: 0.04269\n",
      "Epoch: 112, Time: 1.14877s, Loss: 0.04778\n",
      "Epoch: 113, Time: 1.16577s, Loss: 0.04367\n",
      "update best: 0.46000\n",
      "Epoch: 114, Time: 1.27170s, Loss: 0.04239\n",
      "Epoch: 115, Time: 1.19230s, Loss: 0.04223\n",
      "Epoch: 116, Time: 1.30985s, Loss: 0.04184\n",
      "Epoch: 117, Time: 1.34204s, Loss: 0.04116\n",
      "Epoch: 118, Time: 1.19596s, Loss: 0.04068\n",
      "Epoch: 119, Time: 1.23354s, Loss: 0.03786\n",
      "Epoch: 120, Time: 1.42493s, Loss: 0.04072\n",
      "update best: 0.47500\n",
      "Epoch: 121, Time: 1.24169s, Loss: 0.03664\n",
      "Epoch: 122, Time: 1.24111s, Loss: 0.03947\n",
      "Epoch: 123, Time: 1.23722s, Loss: 0.03868\n",
      "Epoch: 124, Time: 1.31976s, Loss: 0.03620\n",
      "Epoch: 125, Time: 1.19306s, Loss: 0.03710\n",
      "Epoch: 126, Time: 1.18464s, Loss: 0.03816\n",
      "Epoch: 127, Time: 1.23385s, Loss: 0.03773\n",
      "Epoch: 128, Time: 1.15674s, Loss: 0.03621\n",
      "Epoch: 129, Time: 1.22596s, Loss: 0.03730\n",
      "Epoch: 130, Time: 1.14205s, Loss: 0.03664\n",
      "Epoch: 131, Time: 1.15340s, Loss: 0.03373\n",
      "Epoch: 132, Time: 1.32747s, Loss: 0.03674\n",
      "Epoch: 133, Time: 1.22853s, Loss: 0.04360\n",
      "Epoch: 134, Time: 1.34267s, Loss: 0.03768\n",
      "Epoch: 135, Time: 1.41897s, Loss: 0.03557\n",
      "Epoch: 136, Time: 1.31278s, Loss: 0.03471\n",
      "Epoch: 137, Time: 1.20829s, Loss: 0.03593\n",
      "Epoch: 138, Time: 1.20432s, Loss: 0.03424\n",
      "Epoch: 139, Time: 1.29845s, Loss: 0.03539\n",
      "Epoch: 140, Time: 1.38204s, Loss: 0.03442\n",
      "Epoch: 141, Time: 1.27594s, Loss: 0.03629\n",
      "Epoch: 142, Time: 1.30157s, Loss: 0.03312\n",
      "Epoch: 143, Time: 1.18935s, Loss: 0.03841\n",
      "Epoch: 144, Time: 1.25200s, Loss: 0.04076\n",
      "Epoch: 145, Time: 1.23712s, Loss: 0.06876\n",
      "Epoch: 146, Time: 1.13729s, Loss: 0.03650\n",
      "Epoch: 147, Time: 1.23742s, Loss: 0.06332\n",
      "Epoch: 148, Time: 1.35226s, Loss: 0.04392\n",
      "Epoch: 149, Time: 1.48591s, Loss: 0.06565\n",
      "Epoch: 150, Time: 1.18588s, Loss: 0.04007\n",
      "Epoch: 151, Time: 1.20833s, Loss: 0.07563\n",
      "Epoch: 152, Time: 1.35463s, Loss: 0.03581\n",
      "Epoch: 153, Time: 1.26155s, Loss: 0.09672\n",
      "Epoch: 154, Time: 1.18569s, Loss: 0.05547\n",
      "Epoch: 155, Time: 1.18879s, Loss: 0.04690\n",
      "Epoch: 156, Time: 1.28119s, Loss: 0.05561\n",
      "Epoch: 157, Time: 1.25364s, Loss: 0.05781\n",
      "Epoch: 158, Time: 1.22963s, Loss: 0.04668\n",
      "Epoch: 159, Time: 1.18674s, Loss: 0.04285\n",
      "Epoch: 160, Time: 1.33488s, Loss: 0.04534\n",
      "Epoch: 161, Time: 1.34731s, Loss: 0.04604\n",
      "Epoch: 162, Time: 1.34935s, Loss: 0.03965\n",
      "Epoch: 163, Time: 1.26145s, Loss: 0.03661\n",
      "Epoch: 164, Time: 1.27685s, Loss: 0.03958\n",
      "Epoch: 165, Time: 1.24223s, Loss: 0.03301\n",
      "Epoch: 166, Time: 1.29492s, Loss: 0.03590\n",
      "Epoch: 167, Time: 1.16915s, Loss: 0.02961\n",
      "Epoch: 168, Time: 1.18475s, Loss: 0.03390\n",
      "Epoch: 169, Time: 1.22329s, Loss: 0.03066\n",
      "Epoch: 170, Time: 1.19592s, Loss: 0.03081\n",
      "Epoch: 171, Time: 1.24858s, Loss: 0.02582\n",
      "Epoch: 172, Time: 1.29168s, Loss: 0.02749\n",
      "Epoch: 173, Time: 1.19019s, Loss: 0.02687\n",
      "Epoch: 174, Time: 1.21539s, Loss: 0.02652\n",
      "Epoch: 175, Time: 1.30223s, Loss: 0.02704\n",
      "Epoch: 176, Time: 1.52252s, Loss: 0.02565\n",
      "Epoch: 177, Time: 1.42731s, Loss: 0.03043\n",
      "Epoch: 178, Time: 1.26753s, Loss: 0.02406\n",
      "Epoch: 179, Time: 2.60946s, Loss: 0.03481\n",
      "Epoch: 180, Time: 2.52241s, Loss: 0.02621\n",
      "Epoch: 181, Time: 2.60322s, Loss: 0.04958\n",
      "Epoch: 182, Time: 2.56016s, Loss: 0.03141\n",
      "Epoch: 183, Time: 2.58308s, Loss: 0.04558\n",
      "Epoch: 184, Time: 2.49239s, Loss: 0.05380\n",
      "Epoch: 185, Time: 2.60713s, Loss: 0.04330\n",
      "Epoch: 186, Time: 2.59470s, Loss: 0.03930\n",
      "Epoch: 187, Time: 2.72259s, Loss: 0.04647\n",
      "Epoch: 188, Time: 2.51598s, Loss: 0.04012\n",
      "Epoch: 189, Time: 2.54825s, Loss: 0.03249\n",
      "Epoch: 190, Time: 2.59999s, Loss: 0.03584\n",
      "Epoch: 191, Time: 2.76925s, Loss: 0.03525\n",
      "Epoch: 192, Time: 2.05392s, Loss: 0.03017\n",
      "Epoch: 193, Time: 1.00734s, Loss: 0.03143\n",
      "Epoch: 194, Time: 1.02639s, Loss: 0.03074\n",
      "Epoch: 195, Time: 1.19079s, Loss: 0.02683\n",
      "Epoch: 196, Time: 0.99288s, Loss: 0.02414\n",
      "Epoch: 197, Time: 1.00549s, Loss: 0.02461\n",
      "Epoch: 198, Time: 1.13103s, Loss: 0.02479\n",
      "Epoch: 199, Time: 1.02974s, Loss: 0.02296\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 120\n",
      "{'accuracy': 0.45623305439949036, 'f1_score': 0.3653164109596893, 'f1_score -> average@micro': 0.4562330429815793}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 1.27188s, Loss: 3.00386\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.54415s, Loss: 2.70985\n",
      "Epoch: 2, Time: 1.50128s, Loss: 2.37330\n",
      "Epoch: 3, Time: 1.81600s, Loss: 2.18877\n",
      "Epoch: 4, Time: 1.50025s, Loss: 2.05079\n",
      "Epoch: 5, Time: 1.49115s, Loss: 1.92147\n",
      "Epoch: 6, Time: 1.39025s, Loss: 1.80867\n",
      "update best: 0.09000\n",
      "Epoch: 7, Time: 1.39048s, Loss: 1.68405\n",
      "update best: 0.09500\n",
      "Epoch: 8, Time: 1.40198s, Loss: 1.55780\n",
      "Epoch: 9, Time: 1.32872s, Loss: 1.45725\n",
      "Epoch: 10, Time: 1.33453s, Loss: 1.36144\n",
      "Epoch: 11, Time: 1.98168s, Loss: 1.23599\n",
      "Epoch: 12, Time: 1.50997s, Loss: 1.14826\n",
      "Epoch: 13, Time: 1.64627s, Loss: 1.03714\n",
      "Epoch: 14, Time: 1.72242s, Loss: 0.94961\n",
      "update best: 0.10000\n",
      "Epoch: 15, Time: 1.53512s, Loss: 0.87812\n",
      "update best: 0.10500\n",
      "Epoch: 16, Time: 1.35259s, Loss: 0.79208\n",
      "Epoch: 17, Time: 1.35914s, Loss: 0.72543\n",
      "Epoch: 18, Time: 1.38262s, Loss: 0.65884\n",
      "Epoch: 19, Time: 1.37241s, Loss: 0.59730\n",
      "Epoch: 20, Time: 1.41043s, Loss: 0.55186\n",
      "Epoch: 21, Time: 1.45364s, Loss: 0.48377\n",
      "Epoch: 22, Time: 1.33518s, Loss: 0.44027\n",
      "Epoch: 23, Time: 1.42071s, Loss: 0.39254\n",
      "Epoch: 24, Time: 1.34800s, Loss: 0.35922\n",
      "Epoch: 25, Time: 1.33080s, Loss: 0.31876\n",
      "Epoch: 26, Time: 1.29542s, Loss: 0.29382\n",
      "Epoch: 27, Time: 1.43415s, Loss: 0.25786\n",
      "Epoch: 28, Time: 1.37351s, Loss: 0.23379\n",
      "Epoch: 29, Time: 1.43001s, Loss: 0.21166\n",
      "update best: 0.11000\n",
      "Epoch: 30, Time: 1.27605s, Loss: 0.18889\n",
      "Epoch: 31, Time: 1.30442s, Loss: 0.17234\n",
      "Epoch: 32, Time: 1.25549s, Loss: 0.15468\n",
      "update best: 0.12000\n",
      "Epoch: 33, Time: 1.28189s, Loss: 0.14374\n",
      "update best: 0.12500\n",
      "Epoch: 34, Time: 1.25804s, Loss: 0.13073\n",
      "Epoch: 35, Time: 1.24336s, Loss: 0.11533\n",
      "update best: 0.13500\n",
      "Epoch: 36, Time: 1.28270s, Loss: 0.10756\n",
      "update best: 0.14000\n",
      "Epoch: 37, Time: 1.22811s, Loss: 0.09825\n",
      "update best: 0.15000\n",
      "Epoch: 38, Time: 1.37117s, Loss: 0.09213\n",
      "update best: 0.15500\n",
      "Epoch: 39, Time: 1.32696s, Loss: 0.08591\n",
      "Epoch: 40, Time: 1.31704s, Loss: 0.08078\n",
      "Epoch: 41, Time: 1.43949s, Loss: 0.07544\n",
      "Epoch: 42, Time: 1.37561s, Loss: 0.07192\n",
      "Epoch: 43, Time: 1.38291s, Loss: 0.06496\n",
      "Epoch: 44, Time: 1.30007s, Loss: 0.06504\n",
      "Epoch: 45, Time: 1.34068s, Loss: 0.06036\n",
      "Epoch: 46, Time: 1.35244s, Loss: 0.05478\n",
      "Epoch: 47, Time: 1.37770s, Loss: 0.05395\n",
      "Epoch: 48, Time: 1.43613s, Loss: 0.05261\n",
      "Epoch: 49, Time: 1.39924s, Loss: 0.05150\n",
      "Epoch: 50, Time: 1.44943s, Loss: 0.04703\n",
      "Epoch: 51, Time: 1.46634s, Loss: 0.04725\n",
      "Epoch: 52, Time: 1.47481s, Loss: 0.04718\n",
      "Epoch: 53, Time: 1.44230s, Loss: 0.04429\n",
      "update best: 0.16500\n",
      "Epoch: 54, Time: 1.46572s, Loss: 0.04436\n",
      "update best: 0.19500\n",
      "Epoch: 55, Time: 1.43431s, Loss: 0.04274\n",
      "update best: 0.21000\n",
      "Epoch: 56, Time: 1.49898s, Loss: 0.04083\n",
      "Epoch: 57, Time: 1.40041s, Loss: 0.04188\n",
      "update best: 0.23500\n",
      "Epoch: 58, Time: 1.43422s, Loss: 0.04199\n",
      "update best: 0.25000\n",
      "Epoch: 59, Time: 1.37231s, Loss: 0.04150\n",
      "update best: 0.28000\n",
      "Epoch: 60, Time: 1.53890s, Loss: 0.03965\n",
      "update best: 0.28500\n",
      "Epoch: 61, Time: 1.41634s, Loss: 0.03752\n",
      "Epoch: 62, Time: 1.51327s, Loss: 0.03809\n",
      "Epoch: 63, Time: 1.36452s, Loss: 0.03837\n",
      "Epoch: 64, Time: 1.44806s, Loss: 0.03919\n",
      "Epoch: 65, Time: 1.47120s, Loss: 0.03748\n",
      "Epoch: 66, Time: 1.32771s, Loss: 0.03714\n",
      "Epoch: 67, Time: 1.32778s, Loss: 0.03501\n",
      "Epoch: 68, Time: 1.31287s, Loss: 0.03392\n",
      "update best: 0.31000\n",
      "Epoch: 69, Time: 1.32246s, Loss: 0.03251\n",
      "update best: 0.34500\n",
      "Epoch: 70, Time: 1.40177s, Loss: 0.03559\n",
      "update best: 0.37000\n",
      "Epoch: 71, Time: 1.32568s, Loss: 0.03304\n",
      "Epoch: 72, Time: 1.42692s, Loss: 0.03425\n",
      "Epoch: 73, Time: 1.72085s, Loss: 0.03320\n",
      "update best: 0.37500\n",
      "Epoch: 74, Time: 1.43077s, Loss: 0.03200\n",
      "Epoch: 75, Time: 1.35294s, Loss: 0.03212\n",
      "update best: 0.39000\n",
      "Epoch: 76, Time: 1.30638s, Loss: 0.03018\n",
      "Epoch: 77, Time: 1.30985s, Loss: 0.03008\n",
      "update best: 0.39500\n",
      "Epoch: 78, Time: 1.31392s, Loss: 0.03139\n",
      "Epoch: 79, Time: 1.43192s, Loss: 0.02849\n",
      "update best: 0.40000\n",
      "Epoch: 80, Time: 1.40814s, Loss: 0.03067\n",
      "update best: 0.41000\n",
      "Epoch: 81, Time: 1.31110s, Loss: 0.02883\n",
      "update best: 0.45500\n",
      "Epoch: 82, Time: 1.43553s, Loss: 0.02801\n",
      "update best: 0.46000\n",
      "Epoch: 83, Time: 1.42795s, Loss: 0.02717\n",
      "update best: 0.47500\n",
      "Epoch: 84, Time: 1.36155s, Loss: 0.02615\n",
      "Epoch: 85, Time: 1.50270s, Loss: 0.02605\n",
      "Epoch: 86, Time: 1.58583s, Loss: 0.02592\n",
      "Epoch: 87, Time: 1.50066s, Loss: 0.02628\n",
      "Epoch: 88, Time: 1.85431s, Loss: 0.02556\n",
      "Epoch: 89, Time: 1.68659s, Loss: 0.02421\n",
      "Epoch: 90, Time: 1.32606s, Loss: 0.02532\n",
      "Epoch: 91, Time: 1.39409s, Loss: 0.02611\n",
      "Epoch: 92, Time: 1.45525s, Loss: 0.02378\n",
      "Epoch: 93, Time: 1.37677s, Loss: 0.02510\n",
      "Epoch: 94, Time: 1.31949s, Loss: 0.02376\n",
      "Epoch: 95, Time: 1.29086s, Loss: 0.02301\n",
      "Epoch: 96, Time: 1.28152s, Loss: 0.02325\n",
      "Epoch: 97, Time: 1.22093s, Loss: 0.02275\n",
      "Epoch: 98, Time: 1.31907s, Loss: 0.02393\n",
      "update best: 0.48500\n",
      "Epoch: 99, Time: 1.37080s, Loss: 0.02081\n",
      "Epoch: 100, Time: 1.40924s, Loss: 0.02302\n",
      "Epoch: 101, Time: 1.69402s, Loss: 0.02299\n",
      "update best: 0.49500\n",
      "Epoch: 102, Time: 1.32635s, Loss: 0.02197\n",
      "Epoch: 103, Time: 1.33762s, Loss: 0.02165\n",
      "Epoch: 104, Time: 1.32808s, Loss: 0.02216\n",
      "Epoch: 105, Time: 1.32325s, Loss: 0.02217\n",
      "Epoch: 106, Time: 1.39723s, Loss: 0.02029\n",
      "Epoch: 107, Time: 1.37769s, Loss: 0.02081\n",
      "Epoch: 108, Time: 1.46459s, Loss: 0.02086\n",
      "Epoch: 109, Time: 1.31666s, Loss: 0.01997\n",
      "Epoch: 110, Time: 1.33238s, Loss: 0.02177\n",
      "Epoch: 111, Time: 1.56508s, Loss: 0.02051\n",
      "Epoch: 112, Time: 1.42414s, Loss: 0.02007\n",
      "Epoch: 113, Time: 1.27276s, Loss: 0.02031\n",
      "Epoch: 114, Time: 1.27076s, Loss: 0.01989\n",
      "Epoch: 115, Time: 1.33571s, Loss: 0.01988\n",
      "Epoch: 116, Time: 1.46237s, Loss: 0.02025\n",
      "Epoch: 117, Time: 1.29031s, Loss: 0.01742\n",
      "Epoch: 118, Time: 1.35665s, Loss: 0.01813\n",
      "update best: 0.50000\n",
      "Epoch: 119, Time: 1.33894s, Loss: 0.01844\n",
      "Epoch: 120, Time: 1.48070s, Loss: 0.01895\n",
      "Epoch: 121, Time: 1.43016s, Loss: 0.01908\n",
      "Epoch: 122, Time: 1.44818s, Loss: 0.01838\n",
      "Epoch: 123, Time: 1.33738s, Loss: 0.01817\n",
      "Epoch: 124, Time: 1.56268s, Loss: 0.01969\n",
      "Epoch: 125, Time: 1.36915s, Loss: 0.01744\n",
      "Epoch: 126, Time: 1.29028s, Loss: 0.01882\n",
      "Epoch: 127, Time: 1.34672s, Loss: 0.01783\n",
      "Epoch: 128, Time: 1.47206s, Loss: 0.01902\n",
      "Epoch: 129, Time: 1.49847s, Loss: 0.01799\n",
      "Epoch: 130, Time: 1.34965s, Loss: 0.01803\n",
      "Epoch: 131, Time: 1.35305s, Loss: 0.01799\n",
      "Epoch: 132, Time: 1.27873s, Loss: 0.01825\n",
      "Epoch: 133, Time: 1.29053s, Loss: 0.01852\n",
      "Epoch: 134, Time: 1.37004s, Loss: 0.01676\n",
      "Epoch: 135, Time: 1.32739s, Loss: 0.01684\n",
      "Epoch: 136, Time: 1.29580s, Loss: 0.01663\n",
      "Epoch: 137, Time: 1.31958s, Loss: 0.01641\n",
      "Epoch: 138, Time: 1.31170s, Loss: 0.01648\n",
      "Epoch: 139, Time: 1.30674s, Loss: 0.01749\n",
      "Epoch: 140, Time: 1.28187s, Loss: 0.01731\n",
      "Epoch: 141, Time: 1.27024s, Loss: 0.01744\n",
      "Epoch: 142, Time: 1.33814s, Loss: 0.01645\n",
      "Epoch: 143, Time: 1.42886s, Loss: 0.01751\n",
      "Epoch: 144, Time: 1.47848s, Loss: 0.01728\n",
      "Epoch: 145, Time: 1.38144s, Loss: 0.01801\n",
      "Epoch: 146, Time: 1.46823s, Loss: 0.01696\n",
      "Epoch: 147, Time: 1.45598s, Loss: 0.02160\n",
      "Epoch: 148, Time: 1.72187s, Loss: 0.01769\n",
      "Epoch: 149, Time: 1.70991s, Loss: 0.02529\n",
      "Epoch: 150, Time: 1.31159s, Loss: 0.02321\n",
      "Epoch: 151, Time: 1.46281s, Loss: 0.03171\n",
      "Epoch: 152, Time: 1.41087s, Loss: 0.01670\n",
      "Epoch: 153, Time: 1.51190s, Loss: 0.04494\n",
      "Epoch: 154, Time: 1.42654s, Loss: 0.01860\n",
      "Epoch: 155, Time: 1.53499s, Loss: 0.02177\n",
      "Epoch: 156, Time: 1.52821s, Loss: 0.03615\n",
      "Epoch: 157, Time: 1.38554s, Loss: 0.01814\n",
      "Epoch: 158, Time: 1.37736s, Loss: 0.01865\n",
      "Epoch: 159, Time: 1.54945s, Loss: 0.02191\n",
      "Epoch: 160, Time: 1.44442s, Loss: 0.02447\n",
      "Epoch: 161, Time: 1.44328s, Loss: 0.02430\n",
      "Epoch: 162, Time: 1.43570s, Loss: 0.01935\n",
      "Epoch: 163, Time: 1.39545s, Loss: 0.01849\n",
      "Epoch: 164, Time: 1.39675s, Loss: 0.01774\n",
      "Epoch: 165, Time: 1.39179s, Loss: 0.01589\n",
      "Epoch: 166, Time: 1.36605s, Loss: 0.01702\n",
      "Epoch: 167, Time: 1.40766s, Loss: 0.01585\n",
      "Epoch: 168, Time: 1.46108s, Loss: 0.01639\n",
      "Epoch: 169, Time: 1.38070s, Loss: 0.01560\n",
      "Epoch: 170, Time: 1.45014s, Loss: 0.01578\n",
      "Epoch: 171, Time: 1.57744s, Loss: 0.01494\n",
      "Epoch: 172, Time: 1.47759s, Loss: 0.01431\n",
      "Epoch: 173, Time: 1.40481s, Loss: 0.01409\n",
      "Epoch: 174, Time: 1.49832s, Loss: 0.01369\n",
      "Epoch: 175, Time: 1.50229s, Loss: 0.01306\n",
      "Epoch: 176, Time: 1.45149s, Loss: 0.01334\n",
      "Epoch: 177, Time: 1.46486s, Loss: 0.01242\n",
      "Epoch: 178, Time: 1.63365s, Loss: 0.01266\n",
      "Epoch: 179, Time: 2.42034s, Loss: 0.01165\n",
      "Epoch: 180, Time: 1.51814s, Loss: 0.01231\n",
      "Epoch: 181, Time: 1.64823s, Loss: 0.01221\n",
      "Epoch: 182, Time: 1.37344s, Loss: 0.01222\n",
      "Epoch: 183, Time: 1.35596s, Loss: 0.01264\n",
      "Epoch: 184, Time: 1.41736s, Loss: 0.01289\n",
      "Epoch: 185, Time: 1.38307s, Loss: 0.01289\n",
      "Epoch: 186, Time: 1.40860s, Loss: 0.01329\n",
      "Epoch: 187, Time: 1.40430s, Loss: 0.01247\n",
      "Epoch: 188, Time: 1.48603s, Loss: 0.01202\n",
      "Epoch: 189, Time: 1.38514s, Loss: 0.01232\n",
      "Epoch: 190, Time: 1.34340s, Loss: 0.01294\n",
      "Epoch: 191, Time: 1.43782s, Loss: 0.01216\n",
      "Epoch: 192, Time: 1.48778s, Loss: 0.01211\n",
      "Epoch: 193, Time: 1.38422s, Loss: 0.01375\n",
      "Epoch: 194, Time: 1.39332s, Loss: 0.01281\n",
      "Epoch: 195, Time: 1.32660s, Loss: 0.01381\n",
      "Epoch: 196, Time: 1.30333s, Loss: 0.01335\n",
      "Epoch: 197, Time: 1.49123s, Loss: 0.01330\n",
      "Epoch: 198, Time: 1.79635s, Loss: 0.01353\n",
      "Epoch: 199, Time: 1.50184s, Loss: 0.01235\n",
      "\n",
      "train finished!\n",
      "best val: 0.50000\n",
      "test...\n",
      "final result: epoch: 118\n",
      "{'accuracy': 0.5289161801338196, 'f1_score': 0.40503309054521114, 'f1_score -> average@micro': 0.5289161787805227}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.10855s, Loss: 3.00464\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.08103s, Loss: 2.85373\n",
      "Epoch: 2, Time: 0.09561s, Loss: 2.39547\n",
      "Epoch: 3, Time: 0.08431s, Loss: 2.16992\n",
      "Epoch: 4, Time: 0.08556s, Loss: 1.98238\n",
      "Epoch: 5, Time: 0.08011s, Loss: 1.82536\n",
      "update best: 0.07500\n",
      "Epoch: 6, Time: 0.09422s, Loss: 1.69112\n",
      "update best: 0.08000\n",
      "Epoch: 7, Time: 0.08904s, Loss: 1.55686\n",
      "Epoch: 8, Time: 0.09059s, Loss: 1.43632\n",
      "update best: 0.11000\n",
      "Epoch: 9, Time: 0.07604s, Loss: 1.31332\n",
      "update best: 0.22000\n",
      "Epoch: 10, Time: 0.08363s, Loss: 1.20795\n",
      "Epoch: 11, Time: 0.07904s, Loss: 1.09272\n",
      "Epoch: 12, Time: 0.07904s, Loss: 1.00381\n",
      "Epoch: 13, Time: 0.14408s, Loss: 0.90282\n",
      "Epoch: 14, Time: 0.08704s, Loss: 0.81816\n",
      "Epoch: 15, Time: 0.07804s, Loss: 0.74913\n",
      "Epoch: 16, Time: 0.08156s, Loss: 0.66262\n",
      "Epoch: 17, Time: 0.08205s, Loss: 0.60239\n",
      "Epoch: 18, Time: 0.11778s, Loss: 0.53853\n",
      "Epoch: 19, Time: 0.09161s, Loss: 0.48864\n",
      "Epoch: 20, Time: 0.09068s, Loss: 0.42724\n",
      "Epoch: 21, Time: 0.08355s, Loss: 0.38623\n",
      "Epoch: 22, Time: 0.10086s, Loss: 0.34136\n",
      "Epoch: 23, Time: 0.09659s, Loss: 0.30420\n",
      "Epoch: 24, Time: 0.09107s, Loss: 0.27242\n",
      "Epoch: 25, Time: 0.08208s, Loss: 0.23802\n",
      "Epoch: 26, Time: 0.09509s, Loss: 0.21636\n",
      "Epoch: 27, Time: 0.08206s, Loss: 0.19498\n",
      "Epoch: 28, Time: 0.08175s, Loss: 0.17500\n",
      "Epoch: 29, Time: 0.08208s, Loss: 0.15270\n",
      "Epoch: 30, Time: 0.07806s, Loss: 0.13763\n",
      "Epoch: 31, Time: 0.09466s, Loss: 0.12347\n",
      "Epoch: 32, Time: 0.09861s, Loss: 0.11316\n",
      "Epoch: 33, Time: 0.09258s, Loss: 0.10082\n",
      "Epoch: 34, Time: 0.09119s, Loss: 0.09246\n",
      "Epoch: 35, Time: 0.08960s, Loss: 0.08394\n",
      "Epoch: 36, Time: 0.08559s, Loss: 0.07804\n",
      "Epoch: 37, Time: 0.08110s, Loss: 0.07072\n",
      "Epoch: 38, Time: 0.09057s, Loss: 0.06743\n",
      "Epoch: 39, Time: 0.10018s, Loss: 0.06343\n",
      "Epoch: 40, Time: 0.09457s, Loss: 0.05752\n",
      "Epoch: 41, Time: 0.09508s, Loss: 0.05332\n",
      "Epoch: 42, Time: 0.08177s, Loss: 0.05070\n",
      "Epoch: 43, Time: 0.08888s, Loss: 0.04732\n",
      "Epoch: 44, Time: 0.09961s, Loss: 0.04493\n",
      "Epoch: 45, Time: 0.08239s, Loss: 0.04645\n",
      "Epoch: 46, Time: 0.09278s, Loss: 0.04098\n",
      "Epoch: 47, Time: 0.10937s, Loss: 0.04156\n",
      "Epoch: 48, Time: 0.09224s, Loss: 0.03786\n",
      "update best: 0.23500\n",
      "Epoch: 49, Time: 0.08105s, Loss: 0.03783\n",
      "update best: 0.25500\n",
      "Epoch: 50, Time: 0.09759s, Loss: 0.03815\n",
      "update best: 0.26500\n",
      "Epoch: 51, Time: 0.11816s, Loss: 0.03539\n",
      "Epoch: 52, Time: 0.12713s, Loss: 0.03394\n",
      "Epoch: 53, Time: 0.11458s, Loss: 0.03498\n",
      "Epoch: 54, Time: 0.11960s, Loss: 0.03471\n",
      "Epoch: 55, Time: 0.12266s, Loss: 0.03187\n",
      "Epoch: 56, Time: 0.11107s, Loss: 0.03311\n",
      "Epoch: 57, Time: 0.10058s, Loss: 0.03236\n",
      "Epoch: 58, Time: 0.12010s, Loss: 0.03206\n",
      "Epoch: 59, Time: 0.11112s, Loss: 0.03009\n",
      "Epoch: 60, Time: 0.11309s, Loss: 0.03027\n",
      "Epoch: 61, Time: 0.10956s, Loss: 0.03062\n",
      "Epoch: 62, Time: 0.11258s, Loss: 0.03146\n",
      "Epoch: 63, Time: 0.10657s, Loss: 0.02908\n",
      "Epoch: 64, Time: 0.12109s, Loss: 0.03267\n",
      "update best: 0.27000\n",
      "Epoch: 65, Time: 0.10493s, Loss: 0.02796\n",
      "update best: 0.28500\n",
      "Epoch: 66, Time: 0.11657s, Loss: 0.02921\n",
      "update best: 0.31000\n",
      "Epoch: 67, Time: 0.10761s, Loss: 0.02787\n",
      "Epoch: 68, Time: 0.11518s, Loss: 0.02633\n",
      "update best: 0.31500\n",
      "Epoch: 69, Time: 0.11161s, Loss: 0.02667\n",
      "update best: 0.32500\n",
      "Epoch: 70, Time: 0.10609s, Loss: 0.02667\n",
      "update best: 0.33000\n",
      "Epoch: 71, Time: 0.11265s, Loss: 0.02442\n",
      "update best: 0.33500\n",
      "Epoch: 72, Time: 0.10656s, Loss: 0.02611\n",
      "update best: 0.35000\n",
      "Epoch: 73, Time: 0.11760s, Loss: 0.02556\n",
      "Epoch: 74, Time: 0.11409s, Loss: 0.02474\n",
      "Epoch: 75, Time: 0.11714s, Loss: 0.02414\n",
      "Epoch: 76, Time: 0.10258s, Loss: 0.02310\n",
      "Epoch: 77, Time: 0.12112s, Loss: 0.02235\n",
      "Epoch: 78, Time: 0.11957s, Loss: 0.02200\n",
      "Epoch: 79, Time: 0.11412s, Loss: 0.02231\n",
      "Epoch: 80, Time: 0.10962s, Loss: 0.02164\n",
      "update best: 0.35500\n",
      "Epoch: 81, Time: 0.12378s, Loss: 0.02111\n",
      "update best: 0.41000\n",
      "Epoch: 82, Time: 0.11511s, Loss: 0.02094\n",
      "update best: 0.44500\n",
      "Epoch: 83, Time: 0.10360s, Loss: 0.02243\n",
      "Epoch: 84, Time: 0.11059s, Loss: 0.02188\n",
      "Epoch: 85, Time: 0.11810s, Loss: 0.02204\n",
      "Epoch: 86, Time: 0.12082s, Loss: 0.01969\n",
      "Epoch: 87, Time: 0.11518s, Loss: 0.02077\n",
      "Epoch: 88, Time: 0.11263s, Loss: 0.01974\n",
      "Epoch: 89, Time: 0.10610s, Loss: 0.01937\n",
      "Epoch: 90, Time: 0.11911s, Loss: 0.02079\n",
      "Epoch: 91, Time: 0.12612s, Loss: 0.01972\n",
      "Epoch: 92, Time: 0.12459s, Loss: 0.01940\n",
      "Epoch: 93, Time: 0.13528s, Loss: 0.01956\n",
      "Epoch: 94, Time: 0.21866s, Loss: 0.01888\n",
      "Epoch: 95, Time: 0.11109s, Loss: 0.01911\n",
      "Epoch: 96, Time: 0.12051s, Loss: 0.01821\n",
      "Epoch: 97, Time: 0.11910s, Loss: 0.01749\n",
      "Epoch: 98, Time: 0.11008s, Loss: 0.01748\n",
      "Epoch: 99, Time: 0.11158s, Loss: 0.01779\n",
      "Epoch: 100, Time: 0.11512s, Loss: 0.01833\n",
      "Epoch: 101, Time: 0.11421s, Loss: 0.01708\n",
      "Epoch: 102, Time: 0.12238s, Loss: 0.01692\n",
      "Epoch: 103, Time: 0.11256s, Loss: 0.01745\n",
      "update best: 0.45000\n",
      "Epoch: 104, Time: 0.10949s, Loss: 0.01689\n",
      "Epoch: 105, Time: 0.11056s, Loss: 0.01643\n",
      "Epoch: 106, Time: 0.10858s, Loss: 0.01698\n",
      "Epoch: 107, Time: 0.12488s, Loss: 0.01411\n",
      "update best: 0.45500\n",
      "Epoch: 108, Time: 0.11812s, Loss: 0.01557\n",
      "Epoch: 109, Time: 0.11909s, Loss: 0.01596\n",
      "update best: 0.46000\n",
      "Epoch: 110, Time: 0.10959s, Loss: 0.01623\n",
      "Epoch: 111, Time: 0.11502s, Loss: 0.01557\n",
      "update best: 0.47500\n",
      "Epoch: 112, Time: 0.10019s, Loss: 0.01667\n",
      "Epoch: 113, Time: 0.11613s, Loss: 0.01662\n",
      "Epoch: 114, Time: 0.10405s, Loss: 0.01519\n",
      "Epoch: 115, Time: 0.12112s, Loss: 0.01566\n",
      "Epoch: 116, Time: 0.11459s, Loss: 0.01502\n",
      "Epoch: 117, Time: 0.11607s, Loss: 0.01577\n",
      "Epoch: 118, Time: 0.11427s, Loss: 0.01539\n",
      "Epoch: 119, Time: 0.11919s, Loss: 0.01461\n",
      "Epoch: 120, Time: 0.10885s, Loss: 0.01566\n",
      "Epoch: 121, Time: 0.12457s, Loss: 0.01513\n",
      "Epoch: 122, Time: 0.12417s, Loss: 0.01506\n",
      "Epoch: 123, Time: 0.12285s, Loss: 0.01606\n",
      "Epoch: 124, Time: 0.12110s, Loss: 0.01423\n",
      "Epoch: 125, Time: 0.11711s, Loss: 0.01454\n",
      "Epoch: 126, Time: 0.12613s, Loss: 0.01424\n",
      "Epoch: 127, Time: 0.10858s, Loss: 0.01419\n",
      "Epoch: 128, Time: 0.10856s, Loss: 0.01394\n",
      "Epoch: 129, Time: 0.10873s, Loss: 0.01380\n",
      "Epoch: 130, Time: 0.10656s, Loss: 0.01443\n",
      "Epoch: 131, Time: 0.11498s, Loss: 0.01307\n",
      "Epoch: 132, Time: 0.11861s, Loss: 0.01310\n",
      "Epoch: 133, Time: 0.12256s, Loss: 0.01282\n",
      "Epoch: 134, Time: 0.11357s, Loss: 0.01314\n",
      "Epoch: 135, Time: 0.11219s, Loss: 0.01436\n",
      "Epoch: 136, Time: 0.10663s, Loss: 0.01356\n",
      "Epoch: 137, Time: 0.11052s, Loss: 0.01348\n",
      "Epoch: 138, Time: 0.11165s, Loss: 0.01287\n",
      "Epoch: 139, Time: 0.11514s, Loss: 0.01285\n",
      "Epoch: 140, Time: 0.16839s, Loss: 0.01282\n",
      "Epoch: 141, Time: 0.12262s, Loss: 0.01282\n",
      "Epoch: 142, Time: 0.11057s, Loss: 0.01368\n",
      "Epoch: 143, Time: 0.11360s, Loss: 0.01278\n",
      "Epoch: 144, Time: 0.10558s, Loss: 0.01363\n",
      "Epoch: 145, Time: 0.11208s, Loss: 0.01316\n",
      "Epoch: 146, Time: 0.11662s, Loss: 0.01272\n",
      "Epoch: 147, Time: 0.11758s, Loss: 0.01261\n",
      "Epoch: 148, Time: 0.12404s, Loss: 0.01321\n",
      "Epoch: 149, Time: 0.10555s, Loss: 0.01308\n",
      "Epoch: 150, Time: 0.12204s, Loss: 0.01287\n",
      "Epoch: 151, Time: 0.11496s, Loss: 0.01174\n",
      "Epoch: 152, Time: 0.10958s, Loss: 0.01301\n",
      "Epoch: 153, Time: 0.11310s, Loss: 0.01285\n",
      "Epoch: 154, Time: 0.11858s, Loss: 0.01269\n",
      "Epoch: 155, Time: 0.12313s, Loss: 0.01245\n",
      "Epoch: 156, Time: 0.10827s, Loss: 0.01330\n",
      "Epoch: 157, Time: 0.12212s, Loss: 0.01280\n",
      "Epoch: 158, Time: 0.10757s, Loss: 0.01255\n",
      "Epoch: 159, Time: 0.10765s, Loss: 0.01220\n",
      "Epoch: 160, Time: 0.12216s, Loss: 0.01134\n",
      "Epoch: 161, Time: 0.12055s, Loss: 0.01127\n",
      "Epoch: 162, Time: 0.10458s, Loss: 0.01153\n",
      "Epoch: 163, Time: 0.10329s, Loss: 0.01129\n",
      "Epoch: 164, Time: 0.10956s, Loss: 0.01120\n",
      "Epoch: 165, Time: 0.10456s, Loss: 0.01126\n",
      "Epoch: 166, Time: 0.10960s, Loss: 0.01091\n",
      "Epoch: 167, Time: 0.10909s, Loss: 0.01097\n",
      "Epoch: 168, Time: 0.12124s, Loss: 0.01180\n",
      "Epoch: 169, Time: 0.12729s, Loss: 0.01197\n",
      "Epoch: 170, Time: 0.11857s, Loss: 0.01069\n",
      "Epoch: 171, Time: 0.10259s, Loss: 0.01087\n",
      "Epoch: 172, Time: 0.11161s, Loss: 0.01150\n",
      "Epoch: 173, Time: 0.10921s, Loss: 0.01196\n",
      "Epoch: 174, Time: 0.10658s, Loss: 0.01160\n",
      "Epoch: 175, Time: 0.11360s, Loss: 0.01139\n",
      "Epoch: 176, Time: 0.11757s, Loss: 0.01102\n",
      "Epoch: 177, Time: 0.10807s, Loss: 0.01557\n",
      "Epoch: 178, Time: 0.11157s, Loss: 0.03755\n",
      "Epoch: 179, Time: 0.10435s, Loss: 0.01259\n",
      "Epoch: 180, Time: 0.11604s, Loss: 0.04858\n",
      "Epoch: 181, Time: 0.10258s, Loss: 0.04517\n",
      "Epoch: 182, Time: 0.12508s, Loss: 0.01797\n",
      "Epoch: 183, Time: 0.11606s, Loss: 0.02016\n",
      "Epoch: 184, Time: 0.12010s, Loss: 0.02098\n",
      "Epoch: 185, Time: 0.10121s, Loss: 0.02448\n",
      "Epoch: 186, Time: 0.11561s, Loss: 0.02332\n",
      "Epoch: 187, Time: 0.11158s, Loss: 0.02146\n",
      "Epoch: 188, Time: 0.11559s, Loss: 0.01876\n",
      "Epoch: 189, Time: 0.10807s, Loss: 0.01704\n",
      "Epoch: 190, Time: 0.11109s, Loss: 0.01545\n",
      "Epoch: 191, Time: 0.11456s, Loss: 0.01565\n",
      "Epoch: 192, Time: 0.10509s, Loss: 0.01590\n",
      "Epoch: 193, Time: 0.11213s, Loss: 0.01600\n",
      "Epoch: 194, Time: 0.10910s, Loss: 0.01321\n",
      "Epoch: 195, Time: 0.11056s, Loss: 0.01114\n",
      "Epoch: 196, Time: 0.10511s, Loss: 0.01090\n",
      "Epoch: 197, Time: 0.10709s, Loss: 0.01140\n",
      "Epoch: 198, Time: 0.10055s, Loss: 0.01134\n",
      "Epoch: 199, Time: 0.10959s, Loss: 0.01008\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 111\n",
      "{'accuracy': 0.4370983839035034, 'f1_score': 0.3630498297451491, 'f1_score -> average@micro': 0.4370983864058261}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-citation Citaseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is cocitation_citeseer dataset:\n",
       "  ->  num_classes\n",
       "  ->  num_vertices\n",
       "  ->  num_edges\n",
       "  ->  dim_features\n",
       "  ->  features\n",
       "  ->  edge_list\n",
       "  ->  labels\n",
       "  ->  train_mask\n",
       "  ->  val_mask\n",
       "  ->  test_mask\n",
       "Please try `data['name']` to get the specified data."
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = dhg.data.CocitationCiteseer(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['num_vertices'])\n",
    "data['num_edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.07557s, Loss: 1.84384\n",
      "update best: 0.18053\n",
      "Epoch: 1, Time: 0.01852s, Loss: 1.32729\n",
      "Epoch: 2, Time: 0.02073s, Loss: 0.89455\n",
      "Epoch: 3, Time: 0.01821s, Loss: 0.58217\n",
      "Epoch: 4, Time: 0.02025s, Loss: 0.33297\n",
      "Epoch: 5, Time: 0.01954s, Loss: 0.21457\n",
      "Epoch: 6, Time: 0.01855s, Loss: 0.16569\n",
      "update best: 0.18399\n",
      "Epoch: 7, Time: 0.01419s, Loss: 0.11529\n",
      "update best: 0.20258\n",
      "Epoch: 8, Time: 0.02618s, Loss: 0.11493\n",
      "update best: 0.21928\n",
      "Epoch: 9, Time: 0.02598s, Loss: 0.08893\n",
      "update best: 0.22716\n",
      "Epoch: 10, Time: 0.01781s, Loss: 0.04256\n",
      "update best: 0.23503\n",
      "Epoch: 11, Time: 0.01740s, Loss: 0.05914\n",
      "update best: 0.24071\n",
      "Epoch: 12, Time: 0.01607s, Loss: 0.03737\n",
      "update best: 0.24354\n",
      "Epoch: 13, Time: 0.01743s, Loss: 0.03301\n",
      "update best: 0.25457\n",
      "Epoch: 14, Time: 0.01613s, Loss: 0.02690\n",
      "update best: 0.25772\n",
      "Epoch: 15, Time: 0.01786s, Loss: 0.01817\n",
      "update best: 0.25835\n",
      "Epoch: 16, Time: 0.01224s, Loss: 0.04195\n",
      "Epoch: 17, Time: 0.01985s, Loss: 0.04121\n",
      "Epoch: 18, Time: 0.02273s, Loss: 0.01221\n",
      "Epoch: 19, Time: 0.02539s, Loss: 0.01795\n",
      "Epoch: 20, Time: 0.02135s, Loss: 0.03318\n",
      "Epoch: 21, Time: 0.02106s, Loss: 0.01417\n",
      "Epoch: 22, Time: 0.02165s, Loss: 0.01202\n",
      "Epoch: 23, Time: 0.02191s, Loss: 0.00621\n",
      "Epoch: 24, Time: 0.02259s, Loss: 0.01527\n",
      "Epoch: 25, Time: 0.01803s, Loss: 0.00521\n",
      "Epoch: 26, Time: 0.02032s, Loss: 0.01303\n",
      "Epoch: 27, Time: 0.02088s, Loss: 0.01279\n",
      "Epoch: 28, Time: 0.01999s, Loss: 0.00548\n",
      "Epoch: 29, Time: 0.01940s, Loss: 0.00536\n",
      "Epoch: 30, Time: 0.01582s, Loss: 0.01232\n",
      "Epoch: 31, Time: 0.02379s, Loss: 0.00742\n",
      "Epoch: 32, Time: 0.01838s, Loss: 0.00472\n",
      "Epoch: 33, Time: 0.01057s, Loss: 0.01984\n",
      "Epoch: 34, Time: 0.02498s, Loss: 0.00515\n",
      "Epoch: 35, Time: 0.01750s, Loss: 0.00302\n",
      "Epoch: 36, Time: 0.01646s, Loss: 0.01405\n",
      "Epoch: 37, Time: 0.01622s, Loss: 0.00204\n",
      "Epoch: 38, Time: 0.01946s, Loss: 0.00648\n",
      "Epoch: 39, Time: 0.01720s, Loss: 0.00954\n",
      "Epoch: 40, Time: 0.01785s, Loss: 0.00736\n",
      "Epoch: 41, Time: 0.01665s, Loss: 0.00238\n",
      "Epoch: 42, Time: 0.01664s, Loss: 0.00368\n",
      "Epoch: 43, Time: 0.01792s, Loss: 0.00203\n",
      "Epoch: 44, Time: 0.01528s, Loss: 0.00152\n",
      "Epoch: 45, Time: 0.01737s, Loss: 0.00106\n",
      "Epoch: 46, Time: 0.01749s, Loss: 0.00479\n",
      "Epoch: 47, Time: 0.01904s, Loss: 0.00352\n",
      "Epoch: 48, Time: 0.01904s, Loss: 0.01470\n",
      "Epoch: 49, Time: 0.02189s, Loss: 0.00244\n",
      "Epoch: 50, Time: 0.02238s, Loss: 0.00861\n",
      "Epoch: 51, Time: 0.01767s, Loss: 0.00358\n",
      "Epoch: 52, Time: 0.02101s, Loss: 0.00322\n",
      "Epoch: 53, Time: 0.01907s, Loss: 0.00118\n",
      "Epoch: 54, Time: 0.01659s, Loss: 0.00263\n",
      "Epoch: 55, Time: 0.02012s, Loss: 0.01326\n",
      "Epoch: 56, Time: 0.01604s, Loss: 0.00693\n",
      "Epoch: 57, Time: 0.01645s, Loss: 0.00257\n",
      "Epoch: 58, Time: 0.01814s, Loss: 0.00101\n",
      "Epoch: 59, Time: 0.01666s, Loss: 0.00911\n",
      "Epoch: 60, Time: 0.01661s, Loss: 0.00936\n",
      "Epoch: 61, Time: 0.01737s, Loss: 0.00123\n",
      "Epoch: 62, Time: 0.01507s, Loss: 0.01041\n",
      "Epoch: 63, Time: 0.01793s, Loss: 0.01212\n",
      "Epoch: 64, Time: 0.02018s, Loss: 0.00214\n",
      "Epoch: 65, Time: 0.02066s, Loss: 0.00262\n",
      "Epoch: 66, Time: 0.01813s, Loss: 0.00584\n",
      "Epoch: 67, Time: 0.01768s, Loss: 0.00087\n",
      "Epoch: 68, Time: 0.01970s, Loss: 0.01147\n",
      "Epoch: 69, Time: 0.02049s, Loss: 0.01437\n",
      "Epoch: 70, Time: 0.02059s, Loss: 0.00154\n",
      "Epoch: 71, Time: 0.01939s, Loss: 0.00321\n",
      "Epoch: 72, Time: 0.02094s, Loss: 0.00186\n",
      "Epoch: 73, Time: 0.01794s, Loss: 0.00224\n",
      "Epoch: 74, Time: 0.01856s, Loss: 0.00588\n",
      "Epoch: 75, Time: 0.01907s, Loss: 0.02405\n",
      "Epoch: 76, Time: 0.01663s, Loss: 0.00334\n",
      "Epoch: 77, Time: 0.02108s, Loss: 0.00402\n",
      "update best: 0.26402\n",
      "Epoch: 78, Time: 0.01662s, Loss: 0.00632\n",
      "update best: 0.27190\n",
      "Epoch: 79, Time: 0.01623s, Loss: 0.00302\n",
      "update best: 0.27347\n",
      "Epoch: 80, Time: 0.01536s, Loss: 0.00129\n",
      "update best: 0.27599\n",
      "Epoch: 81, Time: 0.02290s, Loss: 0.00393\n",
      "Epoch: 82, Time: 0.01831s, Loss: 0.00084\n",
      "Epoch: 83, Time: 0.01609s, Loss: 0.00363\n",
      "Epoch: 84, Time: 0.01558s, Loss: 0.00176\n",
      "Epoch: 85, Time: 0.01565s, Loss: 0.01176\n",
      "Epoch: 86, Time: 0.01718s, Loss: 0.00149\n",
      "Epoch: 87, Time: 0.00983s, Loss: 0.00122\n",
      "Epoch: 88, Time: 0.01315s, Loss: 0.00072\n",
      "Epoch: 89, Time: 0.01795s, Loss: 0.00815\n",
      "Epoch: 90, Time: 0.01974s, Loss: 0.00086\n",
      "Epoch: 91, Time: 0.01664s, Loss: 0.00212\n",
      "Epoch: 92, Time: 0.01660s, Loss: 0.00074\n",
      "Epoch: 93, Time: 0.01622s, Loss: 0.00070\n",
      "Epoch: 94, Time: 0.01654s, Loss: 0.00234\n",
      "Epoch: 95, Time: 0.01416s, Loss: 0.00231\n",
      "Epoch: 96, Time: 0.01375s, Loss: 0.01695\n",
      "Epoch: 97, Time: 0.01603s, Loss: 0.00428\n",
      "Epoch: 98, Time: 0.01818s, Loss: 0.00508\n",
      "Epoch: 99, Time: 0.01530s, Loss: 0.00125\n",
      "Epoch: 100, Time: 0.01592s, Loss: 0.00322\n",
      "Epoch: 101, Time: 0.01391s, Loss: 0.00112\n",
      "Epoch: 102, Time: 0.01667s, Loss: 0.00270\n",
      "Epoch: 103, Time: 0.01702s, Loss: 0.00323\n",
      "Epoch: 104, Time: 0.01760s, Loss: 0.00200\n",
      "Epoch: 105, Time: 0.02148s, Loss: 0.00109\n",
      "Epoch: 106, Time: 0.01639s, Loss: 0.01693\n",
      "Epoch: 107, Time: 0.01756s, Loss: 0.00453\n",
      "Epoch: 108, Time: 0.01256s, Loss: 0.00070\n",
      "Epoch: 109, Time: 0.01725s, Loss: 0.00992\n",
      "Epoch: 110, Time: 0.01615s, Loss: 0.01284\n",
      "Epoch: 111, Time: 0.02071s, Loss: 0.00394\n",
      "Epoch: 112, Time: 0.01622s, Loss: 0.00095\n",
      "Epoch: 113, Time: 0.01667s, Loss: 0.00139\n",
      "Epoch: 114, Time: 0.01630s, Loss: 0.00704\n",
      "Epoch: 115, Time: 0.02093s, Loss: 0.01456\n",
      "Epoch: 116, Time: 0.01951s, Loss: 0.00099\n",
      "Epoch: 117, Time: 0.01817s, Loss: 0.00307\n",
      "Epoch: 118, Time: 0.01674s, Loss: 0.01291\n",
      "Epoch: 119, Time: 0.01944s, Loss: 0.00089\n",
      "Epoch: 120, Time: 0.01007s, Loss: 0.00867\n",
      "Epoch: 121, Time: 0.01729s, Loss: 0.00143\n",
      "Epoch: 122, Time: 0.01749s, Loss: 0.00413\n",
      "Epoch: 123, Time: 0.01876s, Loss: 0.00115\n",
      "Epoch: 124, Time: 0.02038s, Loss: 0.00473\n",
      "Epoch: 125, Time: 0.01647s, Loss: 0.00423\n",
      "Epoch: 126, Time: 0.01669s, Loss: 0.00284\n",
      "Epoch: 127, Time: 0.01748s, Loss: 0.00151\n",
      "Epoch: 128, Time: 0.01738s, Loss: 0.00751\n",
      "Epoch: 129, Time: 0.01639s, Loss: 0.00503\n",
      "Epoch: 130, Time: 0.02013s, Loss: 0.00140\n",
      "Epoch: 131, Time: 0.01960s, Loss: 0.00309\n",
      "Epoch: 132, Time: 0.01598s, Loss: 0.00381\n",
      "update best: 0.27977\n",
      "Epoch: 133, Time: 0.01861s, Loss: 0.00176\n",
      "update best: 0.28607\n",
      "Epoch: 134, Time: 0.01128s, Loss: 0.00439\n",
      "update best: 0.29332\n",
      "Epoch: 135, Time: 0.01695s, Loss: 0.00276\n",
      "Epoch: 136, Time: 0.01538s, Loss: 0.00186\n",
      "Epoch: 137, Time: 0.01881s, Loss: 0.00234\n",
      "Epoch: 138, Time: 0.01441s, Loss: 0.00368\n",
      "Epoch: 139, Time: 0.01693s, Loss: 0.00487\n",
      "Epoch: 140, Time: 0.02137s, Loss: 0.00286\n",
      "Epoch: 141, Time: 0.01160s, Loss: 0.00101\n",
      "Epoch: 142, Time: 0.01736s, Loss: 0.00221\n",
      "Epoch: 143, Time: 0.01846s, Loss: 0.00689\n",
      "Epoch: 144, Time: 0.01861s, Loss: 0.00191\n",
      "Epoch: 145, Time: 0.01987s, Loss: 0.00288\n",
      "Epoch: 146, Time: 0.01812s, Loss: 0.00674\n",
      "Epoch: 147, Time: 0.01984s, Loss: 0.00450\n",
      "Epoch: 148, Time: 0.01314s, Loss: 0.00107\n",
      "Epoch: 149, Time: 0.01819s, Loss: 0.00232\n",
      "Epoch: 150, Time: 0.01849s, Loss: 0.00186\n",
      "Epoch: 151, Time: 0.01651s, Loss: 0.00094\n",
      "Epoch: 152, Time: 0.01596s, Loss: 0.01419\n",
      "Epoch: 153, Time: 0.01539s, Loss: 0.00092\n",
      "Epoch: 154, Time: 0.01435s, Loss: 0.00103\n",
      "Epoch: 155, Time: 0.01859s, Loss: 0.00089\n",
      "Epoch: 156, Time: 0.01928s, Loss: 0.00860\n",
      "Epoch: 157, Time: 0.02187s, Loss: 0.00186\n",
      "Epoch: 158, Time: 0.01796s, Loss: 0.04007\n",
      "Epoch: 159, Time: 0.02077s, Loss: 0.01148\n",
      "Epoch: 160, Time: 0.01883s, Loss: 0.01324\n",
      "Epoch: 161, Time: 0.01697s, Loss: 0.00500\n",
      "Epoch: 162, Time: 0.01291s, Loss: 0.02147\n",
      "Epoch: 163, Time: 0.01828s, Loss: 0.00268\n",
      "Epoch: 164, Time: 0.01877s, Loss: 0.00114\n",
      "Epoch: 165, Time: 0.01660s, Loss: 0.00103\n",
      "Epoch: 166, Time: 0.01687s, Loss: 0.00143\n",
      "Epoch: 167, Time: 0.01458s, Loss: 0.00802\n",
      "Epoch: 168, Time: 0.01996s, Loss: 0.00196\n",
      "Epoch: 169, Time: 0.02152s, Loss: 0.00107\n",
      "Epoch: 170, Time: 0.02028s, Loss: 0.00500\n",
      "Epoch: 171, Time: 0.01687s, Loss: 0.00304\n",
      "Epoch: 172, Time: 0.02011s, Loss: 0.00600\n",
      "Epoch: 173, Time: 0.02363s, Loss: 0.00318\n",
      "Epoch: 174, Time: 0.02392s, Loss: 0.00424\n",
      "Epoch: 175, Time: 0.02470s, Loss: 0.00479\n",
      "Epoch: 176, Time: 0.01839s, Loss: 0.00095\n",
      "Epoch: 177, Time: 0.01289s, Loss: 0.00309\n",
      "Epoch: 178, Time: 0.01559s, Loss: 0.01341\n",
      "Epoch: 179, Time: 0.01824s, Loss: 0.00103\n",
      "Epoch: 180, Time: 0.02663s, Loss: 0.00252\n",
      "Epoch: 181, Time: 0.01736s, Loss: 0.00145\n",
      "Epoch: 182, Time: 0.01917s, Loss: 0.00156\n",
      "Epoch: 183, Time: 0.02012s, Loss: 0.00154\n",
      "Epoch: 184, Time: 0.02048s, Loss: 0.00170\n",
      "Epoch: 185, Time: 0.01641s, Loss: 0.00569\n",
      "Epoch: 186, Time: 0.01850s, Loss: 0.00530\n",
      "Epoch: 187, Time: 0.01862s, Loss: 0.00718\n",
      "Epoch: 188, Time: 0.01619s, Loss: 0.00135\n",
      "Epoch: 189, Time: 0.02075s, Loss: 0.00858\n",
      "Epoch: 190, Time: 0.01812s, Loss: 0.00070\n",
      "Epoch: 191, Time: 0.01697s, Loss: 0.00161\n",
      "Epoch: 192, Time: 0.01435s, Loss: 0.00187\n",
      "Epoch: 193, Time: 0.02099s, Loss: 0.01581\n",
      "Epoch: 194, Time: 0.01537s, Loss: 0.00251\n",
      "Epoch: 195, Time: 0.02052s, Loss: 0.00978\n",
      "Epoch: 196, Time: 0.01819s, Loss: 0.00208\n",
      "Epoch: 197, Time: 0.01932s, Loss: 0.00614\n",
      "Epoch: 198, Time: 0.01474s, Loss: 0.01333\n",
      "Epoch: 199, Time: 0.02028s, Loss: 0.00244\n",
      "\n",
      "train finished!\n",
      "best val: 0.29332\n",
      "test...\n",
      "final result: epoch: 134\n",
      "{'accuracy': 0.29332074522972107, 'f1_score': 0.24036976724855488, 'f1_score -> average@micro': 0.2933207309388784}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.02666s, Loss: 1.81649\n",
      "update best: 0.21424\n",
      "Epoch: 1, Time: 0.02883s, Loss: 1.43765\n",
      "update best: 0.21865\n",
      "Epoch: 2, Time: 0.02005s, Loss: 1.22796\n",
      "Epoch: 3, Time: 0.01828s, Loss: 1.15794\n",
      "Epoch: 4, Time: 0.01936s, Loss: 1.12441\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.02366s, Loss: 1.10783\n",
      "update best: 0.22275\n",
      "Epoch: 6, Time: 0.01793s, Loss: 1.09250\n",
      "update best: 0.22369\n",
      "Epoch: 7, Time: 0.02164s, Loss: 1.08502\n",
      "update best: 0.22495\n",
      "Epoch: 8, Time: 0.02327s, Loss: 1.07616\n",
      "update best: 0.22527\n",
      "Epoch: 9, Time: 0.02244s, Loss: 1.07437\n",
      "Epoch: 10, Time: 0.01950s, Loss: 1.07368\n",
      "update best: 0.22590\n",
      "Epoch: 11, Time: 0.01640s, Loss: 1.06851\n",
      "Epoch: 12, Time: 0.01686s, Loss: 1.06937\n",
      "Epoch: 13, Time: 0.01804s, Loss: 1.07050\n",
      "Epoch: 14, Time: 0.01849s, Loss: 1.06823\n",
      "update best: 0.22684\n",
      "Epoch: 15, Time: 0.02724s, Loss: 1.06642\n",
      "update best: 0.22810\n",
      "Epoch: 16, Time: 0.04121s, Loss: 1.06693\n",
      "update best: 0.22842\n",
      "Epoch: 17, Time: 0.01856s, Loss: 1.06795\n",
      "update best: 0.22873\n",
      "Epoch: 18, Time: 0.01636s, Loss: 1.06691\n",
      "update best: 0.23031\n",
      "Epoch: 19, Time: 0.01900s, Loss: 1.06681\n",
      "update best: 0.23125\n",
      "Epoch: 20, Time: 0.01937s, Loss: 1.06605\n",
      "update best: 0.23346\n",
      "Epoch: 21, Time: 0.02448s, Loss: 1.06690\n",
      "Epoch: 22, Time: 0.01854s, Loss: 1.06663\n",
      "update best: 0.23377\n",
      "Epoch: 23, Time: 0.02143s, Loss: 1.06689\n",
      "Epoch: 24, Time: 0.02080s, Loss: 1.06694\n",
      "Epoch: 25, Time: 0.01814s, Loss: 1.06539\n",
      "update best: 0.23440\n",
      "Epoch: 26, Time: 0.02194s, Loss: 1.07002\n",
      "Epoch: 27, Time: 0.02151s, Loss: 1.06522\n",
      "Epoch: 28, Time: 0.02571s, Loss: 1.06616\n",
      "update best: 0.23693\n",
      "Epoch: 29, Time: 0.02481s, Loss: 1.06519\n",
      "update best: 0.23913\n",
      "Epoch: 30, Time: 0.01903s, Loss: 1.06896\n",
      "update best: 0.23976\n",
      "Epoch: 31, Time: 0.01825s, Loss: 1.06508\n",
      "update best: 0.24228\n",
      "Epoch: 32, Time: 0.01837s, Loss: 1.06613\n",
      "update best: 0.24354\n",
      "Epoch: 33, Time: 0.02131s, Loss: 1.06547\n",
      "Epoch: 34, Time: 0.01762s, Loss: 1.06507\n",
      "Epoch: 35, Time: 0.02063s, Loss: 1.06493\n",
      "Epoch: 36, Time: 0.02128s, Loss: 1.06490\n",
      "Epoch: 37, Time: 0.01768s, Loss: 1.06516\n",
      "Epoch: 38, Time: 0.01795s, Loss: 1.06532\n",
      "Epoch: 39, Time: 0.01812s, Loss: 1.06980\n",
      "Epoch: 40, Time: 0.01674s, Loss: 1.06490\n",
      "Epoch: 41, Time: 0.01593s, Loss: 1.06702\n",
      "Epoch: 42, Time: 0.01489s, Loss: 1.06530\n",
      "Epoch: 43, Time: 0.01657s, Loss: 1.06540\n",
      "Epoch: 44, Time: 0.01704s, Loss: 1.06647\n",
      "update best: 0.24417\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06484\n",
      "update best: 0.24543\n",
      "Epoch: 46, Time: 0.01660s, Loss: 1.06728\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.01782s, Loss: 1.06481\n",
      "Epoch: 48, Time: 0.01757s, Loss: 1.06632\n",
      "Epoch: 49, Time: 0.01485s, Loss: 1.06521\n",
      "Epoch: 50, Time: 0.01675s, Loss: 1.06575\n",
      "Epoch: 51, Time: 0.01786s, Loss: 1.06525\n",
      "Epoch: 52, Time: 0.02277s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.02166s, Loss: 1.06507\n",
      "Epoch: 54, Time: 0.01743s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01731s, Loss: 1.06621\n",
      "Epoch: 56, Time: 0.01820s, Loss: 1.06738\n",
      "Epoch: 57, Time: 0.01791s, Loss: 1.06510\n",
      "Epoch: 58, Time: 0.02247s, Loss: 1.06537\n",
      "Epoch: 59, Time: 0.02010s, Loss: 1.06517\n",
      "Epoch: 60, Time: 0.01746s, Loss: 1.06534\n",
      "Epoch: 61, Time: 0.01992s, Loss: 1.06506\n",
      "Epoch: 62, Time: 0.01787s, Loss: 1.06508\n",
      "update best: 0.24764\n",
      "Epoch: 63, Time: 0.02259s, Loss: 1.06570\n",
      "update best: 0.24953\n",
      "Epoch: 64, Time: 0.01247s, Loss: 1.06505\n",
      "update best: 0.25142\n",
      "Epoch: 65, Time: 0.01587s, Loss: 1.06507\n",
      "Epoch: 66, Time: 0.01652s, Loss: 1.06519\n",
      "update best: 0.25268\n",
      "Epoch: 67, Time: 0.01768s, Loss: 1.06502\n",
      "update best: 0.25299\n",
      "Epoch: 68, Time: 0.01546s, Loss: 1.06523\n",
      "update best: 0.25331\n",
      "Epoch: 69, Time: 0.01801s, Loss: 1.07305\n",
      "update best: 0.25551\n",
      "Epoch: 70, Time: 0.01831s, Loss: 1.06549\n",
      "update best: 0.25583\n",
      "Epoch: 71, Time: 0.01908s, Loss: 1.06533\n",
      "Epoch: 72, Time: 0.02021s, Loss: 1.06593\n",
      "Epoch: 73, Time: 0.01627s, Loss: 1.06515\n",
      "Epoch: 74, Time: 0.01677s, Loss: 1.06505\n",
      "Epoch: 75, Time: 0.01870s, Loss: 1.06534\n",
      "update best: 0.25677\n",
      "Epoch: 76, Time: 0.01737s, Loss: 1.06526\n",
      "update best: 0.25992\n",
      "Epoch: 77, Time: 0.01953s, Loss: 1.06741\n",
      "update best: 0.26654\n",
      "Epoch: 78, Time: 0.02144s, Loss: 1.06646\n",
      "update best: 0.26906\n",
      "Epoch: 79, Time: 0.02197s, Loss: 1.06549\n",
      "update best: 0.27095\n",
      "Epoch: 80, Time: 0.02261s, Loss: 1.06540\n",
      "update best: 0.27190\n",
      "Epoch: 81, Time: 0.02229s, Loss: 1.06554\n",
      "Epoch: 82, Time: 0.02355s, Loss: 1.06607\n",
      "update best: 0.27473\n",
      "Epoch: 83, Time: 0.01814s, Loss: 1.06520\n",
      "Epoch: 84, Time: 0.01883s, Loss: 1.06574\n",
      "update best: 0.27505\n",
      "Epoch: 85, Time: 0.01675s, Loss: 1.06542\n",
      "update best: 0.27662\n",
      "Epoch: 86, Time: 0.01655s, Loss: 1.06497\n",
      "update best: 0.27788\n",
      "Epoch: 87, Time: 0.01687s, Loss: 1.06536\n",
      "update best: 0.27820\n",
      "Epoch: 88, Time: 0.01637s, Loss: 1.06712\n",
      "update best: 0.27851\n",
      "Epoch: 89, Time: 0.01623s, Loss: 1.06507\n",
      "Epoch: 90, Time: 0.01593s, Loss: 1.06662\n",
      "Epoch: 91, Time: 0.01839s, Loss: 1.06555\n",
      "Epoch: 92, Time: 0.01371s, Loss: 1.06724\n",
      "Epoch: 93, Time: 0.01482s, Loss: 1.06553\n",
      "update best: 0.28040\n",
      "Epoch: 94, Time: 0.01767s, Loss: 1.06508\n",
      "Epoch: 95, Time: 0.01703s, Loss: 1.06521\n",
      "update best: 0.28166\n",
      "Epoch: 96, Time: 0.01775s, Loss: 1.06502\n",
      "update best: 0.28261\n",
      "Epoch: 97, Time: 0.01963s, Loss: 1.06545\n",
      "update best: 0.28387\n",
      "Epoch: 98, Time: 0.02233s, Loss: 1.06513\n",
      "update best: 0.28481\n",
      "Epoch: 99, Time: 0.01748s, Loss: 1.06547\n",
      "Epoch: 100, Time: 0.01556s, Loss: 1.06497\n",
      "Epoch: 101, Time: 0.01438s, Loss: 1.06506\n",
      "update best: 0.28544\n",
      "Epoch: 102, Time: 0.01760s, Loss: 1.06503\n",
      "update best: 0.28639\n",
      "Epoch: 103, Time: 0.01655s, Loss: 1.06542\n",
      "update best: 0.28702\n",
      "Epoch: 104, Time: 0.01242s, Loss: 1.06585\n",
      "Epoch: 105, Time: 0.01357s, Loss: 1.06535\n",
      "Epoch: 106, Time: 0.01611s, Loss: 1.06541\n",
      "Epoch: 107, Time: 0.01720s, Loss: 1.06517\n",
      "Epoch: 108, Time: 0.01844s, Loss: 1.06508\n",
      "Epoch: 109, Time: 0.01629s, Loss: 1.06523\n",
      "Epoch: 110, Time: 0.01776s, Loss: 1.06669\n",
      "Epoch: 111, Time: 0.02685s, Loss: 1.06504\n",
      "Epoch: 112, Time: 0.01608s, Loss: 1.06830\n",
      "Epoch: 113, Time: 0.02013s, Loss: 1.06510\n",
      "Epoch: 114, Time: 0.01823s, Loss: 1.06535\n",
      "Epoch: 115, Time: 0.01609s, Loss: 1.06590\n",
      "Epoch: 116, Time: 0.01215s, Loss: 1.06531\n",
      "Epoch: 117, Time: 0.01296s, Loss: 1.06569\n",
      "Epoch: 118, Time: 0.01771s, Loss: 1.06568\n",
      "Epoch: 119, Time: 0.02355s, Loss: 1.06620\n",
      "Epoch: 120, Time: 0.01563s, Loss: 1.06539\n",
      "Epoch: 121, Time: 0.01465s, Loss: 1.06579\n",
      "Epoch: 122, Time: 0.01447s, Loss: 1.06534\n",
      "Epoch: 123, Time: 0.01752s, Loss: 1.06539\n",
      "Epoch: 124, Time: 0.01862s, Loss: 1.06516\n",
      "Epoch: 125, Time: 0.01149s, Loss: 1.06508\n",
      "update best: 0.28733\n",
      "Epoch: 126, Time: 0.01664s, Loss: 1.06511\n",
      "update best: 0.28922\n",
      "Epoch: 127, Time: 0.01757s, Loss: 1.06536\n",
      "update best: 0.29017\n",
      "Epoch: 128, Time: 0.01645s, Loss: 1.06550\n",
      "update best: 0.29080\n",
      "Epoch: 129, Time: 0.01800s, Loss: 1.06510\n",
      "update best: 0.29238\n",
      "Epoch: 130, Time: 0.01650s, Loss: 1.06519\n",
      "Epoch: 131, Time: 0.02048s, Loss: 1.06574\n",
      "Epoch: 132, Time: 0.01997s, Loss: 1.06540\n",
      "Epoch: 133, Time: 0.02081s, Loss: 1.06570\n",
      "Epoch: 134, Time: 0.01810s, Loss: 1.06556\n",
      "Epoch: 135, Time: 0.01953s, Loss: 1.06589\n",
      "update best: 0.29364\n",
      "Epoch: 136, Time: 0.01920s, Loss: 1.06544\n",
      "Epoch: 137, Time: 0.02303s, Loss: 1.06539\n",
      "Epoch: 138, Time: 0.01508s, Loss: 1.06505\n",
      "Epoch: 139, Time: 0.01842s, Loss: 1.06553\n",
      "Epoch: 140, Time: 0.02009s, Loss: 1.06553\n",
      "Epoch: 141, Time: 0.02001s, Loss: 1.06507\n",
      "Epoch: 142, Time: 0.02050s, Loss: 1.06608\n",
      "Epoch: 143, Time: 0.02168s, Loss: 1.06573\n",
      "Epoch: 144, Time: 0.01856s, Loss: 1.06533\n",
      "Epoch: 145, Time: 0.01972s, Loss: 1.06542\n",
      "Epoch: 146, Time: 0.01655s, Loss: 1.06536\n",
      "Epoch: 147, Time: 0.01834s, Loss: 1.06544\n",
      "Epoch: 148, Time: 0.01313s, Loss: 1.06630\n",
      "Epoch: 149, Time: 0.01821s, Loss: 1.06515\n",
      "Epoch: 150, Time: 0.01853s, Loss: 1.06549\n",
      "Epoch: 151, Time: 0.01937s, Loss: 1.06519\n",
      "Epoch: 152, Time: 0.01679s, Loss: 1.06495\n",
      "Epoch: 153, Time: 0.01791s, Loss: 1.06549\n",
      "Epoch: 154, Time: 0.01480s, Loss: 1.06536\n",
      "Epoch: 155, Time: 0.01520s, Loss: 1.06516\n",
      "Epoch: 156, Time: 0.01627s, Loss: 1.06595\n",
      "Epoch: 157, Time: 0.01555s, Loss: 1.06553\n",
      "Epoch: 158, Time: 0.02305s, Loss: 1.06554\n",
      "Epoch: 159, Time: 0.01351s, Loss: 1.06529\n",
      "Epoch: 160, Time: 0.01414s, Loss: 1.06528\n",
      "Epoch: 161, Time: 0.01765s, Loss: 1.06544\n",
      "Epoch: 162, Time: 0.01710s, Loss: 1.06587\n",
      "Epoch: 163, Time: 0.02061s, Loss: 1.06593\n",
      "Epoch: 164, Time: 0.02033s, Loss: 1.06537\n",
      "Epoch: 165, Time: 0.01972s, Loss: 1.06553\n",
      "Epoch: 166, Time: 0.01385s, Loss: 1.06530\n",
      "Epoch: 167, Time: 0.01608s, Loss: 1.06550\n",
      "Epoch: 168, Time: 0.01672s, Loss: 1.06533\n",
      "Epoch: 169, Time: 0.01599s, Loss: 1.06530\n",
      "Epoch: 170, Time: 0.01611s, Loss: 1.06539\n",
      "Epoch: 171, Time: 0.01605s, Loss: 1.06524\n",
      "Epoch: 172, Time: 0.01934s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01833s, Loss: 1.06556\n",
      "Epoch: 174, Time: 0.01524s, Loss: 1.06522\n",
      "update best: 0.29490\n",
      "Epoch: 175, Time: 0.01827s, Loss: 1.06555\n",
      "update best: 0.29521\n",
      "Epoch: 176, Time: 0.01649s, Loss: 1.06508\n",
      "Epoch: 177, Time: 0.01646s, Loss: 1.06528\n",
      "update best: 0.29647\n",
      "Epoch: 178, Time: 0.02095s, Loss: 1.06525\n",
      "update best: 0.29805\n",
      "Epoch: 179, Time: 0.01807s, Loss: 1.06549\n",
      "Epoch: 180, Time: 0.01872s, Loss: 1.06556\n",
      "Epoch: 181, Time: 0.01824s, Loss: 1.06508\n",
      "Epoch: 182, Time: 0.01404s, Loss: 1.06543\n",
      "Epoch: 183, Time: 0.01711s, Loss: 1.06532\n",
      "Epoch: 184, Time: 0.01564s, Loss: 1.06961\n",
      "Epoch: 185, Time: 0.01837s, Loss: 1.06526\n",
      "Epoch: 186, Time: 0.01663s, Loss: 1.06531\n",
      "Epoch: 187, Time: 0.00980s, Loss: 1.06604\n",
      "Epoch: 188, Time: 0.01678s, Loss: 1.06544\n",
      "Epoch: 189, Time: 0.01867s, Loss: 1.06573\n",
      "Epoch: 190, Time: 0.01688s, Loss: 1.06559\n",
      "Epoch: 191, Time: 0.01976s, Loss: 1.06660\n",
      "Epoch: 192, Time: 0.01817s, Loss: 1.06627\n",
      "Epoch: 193, Time: 0.02570s, Loss: 1.06583\n",
      "Epoch: 194, Time: 0.01776s, Loss: 1.06552\n",
      "Epoch: 195, Time: 0.01549s, Loss: 1.06540\n",
      "Epoch: 196, Time: 0.01602s, Loss: 1.06679\n",
      "Epoch: 197, Time: 0.01553s, Loss: 1.06629\n",
      "Epoch: 198, Time: 0.01786s, Loss: 1.06716\n",
      "Epoch: 199, Time: 0.01856s, Loss: 1.06667\n",
      "\n",
      "train finished!\n",
      "best val: 0.29805\n",
      "test...\n",
      "final result: epoch: 178\n",
      "{'accuracy': 0.29804661870002747, 'f1_score': 0.21974182550479207, 'f1_score -> average@micro': 0.2980466288594833}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.04270s, Loss: 1.82284\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02071s, Loss: 1.51886\n",
      "Epoch: 2, Time: 0.02244s, Loss: 1.30142\n",
      "Epoch: 3, Time: 0.01422s, Loss: 1.19327\n",
      "Epoch: 4, Time: 0.01827s, Loss: 1.14901\n",
      "update best: 0.21456\n",
      "Epoch: 5, Time: 0.01704s, Loss: 1.11603\n",
      "update best: 0.21582\n",
      "Epoch: 6, Time: 0.01643s, Loss: 1.09219\n",
      "update best: 0.21676\n",
      "Epoch: 7, Time: 0.01922s, Loss: 1.08688\n",
      "Epoch: 8, Time: 0.02138s, Loss: 1.08081\n",
      "update best: 0.21802\n",
      "Epoch: 9, Time: 0.02001s, Loss: 1.07927\n",
      "update best: 0.21991\n",
      "Epoch: 10, Time: 0.02502s, Loss: 1.07121\n",
      "update best: 0.22401\n",
      "Epoch: 11, Time: 0.01725s, Loss: 1.07070\n",
      "update best: 0.22842\n",
      "Epoch: 12, Time: 0.01808s, Loss: 1.06910\n",
      "update best: 0.23220\n",
      "Epoch: 13, Time: 0.01767s, Loss: 1.06739\n",
      "update best: 0.23756\n",
      "Epoch: 14, Time: 0.01468s, Loss: 1.06867\n",
      "update best: 0.24228\n",
      "Epoch: 15, Time: 0.02318s, Loss: 1.06965\n",
      "update best: 0.24890\n",
      "Epoch: 16, Time: 0.01846s, Loss: 1.06565\n",
      "update best: 0.25488\n",
      "Epoch: 17, Time: 0.02093s, Loss: 1.06624\n",
      "update best: 0.26024\n",
      "Epoch: 18, Time: 0.01909s, Loss: 1.06569\n",
      "update best: 0.26591\n",
      "Epoch: 19, Time: 0.01976s, Loss: 1.06635\n",
      "update best: 0.26938\n",
      "Epoch: 20, Time: 0.02251s, Loss: 1.06535\n",
      "update best: 0.27347\n",
      "Epoch: 21, Time: 0.01153s, Loss: 1.06856\n",
      "update best: 0.27788\n",
      "Epoch: 22, Time: 0.01665s, Loss: 1.07035\n",
      "update best: 0.28135\n",
      "Epoch: 23, Time: 0.02305s, Loss: 1.06538\n",
      "update best: 0.28670\n",
      "Epoch: 24, Time: 0.02006s, Loss: 1.06527\n",
      "update best: 0.28859\n",
      "Epoch: 25, Time: 0.02235s, Loss: 1.06552\n",
      "update best: 0.29049\n",
      "Epoch: 26, Time: 0.01630s, Loss: 1.06530\n",
      "update best: 0.29080\n",
      "Epoch: 27, Time: 0.02152s, Loss: 1.06492\n",
      "update best: 0.29112\n",
      "Epoch: 28, Time: 0.01894s, Loss: 1.06538\n",
      "update best: 0.29301\n",
      "Epoch: 29, Time: 0.02661s, Loss: 1.06589\n",
      "update best: 0.29553\n",
      "Epoch: 30, Time: 0.02075s, Loss: 1.06627\n",
      "Epoch: 31, Time: 0.02027s, Loss: 1.06568\n",
      "Epoch: 32, Time: 0.01624s, Loss: 1.06576\n",
      "update best: 0.29584\n",
      "Epoch: 33, Time: 0.01920s, Loss: 1.06508\n",
      "update best: 0.29647\n",
      "Epoch: 34, Time: 0.01991s, Loss: 1.06536\n",
      "update best: 0.29679\n",
      "Epoch: 35, Time: 0.01840s, Loss: 1.06501\n",
      "update best: 0.29710\n",
      "Epoch: 36, Time: 0.01724s, Loss: 1.06524\n",
      "Epoch: 37, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 38, Time: 0.02036s, Loss: 1.06499\n",
      "Epoch: 39, Time: 0.01830s, Loss: 1.06507\n",
      "Epoch: 40, Time: 0.01822s, Loss: 1.06540\n",
      "update best: 0.29805\n",
      "Epoch: 41, Time: 0.01940s, Loss: 1.06509\n",
      "Epoch: 42, Time: 0.01928s, Loss: 1.06522\n",
      "update best: 0.29836\n",
      "Epoch: 43, Time: 0.01889s, Loss: 1.06523\n",
      "Epoch: 44, Time: 0.02267s, Loss: 1.06530\n",
      "update best: 0.29931\n",
      "Epoch: 45, Time: 0.02758s, Loss: 1.06553\n",
      "Epoch: 46, Time: 0.02273s, Loss: 1.06594\n",
      "update best: 0.30183\n",
      "Epoch: 47, Time: 0.02789s, Loss: 1.06519\n",
      "update best: 0.30372\n",
      "Epoch: 48, Time: 0.01263s, Loss: 1.06524\n",
      "update best: 0.30466\n",
      "Epoch: 49, Time: 0.01633s, Loss: 1.06493\n",
      "update best: 0.30592\n",
      "Epoch: 50, Time: 0.02007s, Loss: 1.06548\n",
      "Epoch: 51, Time: 0.01813s, Loss: 1.06498\n",
      "Epoch: 52, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.01938s, Loss: 1.06489\n",
      "update best: 0.30718\n",
      "Epoch: 54, Time: 0.01763s, Loss: 1.06500\n",
      "update best: 0.30876\n",
      "Epoch: 55, Time: 0.01670s, Loss: 1.06570\n",
      "update best: 0.31128\n",
      "Epoch: 56, Time: 0.01785s, Loss: 1.06502\n",
      "Epoch: 57, Time: 0.01928s, Loss: 1.06531\n",
      "Epoch: 58, Time: 0.02257s, Loss: 1.06599\n",
      "update best: 0.31159\n",
      "Epoch: 59, Time: 0.02586s, Loss: 1.06529\n",
      "Epoch: 60, Time: 0.02166s, Loss: 1.06604\n",
      "Epoch: 61, Time: 0.01866s, Loss: 1.06585\n",
      "Epoch: 62, Time: 0.02232s, Loss: 1.06490\n",
      "Epoch: 63, Time: 0.01852s, Loss: 1.06505\n",
      "Epoch: 64, Time: 0.02313s, Loss: 1.06505\n",
      "Epoch: 65, Time: 0.01801s, Loss: 1.06512\n",
      "Epoch: 66, Time: 0.02064s, Loss: 1.06598\n",
      "Epoch: 67, Time: 0.02058s, Loss: 1.06529\n",
      "Epoch: 68, Time: 0.01511s, Loss: 1.06910\n",
      "update best: 0.31191\n",
      "Epoch: 69, Time: 0.02213s, Loss: 1.06611\n",
      "update best: 0.31317\n",
      "Epoch: 70, Time: 0.02055s, Loss: 1.06589\n",
      "update best: 0.31411\n",
      "Epoch: 71, Time: 0.01592s, Loss: 1.06512\n",
      "Epoch: 72, Time: 0.02295s, Loss: 1.06520\n",
      "Epoch: 73, Time: 0.02415s, Loss: 1.06497\n",
      "Epoch: 74, Time: 0.01714s, Loss: 1.06511\n",
      "Epoch: 75, Time: 0.01442s, Loss: 1.06539\n",
      "Epoch: 76, Time: 0.02115s, Loss: 1.06501\n",
      "Epoch: 77, Time: 0.01995s, Loss: 1.06772\n",
      "Epoch: 78, Time: 0.01706s, Loss: 1.06505\n",
      "Epoch: 79, Time: 0.01946s, Loss: 1.06511\n",
      "Epoch: 80, Time: 0.01968s, Loss: 1.06532\n",
      "Epoch: 81, Time: 0.02007s, Loss: 1.06583\n",
      "Epoch: 82, Time: 0.01847s, Loss: 1.06540\n",
      "Epoch: 83, Time: 0.01677s, Loss: 1.06522\n",
      "Epoch: 84, Time: 0.01916s, Loss: 1.06552\n",
      "Epoch: 85, Time: 0.01701s, Loss: 1.06588\n",
      "Epoch: 86, Time: 0.01758s, Loss: 1.06530\n",
      "Epoch: 87, Time: 0.01681s, Loss: 1.06513\n",
      "Epoch: 88, Time: 0.01884s, Loss: 1.06695\n",
      "Epoch: 89, Time: 0.01809s, Loss: 1.06502\n",
      "Epoch: 90, Time: 0.01902s, Loss: 1.06542\n",
      "Epoch: 91, Time: 0.01726s, Loss: 1.06501\n",
      "Epoch: 92, Time: 0.01917s, Loss: 1.07111\n",
      "Epoch: 93, Time: 0.02157s, Loss: 1.06501\n",
      "Epoch: 94, Time: 0.01932s, Loss: 1.06579\n",
      "Epoch: 95, Time: 0.01742s, Loss: 1.06544\n",
      "Epoch: 96, Time: 0.01689s, Loss: 1.06592\n",
      "Epoch: 97, Time: 0.02184s, Loss: 1.06546\n",
      "Epoch: 98, Time: 0.01851s, Loss: 1.06559\n",
      "Epoch: 99, Time: 0.01974s, Loss: 1.06655\n",
      "Epoch: 100, Time: 0.02153s, Loss: 1.06613\n",
      "Epoch: 101, Time: 0.02100s, Loss: 1.06529\n",
      "Epoch: 102, Time: 0.02183s, Loss: 1.06513\n",
      "update best: 0.31537\n",
      "Epoch: 103, Time: 0.01861s, Loss: 1.06602\n",
      "update best: 0.31664\n",
      "Epoch: 104, Time: 0.02366s, Loss: 1.06506\n",
      "Epoch: 105, Time: 0.02303s, Loss: 1.06543\n",
      "Epoch: 106, Time: 0.01787s, Loss: 1.06549\n",
      "Epoch: 107, Time: 0.02056s, Loss: 1.06499\n",
      "Epoch: 108, Time: 0.02198s, Loss: 1.06506\n",
      "Epoch: 109, Time: 0.02249s, Loss: 1.06522\n",
      "Epoch: 110, Time: 0.02321s, Loss: 1.06508\n",
      "Epoch: 111, Time: 0.02333s, Loss: 1.06503\n",
      "Epoch: 112, Time: 0.02006s, Loss: 1.06550\n",
      "Epoch: 113, Time: 0.01862s, Loss: 1.06498\n",
      "Epoch: 114, Time: 0.01844s, Loss: 1.06520\n",
      "Epoch: 115, Time: 0.01950s, Loss: 1.06557\n",
      "Epoch: 116, Time: 0.01550s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.01858s, Loss: 1.06584\n",
      "Epoch: 118, Time: 0.01633s, Loss: 1.06530\n",
      "Epoch: 119, Time: 0.02066s, Loss: 1.06533\n",
      "Epoch: 120, Time: 0.01966s, Loss: 1.06518\n",
      "Epoch: 121, Time: 0.01675s, Loss: 1.06587\n",
      "Epoch: 122, Time: 0.02211s, Loss: 1.06538\n",
      "Epoch: 123, Time: 0.01883s, Loss: 1.06525\n",
      "Epoch: 124, Time: 0.01462s, Loss: 1.06792\n",
      "Epoch: 125, Time: 0.01905s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.02171s, Loss: 1.06519\n",
      "Epoch: 127, Time: 0.01829s, Loss: 1.06529\n",
      "Epoch: 128, Time: 0.01958s, Loss: 1.06525\n",
      "Epoch: 129, Time: 0.01915s, Loss: 1.06577\n",
      "Epoch: 130, Time: 0.02259s, Loss: 1.06504\n",
      "Epoch: 131, Time: 0.01623s, Loss: 1.06562\n",
      "Epoch: 132, Time: 0.01713s, Loss: 1.06536\n",
      "Epoch: 133, Time: 0.01912s, Loss: 1.06614\n",
      "Epoch: 134, Time: 0.01647s, Loss: 1.06519\n",
      "Epoch: 135, Time: 0.01784s, Loss: 1.06531\n",
      "Epoch: 136, Time: 0.01811s, Loss: 1.06508\n",
      "Epoch: 137, Time: 0.01808s, Loss: 1.06545\n",
      "Epoch: 138, Time: 0.01983s, Loss: 1.06556\n",
      "Epoch: 139, Time: 0.01699s, Loss: 1.06502\n",
      "Epoch: 140, Time: 0.01636s, Loss: 1.06519\n",
      "Epoch: 141, Time: 0.01850s, Loss: 1.06617\n",
      "Epoch: 142, Time: 0.01634s, Loss: 1.06517\n",
      "Epoch: 143, Time: 0.01739s, Loss: 1.06538\n",
      "Epoch: 144, Time: 0.01833s, Loss: 1.06525\n",
      "Epoch: 145, Time: 0.01783s, Loss: 1.06514\n",
      "Epoch: 146, Time: 0.01796s, Loss: 1.06501\n",
      "Epoch: 147, Time: 0.01856s, Loss: 1.06551\n",
      "Epoch: 148, Time: 0.02472s, Loss: 1.06531\n",
      "Epoch: 149, Time: 0.01940s, Loss: 1.06562\n",
      "Epoch: 150, Time: 0.01549s, Loss: 1.06684\n",
      "Epoch: 151, Time: 0.01839s, Loss: 1.06553\n",
      "Epoch: 152, Time: 0.02012s, Loss: 1.06618\n",
      "Epoch: 153, Time: 0.02237s, Loss: 1.06527\n",
      "Epoch: 154, Time: 0.01830s, Loss: 1.07220\n",
      "Epoch: 155, Time: 0.02025s, Loss: 1.06542\n",
      "Epoch: 156, Time: 0.02134s, Loss: 1.06580\n",
      "Epoch: 157, Time: 0.01560s, Loss: 1.06539\n",
      "Epoch: 158, Time: 0.01388s, Loss: 1.06653\n",
      "Epoch: 159, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 160, Time: 0.01732s, Loss: 1.06567\n",
      "Epoch: 161, Time: 0.01975s, Loss: 1.06511\n",
      "Epoch: 162, Time: 0.01835s, Loss: 1.06567\n",
      "Epoch: 163, Time: 0.01690s, Loss: 1.06527\n",
      "Epoch: 164, Time: 0.01715s, Loss: 1.06890\n",
      "Epoch: 165, Time: 0.02312s, Loss: 1.06546\n",
      "Epoch: 166, Time: 0.01980s, Loss: 1.06614\n",
      "Epoch: 167, Time: 0.02019s, Loss: 1.06497\n",
      "Epoch: 168, Time: 0.01731s, Loss: 1.06529\n",
      "Epoch: 169, Time: 0.01955s, Loss: 1.06562\n",
      "Epoch: 170, Time: 0.01743s, Loss: 1.06597\n",
      "Epoch: 171, Time: 0.01865s, Loss: 1.06532\n",
      "Epoch: 172, Time: 0.01997s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01628s, Loss: 1.06550\n",
      "Epoch: 174, Time: 0.01769s, Loss: 1.06514\n",
      "Epoch: 175, Time: 0.01837s, Loss: 1.06553\n",
      "Epoch: 176, Time: 0.02044s, Loss: 1.06574\n",
      "Epoch: 177, Time: 0.02251s, Loss: 1.06569\n",
      "Epoch: 178, Time: 0.01868s, Loss: 1.06546\n",
      "Epoch: 179, Time: 0.02551s, Loss: 1.06544\n",
      "Epoch: 180, Time: 0.01607s, Loss: 1.06540\n",
      "Epoch: 181, Time: 0.01928s, Loss: 1.06541\n",
      "Epoch: 182, Time: 0.02239s, Loss: 1.06517\n",
      "Epoch: 183, Time: 0.02016s, Loss: 1.06519\n",
      "Epoch: 184, Time: 0.02079s, Loss: 1.06513\n",
      "Epoch: 185, Time: 0.02155s, Loss: 1.06497\n",
      "Epoch: 186, Time: 0.01768s, Loss: 1.06551\n",
      "Epoch: 187, Time: 0.02082s, Loss: 1.06646\n",
      "Epoch: 188, Time: 0.01630s, Loss: 1.06507\n",
      "Epoch: 189, Time: 0.02140s, Loss: 1.06555\n",
      "Epoch: 190, Time: 0.01823s, Loss: 1.06513\n",
      "Epoch: 191, Time: 0.02309s, Loss: 1.06557\n",
      "Epoch: 192, Time: 0.01628s, Loss: 1.06538\n",
      "Epoch: 193, Time: 0.01901s, Loss: 1.06559\n",
      "Epoch: 194, Time: 0.02195s, Loss: 1.06503\n",
      "Epoch: 195, Time: 0.01769s, Loss: 1.06567\n",
      "Epoch: 196, Time: 0.01739s, Loss: 1.06650\n",
      "Epoch: 197, Time: 0.01997s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01885s, Loss: 1.06514\n",
      "Epoch: 199, Time: 0.01803s, Loss: 1.06532\n",
      "\n",
      "train finished!\n",
      "best val: 0.31664\n",
      "test...\n",
      "final result: epoch: 103\n",
      "{'accuracy': 0.3166351616382599, 'f1_score': 0.24460102376042017, 'f1_score -> average@micro': 0.3166351606805293}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhg_datastets = {\n",
    "    \"HouseCommittees\": HouseCommittees(data_path),\n",
    "    \"Cooking200\": Cooking200(data_path),\n",
    "    \"CocitationCiteseer\": CocitationCiteseer(data_path),\n",
    "}\n",
    "\n",
    "# for name, dataset in dhg_datastets.items():\n",
    "#     if not 'train_mask' in dataset.content:\n",
    "#         train_mask, test_mask, val_mask = split_by_ratio(\n",
    "#             num_v = dataset[\"num_vertices\"],\n",
    "#             v_label = dataset[\"labels\"],\n",
    "#             train_ratio = 0.6,\n",
    "#             test_ratio = 0.2,\n",
    "#             val_ratio = 0.2\n",
    "#             )\n",
    "\n",
    "#     dataset._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "train_mask, test_mask, val_mask = split_by_ratio(\n",
    "            num_v = dhg_datastets[\"HouseCommittees\"][\"num_vertices\"],\n",
    "            v_label = dhg_datastets[\"HouseCommittees\"][\"labels\"],\n",
    "            train_ratio = 0.6,\n",
    "            test_ratio = 0.2,\n",
    "            val_ratio = 0.2\n",
    "            )\n",
    "\n",
    "dhg_datastets[\"HouseCommittees\"]._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_densest_subgraphs(hg: Hypergraph, k: int = 3) -> list:\n",
    "    \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "    nodes = set(range(hg.num_v))\n",
    "    subgraphs = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if len(nodes) < 3:  # min_size\n",
    "            break\n",
    "            \n",
    "        current_nodes = set(nodes)\n",
    "        best_subset = None\n",
    "        best_density = -1\n",
    "        \n",
    "        while len(current_nodes) >= 3:\n",
    "            edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "            density = edge_count / len(current_nodes)\n",
    "            \n",
    "            if density > best_density:\n",
    "                best_density = density\n",
    "                best_subset = set(current_nodes)\n",
    "            \n",
    "            # Remove node with lowest degree\n",
    "            degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "            node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "            current_nodes.remove(node_to_remove)\n",
    "        \n",
    "        if best_subset:\n",
    "            subgraphs.append((best_subset, best_density))\n",
    "            nodes -= best_subset\n",
    "    \n",
    "    return subgraphs\n",
    "\n",
    "def preprocess_hypergraph_with_topk(hg: Hypergraph, k: int = 3) -> Hypergraph:\n",
    "    \"\"\"Preprocess hypergraph by focusing on top-k densest subgraphs\"\"\"\n",
    "    subgraphs = get_top_k_densest_subgraphs(hg, k)\n",
    "    if not subgraphs:\n",
    "        return hg\n",
    "    \n",
    "    # Combine all nodes from top-k subgraphs\n",
    "    important_nodes = set()\n",
    "    for subset, _ in subgraphs:\n",
    "        important_nodes.update(subset)\n",
    "    \n",
    "    # Create new hypergraph with only important edges\n",
    "    new_edges = []\n",
    "    for e in hg.e[0]:\n",
    "        if set(e).issubset(important_nodes):\n",
    "            new_edges.append(e)\n",
    "    \n",
    "    return Hypergraph(hg.num_v, new_edges)\n",
    "\n",
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res\n",
    "\n",
    "def run_experiment(data, use_topk=False, k=3):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    \n",
    "    if use_topk:\n",
    "        print(f\"Using Top-{k} Densest Subgraphs preprocessing\")\n",
    "        G = preprocess_hypergraph_with_topk(G, k)\n",
    "    else:\n",
    "        print(\"Using original hypergraph (no Top-k preprocessing)\")\n",
    "    \n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    test_mask = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch, best_val = 0, 0\n",
    "    for epoch in range(200):\n",
    "        # train\n",
    "        train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(net, X, G, lbl, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_epoch = epoch\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HouseCommittees 1290 341\n",
      "tensor([ True, False, False])\n",
      "Cooking200 7403 2755\n",
      "tensor([False, False, False])\n",
      "CocitationCiteseer 3312 1079\n",
      "tensor([False, False, False])\n"
     ]
    }
   ],
   "source": [
    "for name, dataset in dhg_datastets.items():\n",
    "    print(name, dataset[\"num_vertices\"], dataset[\"num_edges\"])\n",
    "    print(dataset[\"train_mask\"][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhg_experiments = {}\n",
    "for name, dataset in dhg_datastets.items():\n",
    "    print(f\"Running experiments on {name} dataset\")\n",
    "    original_result = run_experiment(dataset, use_topk=False)\n",
    "    print(f\"Running experiments on {name} dataset with Top-k densest subgraphs preprocessing...\")\n",
    "    topk_result = run_experiment(dataset, use_topk=True, k=3)\n",
    "    \n",
    "    dhg_experiments[name] = {\n",
    "        \"original_result\": original_result,\n",
    "        \"topk_result\": topk_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run experiments\n",
    "# print(\"=== Baseline Experiment (No Top-k) ===\")\n",
    "# baseline_results = run_experiment(use_topk=False)\n",
    "\n",
    "# print(\"\\n=== Experiment with Top-k Densest Subgraphs ===\")\n",
    "# topk_results = run_experiment(use_topk=True, k=3)\n",
    "\n",
    "# # Compare results\n",
    "# print(\"\\n=== Comparison of Results ===\")\n",
    "# print(f\"Baseline performance: {baseline_results}\")\n",
    "# print(f\"Top-k performance: {topk_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Updated version of run_experiment to work with train/test index\n",
    "def run_experiment_fold(data, train_idx, test_idx, use_topk=False, k=3):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "\n",
    "    if use_topk:\n",
    "        G = preprocess_hypergraph_with_topk(G, k)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "\n",
    "    num_classes = data[\"num_classes\"]\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    models = {\n",
    "        \"HGNN\": HGNN(input_dim, 64, num_classes),\n",
    "        \"HGNNP\": HGNNP(input_dim, 64, num_classes, use_bn=True),\n",
    "        \"UniGCN\": UniGCN(input_dim, 64, num_classes, use_bn=True),\n",
    "    }\n",
    "\n",
    "    train_idx = torch.tensor(train_idx, dtype=torch.long, device=device)\n",
    "    test_idx = torch.tensor(test_idx, dtype=torch.long, device=device)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, net in models.items():\n",
    "        print(f\"  → Training model: {model_name}\")\n",
    "        net = net.to(device)\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        best_state, best_val = None, 0\n",
    "\n",
    "        for epoch in range(100):  # Modify if you want longer training\n",
    "            train(net, X, G, lbl, train_idx, optimizer, epoch)\n",
    "            val_res = infer(net, X, G, lbl, test_idx)\n",
    "            if val_res > best_val:\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "\n",
    "        # Load best weights and evaluate\n",
    "        net.load_state_dict(best_state)\n",
    "        pred_logits = net(X, G)[test_idx]\n",
    "        y_true = lbl[test_idx].cpu().numpy()\n",
    "        y_pred = torch.argmax(pred_logits, dim=1).cpu().numpy()\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "        print(f\"    ↳ {model_name} Fold Result — Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "        results[model_name] = (acc, f1)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Main experiment loop with cross-validation\n",
    "def run_cross_validation(data, dataset_name, use_topk=False, k=3, n_splits=5):\n",
    "    labels = data[\"labels\"].numpy()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = defaultdict(lambda: {\"accuracy\": [], \"f1\": []})\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "        print(f\"\\n[{dataset_name}] Fold {fold + 1}/{n_splits} {'(Top-k)' if use_topk else '(Original)'}\")\n",
    "        results = run_experiment_fold(data, train_idx, test_idx, use_topk, k)\n",
    "\n",
    "        for model_name, (acc, f1) in results.items():\n",
    "            fold_results[model_name][\"accuracy\"].append(acc)\n",
    "            fold_results[model_name][\"f1\"].append(f1)\n",
    "\n",
    "    return fold_results\n",
    "\n",
    "\n",
    "\n",
    "# Plotting function\n",
    "def plot_results(results_dict):\n",
    "    fig, ax = plt.subplots()\n",
    "    for label, scores in results_dict.items():\n",
    "        accs, f1s = scores[\"accuracy\"], scores[\"f1\"]\n",
    "        ax.errorbar(\n",
    "            range(len(accs)), accs, yerr=np.std(accs),\n",
    "            label=f'{label} Acc (mean={np.mean(accs):.3f})', fmt='-o'\n",
    "        )\n",
    "        ax.errorbar(\n",
    "            range(len(f1s)), f1s, yerr=np.std(f1s),\n",
    "            label=f'{label} F1 (mean={np.mean(f1s):.3f})', fmt='-s'\n",
    "        )\n",
    "    ax.set_xlabel(\"Fold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Cross-Validation Results\")\n",
    "    ax.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Running HouseCommittees without Top-k\n",
      "\n",
      "[HouseCommittees] Fold 1/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.23451s, Loss: 1.14359\n",
      "Epoch: 1, Time: 0.07071s, Loss: 1.12440\n",
      "Epoch: 2, Time: 0.06552s, Loss: 1.10567\n",
      "Epoch: 3, Time: 0.08195s, Loss: 1.08194\n",
      "Epoch: 4, Time: 0.06634s, Loss: 1.05557\n",
      "Epoch: 5, Time: 0.06366s, Loss: 1.02410\n",
      "Epoch: 6, Time: 0.06832s, Loss: 0.99154\n",
      "Epoch: 7, Time: 0.06903s, Loss: 0.95628\n",
      "Epoch: 8, Time: 0.06287s, Loss: 0.92484\n",
      "Epoch: 9, Time: 0.06743s, Loss: 0.89437\n",
      "Epoch: 10, Time: 0.06697s, Loss: 0.86566\n",
      "Epoch: 11, Time: 0.06461s, Loss: 0.84183\n",
      "Epoch: 12, Time: 0.06876s, Loss: 0.81883\n",
      "Epoch: 13, Time: 0.07359s, Loss: 0.80158\n",
      "Epoch: 14, Time: 0.06121s, Loss: 0.78698\n",
      "Epoch: 15, Time: 0.06413s, Loss: 0.77292\n",
      "Epoch: 16, Time: 0.06989s, Loss: 0.76298\n",
      "Epoch: 17, Time: 0.07866s, Loss: 0.75429\n",
      "Epoch: 18, Time: 0.08056s, Loss: 0.74777\n",
      "Epoch: 19, Time: 0.07762s, Loss: 0.74069\n",
      "Epoch: 20, Time: 0.07550s, Loss: 0.73535\n",
      "Epoch: 21, Time: 0.08325s, Loss: 0.72989\n",
      "Epoch: 22, Time: 0.08356s, Loss: 0.72588\n",
      "Epoch: 23, Time: 0.08877s, Loss: 0.72190\n",
      "Epoch: 24, Time: 0.08082s, Loss: 0.71895\n",
      "Epoch: 25, Time: 0.08140s, Loss: 0.71581\n",
      "Epoch: 26, Time: 0.07557s, Loss: 0.71374\n",
      "Epoch: 27, Time: 0.08463s, Loss: 0.71110\n",
      "Epoch: 28, Time: 0.07722s, Loss: 0.70921\n",
      "Epoch: 29, Time: 0.07986s, Loss: 0.70773\n",
      "Epoch: 30, Time: 0.06834s, Loss: 0.70623\n",
      "Epoch: 31, Time: 0.07913s, Loss: 0.70494\n",
      "Epoch: 32, Time: 0.06923s, Loss: 0.70315\n",
      "Epoch: 33, Time: 0.07808s, Loss: 0.70267\n",
      "Epoch: 34, Time: 0.06510s, Loss: 0.70174\n",
      "Epoch: 35, Time: 0.07110s, Loss: 0.70036\n",
      "Epoch: 36, Time: 0.07688s, Loss: 0.69965\n",
      "Epoch: 37, Time: 0.06336s, Loss: 0.69934\n",
      "Epoch: 38, Time: 0.06506s, Loss: 0.69852\n",
      "Epoch: 39, Time: 0.07500s, Loss: 0.69791\n",
      "Epoch: 40, Time: 0.07299s, Loss: 0.69730\n",
      "Epoch: 41, Time: 0.06623s, Loss: 0.69698\n",
      "Epoch: 42, Time: 0.07669s, Loss: 0.69692\n",
      "Epoch: 43, Time: 0.07168s, Loss: 0.69667\n",
      "Epoch: 44, Time: 0.07485s, Loss: 0.69618\n",
      "Epoch: 45, Time: 0.07490s, Loss: 0.69618\n",
      "Epoch: 46, Time: 0.08323s, Loss: 0.69591\n",
      "Epoch: 47, Time: 0.07762s, Loss: 0.69565\n",
      "Epoch: 48, Time: 0.08703s, Loss: 0.69551\n",
      "Epoch: 49, Time: 0.07832s, Loss: 0.69572\n",
      "Epoch: 50, Time: 0.07975s, Loss: 0.69514\n",
      "Epoch: 51, Time: 0.07431s, Loss: 0.69503\n",
      "Epoch: 52, Time: 0.09064s, Loss: 0.69517\n",
      "Epoch: 53, Time: 0.08113s, Loss: 0.69440\n",
      "Epoch: 54, Time: 0.06551s, Loss: 0.69484\n",
      "Epoch: 55, Time: 0.07240s, Loss: 0.69459\n",
      "Epoch: 56, Time: 0.06203s, Loss: 0.69457\n",
      "Epoch: 57, Time: 0.05893s, Loss: 0.69459\n",
      "Epoch: 58, Time: 0.06881s, Loss: 0.69508\n",
      "Epoch: 59, Time: 0.07315s, Loss: 0.69454\n",
      "Epoch: 60, Time: 0.07645s, Loss: 0.69400\n",
      "Epoch: 61, Time: 0.06956s, Loss: 0.69421\n",
      "Epoch: 62, Time: 0.07122s, Loss: 0.69478\n",
      "Epoch: 63, Time: 0.07468s, Loss: 0.69368\n",
      "Epoch: 64, Time: 0.06835s, Loss: 0.69461\n",
      "Epoch: 65, Time: 0.06525s, Loss: 0.69409\n",
      "Epoch: 66, Time: 0.08408s, Loss: 0.69368\n",
      "Epoch: 67, Time: 0.07800s, Loss: 0.69392\n",
      "Epoch: 68, Time: 0.07973s, Loss: 0.69400\n",
      "Epoch: 69, Time: 0.07839s, Loss: 0.69434\n",
      "Epoch: 70, Time: 0.07680s, Loss: 0.69448\n",
      "Epoch: 71, Time: 0.09502s, Loss: 0.69410\n",
      "Epoch: 72, Time: 0.09211s, Loss: 0.69435\n",
      "Epoch: 73, Time: 0.08045s, Loss: 0.69395\n",
      "Epoch: 74, Time: 0.08804s, Loss: 0.69414\n",
      "Epoch: 75, Time: 0.07859s, Loss: 0.69395\n",
      "Epoch: 76, Time: 0.09437s, Loss: 0.69368\n",
      "Epoch: 77, Time: 0.08701s, Loss: 0.69378\n",
      "Epoch: 78, Time: 0.06968s, Loss: 0.69373\n",
      "Epoch: 79, Time: 0.06792s, Loss: 0.69344\n",
      "Epoch: 80, Time: 0.06912s, Loss: 0.69336\n",
      "Epoch: 81, Time: 0.07218s, Loss: 0.69407\n",
      "Epoch: 82, Time: 0.07256s, Loss: 0.69388\n",
      "Epoch: 83, Time: 0.07927s, Loss: 0.69401\n",
      "Epoch: 84, Time: 0.07154s, Loss: 0.69433\n",
      "Epoch: 85, Time: 0.06663s, Loss: 0.69388\n",
      "Epoch: 86, Time: 0.07758s, Loss: 0.69331\n",
      "Epoch: 87, Time: 0.06900s, Loss: 0.69352\n",
      "Epoch: 88, Time: 0.06236s, Loss: 0.69346\n",
      "Epoch: 89, Time: 0.06612s, Loss: 0.69372\n",
      "Epoch: 90, Time: 0.07308s, Loss: 0.69389\n",
      "Epoch: 91, Time: 0.08874s, Loss: 0.69343\n",
      "Epoch: 92, Time: 0.08762s, Loss: 0.69325\n",
      "Epoch: 93, Time: 0.09288s, Loss: 0.69342\n",
      "Epoch: 94, Time: 0.09466s, Loss: 0.69353\n",
      "Epoch: 95, Time: 0.09269s, Loss: 0.69375\n",
      "Epoch: 96, Time: 0.08329s, Loss: 0.69342\n",
      "Epoch: 97, Time: 0.09112s, Loss: 0.69349\n",
      "Epoch: 98, Time: 0.08022s, Loss: 0.69366\n",
      "Epoch: 99, Time: 0.08668s, Loss: 0.69292\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.01237s, Loss: 1.07431\n",
      "Epoch: 1, Time: 0.01926s, Loss: 1.00849\n",
      "Epoch: 2, Time: 0.01459s, Loss: 0.94757\n",
      "Epoch: 3, Time: 0.01726s, Loss: 0.89403\n",
      "Epoch: 4, Time: 0.01782s, Loss: 0.85320\n",
      "Epoch: 5, Time: 0.01625s, Loss: 0.82302\n",
      "Epoch: 6, Time: 0.01694s, Loss: 0.78643\n",
      "Epoch: 7, Time: 0.02051s, Loss: 0.76332\n",
      "Epoch: 8, Time: 0.01222s, Loss: 0.73747\n",
      "Epoch: 9, Time: 0.01850s, Loss: 0.71903\n",
      "Epoch: 10, Time: 0.01045s, Loss: 0.69824\n",
      "Epoch: 11, Time: 0.01517s, Loss: 0.68357\n",
      "Epoch: 12, Time: 0.01238s, Loss: 0.67575\n",
      "Epoch: 13, Time: 0.01163s, Loss: 0.66452\n",
      "Epoch: 14, Time: 0.01038s, Loss: 0.65698\n",
      "Epoch: 15, Time: 0.01153s, Loss: 0.65145\n",
      "Epoch: 16, Time: 0.01327s, Loss: 0.64607\n",
      "Epoch: 17, Time: 0.01614s, Loss: 0.64046\n",
      "Epoch: 18, Time: 0.01213s, Loss: 0.63679\n",
      "Epoch: 19, Time: 0.01405s, Loss: 0.63534\n",
      "Epoch: 20, Time: 0.01315s, Loss: 0.63173\n",
      "Epoch: 21, Time: 0.01124s, Loss: 0.63005\n",
      "Epoch: 22, Time: 0.02159s, Loss: 0.62970\n",
      "Epoch: 23, Time: 0.01065s, Loss: 0.62734\n",
      "Epoch: 24, Time: 0.01326s, Loss: 0.62511\n",
      "Epoch: 25, Time: 0.01259s, Loss: 0.62553\n",
      "Epoch: 26, Time: 0.01531s, Loss: 0.62223\n",
      "Epoch: 27, Time: 0.01195s, Loss: 0.62378\n",
      "Epoch: 28, Time: 0.01003s, Loss: 0.62045\n",
      "Epoch: 29, Time: 0.01186s, Loss: 0.62011\n",
      "Epoch: 30, Time: 0.00810s, Loss: 0.61986\n",
      "Epoch: 31, Time: 0.01221s, Loss: 0.61953\n",
      "Epoch: 32, Time: 0.00727s, Loss: 0.61845\n",
      "Epoch: 33, Time: 0.01523s, Loss: 0.61760\n",
      "Epoch: 34, Time: 0.01365s, Loss: 0.61606\n",
      "Epoch: 35, Time: 0.01759s, Loss: 0.61588\n",
      "Epoch: 36, Time: 0.00729s, Loss: 0.61336\n",
      "Epoch: 37, Time: 0.01306s, Loss: 0.61652\n",
      "Epoch: 38, Time: 0.01204s, Loss: 0.61298\n",
      "Epoch: 39, Time: 0.01508s, Loss: 0.61414\n",
      "Epoch: 40, Time: 0.01310s, Loss: 0.61231\n",
      "Epoch: 41, Time: 0.01626s, Loss: 0.61230\n",
      "Epoch: 42, Time: 0.01652s, Loss: 0.61086\n",
      "Epoch: 43, Time: 0.01532s, Loss: 0.61145\n",
      "Epoch: 44, Time: 0.01639s, Loss: 0.60816\n",
      "Epoch: 45, Time: 0.01212s, Loss: 0.61633\n",
      "Epoch: 46, Time: 0.01208s, Loss: 0.61525\n",
      "Epoch: 47, Time: 0.01229s, Loss: 0.61689\n",
      "Epoch: 48, Time: 0.01394s, Loss: 0.61074\n",
      "Epoch: 49, Time: 0.01331s, Loss: 0.62998\n",
      "Epoch: 50, Time: 0.01359s, Loss: 0.61242\n",
      "Epoch: 51, Time: 0.01660s, Loss: 0.61358\n",
      "Epoch: 52, Time: 0.00811s, Loss: 0.61430\n",
      "Epoch: 53, Time: 0.01676s, Loss: 0.61273\n",
      "Epoch: 54, Time: 0.01259s, Loss: 0.61700\n",
      "Epoch: 55, Time: 0.01126s, Loss: 0.61235\n",
      "Epoch: 56, Time: 0.01555s, Loss: 0.61897\n",
      "Epoch: 57, Time: 0.01667s, Loss: 0.61290\n",
      "Epoch: 58, Time: 0.01534s, Loss: 0.62469\n",
      "Epoch: 59, Time: 0.01023s, Loss: 0.60529\n",
      "Epoch: 60, Time: 0.01643s, Loss: 0.61508\n",
      "Epoch: 61, Time: 0.01385s, Loss: 0.61065\n",
      "Epoch: 62, Time: 0.01104s, Loss: 0.61127\n",
      "Epoch: 63, Time: 0.01339s, Loss: 0.61006\n",
      "Epoch: 64, Time: 0.00915s, Loss: 0.60584\n",
      "Epoch: 65, Time: 0.01544s, Loss: 0.60998\n",
      "Epoch: 66, Time: 0.02038s, Loss: 0.60862\n",
      "Epoch: 67, Time: 0.01558s, Loss: 0.60307\n",
      "Epoch: 68, Time: 0.01217s, Loss: 0.61003\n",
      "Epoch: 69, Time: 0.01109s, Loss: 0.60524\n",
      "Epoch: 70, Time: 0.01332s, Loss: 0.60466\n",
      "Epoch: 71, Time: 0.01543s, Loss: 0.60390\n",
      "Epoch: 72, Time: 0.01541s, Loss: 0.60610\n",
      "Epoch: 73, Time: 0.01227s, Loss: 0.60346\n",
      "Epoch: 74, Time: 0.01753s, Loss: 0.60446\n",
      "Epoch: 75, Time: 0.03359s, Loss: 0.60347\n",
      "Epoch: 76, Time: 0.02548s, Loss: 0.60413\n",
      "Epoch: 77, Time: 0.01979s, Loss: 0.60743\n",
      "Epoch: 78, Time: 0.01918s, Loss: 0.60858\n",
      "Epoch: 79, Time: 0.01745s, Loss: 0.60398\n",
      "Epoch: 80, Time: 0.01840s, Loss: 0.60107\n",
      "Epoch: 81, Time: 0.01600s, Loss: 0.60128\n",
      "Epoch: 82, Time: 0.01619s, Loss: 0.60193\n",
      "Epoch: 83, Time: 0.01700s, Loss: 0.60363\n",
      "Epoch: 84, Time: 0.02047s, Loss: 0.60063\n",
      "Epoch: 85, Time: 0.01500s, Loss: 0.60176\n",
      "Epoch: 86, Time: 0.01823s, Loss: 0.60002\n",
      "Epoch: 87, Time: 0.02105s, Loss: 0.59995\n",
      "Epoch: 88, Time: 0.01935s, Loss: 0.60317\n",
      "Epoch: 89, Time: 0.02223s, Loss: 0.59871\n",
      "Epoch: 90, Time: 0.02071s, Loss: 0.60029\n",
      "Epoch: 91, Time: 0.02106s, Loss: 0.60258\n",
      "Epoch: 92, Time: 0.01956s, Loss: 0.60350\n",
      "Epoch: 93, Time: 0.02013s, Loss: 0.60352\n",
      "Epoch: 94, Time: 0.01816s, Loss: 0.60149\n",
      "Epoch: 95, Time: 0.01969s, Loss: 0.60176\n",
      "Epoch: 96, Time: 0.02012s, Loss: 0.60678\n",
      "Epoch: 97, Time: 0.01985s, Loss: 0.60619\n",
      "Epoch: 98, Time: 0.01639s, Loss: 0.60429\n",
      "Epoch: 99, Time: 0.01623s, Loss: 0.60289\n",
      "    ↳ HGNNP Fold Result — Acc: 0.6008, F1: 0.6008\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.01015s, Loss: 1.11105\n",
      "Epoch: 1, Time: 0.01339s, Loss: 0.91987\n",
      "Epoch: 2, Time: 0.01638s, Loss: 0.84844\n",
      "Epoch: 3, Time: 0.01466s, Loss: 0.81838\n",
      "Epoch: 4, Time: 0.01387s, Loss: 0.77895\n",
      "Epoch: 5, Time: 0.01497s, Loss: 0.75910\n",
      "Epoch: 6, Time: 0.01423s, Loss: 0.73660\n",
      "Epoch: 7, Time: 0.01867s, Loss: 0.72450\n",
      "Epoch: 8, Time: 0.01021s, Loss: 0.71411\n",
      "Epoch: 9, Time: 0.01298s, Loss: 0.70354\n",
      "Epoch: 10, Time: 0.01305s, Loss: 0.69705\n",
      "Epoch: 11, Time: 0.01389s, Loss: 0.69038\n",
      "Epoch: 12, Time: 0.01669s, Loss: 0.68599\n",
      "Epoch: 13, Time: 0.01333s, Loss: 0.68235\n",
      "Epoch: 14, Time: 0.01629s, Loss: 0.67662\n",
      "Epoch: 15, Time: 0.01872s, Loss: 0.67276\n",
      "Epoch: 16, Time: 0.01775s, Loss: 0.67086\n",
      "Epoch: 17, Time: 0.01712s, Loss: 0.66835\n",
      "Epoch: 18, Time: 0.01455s, Loss: 0.66501\n",
      "Epoch: 19, Time: 0.01613s, Loss: 0.66375\n",
      "Epoch: 20, Time: 0.01521s, Loss: 0.66119\n",
      "Epoch: 21, Time: 0.01532s, Loss: 0.65910\n",
      "Epoch: 22, Time: 0.01412s, Loss: 0.66013\n",
      "Epoch: 23, Time: 0.01463s, Loss: 0.66153\n",
      "Epoch: 24, Time: 0.01155s, Loss: 0.65565\n",
      "Epoch: 25, Time: 0.01878s, Loss: 0.65998\n",
      "Epoch: 26, Time: 0.01547s, Loss: 0.65442\n",
      "Epoch: 27, Time: 0.01881s, Loss: 0.65798\n",
      "Epoch: 28, Time: 0.01812s, Loss: 0.65318\n",
      "Epoch: 29, Time: 0.01070s, Loss: 0.65692\n",
      "Epoch: 30, Time: 0.01241s, Loss: 0.65304\n",
      "Epoch: 31, Time: 0.01506s, Loss: 0.65493\n",
      "Epoch: 32, Time: 0.01875s, Loss: 0.65405\n",
      "Epoch: 33, Time: 0.01445s, Loss: 0.65029\n",
      "Epoch: 34, Time: 0.01557s, Loss: 0.65496\n",
      "Epoch: 35, Time: 0.01243s, Loss: 0.65189\n",
      "Epoch: 36, Time: 0.01785s, Loss: 0.65386\n",
      "Epoch: 37, Time: 0.01962s, Loss: 0.64779\n",
      "Epoch: 38, Time: 0.01781s, Loss: 0.64809\n",
      "Epoch: 39, Time: 0.01909s, Loss: 0.65069\n",
      "Epoch: 40, Time: 0.01468s, Loss: 0.64800\n",
      "Epoch: 41, Time: 0.01527s, Loss: 0.64798\n",
      "Epoch: 42, Time: 0.01417s, Loss: 0.64378\n",
      "Epoch: 43, Time: 0.01314s, Loss: 0.64381\n",
      "Epoch: 44, Time: 0.01206s, Loss: 0.64270\n",
      "Epoch: 45, Time: 0.01423s, Loss: 0.64410\n",
      "Epoch: 46, Time: 0.01933s, Loss: 0.64457\n",
      "Epoch: 47, Time: 0.01571s, Loss: 0.64455\n",
      "Epoch: 48, Time: 0.01528s, Loss: 0.64279\n",
      "Epoch: 49, Time: 0.01637s, Loss: 0.64735\n",
      "Epoch: 50, Time: 0.01530s, Loss: 0.64259\n",
      "Epoch: 51, Time: 0.01717s, Loss: 0.64942\n",
      "Epoch: 52, Time: 0.01611s, Loss: 0.64335\n",
      "Epoch: 53, Time: 0.01655s, Loss: 0.64527\n",
      "Epoch: 54, Time: 0.01188s, Loss: 0.63773\n",
      "Epoch: 55, Time: 0.00912s, Loss: 0.64490\n",
      "Epoch: 56, Time: 0.01974s, Loss: 0.64351\n",
      "Epoch: 57, Time: 0.01402s, Loss: 0.65162\n",
      "Epoch: 58, Time: 0.01840s, Loss: 0.64133\n",
      "Epoch: 59, Time: 0.01670s, Loss: 0.66347\n",
      "Epoch: 60, Time: 0.01440s, Loss: 0.64452\n",
      "Epoch: 61, Time: 0.01430s, Loss: 0.64418\n",
      "Epoch: 62, Time: 0.01542s, Loss: 0.64700\n",
      "Epoch: 63, Time: 0.01789s, Loss: 0.64427\n",
      "Epoch: 64, Time: 0.01778s, Loss: 0.64003\n",
      "Epoch: 65, Time: 0.02395s, Loss: 0.64720\n",
      "Epoch: 66, Time: 0.01400s, Loss: 0.63700\n",
      "Epoch: 67, Time: 0.01086s, Loss: 0.63773\n",
      "Epoch: 68, Time: 0.01720s, Loss: 0.64340\n",
      "Epoch: 69, Time: 0.01313s, Loss: 0.63667\n",
      "Epoch: 70, Time: 0.01405s, Loss: 0.64151\n",
      "Epoch: 71, Time: 0.01512s, Loss: 0.63805\n",
      "Epoch: 72, Time: 0.01858s, Loss: 0.63937\n",
      "Epoch: 73, Time: 0.01235s, Loss: 0.63785\n",
      "Epoch: 74, Time: 0.01194s, Loss: 0.63855\n",
      "Epoch: 75, Time: 0.01302s, Loss: 0.63758\n",
      "Epoch: 76, Time: 0.01326s, Loss: 0.63878\n",
      "Epoch: 77, Time: 0.01529s, Loss: 0.63612\n",
      "Epoch: 78, Time: 0.01103s, Loss: 0.63846\n",
      "Epoch: 79, Time: 0.01561s, Loss: 0.63488\n",
      "Epoch: 80, Time: 0.00935s, Loss: 0.63789\n",
      "Epoch: 81, Time: 0.01204s, Loss: 0.63363\n",
      "Epoch: 82, Time: 0.01869s, Loss: 0.63604\n",
      "Epoch: 83, Time: 0.01662s, Loss: 0.63378\n",
      "Epoch: 84, Time: 0.01567s, Loss: 0.63639\n",
      "Epoch: 85, Time: 0.00939s, Loss: 0.63647\n",
      "Epoch: 86, Time: 0.02094s, Loss: 0.63295\n",
      "Epoch: 87, Time: 0.01093s, Loss: 0.64178\n",
      "Epoch: 88, Time: 0.01288s, Loss: 0.64283\n",
      "Epoch: 89, Time: 0.01259s, Loss: 0.63483\n",
      "Epoch: 90, Time: 0.01424s, Loss: 0.64222\n",
      "Epoch: 91, Time: 0.01011s, Loss: 0.62900\n",
      "Epoch: 92, Time: 0.01317s, Loss: 0.63479\n",
      "Epoch: 93, Time: 0.01321s, Loss: 0.62996\n",
      "Epoch: 94, Time: 0.01322s, Loss: 0.63255\n",
      "Epoch: 95, Time: 0.01670s, Loss: 0.63862\n",
      "Epoch: 96, Time: 0.01614s, Loss: 0.63163\n",
      "Epoch: 97, Time: 0.01786s, Loss: 0.63711\n",
      "Epoch: 98, Time: 0.01210s, Loss: 0.63234\n",
      "Epoch: 99, Time: 0.01506s, Loss: 0.64923\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5891, F1: 0.5891\n",
      "\n",
      "[HouseCommittees] Fold 2/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.09295s, Loss: 1.10246\n",
      "Epoch: 1, Time: 0.08912s, Loss: 1.08249\n",
      "Epoch: 2, Time: 0.09571s, Loss: 1.06343\n",
      "Epoch: 3, Time: 0.07585s, Loss: 1.04080\n",
      "Epoch: 4, Time: 0.08267s, Loss: 1.01513\n",
      "Epoch: 5, Time: 0.07207s, Loss: 0.98656\n",
      "Epoch: 6, Time: 0.08391s, Loss: 0.95715\n",
      "Epoch: 7, Time: 0.06938s, Loss: 0.92724\n",
      "Epoch: 8, Time: 0.07162s, Loss: 0.89809\n",
      "Epoch: 9, Time: 0.07593s, Loss: 0.87052\n",
      "Epoch: 10, Time: 0.07110s, Loss: 0.84779\n",
      "Epoch: 11, Time: 0.08689s, Loss: 0.82552\n",
      "Epoch: 12, Time: 0.06687s, Loss: 0.80844\n",
      "Epoch: 13, Time: 0.08203s, Loss: 0.79390\n",
      "Epoch: 14, Time: 0.08016s, Loss: 0.77994\n",
      "Epoch: 15, Time: 0.07250s, Loss: 0.77008\n",
      "Epoch: 16, Time: 0.07454s, Loss: 0.75917\n",
      "Epoch: 17, Time: 0.08014s, Loss: 0.75175\n",
      "Epoch: 18, Time: 0.17827s, Loss: 0.74368\n",
      "Epoch: 19, Time: 0.18031s, Loss: 0.73851\n",
      "Epoch: 20, Time: 0.16642s, Loss: 0.73354\n",
      "Epoch: 21, Time: 0.17676s, Loss: 0.72960\n",
      "Epoch: 22, Time: 0.16417s, Loss: 0.72591\n",
      "Epoch: 23, Time: 0.17646s, Loss: 0.72216\n",
      "Epoch: 24, Time: 0.16677s, Loss: 0.71926\n",
      "Epoch: 25, Time: 0.17245s, Loss: 0.71569\n",
      "Epoch: 26, Time: 0.17194s, Loss: 0.71331\n",
      "Epoch: 27, Time: 0.18308s, Loss: 0.71115\n",
      "Epoch: 28, Time: 0.17175s, Loss: 0.70935\n",
      "Epoch: 29, Time: 0.15654s, Loss: 0.70753\n",
      "Epoch: 30, Time: 0.17812s, Loss: 0.70621\n",
      "Epoch: 31, Time: 0.16192s, Loss: 0.70478\n",
      "Epoch: 32, Time: 0.15955s, Loss: 0.70392\n",
      "Epoch: 33, Time: 0.17787s, Loss: 0.70248\n",
      "Epoch: 34, Time: 0.18467s, Loss: 0.70176\n",
      "Epoch: 35, Time: 0.17487s, Loss: 0.70039\n",
      "Epoch: 36, Time: 0.17249s, Loss: 0.70050\n",
      "Epoch: 37, Time: 0.18233s, Loss: 0.69967\n",
      "Epoch: 38, Time: 0.17052s, Loss: 0.69908\n",
      "Epoch: 39, Time: 0.18004s, Loss: 0.69821\n",
      "Epoch: 40, Time: 0.19391s, Loss: 0.69742\n",
      "Epoch: 41, Time: 0.17069s, Loss: 0.69706\n",
      "Epoch: 42, Time: 0.18048s, Loss: 0.69739\n",
      "Epoch: 43, Time: 0.17089s, Loss: 0.69662\n",
      "Epoch: 44, Time: 0.17731s, Loss: 0.69649\n",
      "Epoch: 45, Time: 0.21647s, Loss: 0.69580\n",
      "Epoch: 46, Time: 0.17728s, Loss: 0.69570\n",
      "Epoch: 47, Time: 0.16803s, Loss: 0.69573\n",
      "Epoch: 48, Time: 0.18570s, Loss: 0.69519\n",
      "Epoch: 49, Time: 0.17022s, Loss: 0.69539\n",
      "Epoch: 50, Time: 0.18283s, Loss: 0.69541\n",
      "Epoch: 51, Time: 0.18597s, Loss: 0.69438\n",
      "Epoch: 52, Time: 0.17308s, Loss: 0.69530\n",
      "Epoch: 53, Time: 0.18735s, Loss: 0.69482\n",
      "Epoch: 54, Time: 0.18736s, Loss: 0.69476\n",
      "Epoch: 55, Time: 0.20304s, Loss: 0.69478\n",
      "Epoch: 56, Time: 0.20076s, Loss: 0.69437\n",
      "Epoch: 57, Time: 0.17705s, Loss: 0.69417\n",
      "Epoch: 58, Time: 0.34982s, Loss: 0.69400\n",
      "Epoch: 59, Time: 0.18029s, Loss: 0.69406\n",
      "Epoch: 60, Time: 0.17544s, Loss: 0.69410\n",
      "Epoch: 61, Time: 0.16233s, Loss: 0.69443\n",
      "Epoch: 62, Time: 0.16692s, Loss: 0.69437\n",
      "Epoch: 63, Time: 0.18447s, Loss: 0.69379\n",
      "Epoch: 64, Time: 0.10616s, Loss: 0.69359\n",
      "Epoch: 65, Time: 0.17601s, Loss: 0.69384\n",
      "Epoch: 66, Time: 0.18038s, Loss: 0.69351\n",
      "Epoch: 67, Time: 0.17585s, Loss: 0.69382\n",
      "Epoch: 68, Time: 0.17452s, Loss: 0.69297\n",
      "Epoch: 69, Time: 0.16680s, Loss: 0.69376\n",
      "Epoch: 70, Time: 0.17342s, Loss: 0.69383\n",
      "Epoch: 71, Time: 0.17700s, Loss: 0.69289\n",
      "Epoch: 72, Time: 0.17660s, Loss: 0.69386\n",
      "Epoch: 73, Time: 0.15298s, Loss: 0.69385\n",
      "Epoch: 74, Time: 0.17851s, Loss: 0.69364\n",
      "Epoch: 75, Time: 0.14410s, Loss: 0.69396\n",
      "Epoch: 76, Time: 0.16760s, Loss: 0.69354\n",
      "Epoch: 77, Time: 0.17640s, Loss: 0.69292\n",
      "Epoch: 78, Time: 0.16456s, Loss: 0.69355\n",
      "Epoch: 79, Time: 0.16071s, Loss: 0.69306\n",
      "Epoch: 80, Time: 0.17480s, Loss: 0.69273\n",
      "Epoch: 81, Time: 0.13620s, Loss: 0.69266\n",
      "Epoch: 82, Time: 0.12445s, Loss: 0.69354\n",
      "Epoch: 83, Time: 0.14623s, Loss: 0.69260\n",
      "Epoch: 84, Time: 0.10232s, Loss: 0.69298\n",
      "Epoch: 85, Time: 0.13272s, Loss: 0.69330\n",
      "Epoch: 86, Time: 0.11250s, Loss: 0.69305\n",
      "Epoch: 87, Time: 0.11598s, Loss: 0.69258\n",
      "Epoch: 88, Time: 0.15723s, Loss: 0.69265\n",
      "Epoch: 89, Time: 0.18941s, Loss: 0.69246\n",
      "Epoch: 90, Time: 0.09067s, Loss: 0.69255\n",
      "Epoch: 91, Time: 0.13602s, Loss: 0.69223\n",
      "Epoch: 92, Time: 0.14991s, Loss: 0.69277\n",
      "Epoch: 93, Time: 0.17662s, Loss: 0.69196\n",
      "Epoch: 94, Time: 0.18375s, Loss: 0.69231\n",
      "Epoch: 95, Time: 0.10369s, Loss: 0.69232\n",
      "Epoch: 96, Time: 0.18392s, Loss: 0.69201\n",
      "Epoch: 97, Time: 0.09674s, Loss: 0.69223\n",
      "Epoch: 98, Time: 0.15344s, Loss: 0.69129\n",
      "Epoch: 99, Time: 0.16544s, Loss: 0.69238\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.02766s, Loss: 1.11675\n",
      "Epoch: 1, Time: 0.03603s, Loss: 1.12664\n",
      "Epoch: 2, Time: 0.02944s, Loss: 0.99861\n",
      "Epoch: 3, Time: 0.03341s, Loss: 0.93660\n",
      "Epoch: 4, Time: 0.02671s, Loss: 0.88389\n",
      "Epoch: 5, Time: 0.02771s, Loss: 0.85359\n",
      "Epoch: 6, Time: 0.02656s, Loss: 0.81060\n",
      "Epoch: 7, Time: 0.03088s, Loss: 0.79148\n",
      "Epoch: 8, Time: 0.02296s, Loss: 0.76051\n",
      "Epoch: 9, Time: 0.02660s, Loss: 0.73912\n",
      "Epoch: 10, Time: 0.02141s, Loss: 0.72366\n",
      "Epoch: 11, Time: 0.02687s, Loss: 0.70872\n",
      "Epoch: 12, Time: 0.02868s, Loss: 0.68935\n",
      "Epoch: 13, Time: 0.03225s, Loss: 0.67937\n",
      "Epoch: 14, Time: 0.03027s, Loss: 0.67094\n",
      "Epoch: 15, Time: 0.02741s, Loss: 0.66355\n",
      "Epoch: 16, Time: 0.02419s, Loss: 0.65617\n",
      "Epoch: 17, Time: 0.02957s, Loss: 0.65048\n",
      "Epoch: 18, Time: 0.02459s, Loss: 0.64887\n",
      "Epoch: 19, Time: 0.02791s, Loss: 0.64537\n",
      "Epoch: 20, Time: 0.02535s, Loss: 0.64114\n",
      "Epoch: 21, Time: 0.02454s, Loss: 0.63860\n",
      "Epoch: 22, Time: 0.03027s, Loss: 0.63641\n",
      "Epoch: 23, Time: 0.02345s, Loss: 0.63301\n",
      "Epoch: 24, Time: 0.04623s, Loss: 0.63479\n",
      "Epoch: 25, Time: 0.05016s, Loss: 0.63084\n",
      "Epoch: 26, Time: 0.03287s, Loss: 0.62960\n",
      "Epoch: 27, Time: 0.05991s, Loss: 0.62814\n",
      "Epoch: 28, Time: 0.04967s, Loss: 0.62659\n",
      "Epoch: 29, Time: 0.04255s, Loss: 0.62539\n",
      "Epoch: 30, Time: 0.03248s, Loss: 0.62314\n",
      "Epoch: 31, Time: 0.03044s, Loss: 0.62540\n",
      "Epoch: 32, Time: 0.02571s, Loss: 0.62071\n",
      "Epoch: 33, Time: 0.02817s, Loss: 0.61984\n",
      "Epoch: 34, Time: 0.02457s, Loss: 0.61989\n",
      "Epoch: 35, Time: 0.02178s, Loss: 0.61798\n",
      "Epoch: 36, Time: 0.02184s, Loss: 0.61776\n",
      "Epoch: 37, Time: 0.02209s, Loss: 0.61564\n",
      "Epoch: 38, Time: 0.02350s, Loss: 0.61416\n",
      "Epoch: 39, Time: 0.02916s, Loss: 0.61544\n",
      "Epoch: 40, Time: 0.03024s, Loss: 0.61829\n",
      "Epoch: 41, Time: 0.02841s, Loss: 0.62159\n",
      "Epoch: 42, Time: 0.03135s, Loss: 0.61169\n",
      "Epoch: 43, Time: 0.02935s, Loss: 0.61493\n",
      "Epoch: 44, Time: 0.02883s, Loss: 0.61347\n",
      "Epoch: 45, Time: 0.02629s, Loss: 0.61290\n",
      "Epoch: 46, Time: 0.03122s, Loss: 0.61198\n",
      "Epoch: 47, Time: 0.03116s, Loss: 0.61067\n",
      "Epoch: 48, Time: 0.03080s, Loss: 0.60849\n",
      "Epoch: 49, Time: 0.02870s, Loss: 0.61270\n",
      "Epoch: 50, Time: 0.02993s, Loss: 0.60899\n",
      "Epoch: 51, Time: 0.02557s, Loss: 0.61107\n",
      "Epoch: 52, Time: 0.03213s, Loss: 0.60824\n",
      "Epoch: 53, Time: 0.03345s, Loss: 0.60923\n",
      "Epoch: 54, Time: 0.03610s, Loss: 0.60841\n",
      "Epoch: 55, Time: 0.02889s, Loss: 0.61315\n",
      "Epoch: 56, Time: 0.03215s, Loss: 0.60930\n",
      "Epoch: 57, Time: 0.02852s, Loss: 0.60265\n",
      "Epoch: 58, Time: 0.03203s, Loss: 0.60738\n",
      "Epoch: 59, Time: 0.02749s, Loss: 0.60485\n",
      "Epoch: 60, Time: 0.01599s, Loss: 0.60483\n",
      "Epoch: 61, Time: 0.01627s, Loss: 0.60378\n",
      "Epoch: 62, Time: 0.02496s, Loss: 0.60472\n",
      "Epoch: 63, Time: 0.02255s, Loss: 0.60508\n",
      "Epoch: 64, Time: 0.02592s, Loss: 0.61349\n",
      "Epoch: 65, Time: 0.02278s, Loss: 0.61168\n",
      "Epoch: 66, Time: 0.03170s, Loss: 0.61836\n",
      "Epoch: 67, Time: 0.05456s, Loss: 0.61038\n",
      "Epoch: 68, Time: 0.03592s, Loss: 0.60587\n",
      "Epoch: 69, Time: 0.03385s, Loss: 0.61617\n",
      "Epoch: 70, Time: 0.02828s, Loss: 0.61000\n",
      "Epoch: 71, Time: 0.02687s, Loss: 0.60993\n",
      "Epoch: 72, Time: 0.02388s, Loss: 0.60928\n",
      "Epoch: 73, Time: 0.03625s, Loss: 0.60781\n",
      "Epoch: 74, Time: 0.02423s, Loss: 0.60775\n",
      "Epoch: 75, Time: 0.02925s, Loss: 0.60788\n",
      "Epoch: 76, Time: 0.02767s, Loss: 0.60281\n",
      "Epoch: 77, Time: 0.02561s, Loss: 0.60117\n",
      "Epoch: 78, Time: 0.05546s, Loss: 0.60285\n",
      "Epoch: 79, Time: 0.03079s, Loss: 0.60037\n",
      "Epoch: 80, Time: 0.03470s, Loss: 0.60177\n",
      "Epoch: 81, Time: 0.02838s, Loss: 0.59831\n",
      "Epoch: 82, Time: 0.04917s, Loss: 0.59991\n",
      "Epoch: 83, Time: 0.03752s, Loss: 0.60135\n",
      "Epoch: 84, Time: 0.02269s, Loss: 0.60745\n",
      "Epoch: 85, Time: 0.03012s, Loss: 0.60487\n",
      "Epoch: 86, Time: 0.03777s, Loss: 0.60153\n",
      "Epoch: 87, Time: 0.03081s, Loss: 0.61122\n",
      "Epoch: 88, Time: 0.02877s, Loss: 0.59874\n",
      "Epoch: 89, Time: 0.03184s, Loss: 0.62163\n",
      "Epoch: 90, Time: 0.03209s, Loss: 0.60393\n",
      "Epoch: 91, Time: 0.03238s, Loss: 0.60134\n",
      "Epoch: 92, Time: 0.02218s, Loss: 0.60794\n",
      "Epoch: 93, Time: 0.03331s, Loss: 0.59881\n",
      "Epoch: 94, Time: 0.03463s, Loss: 0.60497\n",
      "Epoch: 95, Time: 0.02910s, Loss: 0.60260\n",
      "Epoch: 96, Time: 0.02826s, Loss: 0.60165\n",
      "Epoch: 97, Time: 0.02939s, Loss: 0.60219\n",
      "Epoch: 98, Time: 0.03575s, Loss: 0.59660\n",
      "Epoch: 99, Time: 0.03232s, Loss: 0.59928\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5310, F1: 0.5310\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03528s, Loss: 1.14238\n",
      "Epoch: 1, Time: 0.03587s, Loss: 0.98038\n",
      "Epoch: 2, Time: 0.03782s, Loss: 0.88648\n",
      "Epoch: 3, Time: 0.03721s, Loss: 0.84907\n",
      "Epoch: 4, Time: 0.02788s, Loss: 0.79924\n",
      "Epoch: 5, Time: 0.03791s, Loss: 0.78260\n",
      "Epoch: 6, Time: 0.03168s, Loss: 0.75872\n",
      "Epoch: 7, Time: 0.03583s, Loss: 0.74502\n",
      "Epoch: 8, Time: 0.03425s, Loss: 0.72953\n",
      "Epoch: 9, Time: 0.03108s, Loss: 0.72072\n",
      "Epoch: 10, Time: 0.03598s, Loss: 0.71160\n",
      "Epoch: 11, Time: 0.03006s, Loss: 0.70156\n",
      "Epoch: 12, Time: 0.06403s, Loss: 0.69798\n",
      "Epoch: 13, Time: 0.02737s, Loss: 0.69121\n",
      "Epoch: 14, Time: 0.03604s, Loss: 0.68679\n",
      "Epoch: 15, Time: 0.02880s, Loss: 0.68169\n",
      "Epoch: 16, Time: 0.03397s, Loss: 0.67909\n",
      "Epoch: 17, Time: 0.03812s, Loss: 0.67344\n",
      "Epoch: 18, Time: 0.03373s, Loss: 0.67212\n",
      "Epoch: 19, Time: 0.03508s, Loss: 0.67020\n",
      "Epoch: 20, Time: 0.03085s, Loss: 0.66733\n",
      "Epoch: 21, Time: 0.03648s, Loss: 0.66776\n",
      "Epoch: 22, Time: 0.02768s, Loss: 0.66423\n",
      "Epoch: 23, Time: 0.03060s, Loss: 0.66431\n",
      "Epoch: 24, Time: 0.02415s, Loss: 0.66365\n",
      "Epoch: 25, Time: 0.02547s, Loss: 0.66372\n",
      "Epoch: 26, Time: 0.02016s, Loss: 0.65873\n",
      "Epoch: 27, Time: 0.01925s, Loss: 0.66375\n",
      "Epoch: 28, Time: 0.01319s, Loss: 0.65589\n",
      "Epoch: 29, Time: 0.03263s, Loss: 0.65651\n",
      "Epoch: 30, Time: 0.03145s, Loss: 0.65514\n",
      "Epoch: 31, Time: 0.02699s, Loss: 0.65533\n",
      "Epoch: 32, Time: 0.03090s, Loss: 0.65532\n",
      "Epoch: 33, Time: 0.02745s, Loss: 0.65304\n",
      "Epoch: 34, Time: 0.03350s, Loss: 0.65279\n",
      "Epoch: 35, Time: 0.03218s, Loss: 0.65456\n",
      "Epoch: 36, Time: 0.02649s, Loss: 0.65042\n",
      "Epoch: 37, Time: 0.02816s, Loss: 0.65006\n",
      "Epoch: 38, Time: 0.02435s, Loss: 0.64809\n",
      "Epoch: 39, Time: 0.03789s, Loss: 0.64696\n",
      "Epoch: 40, Time: 0.04942s, Loss: 0.64824\n",
      "Epoch: 41, Time: 0.03287s, Loss: 0.64723\n",
      "Epoch: 42, Time: 0.02846s, Loss: 0.64533\n",
      "Epoch: 43, Time: 0.03134s, Loss: 0.64385\n",
      "Epoch: 44, Time: 0.02896s, Loss: 0.64780\n",
      "Epoch: 45, Time: 0.02279s, Loss: 0.64579\n",
      "Epoch: 46, Time: 0.02431s, Loss: 0.64802\n",
      "Epoch: 47, Time: 0.02484s, Loss: 0.64762\n",
      "Epoch: 48, Time: 0.03166s, Loss: 0.65411\n",
      "Epoch: 49, Time: 0.03200s, Loss: 0.64410\n",
      "Epoch: 50, Time: 0.03353s, Loss: 0.64980\n",
      "Epoch: 51, Time: 0.01746s, Loss: 0.65062\n",
      "Epoch: 52, Time: 0.03191s, Loss: 0.64540\n",
      "Epoch: 53, Time: 0.02379s, Loss: 0.64226\n",
      "Epoch: 54, Time: 0.02920s, Loss: 0.64290\n",
      "Epoch: 55, Time: 0.02557s, Loss: 0.64241\n",
      "Epoch: 56, Time: 0.02686s, Loss: 0.64574\n",
      "Epoch: 57, Time: 0.02420s, Loss: 0.64219\n",
      "Epoch: 58, Time: 0.03591s, Loss: 0.65146\n",
      "Epoch: 59, Time: 0.03372s, Loss: 0.64965\n",
      "Epoch: 60, Time: 0.03127s, Loss: 0.65465\n",
      "Epoch: 61, Time: 0.03270s, Loss: 0.65061\n",
      "Epoch: 62, Time: 0.03011s, Loss: 0.64880\n",
      "Epoch: 63, Time: 0.03586s, Loss: 0.64470\n",
      "Epoch: 64, Time: 0.03006s, Loss: 0.65087\n",
      "Epoch: 65, Time: 0.02606s, Loss: 0.65257\n",
      "Epoch: 66, Time: 0.02887s, Loss: 0.64815\n",
      "Epoch: 67, Time: 0.03465s, Loss: 0.64505\n",
      "Epoch: 68, Time: 0.03018s, Loss: 0.64421\n",
      "Epoch: 69, Time: 0.02744s, Loss: 0.64135\n",
      "Epoch: 70, Time: 0.04204s, Loss: 0.64310\n",
      "Epoch: 71, Time: 0.01614s, Loss: 0.63988\n",
      "Epoch: 72, Time: 0.02078s, Loss: 0.64551\n",
      "Epoch: 73, Time: 0.01900s, Loss: 0.64508\n",
      "Epoch: 74, Time: 0.02406s, Loss: 0.63962\n",
      "Epoch: 75, Time: 0.03111s, Loss: 0.64869\n",
      "Epoch: 76, Time: 0.03394s, Loss: 0.64179\n",
      "Epoch: 77, Time: 0.03321s, Loss: 0.63959\n",
      "Epoch: 78, Time: 0.02916s, Loss: 0.64584\n",
      "Epoch: 79, Time: 0.01950s, Loss: 0.64089\n",
      "Epoch: 80, Time: 0.01614s, Loss: 0.64793\n",
      "Epoch: 81, Time: 0.01458s, Loss: 0.64622\n",
      "Epoch: 82, Time: 0.01613s, Loss: 0.63646\n",
      "Epoch: 83, Time: 0.01528s, Loss: 0.64738\n",
      "Epoch: 84, Time: 0.01355s, Loss: 0.64670\n",
      "Epoch: 85, Time: 0.02087s, Loss: 0.63609\n",
      "Epoch: 86, Time: 0.03323s, Loss: 0.64739\n",
      "Epoch: 87, Time: 0.03405s, Loss: 0.64421\n",
      "Epoch: 88, Time: 0.03038s, Loss: 0.63833\n",
      "Epoch: 89, Time: 0.03510s, Loss: 0.64584\n",
      "Epoch: 90, Time: 0.03174s, Loss: 0.64468\n",
      "Epoch: 91, Time: 0.02555s, Loss: 0.64021\n",
      "Epoch: 92, Time: 0.02111s, Loss: 0.64418\n",
      "Epoch: 93, Time: 0.02903s, Loss: 0.64054\n",
      "Epoch: 94, Time: 0.01984s, Loss: 0.63720\n",
      "Epoch: 95, Time: 0.02963s, Loss: 0.64471\n",
      "Epoch: 96, Time: 0.02875s, Loss: 0.64270\n",
      "Epoch: 97, Time: 0.02388s, Loss: 0.63952\n",
      "Epoch: 98, Time: 0.02229s, Loss: 0.64325\n",
      "Epoch: 99, Time: 0.02599s, Loss: 0.64112\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5775, F1: 0.5775\n",
      "\n",
      "[HouseCommittees] Fold 3/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.14355s, Loss: 1.07534\n",
      "Epoch: 1, Time: 0.13843s, Loss: 1.05647\n",
      "Epoch: 2, Time: 0.16038s, Loss: 1.03475\n",
      "Epoch: 3, Time: 0.16267s, Loss: 1.01002\n",
      "Epoch: 4, Time: 0.16129s, Loss: 0.98164\n",
      "Epoch: 5, Time: 0.14294s, Loss: 0.95151\n",
      "Epoch: 6, Time: 0.13870s, Loss: 0.92166\n",
      "Epoch: 7, Time: 0.17824s, Loss: 0.89235\n",
      "Epoch: 8, Time: 0.16953s, Loss: 0.86577\n",
      "Epoch: 9, Time: 0.17271s, Loss: 0.84165\n",
      "Epoch: 10, Time: 0.14427s, Loss: 0.81884\n",
      "Epoch: 11, Time: 0.15997s, Loss: 0.80077\n",
      "Epoch: 12, Time: 0.13744s, Loss: 0.78535\n",
      "Epoch: 13, Time: 0.10165s, Loss: 0.77307\n",
      "Epoch: 14, Time: 0.16171s, Loss: 0.76343\n",
      "Epoch: 15, Time: 0.15801s, Loss: 0.75302\n",
      "Epoch: 16, Time: 0.10527s, Loss: 0.74553\n",
      "Epoch: 17, Time: 0.15218s, Loss: 0.73890\n",
      "Epoch: 18, Time: 0.14220s, Loss: 0.73282\n",
      "Epoch: 19, Time: 0.13886s, Loss: 0.72855\n",
      "Epoch: 20, Time: 0.17041s, Loss: 0.72366\n",
      "Epoch: 21, Time: 0.16478s, Loss: 0.72032\n",
      "Epoch: 22, Time: 0.17136s, Loss: 0.71727\n",
      "Epoch: 23, Time: 0.14319s, Loss: 0.71428\n",
      "Epoch: 24, Time: 0.16668s, Loss: 0.71172\n",
      "Epoch: 25, Time: 0.14092s, Loss: 0.70984\n",
      "Epoch: 26, Time: 0.13605s, Loss: 0.70795\n",
      "Epoch: 27, Time: 0.08551s, Loss: 0.70641\n",
      "Epoch: 28, Time: 0.16282s, Loss: 0.70442\n",
      "Epoch: 29, Time: 0.18014s, Loss: 0.70316\n",
      "Epoch: 30, Time: 0.16818s, Loss: 0.70215\n",
      "Epoch: 31, Time: 0.18610s, Loss: 0.70107\n",
      "Epoch: 32, Time: 0.18184s, Loss: 0.70082\n",
      "Epoch: 33, Time: 0.16423s, Loss: 0.69909\n",
      "Epoch: 34, Time: 0.16826s, Loss: 0.69851\n",
      "Epoch: 35, Time: 0.15983s, Loss: 0.69829\n",
      "Epoch: 36, Time: 0.17198s, Loss: 0.69743\n",
      "Epoch: 37, Time: 0.18745s, Loss: 0.69660\n",
      "Epoch: 38, Time: 0.16092s, Loss: 0.69728\n",
      "Epoch: 39, Time: 0.17783s, Loss: 0.69630\n",
      "Epoch: 40, Time: 0.16748s, Loss: 0.69597\n",
      "Epoch: 41, Time: 0.16977s, Loss: 0.69565\n",
      "Epoch: 42, Time: 0.19219s, Loss: 0.69547\n",
      "Epoch: 43, Time: 0.14774s, Loss: 0.69461\n",
      "Epoch: 44, Time: 0.16573s, Loss: 0.69495\n",
      "Epoch: 45, Time: 0.16176s, Loss: 0.69465\n",
      "Epoch: 46, Time: 0.18392s, Loss: 0.69496\n",
      "Epoch: 47, Time: 0.16723s, Loss: 0.69451\n",
      "Epoch: 48, Time: 0.16256s, Loss: 0.69464\n",
      "Epoch: 49, Time: 0.16321s, Loss: 0.69423\n",
      "Epoch: 50, Time: 0.14202s, Loss: 0.69352\n",
      "Epoch: 51, Time: 0.13670s, Loss: 0.69400\n",
      "Epoch: 52, Time: 0.15004s, Loss: 0.69427\n",
      "Epoch: 53, Time: 0.13812s, Loss: 0.69422\n",
      "Epoch: 54, Time: 0.15319s, Loss: 0.69350\n",
      "Epoch: 55, Time: 0.16516s, Loss: 0.69478\n",
      "Epoch: 56, Time: 0.15167s, Loss: 0.69432\n",
      "Epoch: 57, Time: 0.14705s, Loss: 0.69370\n",
      "Epoch: 58, Time: 0.16593s, Loss: 0.69309\n",
      "Epoch: 59, Time: 0.16502s, Loss: 0.69321\n",
      "Epoch: 60, Time: 0.16662s, Loss: 0.69337\n",
      "Epoch: 61, Time: 0.17364s, Loss: 0.69333\n",
      "Epoch: 62, Time: 0.17765s, Loss: 0.69389\n",
      "Epoch: 63, Time: 0.16831s, Loss: 0.69313\n",
      "Epoch: 64, Time: 0.16990s, Loss: 0.69361\n",
      "Epoch: 65, Time: 0.17438s, Loss: 0.69334\n",
      "Epoch: 66, Time: 0.17380s, Loss: 0.69321\n",
      "Epoch: 67, Time: 0.15582s, Loss: 0.69294\n",
      "Epoch: 68, Time: 0.17165s, Loss: 0.69284\n",
      "Epoch: 69, Time: 0.17044s, Loss: 0.69272\n",
      "Epoch: 70, Time: 0.17459s, Loss: 0.69337\n",
      "Epoch: 71, Time: 0.17226s, Loss: 0.69349\n",
      "Epoch: 72, Time: 0.18763s, Loss: 0.69280\n",
      "Epoch: 73, Time: 0.17334s, Loss: 0.69359\n",
      "Epoch: 74, Time: 0.18341s, Loss: 0.69297\n",
      "Epoch: 75, Time: 0.17310s, Loss: 0.69300\n",
      "Epoch: 76, Time: 0.18470s, Loss: 0.69331\n",
      "Epoch: 77, Time: 0.19383s, Loss: 0.69348\n",
      "Epoch: 78, Time: 0.17728s, Loss: 0.69306\n",
      "Epoch: 79, Time: 0.17710s, Loss: 0.69278\n",
      "Epoch: 80, Time: 0.18784s, Loss: 0.69328\n",
      "Epoch: 81, Time: 0.19120s, Loss: 0.69248\n",
      "Epoch: 82, Time: 0.32244s, Loss: 0.69329\n",
      "Epoch: 83, Time: 0.17767s, Loss: 0.69289\n",
      "Epoch: 84, Time: 0.16308s, Loss: 0.69293\n",
      "Epoch: 85, Time: 0.17941s, Loss: 0.69254\n",
      "Epoch: 86, Time: 0.19466s, Loss: 0.69319\n",
      "Epoch: 87, Time: 0.17218s, Loss: 0.69222\n",
      "Epoch: 88, Time: 0.16927s, Loss: 0.69315\n",
      "Epoch: 89, Time: 0.17006s, Loss: 0.69304\n",
      "Epoch: 90, Time: 0.18267s, Loss: 0.69319\n",
      "Epoch: 91, Time: 0.14720s, Loss: 0.69296\n",
      "Epoch: 92, Time: 0.16602s, Loss: 0.69270\n",
      "Epoch: 93, Time: 0.14067s, Loss: 0.69278\n",
      "Epoch: 94, Time: 0.16370s, Loss: 0.69278\n",
      "Epoch: 95, Time: 0.17272s, Loss: 0.69309\n",
      "Epoch: 96, Time: 0.16506s, Loss: 0.69333\n",
      "Epoch: 97, Time: 0.15439s, Loss: 0.69285\n",
      "Epoch: 98, Time: 0.16805s, Loss: 0.69280\n",
      "Epoch: 99, Time: 0.16191s, Loss: 0.69295\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03427s, Loss: 1.13404\n",
      "Epoch: 1, Time: 0.03626s, Loss: 1.05258\n",
      "Epoch: 2, Time: 0.02346s, Loss: 0.98959\n",
      "Epoch: 3, Time: 0.02916s, Loss: 0.93923\n",
      "Epoch: 4, Time: 0.02692s, Loss: 0.88743\n",
      "Epoch: 5, Time: 0.02557s, Loss: 0.83372\n",
      "Epoch: 6, Time: 0.03056s, Loss: 0.80787\n",
      "Epoch: 7, Time: 0.02139s, Loss: 0.77523\n",
      "Epoch: 8, Time: 0.02997s, Loss: 0.74795\n",
      "Epoch: 9, Time: 0.02361s, Loss: 0.72320\n",
      "Epoch: 10, Time: 0.02885s, Loss: 0.70842\n",
      "Epoch: 11, Time: 0.01961s, Loss: 0.68875\n",
      "Epoch: 12, Time: 0.01944s, Loss: 0.67843\n",
      "Epoch: 13, Time: 0.02501s, Loss: 0.66680\n",
      "Epoch: 14, Time: 0.02171s, Loss: 0.65832\n",
      "Epoch: 15, Time: 0.02846s, Loss: 0.65247\n",
      "Epoch: 16, Time: 0.02691s, Loss: 0.64535\n",
      "Epoch: 17, Time: 0.02895s, Loss: 0.64033\n",
      "Epoch: 18, Time: 0.01996s, Loss: 0.63882\n",
      "Epoch: 19, Time: 0.02785s, Loss: 0.63434\n",
      "Epoch: 20, Time: 0.03092s, Loss: 0.63265\n",
      "Epoch: 21, Time: 0.02980s, Loss: 0.63048\n",
      "Epoch: 22, Time: 0.03872s, Loss: 0.62991\n",
      "Epoch: 23, Time: 0.02603s, Loss: 0.62733\n",
      "Epoch: 24, Time: 0.02987s, Loss: 0.62497\n",
      "Epoch: 25, Time: 0.02007s, Loss: 0.62395\n",
      "Epoch: 26, Time: 0.02468s, Loss: 0.62190\n",
      "Epoch: 27, Time: 0.03219s, Loss: 0.62122\n",
      "Epoch: 28, Time: 0.02368s, Loss: 0.61884\n",
      "Epoch: 29, Time: 0.02990s, Loss: 0.61754\n",
      "Epoch: 30, Time: 0.02421s, Loss: 0.61589\n",
      "Epoch: 31, Time: 0.02476s, Loss: 0.61642\n",
      "Epoch: 32, Time: 0.02378s, Loss: 0.61489\n",
      "Epoch: 33, Time: 0.02319s, Loss: 0.61453\n",
      "Epoch: 34, Time: 0.02811s, Loss: 0.61470\n",
      "Epoch: 35, Time: 0.01847s, Loss: 0.61163\n",
      "Epoch: 36, Time: 0.02641s, Loss: 0.61163\n",
      "Epoch: 37, Time: 0.03298s, Loss: 0.61290\n",
      "Epoch: 38, Time: 0.02529s, Loss: 0.60985\n",
      "Epoch: 39, Time: 0.02734s, Loss: 0.60760\n",
      "Epoch: 40, Time: 0.02692s, Loss: 0.61005\n",
      "Epoch: 41, Time: 0.02656s, Loss: 0.61020\n",
      "Epoch: 42, Time: 0.02894s, Loss: 0.60494\n",
      "Epoch: 43, Time: 0.02520s, Loss: 0.60975\n",
      "Epoch: 44, Time: 0.01604s, Loss: 0.60510\n",
      "Epoch: 45, Time: 0.01949s, Loss: 0.60463\n",
      "Epoch: 46, Time: 0.02614s, Loss: 0.60462\n",
      "Epoch: 47, Time: 0.02584s, Loss: 0.61080\n",
      "Epoch: 48, Time: 0.02837s, Loss: 0.60774\n",
      "Epoch: 49, Time: 0.03055s, Loss: 0.61064\n",
      "Epoch: 50, Time: 0.02353s, Loss: 0.60237\n",
      "Epoch: 51, Time: 0.01937s, Loss: 0.60931\n",
      "Epoch: 52, Time: 0.02959s, Loss: 0.59967\n",
      "Epoch: 53, Time: 0.02216s, Loss: 0.60425\n",
      "Epoch: 54, Time: 0.02778s, Loss: 0.59980\n",
      "Epoch: 55, Time: 0.03052s, Loss: 0.60124\n",
      "Epoch: 56, Time: 0.02150s, Loss: 0.60285\n",
      "Epoch: 57, Time: 0.02458s, Loss: 0.60133\n",
      "Epoch: 58, Time: 0.03066s, Loss: 0.59900\n",
      "Epoch: 59, Time: 0.02460s, Loss: 0.60715\n",
      "Epoch: 60, Time: 0.02777s, Loss: 0.61711\n",
      "Epoch: 61, Time: 0.03535s, Loss: 0.60427\n",
      "Epoch: 62, Time: 0.01742s, Loss: 0.60557\n",
      "Epoch: 63, Time: 0.02274s, Loss: 0.60528\n",
      "Epoch: 64, Time: 0.02394s, Loss: 0.60237\n",
      "Epoch: 65, Time: 0.02703s, Loss: 0.61001\n",
      "Epoch: 66, Time: 0.03060s, Loss: 0.61360\n",
      "Epoch: 67, Time: 0.02499s, Loss: 0.59823\n",
      "Epoch: 68, Time: 0.02325s, Loss: 0.60265\n",
      "Epoch: 69, Time: 0.02464s, Loss: 0.59937\n",
      "Epoch: 70, Time: 0.02253s, Loss: 0.60228\n",
      "Epoch: 71, Time: 0.03075s, Loss: 0.60728\n",
      "Epoch: 72, Time: 0.02249s, Loss: 0.59546\n",
      "Epoch: 73, Time: 0.02952s, Loss: 0.60180\n",
      "Epoch: 74, Time: 0.03063s, Loss: 0.59693\n",
      "Epoch: 75, Time: 0.02238s, Loss: 0.59697\n",
      "Epoch: 76, Time: 0.03448s, Loss: 0.59434\n",
      "Epoch: 77, Time: 0.02086s, Loss: 0.59451\n",
      "Epoch: 78, Time: 0.02638s, Loss: 0.59815\n",
      "Epoch: 79, Time: 0.02767s, Loss: 0.59958\n",
      "Epoch: 80, Time: 0.02952s, Loss: 0.59249\n",
      "Epoch: 81, Time: 0.02455s, Loss: 0.59567\n",
      "Epoch: 82, Time: 0.03402s, Loss: 0.59464\n",
      "Epoch: 83, Time: 0.03050s, Loss: 0.60303\n",
      "Epoch: 84, Time: 0.03115s, Loss: 0.61975\n",
      "Epoch: 85, Time: 0.03250s, Loss: 0.59544\n",
      "Epoch: 86, Time: 0.02689s, Loss: 0.60032\n",
      "Epoch: 87, Time: 0.02053s, Loss: 0.60399\n",
      "Epoch: 88, Time: 0.03159s, Loss: 0.59892\n",
      "Epoch: 89, Time: 0.01971s, Loss: 0.60453\n",
      "Epoch: 90, Time: 0.02137s, Loss: 0.60023\n",
      "Epoch: 91, Time: 0.02830s, Loss: 0.59923\n",
      "Epoch: 92, Time: 0.02239s, Loss: 0.60259\n",
      "Epoch: 93, Time: 0.02883s, Loss: 0.60058\n",
      "Epoch: 94, Time: 0.02938s, Loss: 0.61051\n",
      "Epoch: 95, Time: 0.03097s, Loss: 0.59791\n",
      "Epoch: 96, Time: 0.03007s, Loss: 0.60360\n",
      "Epoch: 97, Time: 0.03491s, Loss: 0.59745\n",
      "Epoch: 98, Time: 0.03076s, Loss: 0.60072\n",
      "Epoch: 99, Time: 0.03241s, Loss: 0.60788\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5620, F1: 0.5620\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.02477s, Loss: 1.09212\n",
      "Epoch: 1, Time: 0.03123s, Loss: 0.90909\n",
      "Epoch: 2, Time: 0.02473s, Loss: 0.85265\n",
      "Epoch: 3, Time: 0.02997s, Loss: 0.80366\n",
      "Epoch: 4, Time: 0.03129s, Loss: 0.77270\n",
      "Epoch: 5, Time: 0.02455s, Loss: 0.75072\n",
      "Epoch: 6, Time: 0.03537s, Loss: 0.73332\n",
      "Epoch: 7, Time: 0.02832s, Loss: 0.72331\n",
      "Epoch: 8, Time: 0.02878s, Loss: 0.71422\n",
      "Epoch: 9, Time: 0.03145s, Loss: 0.70452\n",
      "Epoch: 10, Time: 0.01980s, Loss: 0.69856\n",
      "Epoch: 11, Time: 0.02479s, Loss: 0.69160\n",
      "Epoch: 12, Time: 0.03133s, Loss: 0.68807\n",
      "Epoch: 13, Time: 0.02933s, Loss: 0.68593\n",
      "Epoch: 14, Time: 0.02582s, Loss: 0.68146\n",
      "Epoch: 15, Time: 0.03685s, Loss: 0.67776\n",
      "Epoch: 16, Time: 0.02651s, Loss: 0.67579\n",
      "Epoch: 17, Time: 0.02458s, Loss: 0.67341\n",
      "Epoch: 18, Time: 0.03440s, Loss: 0.67134\n",
      "Epoch: 19, Time: 0.03442s, Loss: 0.66856\n",
      "Epoch: 20, Time: 0.02638s, Loss: 0.66937\n",
      "Epoch: 21, Time: 0.02934s, Loss: 0.66567\n",
      "Epoch: 22, Time: 0.02545s, Loss: 0.66839\n",
      "Epoch: 23, Time: 0.03140s, Loss: 0.66401\n",
      "Epoch: 24, Time: 0.02047s, Loss: 0.66250\n",
      "Epoch: 25, Time: 0.02549s, Loss: 0.66263\n",
      "Epoch: 26, Time: 0.02652s, Loss: 0.66153\n",
      "Epoch: 27, Time: 0.01934s, Loss: 0.66317\n",
      "Epoch: 28, Time: 0.02544s, Loss: 0.65777\n",
      "Epoch: 29, Time: 0.02167s, Loss: 0.65749\n",
      "Epoch: 30, Time: 0.02429s, Loss: 0.65803\n",
      "Epoch: 31, Time: 0.01721s, Loss: 0.65411\n",
      "Epoch: 32, Time: 0.02611s, Loss: 0.65611\n",
      "Epoch: 33, Time: 0.02186s, Loss: 0.65437\n",
      "Epoch: 34, Time: 0.02276s, Loss: 0.65441\n",
      "Epoch: 35, Time: 0.02027s, Loss: 0.65875\n",
      "Epoch: 36, Time: 0.02669s, Loss: 0.65105\n",
      "Epoch: 37, Time: 0.02281s, Loss: 0.65698\n",
      "Epoch: 38, Time: 0.01927s, Loss: 0.65018\n",
      "Epoch: 39, Time: 0.02145s, Loss: 0.65609\n",
      "Epoch: 40, Time: 0.02147s, Loss: 0.64944\n",
      "Epoch: 41, Time: 0.01968s, Loss: 0.65405\n",
      "Epoch: 42, Time: 0.02887s, Loss: 0.65354\n",
      "Epoch: 43, Time: 0.01944s, Loss: 0.65403\n",
      "Epoch: 44, Time: 0.02858s, Loss: 0.65053\n",
      "Epoch: 45, Time: 0.03045s, Loss: 0.65541\n",
      "Epoch: 46, Time: 0.02797s, Loss: 0.64722\n",
      "Epoch: 47, Time: 0.02390s, Loss: 0.64810\n",
      "Epoch: 48, Time: 0.02875s, Loss: 0.64954\n",
      "Epoch: 49, Time: 0.01811s, Loss: 0.64723\n",
      "Epoch: 50, Time: 0.02406s, Loss: 0.65571\n",
      "Epoch: 51, Time: 0.02370s, Loss: 0.64804\n",
      "Epoch: 52, Time: 0.02575s, Loss: 0.65059\n",
      "Epoch: 53, Time: 0.02650s, Loss: 0.65424\n",
      "Epoch: 54, Time: 0.01658s, Loss: 0.64695\n",
      "Epoch: 55, Time: 0.02674s, Loss: 0.65882\n",
      "Epoch: 56, Time: 0.02533s, Loss: 0.64427\n",
      "Epoch: 57, Time: 0.02661s, Loss: 0.64927\n",
      "Epoch: 58, Time: 0.02796s, Loss: 0.66181\n",
      "Epoch: 59, Time: 0.02171s, Loss: 0.64755\n",
      "Epoch: 60, Time: 0.02442s, Loss: 0.65334\n",
      "Epoch: 61, Time: 0.02823s, Loss: 0.64631\n",
      "Epoch: 62, Time: 0.03048s, Loss: 0.65127\n",
      "Epoch: 63, Time: 0.02350s, Loss: 0.64926\n",
      "Epoch: 64, Time: 0.02488s, Loss: 0.64826\n",
      "Epoch: 65, Time: 0.02443s, Loss: 0.64680\n",
      "Epoch: 66, Time: 0.02664s, Loss: 0.64714\n",
      "Epoch: 67, Time: 0.02449s, Loss: 0.64541\n",
      "Epoch: 68, Time: 0.02184s, Loss: 0.65168\n",
      "Epoch: 69, Time: 0.02775s, Loss: 0.64493\n",
      "Epoch: 70, Time: 0.02526s, Loss: 0.64280\n",
      "Epoch: 71, Time: 0.01994s, Loss: 0.64383\n",
      "Epoch: 72, Time: 0.03579s, Loss: 0.64180\n",
      "Epoch: 73, Time: 0.02664s, Loss: 0.64275\n",
      "Epoch: 74, Time: 0.03227s, Loss: 0.64016\n",
      "Epoch: 75, Time: 0.02566s, Loss: 0.64513\n",
      "Epoch: 76, Time: 0.03489s, Loss: 0.64361\n",
      "Epoch: 77, Time: 0.02809s, Loss: 0.64362\n",
      "Epoch: 78, Time: 0.02466s, Loss: 0.64176\n",
      "Epoch: 79, Time: 0.02490s, Loss: 0.64146\n",
      "Epoch: 80, Time: 0.03419s, Loss: 0.64468\n",
      "Epoch: 81, Time: 0.03640s, Loss: 0.63646\n",
      "Epoch: 82, Time: 0.02990s, Loss: 0.64183\n",
      "Epoch: 83, Time: 0.01951s, Loss: 0.64205\n",
      "Epoch: 84, Time: 0.02851s, Loss: 0.63479\n",
      "Epoch: 85, Time: 0.02648s, Loss: 0.64357\n",
      "Epoch: 86, Time: 0.02163s, Loss: 0.64349\n",
      "Epoch: 87, Time: 0.02750s, Loss: 0.63757\n",
      "Epoch: 88, Time: 0.01522s, Loss: 0.64288\n",
      "Epoch: 89, Time: 0.02271s, Loss: 0.64329\n",
      "Epoch: 90, Time: 0.02844s, Loss: 0.63354\n",
      "Epoch: 91, Time: 0.02505s, Loss: 0.64369\n",
      "Epoch: 92, Time: 0.02285s, Loss: 0.63843\n",
      "Epoch: 93, Time: 0.02773s, Loss: 0.64293\n",
      "Epoch: 94, Time: 0.03123s, Loss: 0.65201\n",
      "Epoch: 95, Time: 0.03281s, Loss: 0.63872\n",
      "Epoch: 96, Time: 0.02836s, Loss: 0.63974\n",
      "Epoch: 97, Time: 0.04396s, Loss: 0.64096\n",
      "Epoch: 98, Time: 0.02296s, Loss: 0.63813\n",
      "Epoch: 99, Time: 0.02992s, Loss: 0.63811\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5659, F1: 0.5659\n",
      "\n",
      "[HouseCommittees] Fold 4/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.20326s, Loss: 1.14526\n",
      "Epoch: 1, Time: 0.16485s, Loss: 1.12668\n",
      "Epoch: 2, Time: 0.13964s, Loss: 1.10459\n",
      "Epoch: 3, Time: 0.15094s, Loss: 1.07693\n",
      "Epoch: 4, Time: 0.15622s, Loss: 1.04604\n",
      "Epoch: 5, Time: 0.15550s, Loss: 1.01216\n",
      "Epoch: 6, Time: 0.17416s, Loss: 0.97616\n",
      "Epoch: 7, Time: 0.13684s, Loss: 0.94373\n",
      "Epoch: 8, Time: 0.15984s, Loss: 0.90956\n",
      "Epoch: 9, Time: 0.15375s, Loss: 0.88046\n",
      "Epoch: 10, Time: 0.16693s, Loss: 0.85320\n",
      "Epoch: 11, Time: 0.18567s, Loss: 0.82902\n",
      "Epoch: 12, Time: 0.16596s, Loss: 0.81049\n",
      "Epoch: 13, Time: 0.16914s, Loss: 0.79544\n",
      "Epoch: 14, Time: 0.15766s, Loss: 0.78015\n",
      "Epoch: 15, Time: 0.16814s, Loss: 0.76912\n",
      "Epoch: 16, Time: 0.16363s, Loss: 0.75911\n",
      "Epoch: 17, Time: 0.16616s, Loss: 0.75099\n",
      "Epoch: 18, Time: 0.16815s, Loss: 0.74478\n",
      "Epoch: 19, Time: 0.17891s, Loss: 0.73779\n",
      "Epoch: 20, Time: 0.17053s, Loss: 0.73278\n",
      "Epoch: 21, Time: 0.14366s, Loss: 0.72927\n",
      "Epoch: 22, Time: 0.15214s, Loss: 0.72459\n",
      "Epoch: 23, Time: 0.14180s, Loss: 0.72149\n",
      "Epoch: 24, Time: 0.13641s, Loss: 0.71806\n",
      "Epoch: 25, Time: 0.12053s, Loss: 0.71510\n",
      "Epoch: 26, Time: 0.12960s, Loss: 0.71267\n",
      "Epoch: 27, Time: 0.16014s, Loss: 0.71045\n",
      "Epoch: 28, Time: 0.15376s, Loss: 0.70857\n",
      "Epoch: 29, Time: 0.16791s, Loss: 0.70715\n",
      "Epoch: 30, Time: 0.17441s, Loss: 0.70572\n",
      "Epoch: 31, Time: 0.15321s, Loss: 0.70420\n",
      "Epoch: 32, Time: 0.15758s, Loss: 0.70267\n",
      "Epoch: 33, Time: 0.12269s, Loss: 0.70169\n",
      "Epoch: 34, Time: 0.14124s, Loss: 0.70141\n",
      "Epoch: 35, Time: 0.13308s, Loss: 0.69984\n",
      "Epoch: 36, Time: 0.16638s, Loss: 0.69915\n",
      "Epoch: 37, Time: 0.17226s, Loss: 0.69862\n",
      "Epoch: 38, Time: 0.16259s, Loss: 0.69805\n",
      "Epoch: 39, Time: 0.16821s, Loss: 0.69769\n",
      "Epoch: 40, Time: 0.15913s, Loss: 0.69705\n",
      "Epoch: 41, Time: 0.16194s, Loss: 0.69723\n",
      "Epoch: 42, Time: 0.16085s, Loss: 0.69635\n",
      "Epoch: 43, Time: 0.13482s, Loss: 0.69596\n",
      "Epoch: 44, Time: 0.17891s, Loss: 0.69593\n",
      "Epoch: 45, Time: 0.15891s, Loss: 0.69538\n",
      "Epoch: 46, Time: 0.16248s, Loss: 0.69525\n",
      "Epoch: 47, Time: 0.13363s, Loss: 0.69509\n",
      "Epoch: 48, Time: 0.12762s, Loss: 0.69519\n",
      "Epoch: 49, Time: 0.14385s, Loss: 0.69470\n",
      "Epoch: 50, Time: 0.16390s, Loss: 0.69486\n",
      "Epoch: 51, Time: 0.17354s, Loss: 0.69444\n",
      "Epoch: 52, Time: 0.13615s, Loss: 0.69457\n",
      "Epoch: 53, Time: 0.15519s, Loss: 0.69421\n",
      "Epoch: 54, Time: 0.17017s, Loss: 0.69403\n",
      "Epoch: 55, Time: 0.15135s, Loss: 0.69408\n",
      "Epoch: 56, Time: 0.17954s, Loss: 0.69383\n",
      "Epoch: 57, Time: 0.18355s, Loss: 0.69422\n",
      "Epoch: 58, Time: 0.17618s, Loss: 0.69435\n",
      "Epoch: 59, Time: 0.17998s, Loss: 0.69403\n",
      "Epoch: 60, Time: 0.18243s, Loss: 0.69380\n",
      "Epoch: 61, Time: 0.17275s, Loss: 0.69378\n",
      "Epoch: 62, Time: 0.15269s, Loss: 0.69377\n",
      "Epoch: 63, Time: 0.16590s, Loss: 0.69429\n",
      "Epoch: 64, Time: 0.16836s, Loss: 0.69364\n",
      "Epoch: 65, Time: 0.15218s, Loss: 0.69366\n",
      "Epoch: 66, Time: 0.17431s, Loss: 0.69371\n",
      "Epoch: 67, Time: 0.15916s, Loss: 0.69366\n",
      "Epoch: 68, Time: 0.18245s, Loss: 0.69340\n",
      "Epoch: 69, Time: 0.14570s, Loss: 0.69372\n",
      "Epoch: 70, Time: 0.15262s, Loss: 0.69360\n",
      "Epoch: 71, Time: 0.16483s, Loss: 0.69361\n",
      "Epoch: 72, Time: 0.15748s, Loss: 0.69353\n",
      "Epoch: 73, Time: 0.16812s, Loss: 0.69393\n",
      "Epoch: 74, Time: 0.17944s, Loss: 0.69321\n",
      "Epoch: 75, Time: 0.13901s, Loss: 0.69335\n",
      "Epoch: 76, Time: 0.15495s, Loss: 0.69335\n",
      "Epoch: 77, Time: 0.16231s, Loss: 0.69343\n",
      "Epoch: 78, Time: 0.17019s, Loss: 0.69329\n",
      "Epoch: 79, Time: 0.15951s, Loss: 0.69324\n",
      "Epoch: 80, Time: 0.13882s, Loss: 0.69288\n",
      "Epoch: 81, Time: 0.14884s, Loss: 0.69316\n",
      "Epoch: 82, Time: 0.15672s, Loss: 0.69324\n",
      "Epoch: 83, Time: 0.17143s, Loss: 0.69312\n",
      "Epoch: 84, Time: 0.15266s, Loss: 0.69313\n",
      "Epoch: 85, Time: 0.17211s, Loss: 0.69291\n",
      "Epoch: 86, Time: 0.17858s, Loss: 0.69324\n",
      "Epoch: 87, Time: 0.17300s, Loss: 0.69389\n",
      "Epoch: 88, Time: 0.17116s, Loss: 0.69281\n",
      "Epoch: 89, Time: 0.13884s, Loss: 0.69243\n",
      "Epoch: 90, Time: 0.14472s, Loss: 0.69343\n",
      "Epoch: 91, Time: 0.13473s, Loss: 0.69348\n",
      "Epoch: 92, Time: 0.17085s, Loss: 0.69309\n",
      "Epoch: 93, Time: 0.16973s, Loss: 0.69274\n",
      "Epoch: 94, Time: 0.19413s, Loss: 0.69301\n",
      "Epoch: 95, Time: 0.17123s, Loss: 0.69339\n",
      "Epoch: 96, Time: 0.17610s, Loss: 0.69290\n",
      "Epoch: 97, Time: 0.17137s, Loss: 0.69307\n",
      "Epoch: 98, Time: 0.16679s, Loss: 0.69278\n",
      "Epoch: 99, Time: 0.17105s, Loss: 0.69286\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.02576s, Loss: 1.13865\n",
      "Epoch: 1, Time: 0.03115s, Loss: 1.07195\n",
      "Epoch: 2, Time: 0.02799s, Loss: 1.00141\n",
      "Epoch: 3, Time: 0.02394s, Loss: 0.95119\n",
      "Epoch: 4, Time: 0.03681s, Loss: 0.92126\n",
      "Epoch: 5, Time: 0.03094s, Loss: 0.86689\n",
      "Epoch: 6, Time: 0.02942s, Loss: 0.83634\n",
      "Epoch: 7, Time: 0.02634s, Loss: 0.79786\n",
      "Epoch: 8, Time: 0.02683s, Loss: 0.77300\n",
      "Epoch: 9, Time: 0.03651s, Loss: 0.74975\n",
      "Epoch: 10, Time: 0.02938s, Loss: 0.72678\n",
      "Epoch: 11, Time: 0.02659s, Loss: 0.71018\n",
      "Epoch: 12, Time: 0.03281s, Loss: 0.69293\n",
      "Epoch: 13, Time: 0.03017s, Loss: 0.68593\n",
      "Epoch: 14, Time: 0.03070s, Loss: 0.67585\n",
      "Epoch: 15, Time: 0.03137s, Loss: 0.66504\n",
      "Epoch: 16, Time: 0.02882s, Loss: 0.65892\n",
      "Epoch: 17, Time: 0.02793s, Loss: 0.65482\n",
      "Epoch: 18, Time: 0.02611s, Loss: 0.65064\n",
      "Epoch: 19, Time: 0.03024s, Loss: 0.64443\n",
      "Epoch: 20, Time: 0.03028s, Loss: 0.64454\n",
      "Epoch: 21, Time: 0.03018s, Loss: 0.64169\n",
      "Epoch: 22, Time: 0.02966s, Loss: 0.63968\n",
      "Epoch: 23, Time: 0.02820s, Loss: 0.63925\n",
      "Epoch: 24, Time: 0.03142s, Loss: 0.63518\n",
      "Epoch: 25, Time: 0.02970s, Loss: 0.63511\n",
      "Epoch: 26, Time: 0.02724s, Loss: 0.63287\n",
      "Epoch: 27, Time: 0.03621s, Loss: 0.63180\n",
      "Epoch: 28, Time: 0.03157s, Loss: 0.62924\n",
      "Epoch: 29, Time: 0.03058s, Loss: 0.63186\n",
      "Epoch: 30, Time: 0.03192s, Loss: 0.62742\n",
      "Epoch: 31, Time: 0.02975s, Loss: 0.62613\n",
      "Epoch: 32, Time: 0.02806s, Loss: 0.62776\n",
      "Epoch: 33, Time: 0.02894s, Loss: 0.62476\n",
      "Epoch: 34, Time: 0.03096s, Loss: 0.62558\n",
      "Epoch: 35, Time: 0.03028s, Loss: 0.62346\n",
      "Epoch: 36, Time: 0.02745s, Loss: 0.62243\n",
      "Epoch: 37, Time: 0.03313s, Loss: 0.62351\n",
      "Epoch: 38, Time: 0.02543s, Loss: 0.62124\n",
      "Epoch: 39, Time: 0.03036s, Loss: 0.62006\n",
      "Epoch: 40, Time: 0.02808s, Loss: 0.61898\n",
      "Epoch: 41, Time: 0.02795s, Loss: 0.62446\n",
      "Epoch: 42, Time: 0.05704s, Loss: 0.62247\n",
      "Epoch: 43, Time: 0.03069s, Loss: 0.62168\n",
      "Epoch: 44, Time: 0.02286s, Loss: 0.62241\n",
      "Epoch: 45, Time: 0.02218s, Loss: 0.61788\n",
      "Epoch: 46, Time: 0.02877s, Loss: 0.62237\n",
      "Epoch: 47, Time: 0.02999s, Loss: 0.61837\n",
      "Epoch: 48, Time: 0.03203s, Loss: 0.61819\n",
      "Epoch: 49, Time: 0.02534s, Loss: 0.62140\n",
      "Epoch: 50, Time: 0.03072s, Loss: 0.63186\n",
      "Epoch: 51, Time: 0.03309s, Loss: 0.61737\n",
      "Epoch: 52, Time: 0.03036s, Loss: 0.62182\n",
      "Epoch: 53, Time: 0.02877s, Loss: 0.62007\n",
      "Epoch: 54, Time: 0.03108s, Loss: 0.61528\n",
      "Epoch: 55, Time: 0.03570s, Loss: 0.61784\n",
      "Epoch: 56, Time: 0.03253s, Loss: 0.61684\n",
      "Epoch: 57, Time: 0.02860s, Loss: 0.61535\n",
      "Epoch: 58, Time: 0.03116s, Loss: 0.62058\n",
      "Epoch: 59, Time: 0.03106s, Loss: 0.61564\n",
      "Epoch: 60, Time: 0.02779s, Loss: 0.62329\n",
      "Epoch: 61, Time: 0.02705s, Loss: 0.61188\n",
      "Epoch: 62, Time: 0.02967s, Loss: 0.61729\n",
      "Epoch: 63, Time: 0.03216s, Loss: 0.61700\n",
      "Epoch: 64, Time: 0.02242s, Loss: 0.61209\n",
      "Epoch: 65, Time: 0.03239s, Loss: 0.61264\n",
      "Epoch: 66, Time: 0.02658s, Loss: 0.61977\n",
      "Epoch: 67, Time: 0.03272s, Loss: 0.61260\n",
      "Epoch: 68, Time: 0.02535s, Loss: 0.61433\n",
      "Epoch: 69, Time: 0.03106s, Loss: 0.61518\n",
      "Epoch: 70, Time: 0.02837s, Loss: 0.61086\n",
      "Epoch: 71, Time: 0.02586s, Loss: 0.61057\n",
      "Epoch: 72, Time: 0.03250s, Loss: 0.60962\n",
      "Epoch: 73, Time: 0.02414s, Loss: 0.60614\n",
      "Epoch: 74, Time: 0.02627s, Loss: 0.61363\n",
      "Epoch: 75, Time: 0.02889s, Loss: 0.60772\n",
      "Epoch: 76, Time: 0.03113s, Loss: 0.61245\n",
      "Epoch: 77, Time: 0.03042s, Loss: 0.61525\n",
      "Epoch: 78, Time: 0.02437s, Loss: 0.61176\n",
      "Epoch: 79, Time: 0.03233s, Loss: 0.62205\n",
      "Epoch: 80, Time: 0.03397s, Loss: 0.62247\n",
      "Epoch: 81, Time: 0.02813s, Loss: 0.60939\n",
      "Epoch: 82, Time: 0.02652s, Loss: 0.62016\n",
      "Epoch: 83, Time: 0.03552s, Loss: 0.61553\n",
      "Epoch: 84, Time: 0.03236s, Loss: 0.61161\n",
      "Epoch: 85, Time: 0.03131s, Loss: 0.61660\n",
      "Epoch: 86, Time: 0.03561s, Loss: 0.60780\n",
      "Epoch: 87, Time: 0.03039s, Loss: 0.62000\n",
      "Epoch: 88, Time: 0.03219s, Loss: 0.62413\n",
      "Epoch: 89, Time: 0.02803s, Loss: 0.60994\n",
      "Epoch: 90, Time: 0.03360s, Loss: 0.61027\n",
      "Epoch: 91, Time: 0.03318s, Loss: 0.61393\n",
      "Epoch: 92, Time: 0.03517s, Loss: 0.60823\n",
      "Epoch: 93, Time: 0.02631s, Loss: 0.61199\n",
      "Epoch: 94, Time: 0.02847s, Loss: 0.60984\n",
      "Epoch: 95, Time: 0.03031s, Loss: 0.61716\n",
      "Epoch: 96, Time: 0.03094s, Loss: 0.60919\n",
      "Epoch: 97, Time: 0.04120s, Loss: 0.60903\n",
      "Epoch: 98, Time: 0.02822s, Loss: 0.60796\n",
      "Epoch: 99, Time: 0.03249s, Loss: 0.61324\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5775, F1: 0.5775\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03060s, Loss: 1.05861\n",
      "Epoch: 1, Time: 0.04101s, Loss: 0.90471\n",
      "Epoch: 2, Time: 0.03180s, Loss: 0.85442\n",
      "Epoch: 3, Time: 0.03713s, Loss: 0.80730\n",
      "Epoch: 4, Time: 0.03676s, Loss: 0.77691\n",
      "Epoch: 5, Time: 0.03189s, Loss: 0.75712\n",
      "Epoch: 6, Time: 0.03317s, Loss: 0.74075\n",
      "Epoch: 7, Time: 0.03215s, Loss: 0.72445\n",
      "Epoch: 8, Time: 0.03213s, Loss: 0.71961\n",
      "Epoch: 9, Time: 0.03217s, Loss: 0.70905\n",
      "Epoch: 10, Time: 0.03383s, Loss: 0.70180\n",
      "Epoch: 11, Time: 0.03048s, Loss: 0.69556\n",
      "Epoch: 12, Time: 0.03495s, Loss: 0.69262\n",
      "Epoch: 13, Time: 0.03082s, Loss: 0.68825\n",
      "Epoch: 14, Time: 0.03389s, Loss: 0.68276\n",
      "Epoch: 15, Time: 0.03498s, Loss: 0.68152\n",
      "Epoch: 16, Time: 0.03303s, Loss: 0.67771\n",
      "Epoch: 17, Time: 0.03083s, Loss: 0.67590\n",
      "Epoch: 18, Time: 0.03193s, Loss: 0.67400\n",
      "Epoch: 19, Time: 0.02512s, Loss: 0.67138\n",
      "Epoch: 20, Time: 0.02806s, Loss: 0.67170\n",
      "Epoch: 21, Time: 0.03134s, Loss: 0.67107\n",
      "Epoch: 22, Time: 0.03245s, Loss: 0.66616\n",
      "Epoch: 23, Time: 0.03048s, Loss: 0.66829\n",
      "Epoch: 24, Time: 0.03164s, Loss: 0.66627\n",
      "Epoch: 25, Time: 0.02920s, Loss: 0.66335\n",
      "Epoch: 26, Time: 0.03234s, Loss: 0.66392\n",
      "Epoch: 27, Time: 0.03304s, Loss: 0.66279\n",
      "Epoch: 28, Time: 0.03076s, Loss: 0.66558\n",
      "Epoch: 29, Time: 0.02990s, Loss: 0.66099\n",
      "Epoch: 30, Time: 0.03254s, Loss: 0.66055\n",
      "Epoch: 31, Time: 0.03242s, Loss: 0.66017\n",
      "Epoch: 32, Time: 0.03074s, Loss: 0.65835\n",
      "Epoch: 33, Time: 0.03256s, Loss: 0.66319\n",
      "Epoch: 34, Time: 0.02668s, Loss: 0.65711\n",
      "Epoch: 35, Time: 0.03275s, Loss: 0.67085\n",
      "Epoch: 36, Time: 0.03302s, Loss: 0.65620\n",
      "Epoch: 37, Time: 0.03302s, Loss: 0.65870\n",
      "Epoch: 38, Time: 0.02600s, Loss: 0.65945\n",
      "Epoch: 39, Time: 0.03045s, Loss: 0.65569\n",
      "Epoch: 40, Time: 0.03358s, Loss: 0.65716\n",
      "Epoch: 41, Time: 0.03067s, Loss: 0.65617\n",
      "Epoch: 42, Time: 0.03271s, Loss: 0.65485\n",
      "Epoch: 43, Time: 0.02796s, Loss: 0.65453\n",
      "Epoch: 44, Time: 0.02899s, Loss: 0.65183\n",
      "Epoch: 45, Time: 0.03224s, Loss: 0.65947\n",
      "Epoch: 46, Time: 0.03224s, Loss: 0.65440\n",
      "Epoch: 47, Time: 0.02437s, Loss: 0.65493\n",
      "Epoch: 48, Time: 0.03254s, Loss: 0.65162\n",
      "Epoch: 49, Time: 0.03180s, Loss: 0.65243\n",
      "Epoch: 50, Time: 0.03166s, Loss: 0.65314\n",
      "Epoch: 51, Time: 0.03572s, Loss: 0.64946\n",
      "Epoch: 52, Time: 0.02954s, Loss: 0.65133\n",
      "Epoch: 53, Time: 0.02469s, Loss: 0.64954\n",
      "Epoch: 54, Time: 0.02847s, Loss: 0.65034\n",
      "Epoch: 55, Time: 0.02398s, Loss: 0.66633\n",
      "Epoch: 56, Time: 0.02807s, Loss: 0.65608\n",
      "Epoch: 57, Time: 0.02746s, Loss: 0.66793\n",
      "Epoch: 58, Time: 0.03014s, Loss: 0.64679\n",
      "Epoch: 59, Time: 0.03281s, Loss: 0.66674\n",
      "Epoch: 60, Time: 0.02574s, Loss: 0.65654\n",
      "Epoch: 61, Time: 0.03547s, Loss: 0.65339\n",
      "Epoch: 62, Time: 0.03096s, Loss: 0.66066\n",
      "Epoch: 63, Time: 0.02822s, Loss: 0.65128\n",
      "Epoch: 64, Time: 0.02592s, Loss: 0.65384\n",
      "Epoch: 65, Time: 0.02423s, Loss: 0.65999\n",
      "Epoch: 66, Time: 0.02667s, Loss: 0.64938\n",
      "Epoch: 67, Time: 0.03077s, Loss: 0.64942\n",
      "Epoch: 68, Time: 0.03292s, Loss: 0.65830\n",
      "Epoch: 69, Time: 0.03152s, Loss: 0.65140\n",
      "Epoch: 70, Time: 0.03518s, Loss: 0.65264\n",
      "Epoch: 71, Time: 0.03042s, Loss: 0.65480\n",
      "Epoch: 72, Time: 0.03446s, Loss: 0.65433\n",
      "Epoch: 73, Time: 0.02782s, Loss: 0.64958\n",
      "Epoch: 74, Time: 0.03551s, Loss: 0.65156\n",
      "Epoch: 75, Time: 0.02562s, Loss: 0.65120\n",
      "Epoch: 76, Time: 0.03468s, Loss: 0.64869\n",
      "Epoch: 77, Time: 0.03176s, Loss: 0.65133\n",
      "Epoch: 78, Time: 0.02974s, Loss: 0.65198\n",
      "Epoch: 79, Time: 0.03291s, Loss: 0.64988\n",
      "Epoch: 80, Time: 0.02750s, Loss: 0.65631\n",
      "Epoch: 81, Time: 0.03178s, Loss: 0.65060\n",
      "Epoch: 82, Time: 0.02702s, Loss: 0.65922\n",
      "Epoch: 83, Time: 0.03390s, Loss: 0.64867\n",
      "Epoch: 84, Time: 0.03253s, Loss: 0.65001\n",
      "Epoch: 85, Time: 0.03457s, Loss: 0.65867\n",
      "Epoch: 86, Time: 0.03194s, Loss: 0.65059\n",
      "Epoch: 87, Time: 0.02675s, Loss: 0.65075\n",
      "Epoch: 88, Time: 0.21234s, Loss: 0.64831\n",
      "Epoch: 89, Time: 0.06666s, Loss: 0.64814\n",
      "Epoch: 90, Time: 0.03315s, Loss: 0.64683\n",
      "Epoch: 91, Time: 0.03180s, Loss: 0.64975\n",
      "Epoch: 92, Time: 0.03090s, Loss: 0.64739\n",
      "Epoch: 93, Time: 0.02322s, Loss: 0.64496\n",
      "Epoch: 94, Time: 0.02608s, Loss: 0.65429\n",
      "Epoch: 95, Time: 0.03320s, Loss: 0.65177\n",
      "Epoch: 96, Time: 0.02819s, Loss: 0.64447\n",
      "Epoch: 97, Time: 0.03037s, Loss: 0.66128\n",
      "Epoch: 98, Time: 0.02793s, Loss: 0.64554\n",
      "Epoch: 99, Time: 0.02470s, Loss: 0.64741\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5853, F1: 0.5853\n",
      "\n",
      "[HouseCommittees] Fold 5/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.21481s, Loss: 1.12280\n",
      "Epoch: 1, Time: 0.17060s, Loss: 1.10019\n",
      "Epoch: 2, Time: 0.19180s, Loss: 1.07516\n",
      "Epoch: 3, Time: 0.16207s, Loss: 1.04685\n",
      "Epoch: 4, Time: 0.16885s, Loss: 1.01558\n",
      "Epoch: 5, Time: 0.16815s, Loss: 0.98208\n",
      "Epoch: 6, Time: 0.17437s, Loss: 0.94719\n",
      "Epoch: 7, Time: 0.18334s, Loss: 0.91433\n",
      "Epoch: 8, Time: 0.16479s, Loss: 0.88305\n",
      "Epoch: 9, Time: 0.16090s, Loss: 0.85868\n",
      "Epoch: 10, Time: 0.17435s, Loss: 0.83554\n",
      "Epoch: 11, Time: 0.17952s, Loss: 0.81429\n",
      "Epoch: 12, Time: 0.17775s, Loss: 0.79910\n",
      "Epoch: 13, Time: 0.18259s, Loss: 0.78344\n",
      "Epoch: 14, Time: 0.17895s, Loss: 0.77263\n",
      "Epoch: 15, Time: 0.17511s, Loss: 0.76137\n",
      "Epoch: 16, Time: 0.16775s, Loss: 0.75210\n",
      "Epoch: 17, Time: 0.17428s, Loss: 0.74568\n",
      "Epoch: 18, Time: 0.16921s, Loss: 0.73987\n",
      "Epoch: 19, Time: 0.19318s, Loss: 0.73440\n",
      "Epoch: 20, Time: 0.16969s, Loss: 0.73099\n",
      "Epoch: 21, Time: 0.18245s, Loss: 0.72574\n",
      "Epoch: 22, Time: 0.17545s, Loss: 0.72161\n",
      "Epoch: 23, Time: 0.17057s, Loss: 0.71874\n",
      "Epoch: 24, Time: 0.18239s, Loss: 0.71548\n",
      "Epoch: 25, Time: 0.21216s, Loss: 0.71326\n",
      "Epoch: 26, Time: 0.17390s, Loss: 0.71147\n",
      "Epoch: 27, Time: 0.18112s, Loss: 0.70950\n",
      "Epoch: 28, Time: 0.17354s, Loss: 0.70780\n",
      "Epoch: 29, Time: 0.18313s, Loss: 0.70633\n",
      "Epoch: 30, Time: 0.18095s, Loss: 0.70453\n",
      "Epoch: 31, Time: 0.18868s, Loss: 0.70340\n",
      "Epoch: 32, Time: 0.18765s, Loss: 0.70274\n",
      "Epoch: 33, Time: 0.17841s, Loss: 0.70180\n",
      "Epoch: 34, Time: 0.18786s, Loss: 0.70072\n",
      "Epoch: 35, Time: 0.16510s, Loss: 0.70005\n",
      "Epoch: 36, Time: 0.17643s, Loss: 0.69940\n",
      "Epoch: 37, Time: 0.17109s, Loss: 0.69888\n",
      "Epoch: 38, Time: 0.17025s, Loss: 0.69816\n",
      "Epoch: 39, Time: 0.16047s, Loss: 0.69780\n",
      "Epoch: 40, Time: 0.17280s, Loss: 0.69777\n",
      "Epoch: 41, Time: 0.17540s, Loss: 0.69714\n",
      "Epoch: 42, Time: 0.18065s, Loss: 0.69627\n",
      "Epoch: 43, Time: 0.15551s, Loss: 0.69644\n",
      "Epoch: 44, Time: 0.15959s, Loss: 0.69622\n",
      "Epoch: 45, Time: 0.15952s, Loss: 0.69579\n",
      "Epoch: 46, Time: 0.17124s, Loss: 0.69559\n",
      "Epoch: 47, Time: 0.14734s, Loss: 0.69550\n",
      "Epoch: 48, Time: 0.16477s, Loss: 0.69487\n",
      "Epoch: 49, Time: 0.14171s, Loss: 0.69497\n",
      "Epoch: 50, Time: 0.14906s, Loss: 0.69514\n",
      "Epoch: 51, Time: 0.16641s, Loss: 0.69502\n",
      "Epoch: 52, Time: 0.14297s, Loss: 0.69475\n",
      "Epoch: 53, Time: 0.15330s, Loss: 0.69448\n",
      "Epoch: 54, Time: 0.14745s, Loss: 0.69430\n",
      "Epoch: 55, Time: 0.16404s, Loss: 0.69438\n",
      "Epoch: 56, Time: 0.16372s, Loss: 0.69428\n",
      "Epoch: 57, Time: 0.16329s, Loss: 0.69452\n",
      "Epoch: 58, Time: 0.15137s, Loss: 0.69415\n",
      "Epoch: 59, Time: 0.15846s, Loss: 0.69426\n",
      "Epoch: 60, Time: 0.17873s, Loss: 0.69399\n",
      "Epoch: 61, Time: 0.14129s, Loss: 0.69366\n",
      "Epoch: 62, Time: 0.14105s, Loss: 0.69371\n",
      "Epoch: 63, Time: 0.16286s, Loss: 0.69361\n",
      "Epoch: 64, Time: 0.16431s, Loss: 0.69424\n",
      "Epoch: 65, Time: 0.15419s, Loss: 0.69347\n",
      "Epoch: 66, Time: 0.15279s, Loss: 0.69398\n",
      "Epoch: 67, Time: 0.17167s, Loss: 0.69357\n",
      "Epoch: 68, Time: 0.14854s, Loss: 0.69352\n",
      "Epoch: 69, Time: 0.17142s, Loss: 0.69339\n",
      "Epoch: 70, Time: 0.14038s, Loss: 0.69375\n",
      "Epoch: 71, Time: 0.16723s, Loss: 0.69296\n",
      "Epoch: 72, Time: 0.15186s, Loss: 0.69344\n",
      "Epoch: 73, Time: 0.17544s, Loss: 0.69321\n",
      "Epoch: 74, Time: 0.16286s, Loss: 0.69320\n",
      "Epoch: 75, Time: 0.16296s, Loss: 0.69323\n",
      "Epoch: 76, Time: 0.17894s, Loss: 0.69319\n",
      "Epoch: 77, Time: 0.16949s, Loss: 0.69299\n",
      "Epoch: 78, Time: 0.18908s, Loss: 0.69309\n",
      "Epoch: 79, Time: 0.16678s, Loss: 0.69296\n",
      "Epoch: 80, Time: 0.16678s, Loss: 0.69346\n",
      "Epoch: 81, Time: 0.17366s, Loss: 0.69287\n",
      "Epoch: 82, Time: 0.16974s, Loss: 0.69237\n",
      "Epoch: 83, Time: 0.14748s, Loss: 0.69266\n",
      "Epoch: 84, Time: 0.15606s, Loss: 0.69300\n",
      "Epoch: 85, Time: 0.15426s, Loss: 0.69270\n",
      "Epoch: 86, Time: 0.16275s, Loss: 0.69269\n",
      "Epoch: 87, Time: 0.16390s, Loss: 0.69243\n",
      "Epoch: 88, Time: 0.12751s, Loss: 0.69258\n",
      "Epoch: 89, Time: 0.14667s, Loss: 0.69317\n",
      "Epoch: 90, Time: 0.15231s, Loss: 0.69221\n",
      "Epoch: 91, Time: 0.14777s, Loss: 0.69238\n",
      "Epoch: 92, Time: 0.13846s, Loss: 0.69181\n",
      "Epoch: 93, Time: 0.14796s, Loss: 0.69207\n",
      "Epoch: 94, Time: 0.15581s, Loss: 0.69252\n",
      "Epoch: 95, Time: 0.16776s, Loss: 0.69273\n",
      "Epoch: 96, Time: 0.16899s, Loss: 0.69215\n",
      "Epoch: 97, Time: 0.14937s, Loss: 0.69230\n",
      "Epoch: 98, Time: 0.15017s, Loss: 0.69207\n",
      "Epoch: 99, Time: 0.15830s, Loss: 0.69170\n",
      "    ↳ HGNN Fold Result — Acc: 0.5271, F1: 0.5271\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03311s, Loss: 1.07853\n",
      "Epoch: 1, Time: 0.02117s, Loss: 1.01738\n",
      "Epoch: 2, Time: 0.02226s, Loss: 0.95994\n",
      "Epoch: 3, Time: 0.03196s, Loss: 0.91458\n",
      "Epoch: 4, Time: 0.03038s, Loss: 0.90171\n",
      "Epoch: 5, Time: 0.02849s, Loss: 0.84585\n",
      "Epoch: 6, Time: 0.02815s, Loss: 0.82153\n",
      "Epoch: 7, Time: 0.02753s, Loss: 0.78884\n",
      "Epoch: 8, Time: 0.02996s, Loss: 0.77393\n",
      "Epoch: 9, Time: 0.02474s, Loss: 0.74846\n",
      "Epoch: 10, Time: 0.02625s, Loss: 0.73264\n",
      "Epoch: 11, Time: 0.01444s, Loss: 0.71483\n",
      "Epoch: 12, Time: 0.02643s, Loss: 0.70435\n",
      "Epoch: 13, Time: 0.02612s, Loss: 0.69312\n",
      "Epoch: 14, Time: 0.02464s, Loss: 0.68439\n",
      "Epoch: 15, Time: 0.02841s, Loss: 0.67291\n",
      "Epoch: 16, Time: 0.01810s, Loss: 0.66774\n",
      "Epoch: 17, Time: 0.03079s, Loss: 0.66350\n",
      "Epoch: 18, Time: 0.02482s, Loss: 0.65988\n",
      "Epoch: 19, Time: 0.02358s, Loss: 0.65558\n",
      "Epoch: 20, Time: 0.02664s, Loss: 0.65224\n",
      "Epoch: 21, Time: 0.02427s, Loss: 0.64889\n",
      "Epoch: 22, Time: 0.02192s, Loss: 0.64648\n",
      "Epoch: 23, Time: 0.02486s, Loss: 0.64435\n",
      "Epoch: 24, Time: 0.02263s, Loss: 0.64367\n",
      "Epoch: 25, Time: 0.02608s, Loss: 0.63986\n",
      "Epoch: 26, Time: 0.04091s, Loss: 0.63860\n",
      "Epoch: 27, Time: 0.01976s, Loss: 0.63792\n",
      "Epoch: 28, Time: 0.02645s, Loss: 0.63585\n",
      "Epoch: 29, Time: 0.02481s, Loss: 0.63596\n",
      "Epoch: 30, Time: 0.02901s, Loss: 0.63442\n",
      "Epoch: 31, Time: 0.03267s, Loss: 0.63264\n",
      "Epoch: 32, Time: 0.02489s, Loss: 0.63062\n",
      "Epoch: 33, Time: 0.02887s, Loss: 0.62925\n",
      "Epoch: 34, Time: 0.02529s, Loss: 0.62884\n",
      "Epoch: 35, Time: 0.02299s, Loss: 0.62767\n",
      "Epoch: 36, Time: 0.02422s, Loss: 0.62696\n",
      "Epoch: 37, Time: 0.02838s, Loss: 0.62494\n",
      "Epoch: 38, Time: 0.02353s, Loss: 0.62345\n",
      "Epoch: 39, Time: 0.02168s, Loss: 0.62243\n",
      "Epoch: 40, Time: 0.02343s, Loss: 0.62220\n",
      "Epoch: 41, Time: 0.02839s, Loss: 0.61985\n",
      "Epoch: 42, Time: 0.02298s, Loss: 0.62135\n",
      "Epoch: 43, Time: 0.02377s, Loss: 0.61953\n",
      "Epoch: 44, Time: 0.03187s, Loss: 0.62113\n",
      "Epoch: 45, Time: 0.01987s, Loss: 0.62508\n",
      "Epoch: 46, Time: 0.02892s, Loss: 0.64237\n",
      "Epoch: 47, Time: 0.02494s, Loss: 0.62425\n",
      "Epoch: 48, Time: 0.02449s, Loss: 0.62018\n",
      "Epoch: 49, Time: 0.02394s, Loss: 0.61792\n",
      "Epoch: 50, Time: 0.02193s, Loss: 0.61686\n",
      "Epoch: 51, Time: 0.02909s, Loss: 0.62343\n",
      "Epoch: 52, Time: 0.02673s, Loss: 0.61461\n",
      "Epoch: 53, Time: 0.02663s, Loss: 0.62262\n",
      "Epoch: 54, Time: 0.02011s, Loss: 0.61802\n",
      "Epoch: 55, Time: 0.03195s, Loss: 0.61432\n",
      "Epoch: 56, Time: 0.02038s, Loss: 0.61657\n",
      "Epoch: 57, Time: 0.01955s, Loss: 0.61865\n",
      "Epoch: 58, Time: 0.02969s, Loss: 0.61354\n",
      "Epoch: 59, Time: 0.02999s, Loss: 0.61353\n",
      "Epoch: 60, Time: 0.02097s, Loss: 0.61331\n",
      "Epoch: 61, Time: 0.01836s, Loss: 0.61216\n",
      "Epoch: 62, Time: 0.02271s, Loss: 0.61420\n",
      "Epoch: 63, Time: 0.02872s, Loss: 0.61099\n",
      "Epoch: 64, Time: 0.03217s, Loss: 0.61634\n",
      "Epoch: 65, Time: 0.02302s, Loss: 0.61032\n",
      "Epoch: 66, Time: 0.02426s, Loss: 0.60904\n",
      "Epoch: 67, Time: 0.02943s, Loss: 0.61776\n",
      "Epoch: 68, Time: 0.02887s, Loss: 0.60579\n",
      "Epoch: 69, Time: 0.01839s, Loss: 0.60504\n",
      "Epoch: 70, Time: 0.02422s, Loss: 0.61466\n",
      "Epoch: 71, Time: 0.01977s, Loss: 0.60815\n",
      "Epoch: 72, Time: 0.02207s, Loss: 0.61763\n",
      "Epoch: 73, Time: 0.01828s, Loss: 0.60688\n",
      "Epoch: 74, Time: 0.03108s, Loss: 0.61310\n",
      "Epoch: 75, Time: 0.01572s, Loss: 0.60542\n",
      "Epoch: 76, Time: 0.01707s, Loss: 0.61219\n",
      "Epoch: 77, Time: 0.03022s, Loss: 0.61553\n",
      "Epoch: 78, Time: 0.02732s, Loss: 0.61495\n",
      "Epoch: 79, Time: 0.02265s, Loss: 0.61689\n",
      "Epoch: 80, Time: 0.02851s, Loss: 0.60669\n",
      "Epoch: 81, Time: 0.02210s, Loss: 0.60969\n",
      "Epoch: 82, Time: 0.01619s, Loss: 0.61929\n",
      "Epoch: 83, Time: 0.02443s, Loss: 0.61542\n",
      "Epoch: 84, Time: 0.02955s, Loss: 0.60436\n",
      "Epoch: 85, Time: 0.02612s, Loss: 0.60759\n",
      "Epoch: 86, Time: 0.03129s, Loss: 0.60884\n",
      "Epoch: 87, Time: 0.02666s, Loss: 0.60636\n",
      "Epoch: 88, Time: 0.01947s, Loss: 0.60711\n",
      "Epoch: 89, Time: 0.02033s, Loss: 0.60653\n",
      "Epoch: 90, Time: 0.02499s, Loss: 0.60724\n",
      "Epoch: 91, Time: 0.02621s, Loss: 0.60746\n",
      "Epoch: 92, Time: 0.02258s, Loss: 0.60360\n",
      "Epoch: 93, Time: 0.03076s, Loss: 0.60428\n",
      "Epoch: 94, Time: 0.02602s, Loss: 0.60080\n",
      "Epoch: 95, Time: 0.02905s, Loss: 0.60050\n",
      "Epoch: 96, Time: 0.03239s, Loss: 0.60541\n",
      "Epoch: 97, Time: 0.01608s, Loss: 0.60161\n",
      "Epoch: 98, Time: 0.02431s, Loss: 0.60571\n",
      "Epoch: 99, Time: 0.02872s, Loss: 0.63271\n",
      "    ↳ HGNNP Fold Result — Acc: 0.6085, F1: 0.6085\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03164s, Loss: 1.07392\n",
      "Epoch: 1, Time: 0.02791s, Loss: 0.92304\n",
      "Epoch: 2, Time: 0.02091s, Loss: 0.85516\n",
      "Epoch: 3, Time: 0.02442s, Loss: 0.80914\n",
      "Epoch: 4, Time: 0.02651s, Loss: 0.77285\n",
      "Epoch: 5, Time: 0.02554s, Loss: 0.74954\n",
      "Epoch: 6, Time: 0.01806s, Loss: 0.73713\n",
      "Epoch: 7, Time: 0.03026s, Loss: 0.72084\n",
      "Epoch: 8, Time: 0.02910s, Loss: 0.71354\n",
      "Epoch: 9, Time: 0.02245s, Loss: 0.70617\n",
      "Epoch: 10, Time: 0.02948s, Loss: 0.69790\n",
      "Epoch: 11, Time: 0.02936s, Loss: 0.69257\n",
      "Epoch: 12, Time: 0.02471s, Loss: 0.68921\n",
      "Epoch: 13, Time: 0.02649s, Loss: 0.68574\n",
      "Epoch: 14, Time: 0.02822s, Loss: 0.68108\n",
      "Epoch: 15, Time: 0.03014s, Loss: 0.67826\n",
      "Epoch: 16, Time: 0.02625s, Loss: 0.67601\n",
      "Epoch: 17, Time: 0.02176s, Loss: 0.67485\n",
      "Epoch: 18, Time: 0.02269s, Loss: 0.67246\n",
      "Epoch: 19, Time: 0.02184s, Loss: 0.67301\n",
      "Epoch: 20, Time: 0.03568s, Loss: 0.66966\n",
      "Epoch: 21, Time: 0.02141s, Loss: 0.66905\n",
      "Epoch: 22, Time: 0.02153s, Loss: 0.66836\n",
      "Epoch: 23, Time: 0.01920s, Loss: 0.66551\n",
      "Epoch: 24, Time: 0.02179s, Loss: 0.66763\n",
      "Epoch: 25, Time: 0.02421s, Loss: 0.66365\n",
      "Epoch: 26, Time: 0.02872s, Loss: 0.66274\n",
      "Epoch: 27, Time: 0.02915s, Loss: 0.66283\n",
      "Epoch: 28, Time: 0.02043s, Loss: 0.66281\n",
      "Epoch: 29, Time: 0.02894s, Loss: 0.66001\n",
      "Epoch: 30, Time: 0.02384s, Loss: 0.66219\n",
      "Epoch: 31, Time: 0.02592s, Loss: 0.65908\n",
      "Epoch: 32, Time: 0.02653s, Loss: 0.66447\n",
      "Epoch: 33, Time: 0.02410s, Loss: 0.65730\n",
      "Epoch: 34, Time: 0.02615s, Loss: 0.65975\n",
      "Epoch: 35, Time: 0.02518s, Loss: 0.65722\n",
      "Epoch: 36, Time: 0.02676s, Loss: 0.66501\n",
      "Epoch: 37, Time: 0.02939s, Loss: 0.65767\n",
      "Epoch: 38, Time: 0.02291s, Loss: 0.67005\n",
      "Epoch: 39, Time: 0.03110s, Loss: 0.65643\n",
      "Epoch: 40, Time: 0.02461s, Loss: 0.65871\n",
      "Epoch: 41, Time: 0.03137s, Loss: 0.65797\n",
      "Epoch: 42, Time: 0.02521s, Loss: 0.65668\n",
      "Epoch: 43, Time: 0.02938s, Loss: 0.65701\n",
      "Epoch: 44, Time: 0.03372s, Loss: 0.65421\n",
      "Epoch: 45, Time: 0.03166s, Loss: 0.65239\n",
      "Epoch: 46, Time: 0.03072s, Loss: 0.65486\n",
      "Epoch: 47, Time: 0.02701s, Loss: 0.65535\n",
      "Epoch: 48, Time: 0.03452s, Loss: 0.65364\n",
      "Epoch: 49, Time: 0.03059s, Loss: 0.65144\n",
      "Epoch: 50, Time: 0.03293s, Loss: 0.65170\n",
      "Epoch: 51, Time: 0.02819s, Loss: 0.64874\n",
      "Epoch: 52, Time: 0.02926s, Loss: 0.64916\n",
      "Epoch: 53, Time: 0.02887s, Loss: 0.65301\n",
      "Epoch: 54, Time: 0.02509s, Loss: 0.64807\n",
      "Epoch: 55, Time: 0.02817s, Loss: 0.64907\n",
      "Epoch: 56, Time: 0.02985s, Loss: 0.65115\n",
      "Epoch: 57, Time: 0.02761s, Loss: 0.65274\n",
      "Epoch: 58, Time: 0.03065s, Loss: 0.64688\n",
      "Epoch: 59, Time: 0.03391s, Loss: 0.64746\n",
      "Epoch: 60, Time: 0.03547s, Loss: 0.64445\n",
      "Epoch: 61, Time: 0.03422s, Loss: 0.64699\n",
      "Epoch: 62, Time: 0.03034s, Loss: 0.64583\n",
      "Epoch: 63, Time: 0.03234s, Loss: 0.64420\n",
      "Epoch: 64, Time: 0.02620s, Loss: 0.64406\n",
      "Epoch: 65, Time: 0.05165s, Loss: 0.64213\n",
      "Epoch: 66, Time: 0.03313s, Loss: 0.66050\n",
      "Epoch: 67, Time: 0.03274s, Loss: 0.65369\n",
      "Epoch: 68, Time: 0.03101s, Loss: 0.65242\n",
      "Epoch: 69, Time: 0.03118s, Loss: 0.65805\n",
      "Epoch: 70, Time: 0.02944s, Loss: 0.64522\n",
      "Epoch: 71, Time: 0.03317s, Loss: 0.65344\n",
      "Epoch: 72, Time: 0.03240s, Loss: 0.65028\n",
      "Epoch: 73, Time: 0.03119s, Loss: 0.64843\n",
      "Epoch: 74, Time: 0.03263s, Loss: 0.64378\n",
      "Epoch: 75, Time: 0.03294s, Loss: 0.64550\n",
      "Epoch: 76, Time: 0.03290s, Loss: 0.64754\n",
      "Epoch: 77, Time: 0.03099s, Loss: 0.64376\n",
      "Epoch: 78, Time: 0.02897s, Loss: 0.64246\n",
      "Epoch: 79, Time: 0.03543s, Loss: 0.64347\n",
      "Epoch: 80, Time: 0.03915s, Loss: 0.65035\n",
      "Epoch: 81, Time: 0.02852s, Loss: 0.65069\n",
      "Epoch: 82, Time: 0.02572s, Loss: 0.64528\n",
      "Epoch: 83, Time: 0.03358s, Loss: 0.64120\n",
      "Epoch: 84, Time: 0.02975s, Loss: 0.65192\n",
      "Epoch: 85, Time: 0.02778s, Loss: 0.64446\n",
      "Epoch: 86, Time: 0.03257s, Loss: 0.64420\n",
      "Epoch: 87, Time: 0.03105s, Loss: 0.64796\n",
      "Epoch: 88, Time: 0.03084s, Loss: 0.64468\n",
      "Epoch: 89, Time: 0.03408s, Loss: 0.64490\n",
      "Epoch: 90, Time: 0.03597s, Loss: 0.64278\n",
      "Epoch: 91, Time: 0.03460s, Loss: 0.64176\n",
      "Epoch: 92, Time: 0.03320s, Loss: 0.64191\n",
      "Epoch: 93, Time: 0.03079s, Loss: 0.63718\n",
      "Epoch: 94, Time: 0.03352s, Loss: 0.64227\n",
      "Epoch: 95, Time: 0.03358s, Loss: 0.63813\n",
      "Epoch: 96, Time: 0.02935s, Loss: 0.64363\n",
      "Epoch: 97, Time: 0.02872s, Loss: 0.64220\n",
      "Epoch: 98, Time: 0.02903s, Loss: 0.64059\n",
      "Epoch: 99, Time: 0.02767s, Loss: 0.63556\n",
      "    ↳ UniGCN Fold Result — Acc: 0.6085, F1: 0.6085\n",
      "\n",
      "==> Running HouseCommittees with Top-k\n",
      "\n",
      "[HouseCommittees] Fold 1/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.22720s, Loss: 1.12324\n",
      "Epoch: 1, Time: 0.19981s, Loss: 1.10228\n",
      "Epoch: 2, Time: 0.18194s, Loss: 1.08304\n",
      "Epoch: 3, Time: 0.19020s, Loss: 1.05912\n",
      "Epoch: 4, Time: 0.19650s, Loss: 1.03116\n",
      "Epoch: 5, Time: 0.18744s, Loss: 1.00201\n",
      "Epoch: 6, Time: 0.18586s, Loss: 0.96827\n",
      "Epoch: 7, Time: 0.19381s, Loss: 0.93701\n",
      "Epoch: 8, Time: 0.17840s, Loss: 0.90702\n",
      "Epoch: 9, Time: 0.17661s, Loss: 0.87709\n",
      "Epoch: 10, Time: 0.19385s, Loss: 0.85265\n",
      "Epoch: 11, Time: 0.18423s, Loss: 0.83081\n",
      "Epoch: 12, Time: 0.19583s, Loss: 0.81204\n",
      "Epoch: 13, Time: 0.18946s, Loss: 0.79524\n",
      "Epoch: 14, Time: 0.18915s, Loss: 0.78305\n",
      "Epoch: 15, Time: 0.18391s, Loss: 0.77099\n",
      "Epoch: 16, Time: 0.18187s, Loss: 0.76143\n",
      "Epoch: 17, Time: 0.18628s, Loss: 0.75296\n",
      "Epoch: 18, Time: 0.17138s, Loss: 0.74562\n",
      "Epoch: 19, Time: 0.17871s, Loss: 0.73976\n",
      "Epoch: 20, Time: 0.17679s, Loss: 0.73446\n",
      "Epoch: 21, Time: 0.18754s, Loss: 0.73046\n",
      "Epoch: 22, Time: 0.18020s, Loss: 0.72589\n",
      "Epoch: 23, Time: 0.14798s, Loss: 0.72243\n",
      "Epoch: 24, Time: 0.12483s, Loss: 0.71995\n",
      "Epoch: 25, Time: 0.13487s, Loss: 0.71685\n",
      "Epoch: 26, Time: 0.13747s, Loss: 0.71460\n",
      "Epoch: 27, Time: 0.12265s, Loss: 0.71211\n",
      "Epoch: 28, Time: 0.14492s, Loss: 0.70977\n",
      "Epoch: 29, Time: 0.15707s, Loss: 0.70812\n",
      "Epoch: 30, Time: 0.12834s, Loss: 0.70686\n",
      "Epoch: 31, Time: 0.12799s, Loss: 0.70514\n",
      "Epoch: 32, Time: 0.12681s, Loss: 0.70424\n",
      "Epoch: 33, Time: 0.15786s, Loss: 0.70255\n",
      "Epoch: 34, Time: 0.11562s, Loss: 0.70174\n",
      "Epoch: 35, Time: 0.17845s, Loss: 0.70105\n",
      "Epoch: 36, Time: 0.18148s, Loss: 0.70058\n",
      "Epoch: 37, Time: 0.18845s, Loss: 0.69924\n",
      "Epoch: 38, Time: 0.18430s, Loss: 0.69889\n",
      "Epoch: 39, Time: 0.17690s, Loss: 0.69815\n",
      "Epoch: 40, Time: 0.19939s, Loss: 0.69795\n",
      "Epoch: 41, Time: 0.18370s, Loss: 0.69708\n",
      "Epoch: 42, Time: 0.18332s, Loss: 0.69701\n",
      "Epoch: 43, Time: 0.18941s, Loss: 0.69681\n",
      "Epoch: 44, Time: 0.17808s, Loss: 0.69650\n",
      "Epoch: 45, Time: 0.18285s, Loss: 0.69628\n",
      "Epoch: 46, Time: 0.17746s, Loss: 0.69582\n",
      "Epoch: 47, Time: 0.18052s, Loss: 0.69575\n",
      "Epoch: 48, Time: 0.18240s, Loss: 0.69539\n",
      "Epoch: 49, Time: 0.18095s, Loss: 0.69534\n",
      "Epoch: 50, Time: 0.19618s, Loss: 0.69475\n",
      "Epoch: 51, Time: 0.17932s, Loss: 0.69503\n",
      "Epoch: 52, Time: 0.18331s, Loss: 0.69505\n",
      "Epoch: 53, Time: 0.18370s, Loss: 0.69460\n",
      "Epoch: 54, Time: 0.19014s, Loss: 0.69443\n",
      "Epoch: 55, Time: 0.17999s, Loss: 0.69449\n",
      "Epoch: 56, Time: 0.16946s, Loss: 0.69467\n",
      "Epoch: 57, Time: 0.18997s, Loss: 0.69420\n",
      "Epoch: 58, Time: 0.19104s, Loss: 0.69435\n",
      "Epoch: 59, Time: 0.17643s, Loss: 0.69455\n",
      "Epoch: 60, Time: 0.16885s, Loss: 0.69399\n",
      "Epoch: 61, Time: 0.18197s, Loss: 0.69412\n",
      "Epoch: 62, Time: 0.18465s, Loss: 0.69422\n",
      "Epoch: 63, Time: 0.19259s, Loss: 0.69416\n",
      "Epoch: 64, Time: 0.18854s, Loss: 0.69394\n",
      "Epoch: 65, Time: 0.19616s, Loss: 0.69442\n",
      "Epoch: 66, Time: 0.18832s, Loss: 0.69377\n",
      "Epoch: 67, Time: 0.18839s, Loss: 0.69383\n",
      "Epoch: 68, Time: 0.18726s, Loss: 0.69372\n",
      "Epoch: 69, Time: 0.18103s, Loss: 0.69399\n",
      "Epoch: 70, Time: 0.18108s, Loss: 0.69391\n",
      "Epoch: 71, Time: 0.18153s, Loss: 0.69377\n",
      "Epoch: 72, Time: 0.18602s, Loss: 0.69410\n",
      "Epoch: 73, Time: 0.17234s, Loss: 0.69300\n",
      "Epoch: 74, Time: 0.20396s, Loss: 0.69368\n",
      "Epoch: 75, Time: 0.18193s, Loss: 0.69392\n",
      "Epoch: 76, Time: 0.18846s, Loss: 0.69365\n",
      "Epoch: 77, Time: 0.18493s, Loss: 0.69375\n",
      "Epoch: 78, Time: 0.19334s, Loss: 0.69395\n",
      "Epoch: 79, Time: 0.18496s, Loss: 0.69374\n",
      "Epoch: 80, Time: 0.17525s, Loss: 0.69395\n",
      "Epoch: 81, Time: 0.17735s, Loss: 0.69346\n",
      "Epoch: 82, Time: 0.19065s, Loss: 0.69362\n",
      "Epoch: 83, Time: 0.18672s, Loss: 0.69309\n",
      "Epoch: 84, Time: 0.19227s, Loss: 0.69377\n",
      "Epoch: 85, Time: 0.18133s, Loss: 0.69360\n",
      "Epoch: 86, Time: 0.18622s, Loss: 0.69346\n",
      "Epoch: 87, Time: 0.18737s, Loss: 0.69274\n",
      "Epoch: 88, Time: 0.18614s, Loss: 0.69378\n",
      "Epoch: 89, Time: 0.19176s, Loss: 0.69373\n",
      "Epoch: 90, Time: 0.18773s, Loss: 0.69390\n",
      "Epoch: 91, Time: 0.17514s, Loss: 0.69397\n",
      "Epoch: 92, Time: 0.20093s, Loss: 0.69334\n",
      "Epoch: 93, Time: 0.19185s, Loss: 0.69319\n",
      "Epoch: 94, Time: 0.18331s, Loss: 0.69353\n",
      "Epoch: 95, Time: 0.17835s, Loss: 0.69366\n",
      "Epoch: 96, Time: 0.18443s, Loss: 0.69312\n",
      "Epoch: 97, Time: 0.17962s, Loss: 0.69322\n",
      "Epoch: 98, Time: 0.18795s, Loss: 0.69375\n",
      "Epoch: 99, Time: 0.18635s, Loss: 0.69306\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03213s, Loss: 1.07429\n",
      "Epoch: 1, Time: 0.02974s, Loss: 0.98814\n",
      "Epoch: 2, Time: 0.03280s, Loss: 0.94016\n",
      "Epoch: 3, Time: 0.03277s, Loss: 0.88351\n",
      "Epoch: 4, Time: 0.02825s, Loss: 0.83316\n",
      "Epoch: 5, Time: 0.03268s, Loss: 0.81032\n",
      "Epoch: 6, Time: 0.03157s, Loss: 0.77289\n",
      "Epoch: 7, Time: 0.03325s, Loss: 0.74646\n",
      "Epoch: 8, Time: 0.02754s, Loss: 0.72526\n",
      "Epoch: 9, Time: 0.02986s, Loss: 0.70445\n",
      "Epoch: 10, Time: 0.04043s, Loss: 0.68913\n",
      "Epoch: 11, Time: 0.03316s, Loss: 0.67523\n",
      "Epoch: 12, Time: 0.02986s, Loss: 0.66400\n",
      "Epoch: 13, Time: 0.03088s, Loss: 0.65750\n",
      "Epoch: 14, Time: 0.03405s, Loss: 0.64940\n",
      "Epoch: 15, Time: 0.02966s, Loss: 0.64359\n",
      "Epoch: 16, Time: 0.03106s, Loss: 0.64075\n",
      "Epoch: 17, Time: 0.03213s, Loss: 0.63903\n",
      "Epoch: 18, Time: 0.02932s, Loss: 0.63419\n",
      "Epoch: 19, Time: 0.03578s, Loss: 0.63125\n",
      "Epoch: 20, Time: 0.03076s, Loss: 0.63021\n",
      "Epoch: 21, Time: 0.03324s, Loss: 0.62952\n",
      "Epoch: 22, Time: 0.03310s, Loss: 0.62823\n",
      "Epoch: 23, Time: 0.03342s, Loss: 0.62719\n",
      "Epoch: 24, Time: 0.03062s, Loss: 0.62420\n",
      "Epoch: 25, Time: 0.03388s, Loss: 0.62363\n",
      "Epoch: 26, Time: 0.02436s, Loss: 0.62484\n",
      "Epoch: 27, Time: 0.03211s, Loss: 0.62207\n",
      "Epoch: 28, Time: 0.03029s, Loss: 0.62024\n",
      "Epoch: 29, Time: 0.03203s, Loss: 0.61959\n",
      "Epoch: 30, Time: 0.02948s, Loss: 0.61817\n",
      "Epoch: 31, Time: 0.03147s, Loss: 0.61822\n",
      "Epoch: 32, Time: 0.03185s, Loss: 0.61931\n",
      "Epoch: 33, Time: 0.03004s, Loss: 0.61438\n",
      "Epoch: 34, Time: 0.03106s, Loss: 0.61915\n",
      "Epoch: 35, Time: 0.03130s, Loss: 0.61715\n",
      "Epoch: 36, Time: 0.03333s, Loss: 0.61388\n",
      "Epoch: 37, Time: 0.03472s, Loss: 0.62017\n",
      "Epoch: 38, Time: 0.03487s, Loss: 0.61617\n",
      "Epoch: 39, Time: 0.02850s, Loss: 0.61772\n",
      "Epoch: 40, Time: 0.03177s, Loss: 0.62929\n",
      "Epoch: 41, Time: 0.03068s, Loss: 0.61240\n",
      "Epoch: 42, Time: 0.03200s, Loss: 0.62519\n",
      "Epoch: 43, Time: 0.03492s, Loss: 0.61417\n",
      "Epoch: 44, Time: 0.02448s, Loss: 0.62304\n",
      "Epoch: 45, Time: 0.03026s, Loss: 0.61147\n",
      "Epoch: 46, Time: 0.03203s, Loss: 0.62409\n",
      "Epoch: 47, Time: 0.03212s, Loss: 0.61345\n",
      "Epoch: 48, Time: 0.03420s, Loss: 0.61614\n",
      "Epoch: 49, Time: 0.02852s, Loss: 0.61343\n",
      "Epoch: 50, Time: 0.03290s, Loss: 0.61234\n",
      "Epoch: 51, Time: 0.03005s, Loss: 0.61267\n",
      "Epoch: 52, Time: 0.02975s, Loss: 0.60815\n",
      "Epoch: 53, Time: 0.03349s, Loss: 0.61483\n",
      "Epoch: 54, Time: 0.03042s, Loss: 0.60748\n",
      "Epoch: 55, Time: 0.03243s, Loss: 0.61340\n",
      "Epoch: 56, Time: 0.02446s, Loss: 0.60886\n",
      "Epoch: 57, Time: 0.02788s, Loss: 0.61011\n",
      "Epoch: 58, Time: 0.03046s, Loss: 0.60890\n",
      "Epoch: 59, Time: 0.03116s, Loss: 0.61074\n",
      "Epoch: 60, Time: 0.03807s, Loss: 0.61192\n",
      "Epoch: 61, Time: 0.03276s, Loss: 0.61129\n",
      "Epoch: 62, Time: 0.03024s, Loss: 0.60756\n",
      "Epoch: 63, Time: 0.02747s, Loss: 0.61549\n",
      "Epoch: 64, Time: 0.03234s, Loss: 0.61092\n",
      "Epoch: 65, Time: 0.02178s, Loss: 0.61197\n",
      "Epoch: 66, Time: 0.02658s, Loss: 0.60505\n",
      "Epoch: 67, Time: 0.03447s, Loss: 0.60359\n",
      "Epoch: 68, Time: 0.03368s, Loss: 0.60497\n",
      "Epoch: 69, Time: 0.03576s, Loss: 0.60079\n",
      "Epoch: 70, Time: 0.03128s, Loss: 0.60355\n",
      "Epoch: 71, Time: 0.01832s, Loss: 0.60282\n",
      "Epoch: 72, Time: 0.02445s, Loss: 0.60570\n",
      "Epoch: 73, Time: 0.02869s, Loss: 0.60462\n",
      "Epoch: 74, Time: 0.03175s, Loss: 0.60244\n",
      "Epoch: 75, Time: 0.02535s, Loss: 0.60272\n",
      "Epoch: 76, Time: 0.03531s, Loss: 0.60552\n",
      "Epoch: 77, Time: 0.03275s, Loss: 0.60533\n",
      "Epoch: 78, Time: 0.02636s, Loss: 0.59820\n",
      "Epoch: 79, Time: 0.03080s, Loss: 0.60920\n",
      "Epoch: 80, Time: 0.02491s, Loss: 0.60545\n",
      "Epoch: 81, Time: 0.02894s, Loss: 0.60897\n",
      "Epoch: 82, Time: 0.03491s, Loss: 0.60776\n",
      "Epoch: 83, Time: 0.03182s, Loss: 0.60334\n",
      "Epoch: 84, Time: 0.03328s, Loss: 0.61311\n",
      "Epoch: 85, Time: 0.03238s, Loss: 0.59954\n",
      "Epoch: 86, Time: 0.02761s, Loss: 0.61017\n",
      "Epoch: 87, Time: 0.02384s, Loss: 0.60446\n",
      "Epoch: 88, Time: 0.02962s, Loss: 0.62269\n",
      "Epoch: 89, Time: 0.02765s, Loss: 0.60535\n",
      "Epoch: 90, Time: 0.03207s, Loss: 0.61027\n",
      "Epoch: 91, Time: 0.02667s, Loss: 0.60862\n",
      "Epoch: 92, Time: 0.02970s, Loss: 0.60479\n",
      "Epoch: 93, Time: 0.02707s, Loss: 0.60140\n",
      "Epoch: 94, Time: 0.02524s, Loss: 0.60324\n",
      "Epoch: 95, Time: 0.02650s, Loss: 0.60051\n",
      "Epoch: 96, Time: 0.02968s, Loss: 0.60246\n",
      "Epoch: 97, Time: 0.02865s, Loss: 0.59887\n",
      "Epoch: 98, Time: 0.02425s, Loss: 0.60285\n",
      "Epoch: 99, Time: 0.02857s, Loss: 0.59914\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5969, F1: 0.5969\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03955s, Loss: 1.13695\n",
      "Epoch: 1, Time: 0.03001s, Loss: 0.96556\n",
      "Epoch: 2, Time: 0.02375s, Loss: 0.87246\n",
      "Epoch: 3, Time: 0.03405s, Loss: 0.82811\n",
      "Epoch: 4, Time: 0.03160s, Loss: 0.78695\n",
      "Epoch: 5, Time: 0.03314s, Loss: 0.76476\n",
      "Epoch: 6, Time: 0.03134s, Loss: 0.74469\n",
      "Epoch: 7, Time: 0.04059s, Loss: 0.73068\n",
      "Epoch: 8, Time: 0.03247s, Loss: 0.71979\n",
      "Epoch: 9, Time: 0.02566s, Loss: 0.71044\n",
      "Epoch: 10, Time: 0.03744s, Loss: 0.70161\n",
      "Epoch: 11, Time: 0.03189s, Loss: 0.69492\n",
      "Epoch: 12, Time: 0.03297s, Loss: 0.69034\n",
      "Epoch: 13, Time: 0.02697s, Loss: 0.68696\n",
      "Epoch: 14, Time: 0.03032s, Loss: 0.68032\n",
      "Epoch: 15, Time: 0.03387s, Loss: 0.67845\n",
      "Epoch: 16, Time: 0.03264s, Loss: 0.67319\n",
      "Epoch: 17, Time: 0.03156s, Loss: 0.67067\n",
      "Epoch: 18, Time: 0.03247s, Loss: 0.66765\n",
      "Epoch: 19, Time: 0.03041s, Loss: 0.66426\n",
      "Epoch: 20, Time: 0.02662s, Loss: 0.66405\n",
      "Epoch: 21, Time: 0.03475s, Loss: 0.66146\n",
      "Epoch: 22, Time: 0.03744s, Loss: 0.65978\n",
      "Epoch: 23, Time: 0.03536s, Loss: 0.65635\n",
      "Epoch: 24, Time: 0.03576s, Loss: 0.66243\n",
      "Epoch: 25, Time: 0.03005s, Loss: 0.65675\n",
      "Epoch: 26, Time: 0.03161s, Loss: 0.65556\n",
      "Epoch: 27, Time: 0.03542s, Loss: 0.65340\n",
      "Epoch: 28, Time: 0.03013s, Loss: 0.65374\n",
      "Epoch: 29, Time: 0.03550s, Loss: 0.65047\n",
      "Epoch: 30, Time: 0.03346s, Loss: 0.65260\n",
      "Epoch: 31, Time: 0.03227s, Loss: 0.65042\n",
      "Epoch: 32, Time: 0.03420s, Loss: 0.65329\n",
      "Epoch: 33, Time: 0.03085s, Loss: 0.65535\n",
      "Epoch: 34, Time: 0.02322s, Loss: 0.64810\n",
      "Epoch: 35, Time: 0.03466s, Loss: 0.65348\n",
      "Epoch: 36, Time: 0.02715s, Loss: 0.64757\n",
      "Epoch: 37, Time: 0.03197s, Loss: 0.64921\n",
      "Epoch: 38, Time: 0.03061s, Loss: 0.64652\n",
      "Epoch: 39, Time: 0.03456s, Loss: 0.64552\n",
      "Epoch: 40, Time: 0.03112s, Loss: 0.65123\n",
      "Epoch: 41, Time: 0.02980s, Loss: 0.64584\n",
      "Epoch: 42, Time: 0.02831s, Loss: 0.64580\n",
      "Epoch: 43, Time: 0.03046s, Loss: 0.64664\n",
      "Epoch: 44, Time: 0.03430s, Loss: 0.65945\n",
      "Epoch: 45, Time: 0.02691s, Loss: 0.64593\n",
      "Epoch: 46, Time: 0.03325s, Loss: 0.65805\n",
      "Epoch: 47, Time: 0.02715s, Loss: 0.64695\n",
      "Epoch: 48, Time: 0.03269s, Loss: 0.64344\n",
      "Epoch: 49, Time: 0.03307s, Loss: 0.64792\n",
      "Epoch: 50, Time: 0.03025s, Loss: 0.64492\n",
      "Epoch: 51, Time: 0.03074s, Loss: 0.64100\n",
      "Epoch: 52, Time: 0.03150s, Loss: 0.64639\n",
      "Epoch: 53, Time: 0.03471s, Loss: 0.64545\n",
      "Epoch: 54, Time: 0.03323s, Loss: 0.63705\n",
      "Epoch: 55, Time: 0.03193s, Loss: 0.65052\n",
      "Epoch: 56, Time: 0.03701s, Loss: 0.64050\n",
      "Epoch: 57, Time: 0.03245s, Loss: 0.64814\n",
      "Epoch: 58, Time: 0.03217s, Loss: 0.64080\n",
      "Epoch: 59, Time: 0.03380s, Loss: 0.64846\n",
      "Epoch: 60, Time: 0.03246s, Loss: 0.63900\n",
      "Epoch: 61, Time: 0.03578s, Loss: 0.64361\n",
      "Epoch: 62, Time: 0.03151s, Loss: 0.64089\n",
      "Epoch: 63, Time: 0.04018s, Loss: 0.63787\n",
      "Epoch: 64, Time: 0.03232s, Loss: 0.64481\n",
      "Epoch: 65, Time: 0.03385s, Loss: 0.64753\n",
      "Epoch: 66, Time: 0.03614s, Loss: 0.63624\n",
      "Epoch: 67, Time: 0.03018s, Loss: 0.64678\n",
      "Epoch: 68, Time: 0.02695s, Loss: 0.63878\n",
      "Epoch: 69, Time: 0.03339s, Loss: 0.64231\n",
      "Epoch: 70, Time: 0.03291s, Loss: 0.64210\n",
      "Epoch: 71, Time: 0.03766s, Loss: 0.63567\n",
      "Epoch: 72, Time: 0.02559s, Loss: 0.64409\n",
      "Epoch: 73, Time: 0.03194s, Loss: 0.63612\n",
      "Epoch: 74, Time: 0.03400s, Loss: 0.64271\n",
      "Epoch: 75, Time: 0.03044s, Loss: 0.64189\n",
      "Epoch: 76, Time: 0.02966s, Loss: 0.64039\n",
      "Epoch: 77, Time: 0.03164s, Loss: 0.63911\n",
      "Epoch: 78, Time: 0.03221s, Loss: 0.63600\n",
      "Epoch: 79, Time: 0.03242s, Loss: 0.63933\n",
      "Epoch: 80, Time: 0.03079s, Loss: 0.63791\n",
      "Epoch: 81, Time: 0.03246s, Loss: 0.64172\n",
      "Epoch: 82, Time: 0.03123s, Loss: 0.63674\n",
      "Epoch: 83, Time: 0.03439s, Loss: 0.63411\n",
      "Epoch: 84, Time: 0.02777s, Loss: 0.64348\n",
      "Epoch: 85, Time: 0.02946s, Loss: 0.63714\n",
      "Epoch: 86, Time: 0.03392s, Loss: 0.63822\n",
      "Epoch: 87, Time: 0.03230s, Loss: 0.63854\n",
      "Epoch: 88, Time: 0.03218s, Loss: 0.63733\n",
      "Epoch: 89, Time: 0.03385s, Loss: 0.63846\n",
      "Epoch: 90, Time: 0.03216s, Loss: 0.63729\n",
      "Epoch: 91, Time: 0.03033s, Loss: 0.63956\n",
      "Epoch: 92, Time: 0.03204s, Loss: 0.62905\n",
      "Epoch: 93, Time: 0.03231s, Loss: 0.63520\n",
      "Epoch: 94, Time: 0.02849s, Loss: 0.63554\n",
      "Epoch: 95, Time: 0.03519s, Loss: 0.64471\n",
      "Epoch: 96, Time: 0.03327s, Loss: 0.63885\n",
      "Epoch: 97, Time: 0.03460s, Loss: 0.63529\n",
      "Epoch: 98, Time: 0.03180s, Loss: 0.64472\n",
      "Epoch: 99, Time: 0.03108s, Loss: 0.63457\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5930, F1: 0.5930\n",
      "\n",
      "[HouseCommittees] Fold 2/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.23987s, Loss: 1.07529\n",
      "Epoch: 1, Time: 0.18936s, Loss: 1.05268\n",
      "Epoch: 2, Time: 0.18845s, Loss: 1.02785\n",
      "Epoch: 3, Time: 0.16072s, Loss: 1.00015\n",
      "Epoch: 4, Time: 0.18423s, Loss: 0.96966\n",
      "Epoch: 5, Time: 0.19253s, Loss: 0.93931\n",
      "Epoch: 6, Time: 0.17464s, Loss: 0.90945\n",
      "Epoch: 7, Time: 0.17957s, Loss: 0.88036\n",
      "Epoch: 8, Time: 0.18220s, Loss: 0.85773\n",
      "Epoch: 9, Time: 0.19543s, Loss: 0.83343\n",
      "Epoch: 10, Time: 0.18148s, Loss: 0.81450\n",
      "Epoch: 11, Time: 0.17320s, Loss: 0.79929\n",
      "Epoch: 12, Time: 0.18498s, Loss: 0.78514\n",
      "Epoch: 13, Time: 0.20018s, Loss: 0.77267\n",
      "Epoch: 14, Time: 0.19496s, Loss: 0.76390\n",
      "Epoch: 15, Time: 0.18186s, Loss: 0.75388\n",
      "Epoch: 16, Time: 0.18287s, Loss: 0.74676\n",
      "Epoch: 17, Time: 0.18878s, Loss: 0.74085\n",
      "Epoch: 18, Time: 0.18304s, Loss: 0.73528\n",
      "Epoch: 19, Time: 0.18292s, Loss: 0.73096\n",
      "Epoch: 20, Time: 0.17572s, Loss: 0.72722\n",
      "Epoch: 21, Time: 0.19524s, Loss: 0.72321\n",
      "Epoch: 22, Time: 0.17765s, Loss: 0.71986\n",
      "Epoch: 23, Time: 0.18720s, Loss: 0.71649\n",
      "Epoch: 24, Time: 0.18734s, Loss: 0.71423\n",
      "Epoch: 25, Time: 0.18850s, Loss: 0.71116\n",
      "Epoch: 26, Time: 0.18627s, Loss: 0.70992\n",
      "Epoch: 27, Time: 0.18811s, Loss: 0.70789\n",
      "Epoch: 28, Time: 0.18932s, Loss: 0.70633\n",
      "Epoch: 29, Time: 0.17978s, Loss: 0.70472\n",
      "Epoch: 30, Time: 0.18018s, Loss: 0.70375\n",
      "Epoch: 31, Time: 0.18757s, Loss: 0.70253\n",
      "Epoch: 32, Time: 0.17989s, Loss: 0.70131\n",
      "Epoch: 33, Time: 0.19219s, Loss: 0.70074\n",
      "Epoch: 34, Time: 0.18524s, Loss: 0.70014\n",
      "Epoch: 35, Time: 0.18344s, Loss: 0.69927\n",
      "Epoch: 36, Time: 0.18755s, Loss: 0.69864\n",
      "Epoch: 37, Time: 0.19012s, Loss: 0.69757\n",
      "Epoch: 38, Time: 0.18951s, Loss: 0.69736\n",
      "Epoch: 39, Time: 0.19419s, Loss: 0.69673\n",
      "Epoch: 40, Time: 0.19318s, Loss: 0.69656\n",
      "Epoch: 41, Time: 0.18258s, Loss: 0.69634\n",
      "Epoch: 42, Time: 0.18247s, Loss: 0.69581\n",
      "Epoch: 43, Time: 0.17900s, Loss: 0.69520\n",
      "Epoch: 44, Time: 0.18519s, Loss: 0.69591\n",
      "Epoch: 45, Time: 0.18637s, Loss: 0.69518\n",
      "Epoch: 46, Time: 0.17902s, Loss: 0.69559\n",
      "Epoch: 47, Time: 0.18619s, Loss: 0.69544\n",
      "Epoch: 48, Time: 0.19839s, Loss: 0.69465\n",
      "Epoch: 49, Time: 0.19420s, Loss: 0.69446\n",
      "Epoch: 50, Time: 0.19387s, Loss: 0.69448\n",
      "Epoch: 51, Time: 0.17949s, Loss: 0.69465\n",
      "Epoch: 52, Time: 0.18635s, Loss: 0.69447\n",
      "Epoch: 53, Time: 0.18624s, Loss: 0.69397\n",
      "Epoch: 54, Time: 0.19004s, Loss: 0.69402\n",
      "Epoch: 55, Time: 0.18537s, Loss: 0.69389\n",
      "Epoch: 56, Time: 0.18576s, Loss: 0.69367\n",
      "Epoch: 57, Time: 0.19083s, Loss: 0.69409\n",
      "Epoch: 58, Time: 0.16840s, Loss: 0.69417\n",
      "Epoch: 59, Time: 0.18578s, Loss: 0.69435\n",
      "Epoch: 60, Time: 0.18922s, Loss: 0.69392\n",
      "Epoch: 61, Time: 0.18634s, Loss: 0.69408\n",
      "Epoch: 62, Time: 0.18275s, Loss: 0.69407\n",
      "Epoch: 63, Time: 0.17097s, Loss: 0.69346\n",
      "Epoch: 64, Time: 0.18298s, Loss: 0.69361\n",
      "Epoch: 65, Time: 0.18279s, Loss: 0.69371\n",
      "Epoch: 66, Time: 0.19372s, Loss: 0.69418\n",
      "Epoch: 67, Time: 0.18249s, Loss: 0.69348\n",
      "Epoch: 68, Time: 0.18445s, Loss: 0.69344\n",
      "Epoch: 69, Time: 0.17139s, Loss: 0.69338\n",
      "Epoch: 70, Time: 0.17922s, Loss: 0.69371\n",
      "Epoch: 71, Time: 0.19297s, Loss: 0.69330\n",
      "Epoch: 72, Time: 0.19334s, Loss: 0.69337\n",
      "Epoch: 73, Time: 0.18860s, Loss: 0.69352\n",
      "Epoch: 74, Time: 0.18394s, Loss: 0.69304\n",
      "Epoch: 75, Time: 0.17931s, Loss: 0.69345\n",
      "Epoch: 76, Time: 0.17353s, Loss: 0.69349\n",
      "Epoch: 77, Time: 0.20093s, Loss: 0.69333\n",
      "Epoch: 78, Time: 0.18631s, Loss: 0.69350\n",
      "Epoch: 79, Time: 0.18196s, Loss: 0.69274\n",
      "Epoch: 80, Time: 0.17780s, Loss: 0.69316\n",
      "Epoch: 81, Time: 0.16826s, Loss: 0.69320\n",
      "Epoch: 82, Time: 0.18226s, Loss: 0.69218\n",
      "Epoch: 83, Time: 0.19895s, Loss: 0.69288\n",
      "Epoch: 84, Time: 0.19350s, Loss: 0.69353\n",
      "Epoch: 85, Time: 0.18089s, Loss: 0.69339\n",
      "Epoch: 86, Time: 0.17586s, Loss: 0.69277\n",
      "Epoch: 87, Time: 0.17542s, Loss: 0.69300\n",
      "Epoch: 88, Time: 0.17657s, Loss: 0.69308\n",
      "Epoch: 89, Time: 0.17755s, Loss: 0.69288\n",
      "Epoch: 90, Time: 0.18642s, Loss: 0.69224\n",
      "Epoch: 91, Time: 0.19632s, Loss: 0.69336\n",
      "Epoch: 92, Time: 0.17161s, Loss: 0.69320\n",
      "Epoch: 93, Time: 0.17878s, Loss: 0.69257\n",
      "Epoch: 94, Time: 0.18131s, Loss: 0.69206\n",
      "Epoch: 95, Time: 0.17900s, Loss: 0.69271\n",
      "Epoch: 96, Time: 0.17097s, Loss: 0.69317\n",
      "Epoch: 97, Time: 0.18779s, Loss: 0.69222\n",
      "Epoch: 98, Time: 0.17830s, Loss: 0.69260\n",
      "Epoch: 99, Time: 0.18097s, Loss: 0.69297\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03156s, Loss: 1.12315\n",
      "Epoch: 1, Time: 0.03385s, Loss: 1.06649\n",
      "Epoch: 2, Time: 0.02995s, Loss: 0.99768\n",
      "Epoch: 3, Time: 0.03208s, Loss: 0.93705\n",
      "Epoch: 4, Time: 0.01871s, Loss: 0.89716\n",
      "Epoch: 5, Time: 0.02715s, Loss: 0.85333\n",
      "Epoch: 6, Time: 0.03249s, Loss: 0.81060\n",
      "Epoch: 7, Time: 0.03174s, Loss: 0.78493\n",
      "Epoch: 8, Time: 0.03308s, Loss: 0.75368\n",
      "Epoch: 9, Time: 0.02648s, Loss: 0.72994\n",
      "Epoch: 10, Time: 0.03559s, Loss: 0.71015\n",
      "Epoch: 11, Time: 0.02913s, Loss: 0.69495\n",
      "Epoch: 12, Time: 0.03258s, Loss: 0.68113\n",
      "Epoch: 13, Time: 0.03539s, Loss: 0.67124\n",
      "Epoch: 14, Time: 0.02947s, Loss: 0.66409\n",
      "Epoch: 15, Time: 0.02987s, Loss: 0.65648\n",
      "Epoch: 16, Time: 0.02955s, Loss: 0.65005\n",
      "Epoch: 17, Time: 0.03123s, Loss: 0.64546\n",
      "Epoch: 18, Time: 0.02560s, Loss: 0.64319\n",
      "Epoch: 19, Time: 0.03008s, Loss: 0.64017\n",
      "Epoch: 20, Time: 0.02483s, Loss: 0.63670\n",
      "Epoch: 21, Time: 0.03181s, Loss: 0.63497\n",
      "Epoch: 22, Time: 0.02996s, Loss: 0.63371\n",
      "Epoch: 23, Time: 0.02591s, Loss: 0.63226\n",
      "Epoch: 24, Time: 0.03453s, Loss: 0.63088\n",
      "Epoch: 25, Time: 0.02995s, Loss: 0.63042\n",
      "Epoch: 26, Time: 0.02842s, Loss: 0.62649\n",
      "Epoch: 27, Time: 0.03067s, Loss: 0.62846\n",
      "Epoch: 28, Time: 0.03249s, Loss: 0.62516\n",
      "Epoch: 29, Time: 0.03051s, Loss: 0.62505\n",
      "Epoch: 30, Time: 0.03572s, Loss: 0.62387\n",
      "Epoch: 31, Time: 0.02701s, Loss: 0.62336\n",
      "Epoch: 32, Time: 0.03191s, Loss: 0.62007\n",
      "Epoch: 33, Time: 0.02799s, Loss: 0.62278\n",
      "Epoch: 34, Time: 0.02851s, Loss: 0.61925\n",
      "Epoch: 35, Time: 0.02787s, Loss: 0.61962\n",
      "Epoch: 36, Time: 0.02757s, Loss: 0.61805\n",
      "Epoch: 37, Time: 0.02779s, Loss: 0.61621\n",
      "Epoch: 38, Time: 0.02345s, Loss: 0.61711\n",
      "Epoch: 39, Time: 0.03184s, Loss: 0.61815\n",
      "Epoch: 40, Time: 0.03103s, Loss: 0.61675\n",
      "Epoch: 41, Time: 0.03052s, Loss: 0.61364\n",
      "Epoch: 42, Time: 0.03411s, Loss: 0.61613\n",
      "Epoch: 43, Time: 0.02440s, Loss: 0.61203\n",
      "Epoch: 44, Time: 0.03032s, Loss: 0.61705\n",
      "Epoch: 45, Time: 0.02423s, Loss: 0.62059\n",
      "Epoch: 46, Time: 0.02511s, Loss: 0.61112\n",
      "Epoch: 47, Time: 0.03158s, Loss: 0.61271\n",
      "Epoch: 48, Time: 0.03438s, Loss: 0.61362\n",
      "Epoch: 49, Time: 0.05308s, Loss: 0.61147\n",
      "Epoch: 50, Time: 0.03004s, Loss: 0.61627\n",
      "Epoch: 51, Time: 0.02644s, Loss: 0.61299\n",
      "Epoch: 52, Time: 0.01729s, Loss: 0.61090\n",
      "Epoch: 53, Time: 0.02667s, Loss: 0.61815\n",
      "Epoch: 54, Time: 0.02689s, Loss: 0.60703\n",
      "Epoch: 55, Time: 0.03288s, Loss: 0.61190\n",
      "Epoch: 56, Time: 0.03222s, Loss: 0.60563\n",
      "Epoch: 57, Time: 0.02489s, Loss: 0.60704\n",
      "Epoch: 58, Time: 0.02843s, Loss: 0.60723\n",
      "Epoch: 59, Time: 0.03464s, Loss: 0.60434\n",
      "Epoch: 60, Time: 0.03428s, Loss: 0.60718\n",
      "Epoch: 61, Time: 0.02968s, Loss: 0.60339\n",
      "Epoch: 62, Time: 0.02503s, Loss: 0.60522\n",
      "Epoch: 63, Time: 0.03123s, Loss: 0.60896\n",
      "Epoch: 64, Time: 0.03067s, Loss: 0.60631\n",
      "Epoch: 65, Time: 0.03197s, Loss: 0.60581\n",
      "Epoch: 66, Time: 0.02583s, Loss: 0.60660\n",
      "Epoch: 67, Time: 0.02928s, Loss: 0.60541\n",
      "Epoch: 68, Time: 0.03106s, Loss: 0.60229\n",
      "Epoch: 69, Time: 0.03005s, Loss: 0.60085\n",
      "Epoch: 70, Time: 0.02799s, Loss: 0.60116\n",
      "Epoch: 71, Time: 0.02955s, Loss: 0.60006\n",
      "Epoch: 72, Time: 0.03037s, Loss: 0.60057\n",
      "Epoch: 73, Time: 0.03062s, Loss: 0.60258\n",
      "Epoch: 74, Time: 0.02805s, Loss: 0.60133\n",
      "Epoch: 75, Time: 0.03066s, Loss: 0.60238\n",
      "Epoch: 76, Time: 0.02490s, Loss: 0.62769\n",
      "Epoch: 77, Time: 0.03097s, Loss: 0.66141\n",
      "Epoch: 78, Time: 0.03203s, Loss: 0.61419\n",
      "Epoch: 79, Time: 0.03471s, Loss: 0.63246\n",
      "Epoch: 80, Time: 0.04490s, Loss: 0.62871\n",
      "Epoch: 81, Time: 0.03325s, Loss: 0.62628\n",
      "Epoch: 82, Time: 0.03190s, Loss: 0.60557\n",
      "Epoch: 83, Time: 0.02850s, Loss: 0.62681\n",
      "Epoch: 84, Time: 0.02850s, Loss: 0.61835\n",
      "Epoch: 85, Time: 0.03084s, Loss: 0.61257\n",
      "Epoch: 86, Time: 0.03414s, Loss: 0.62365\n",
      "Epoch: 87, Time: 0.03086s, Loss: 0.61236\n",
      "Epoch: 88, Time: 0.03112s, Loss: 0.60779\n",
      "Epoch: 89, Time: 0.03858s, Loss: 0.61759\n",
      "Epoch: 90, Time: 0.03023s, Loss: 0.61017\n",
      "Epoch: 91, Time: 0.03175s, Loss: 0.60779\n",
      "Epoch: 92, Time: 0.02886s, Loss: 0.60955\n",
      "Epoch: 93, Time: 0.02559s, Loss: 0.60735\n",
      "Epoch: 94, Time: 0.03371s, Loss: 0.60920\n",
      "Epoch: 95, Time: 0.03295s, Loss: 0.61007\n",
      "Epoch: 96, Time: 0.03233s, Loss: 0.60487\n",
      "Epoch: 97, Time: 0.02836s, Loss: 0.60431\n",
      "Epoch: 98, Time: 0.03191s, Loss: 0.60956\n",
      "Epoch: 99, Time: 0.03465s, Loss: 0.60726\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5543, F1: 0.5543\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.02453s, Loss: 1.07779\n",
      "Epoch: 1, Time: 0.03310s, Loss: 0.91868\n",
      "Epoch: 2, Time: 0.03431s, Loss: 0.88328\n",
      "Epoch: 3, Time: 0.02507s, Loss: 0.83430\n",
      "Epoch: 4, Time: 0.03170s, Loss: 0.79292\n",
      "Epoch: 5, Time: 0.02999s, Loss: 0.76998\n",
      "Epoch: 6, Time: 0.03060s, Loss: 0.74994\n",
      "Epoch: 7, Time: 0.03040s, Loss: 0.73778\n",
      "Epoch: 8, Time: 0.03715s, Loss: 0.72469\n",
      "Epoch: 9, Time: 0.03660s, Loss: 0.71172\n",
      "Epoch: 10, Time: 0.03169s, Loss: 0.70514\n",
      "Epoch: 11, Time: 0.02486s, Loss: 0.69722\n",
      "Epoch: 12, Time: 0.03063s, Loss: 0.69199\n",
      "Epoch: 13, Time: 0.03018s, Loss: 0.68579\n",
      "Epoch: 14, Time: 0.03269s, Loss: 0.68182\n",
      "Epoch: 15, Time: 0.02792s, Loss: 0.67797\n",
      "Epoch: 16, Time: 0.03361s, Loss: 0.67561\n",
      "Epoch: 17, Time: 0.02660s, Loss: 0.67426\n",
      "Epoch: 18, Time: 0.03366s, Loss: 0.66942\n",
      "Epoch: 19, Time: 0.03104s, Loss: 0.66863\n",
      "Epoch: 20, Time: 0.03170s, Loss: 0.66568\n",
      "Epoch: 21, Time: 0.03506s, Loss: 0.66481\n",
      "Epoch: 22, Time: 0.03179s, Loss: 0.66335\n",
      "Epoch: 23, Time: 0.03258s, Loss: 0.66020\n",
      "Epoch: 24, Time: 0.03221s, Loss: 0.66153\n",
      "Epoch: 25, Time: 0.03198s, Loss: 0.65863\n",
      "Epoch: 26, Time: 0.03217s, Loss: 0.65836\n",
      "Epoch: 27, Time: 0.02613s, Loss: 0.65499\n",
      "Epoch: 28, Time: 0.03090s, Loss: 0.65486\n",
      "Epoch: 29, Time: 0.03391s, Loss: 0.65384\n",
      "Epoch: 30, Time: 0.03762s, Loss: 0.65300\n",
      "Epoch: 31, Time: 0.03125s, Loss: 0.65292\n",
      "Epoch: 32, Time: 0.02868s, Loss: 0.65057\n",
      "Epoch: 33, Time: 0.03074s, Loss: 0.65024\n",
      "Epoch: 34, Time: 0.02983s, Loss: 0.65114\n",
      "Epoch: 35, Time: 0.02852s, Loss: 0.65139\n",
      "Epoch: 36, Time: 0.02701s, Loss: 0.66081\n",
      "Epoch: 37, Time: 0.03073s, Loss: 0.64896\n",
      "Epoch: 38, Time: 0.02827s, Loss: 0.65046\n",
      "Epoch: 39, Time: 0.03101s, Loss: 0.64751\n",
      "Epoch: 40, Time: 0.03109s, Loss: 0.65870\n",
      "Epoch: 41, Time: 0.03293s, Loss: 0.64582\n",
      "Epoch: 42, Time: 0.03292s, Loss: 0.66160\n",
      "Epoch: 43, Time: 0.03358s, Loss: 0.64778\n",
      "Epoch: 44, Time: 0.02894s, Loss: 0.65499\n",
      "Epoch: 45, Time: 0.02605s, Loss: 0.64854\n",
      "Epoch: 46, Time: 0.03296s, Loss: 0.64975\n",
      "Epoch: 47, Time: 0.03435s, Loss: 0.65344\n",
      "Epoch: 48, Time: 0.03159s, Loss: 0.64525\n",
      "Epoch: 49, Time: 0.03155s, Loss: 0.64813\n",
      "Epoch: 50, Time: 0.03506s, Loss: 0.65685\n",
      "Epoch: 51, Time: 0.03343s, Loss: 0.65046\n",
      "Epoch: 52, Time: 0.03190s, Loss: 0.64979\n",
      "Epoch: 53, Time: 0.03421s, Loss: 0.65613\n",
      "Epoch: 54, Time: 0.03047s, Loss: 0.64982\n",
      "Epoch: 55, Time: 0.03024s, Loss: 0.64464\n",
      "Epoch: 56, Time: 0.02435s, Loss: 0.64687\n",
      "Epoch: 57, Time: 0.03039s, Loss: 0.65349\n",
      "Epoch: 58, Time: 0.03490s, Loss: 0.64306\n",
      "Epoch: 59, Time: 0.03022s, Loss: 0.64700\n",
      "Epoch: 60, Time: 0.03271s, Loss: 0.64488\n",
      "Epoch: 61, Time: 0.03696s, Loss: 0.64399\n",
      "Epoch: 62, Time: 0.03137s, Loss: 0.64366\n",
      "Epoch: 63, Time: 0.03505s, Loss: 0.64571\n",
      "Epoch: 64, Time: 0.03270s, Loss: 0.64177\n",
      "Epoch: 65, Time: 0.03633s, Loss: 0.64105\n",
      "Epoch: 66, Time: 0.03780s, Loss: 0.64051\n",
      "Epoch: 67, Time: 0.03322s, Loss: 0.63828\n",
      "Epoch: 68, Time: 0.03387s, Loss: 0.63935\n",
      "Epoch: 69, Time: 0.03350s, Loss: 0.63781\n",
      "Epoch: 70, Time: 0.03288s, Loss: 0.64019\n",
      "Epoch: 71, Time: 0.03265s, Loss: 0.63687\n",
      "Epoch: 72, Time: 0.03280s, Loss: 0.63795\n",
      "Epoch: 73, Time: 0.03087s, Loss: 0.63844\n",
      "Epoch: 74, Time: 0.03095s, Loss: 0.63926\n",
      "Epoch: 75, Time: 0.03242s, Loss: 0.63941\n",
      "Epoch: 76, Time: 0.03476s, Loss: 0.63996\n",
      "Epoch: 77, Time: 0.02864s, Loss: 0.63722\n",
      "Epoch: 78, Time: 0.03737s, Loss: 0.63876\n",
      "Epoch: 79, Time: 0.03026s, Loss: 0.64577\n",
      "Epoch: 80, Time: 0.03431s, Loss: 0.63909\n",
      "Epoch: 81, Time: 0.03119s, Loss: 0.63504\n",
      "Epoch: 82, Time: 0.03198s, Loss: 0.64045\n",
      "Epoch: 83, Time: 0.02942s, Loss: 0.63764\n",
      "Epoch: 84, Time: 0.03075s, Loss: 0.63659\n",
      "Epoch: 85, Time: 0.03223s, Loss: 0.63958\n",
      "Epoch: 86, Time: 0.03322s, Loss: 0.63656\n",
      "Epoch: 87, Time: 0.03488s, Loss: 0.63497\n",
      "Epoch: 88, Time: 0.02734s, Loss: 0.63482\n",
      "Epoch: 89, Time: 0.03623s, Loss: 0.63895\n",
      "Epoch: 90, Time: 0.03151s, Loss: 0.63657\n",
      "Epoch: 91, Time: 0.03347s, Loss: 0.63542\n",
      "Epoch: 92, Time: 0.03101s, Loss: 0.63702\n",
      "Epoch: 93, Time: 0.03224s, Loss: 0.63153\n",
      "Epoch: 94, Time: 0.02524s, Loss: 0.63687\n",
      "Epoch: 95, Time: 0.03125s, Loss: 0.63630\n",
      "Epoch: 96, Time: 0.03044s, Loss: 0.63240\n",
      "Epoch: 97, Time: 0.03164s, Loss: 0.63961\n",
      "Epoch: 98, Time: 0.03507s, Loss: 0.63315\n",
      "Epoch: 99, Time: 0.03185s, Loss: 0.63660\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "\n",
      "[HouseCommittees] Fold 3/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.22995s, Loss: 1.11314\n",
      "Epoch: 1, Time: 0.18469s, Loss: 1.08995\n",
      "Epoch: 2, Time: 0.19606s, Loss: 1.06864\n",
      "Epoch: 3, Time: 0.19287s, Loss: 1.04377\n",
      "Epoch: 4, Time: 0.17022s, Loss: 1.01394\n",
      "Epoch: 5, Time: 0.18585s, Loss: 0.98460\n",
      "Epoch: 6, Time: 0.19015s, Loss: 0.95335\n",
      "Epoch: 7, Time: 0.19771s, Loss: 0.92303\n",
      "Epoch: 8, Time: 0.18727s, Loss: 0.89328\n",
      "Epoch: 9, Time: 0.19344s, Loss: 0.86787\n",
      "Epoch: 10, Time: 0.18808s, Loss: 0.84366\n",
      "Epoch: 11, Time: 0.18617s, Loss: 0.82381\n",
      "Epoch: 12, Time: 0.16974s, Loss: 0.80523\n",
      "Epoch: 13, Time: 0.18795s, Loss: 0.79099\n",
      "Epoch: 14, Time: 0.17815s, Loss: 0.77679\n",
      "Epoch: 15, Time: 0.20194s, Loss: 0.76635\n",
      "Epoch: 16, Time: 0.18124s, Loss: 0.75809\n",
      "Epoch: 17, Time: 0.18752s, Loss: 0.75032\n",
      "Epoch: 18, Time: 0.15775s, Loss: 0.74353\n",
      "Epoch: 19, Time: 0.17209s, Loss: 0.73714\n",
      "Epoch: 20, Time: 0.17710s, Loss: 0.73331\n",
      "Epoch: 21, Time: 0.19123s, Loss: 0.72769\n",
      "Epoch: 22, Time: 0.15388s, Loss: 0.72428\n",
      "Epoch: 23, Time: 0.16337s, Loss: 0.72100\n",
      "Epoch: 24, Time: 0.17019s, Loss: 0.71824\n",
      "Epoch: 25, Time: 0.17653s, Loss: 0.71573\n",
      "Epoch: 26, Time: 0.16725s, Loss: 0.71243\n",
      "Epoch: 27, Time: 0.19172s, Loss: 0.71063\n",
      "Epoch: 28, Time: 0.17158s, Loss: 0.70848\n",
      "Epoch: 29, Time: 0.14449s, Loss: 0.70704\n",
      "Epoch: 30, Time: 0.12543s, Loss: 0.70538\n",
      "Epoch: 31, Time: 0.19625s, Loss: 0.70422\n",
      "Epoch: 32, Time: 0.17810s, Loss: 0.70271\n",
      "Epoch: 33, Time: 0.16540s, Loss: 0.70165\n",
      "Epoch: 34, Time: 0.12855s, Loss: 0.70067\n",
      "Epoch: 35, Time: 0.19172s, Loss: 0.70036\n",
      "Epoch: 36, Time: 0.19172s, Loss: 0.69941\n",
      "Epoch: 37, Time: 0.18322s, Loss: 0.69912\n",
      "Epoch: 38, Time: 0.18076s, Loss: 0.69790\n",
      "Epoch: 39, Time: 0.19722s, Loss: 0.69822\n",
      "Epoch: 40, Time: 0.16956s, Loss: 0.69757\n",
      "Epoch: 41, Time: 0.16772s, Loss: 0.69678\n",
      "Epoch: 42, Time: 0.18010s, Loss: 0.69696\n",
      "Epoch: 43, Time: 0.16909s, Loss: 0.69657\n",
      "Epoch: 44, Time: 0.18527s, Loss: 0.69571\n",
      "Epoch: 45, Time: 0.19207s, Loss: 0.69610\n",
      "Epoch: 46, Time: 0.18065s, Loss: 0.69590\n",
      "Epoch: 47, Time: 0.17917s, Loss: 0.69582\n",
      "Epoch: 48, Time: 0.18057s, Loss: 0.69545\n",
      "Epoch: 49, Time: 0.17818s, Loss: 0.69541\n",
      "Epoch: 50, Time: 0.19024s, Loss: 0.69502\n",
      "Epoch: 51, Time: 0.18399s, Loss: 0.69490\n",
      "Epoch: 52, Time: 0.18725s, Loss: 0.69473\n",
      "Epoch: 53, Time: 0.19178s, Loss: 0.69448\n",
      "Epoch: 54, Time: 0.19271s, Loss: 0.69402\n",
      "Epoch: 55, Time: 0.18689s, Loss: 0.69463\n",
      "Epoch: 56, Time: 0.18548s, Loss: 0.69458\n",
      "Epoch: 57, Time: 0.18698s, Loss: 0.69421\n",
      "Epoch: 58, Time: 0.20000s, Loss: 0.69332\n",
      "Epoch: 59, Time: 0.19528s, Loss: 0.69458\n",
      "Epoch: 60, Time: 0.19106s, Loss: 0.69398\n",
      "Epoch: 61, Time: 0.20969s, Loss: 0.69410\n",
      "Epoch: 62, Time: 0.19035s, Loss: 0.69381\n",
      "Epoch: 63, Time: 0.19775s, Loss: 0.69420\n",
      "Epoch: 64, Time: 0.17810s, Loss: 0.69442\n",
      "Epoch: 65, Time: 0.19505s, Loss: 0.69412\n",
      "Epoch: 66, Time: 0.15289s, Loss: 0.69400\n",
      "Epoch: 67, Time: 0.10402s, Loss: 0.69414\n",
      "Epoch: 68, Time: 0.10626s, Loss: 0.69383\n",
      "Epoch: 69, Time: 0.19058s, Loss: 0.69356\n",
      "Epoch: 70, Time: 0.18800s, Loss: 0.69388\n",
      "Epoch: 71, Time: 0.18931s, Loss: 0.69408\n",
      "Epoch: 72, Time: 0.20027s, Loss: 0.69333\n",
      "Epoch: 73, Time: 0.18442s, Loss: 0.69369\n",
      "Epoch: 74, Time: 0.18687s, Loss: 0.69378\n",
      "Epoch: 75, Time: 0.19228s, Loss: 0.69413\n",
      "Epoch: 76, Time: 0.19658s, Loss: 0.69336\n",
      "Epoch: 77, Time: 0.18735s, Loss: 0.69403\n",
      "Epoch: 78, Time: 0.18356s, Loss: 0.69354\n",
      "Epoch: 79, Time: 0.18793s, Loss: 0.69393\n",
      "Epoch: 80, Time: 0.17978s, Loss: 0.69313\n",
      "Epoch: 81, Time: 0.20502s, Loss: 0.69370\n",
      "Epoch: 82, Time: 0.19873s, Loss: 0.69354\n",
      "Epoch: 83, Time: 0.18966s, Loss: 0.69381\n",
      "Epoch: 84, Time: 0.18215s, Loss: 0.69362\n",
      "Epoch: 85, Time: 0.19140s, Loss: 0.69303\n",
      "Epoch: 86, Time: 0.19579s, Loss: 0.69276\n",
      "Epoch: 87, Time: 0.19671s, Loss: 0.69401\n",
      "Epoch: 88, Time: 0.18944s, Loss: 0.69322\n",
      "Epoch: 89, Time: 0.18644s, Loss: 0.69317\n",
      "Epoch: 90, Time: 0.18946s, Loss: 0.69289\n",
      "Epoch: 91, Time: 0.18815s, Loss: 0.69303\n",
      "Epoch: 92, Time: 0.17811s, Loss: 0.69357\n",
      "Epoch: 93, Time: 0.19152s, Loss: 0.69274\n",
      "Epoch: 94, Time: 0.18991s, Loss: 0.69306\n",
      "Epoch: 95, Time: 0.20146s, Loss: 0.69295\n",
      "Epoch: 96, Time: 0.18960s, Loss: 0.69298\n",
      "Epoch: 97, Time: 0.18909s, Loss: 0.69310\n",
      "Epoch: 98, Time: 0.19926s, Loss: 0.69347\n",
      "Epoch: 99, Time: 0.18183s, Loss: 0.69321\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03087s, Loss: 1.09236\n",
      "Epoch: 1, Time: 0.03097s, Loss: 1.08425\n",
      "Epoch: 2, Time: 0.02480s, Loss: 0.96263\n",
      "Epoch: 3, Time: 0.02978s, Loss: 0.92791\n",
      "Epoch: 4, Time: 0.03328s, Loss: 0.89398\n",
      "Epoch: 5, Time: 0.03806s, Loss: 0.84936\n",
      "Epoch: 6, Time: 0.02944s, Loss: 0.82979\n",
      "Epoch: 7, Time: 0.03127s, Loss: 0.79421\n",
      "Epoch: 8, Time: 0.03292s, Loss: 0.76984\n",
      "Epoch: 9, Time: 0.03111s, Loss: 0.74597\n",
      "Epoch: 10, Time: 0.03293s, Loss: 0.72713\n",
      "Epoch: 11, Time: 0.03088s, Loss: 0.71072\n",
      "Epoch: 12, Time: 0.02250s, Loss: 0.69130\n",
      "Epoch: 13, Time: 0.02457s, Loss: 0.68104\n",
      "Epoch: 14, Time: 0.02431s, Loss: 0.66970\n",
      "Epoch: 15, Time: 0.03496s, Loss: 0.66349\n",
      "Epoch: 16, Time: 0.03215s, Loss: 0.65813\n",
      "Epoch: 17, Time: 0.03430s, Loss: 0.65257\n",
      "Epoch: 18, Time: 0.03057s, Loss: 0.64780\n",
      "Epoch: 19, Time: 0.03137s, Loss: 0.64583\n",
      "Epoch: 20, Time: 0.02255s, Loss: 0.64057\n",
      "Epoch: 21, Time: 0.02613s, Loss: 0.63808\n",
      "Epoch: 22, Time: 0.03001s, Loss: 0.63595\n",
      "Epoch: 23, Time: 0.02583s, Loss: 0.63329\n",
      "Epoch: 24, Time: 0.02837s, Loss: 0.63300\n",
      "Epoch: 25, Time: 0.03049s, Loss: 0.63037\n",
      "Epoch: 26, Time: 0.02673s, Loss: 0.62706\n",
      "Epoch: 27, Time: 0.03219s, Loss: 0.62788\n",
      "Epoch: 28, Time: 0.03126s, Loss: 0.62662\n",
      "Epoch: 29, Time: 0.03352s, Loss: 0.62447\n",
      "Epoch: 30, Time: 0.03005s, Loss: 0.62292\n",
      "Epoch: 31, Time: 0.03042s, Loss: 0.62081\n",
      "Epoch: 32, Time: 0.02617s, Loss: 0.61930\n",
      "Epoch: 33, Time: 0.02420s, Loss: 0.61853\n",
      "Epoch: 34, Time: 0.03642s, Loss: 0.61878\n",
      "Epoch: 35, Time: 0.03025s, Loss: 0.61714\n",
      "Epoch: 36, Time: 0.03473s, Loss: 0.61582\n",
      "Epoch: 37, Time: 0.03021s, Loss: 0.61395\n",
      "Epoch: 38, Time: 0.02876s, Loss: 0.61322\n",
      "Epoch: 39, Time: 0.03330s, Loss: 0.61233\n",
      "Epoch: 40, Time: 0.03382s, Loss: 0.61202\n",
      "Epoch: 41, Time: 0.03400s, Loss: 0.61114\n",
      "Epoch: 42, Time: 0.02579s, Loss: 0.61541\n",
      "Epoch: 43, Time: 0.02856s, Loss: 0.60949\n",
      "Epoch: 44, Time: 0.02770s, Loss: 0.61068\n",
      "Epoch: 45, Time: 0.03021s, Loss: 0.60933\n",
      "Epoch: 46, Time: 0.02768s, Loss: 0.61112\n",
      "Epoch: 47, Time: 0.02242s, Loss: 0.60555\n",
      "Epoch: 48, Time: 0.03386s, Loss: 0.61058\n",
      "Epoch: 49, Time: 0.03222s, Loss: 0.61930\n",
      "Epoch: 50, Time: 0.03221s, Loss: 0.60635\n",
      "Epoch: 51, Time: 0.02756s, Loss: 0.60344\n",
      "Epoch: 52, Time: 0.02869s, Loss: 0.60932\n",
      "Epoch: 53, Time: 0.02656s, Loss: 0.60356\n",
      "Epoch: 54, Time: 0.03163s, Loss: 0.60675\n",
      "Epoch: 55, Time: 0.02853s, Loss: 0.61501\n",
      "Epoch: 56, Time: 0.03338s, Loss: 0.60502\n",
      "Epoch: 57, Time: 0.02648s, Loss: 0.60546\n",
      "Epoch: 58, Time: 0.03273s, Loss: 0.60798\n",
      "Epoch: 59, Time: 0.03347s, Loss: 0.60197\n",
      "Epoch: 60, Time: 0.03099s, Loss: 0.60442\n",
      "Epoch: 61, Time: 0.03049s, Loss: 0.60580\n",
      "Epoch: 62, Time: 0.02464s, Loss: 0.60316\n",
      "Epoch: 63, Time: 0.02675s, Loss: 0.60712\n",
      "Epoch: 64, Time: 0.02426s, Loss: 0.60675\n",
      "Epoch: 65, Time: 0.03439s, Loss: 0.60970\n",
      "Epoch: 66, Time: 0.03242s, Loss: 0.60763\n",
      "Epoch: 67, Time: 0.02479s, Loss: 0.60116\n",
      "Epoch: 68, Time: 0.02997s, Loss: 0.60619\n",
      "Epoch: 69, Time: 0.03502s, Loss: 0.60319\n",
      "Epoch: 70, Time: 0.03173s, Loss: 0.59704\n",
      "Epoch: 71, Time: 0.03536s, Loss: 0.60015\n",
      "Epoch: 72, Time: 0.03313s, Loss: 0.60379\n",
      "Epoch: 73, Time: 0.03173s, Loss: 0.59756\n",
      "Epoch: 74, Time: 0.02829s, Loss: 0.60027\n",
      "Epoch: 75, Time: 0.02800s, Loss: 0.60555\n",
      "Epoch: 76, Time: 0.02862s, Loss: 0.60793\n",
      "Epoch: 77, Time: 0.03034s, Loss: 0.60045\n",
      "Epoch: 78, Time: 0.03103s, Loss: 0.60185\n",
      "Epoch: 79, Time: 0.02978s, Loss: 0.60063\n",
      "Epoch: 80, Time: 0.03137s, Loss: 0.60554\n",
      "Epoch: 81, Time: 0.03060s, Loss: 0.60071\n",
      "Epoch: 82, Time: 0.02669s, Loss: 0.59817\n",
      "Epoch: 83, Time: 0.03207s, Loss: 0.59626\n",
      "Epoch: 84, Time: 0.03294s, Loss: 0.59659\n",
      "Epoch: 85, Time: 0.02159s, Loss: 0.59633\n",
      "Epoch: 86, Time: 0.03685s, Loss: 0.59646\n",
      "Epoch: 87, Time: 0.03303s, Loss: 0.59876\n",
      "Epoch: 88, Time: 0.03945s, Loss: 0.59878\n",
      "Epoch: 89, Time: 0.02803s, Loss: 0.59268\n",
      "Epoch: 90, Time: 0.03540s, Loss: 0.60294\n",
      "Epoch: 91, Time: 0.03245s, Loss: 0.60240\n",
      "Epoch: 92, Time: 0.02907s, Loss: 0.59789\n",
      "Epoch: 93, Time: 0.02647s, Loss: 0.60295\n",
      "Epoch: 94, Time: 0.03149s, Loss: 0.60884\n",
      "Epoch: 95, Time: 0.03244s, Loss: 0.59688\n",
      "Epoch: 96, Time: 0.02767s, Loss: 0.61312\n",
      "Epoch: 97, Time: 0.03539s, Loss: 0.59724\n",
      "Epoch: 98, Time: 0.02887s, Loss: 0.60087\n",
      "Epoch: 99, Time: 0.03963s, Loss: 0.60221\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5543, F1: 0.5543\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03267s, Loss: 1.15471\n",
      "Epoch: 1, Time: 0.03900s, Loss: 0.93084\n",
      "Epoch: 2, Time: 0.03796s, Loss: 0.86500\n",
      "Epoch: 3, Time: 0.03270s, Loss: 0.81298\n",
      "Epoch: 4, Time: 0.03689s, Loss: 0.78161\n",
      "Epoch: 5, Time: 0.03252s, Loss: 0.76612\n",
      "Epoch: 6, Time: 0.02660s, Loss: 0.74777\n",
      "Epoch: 7, Time: 0.03324s, Loss: 0.73354\n",
      "Epoch: 8, Time: 0.03359s, Loss: 0.72222\n",
      "Epoch: 9, Time: 0.02895s, Loss: 0.71624\n",
      "Epoch: 10, Time: 0.02483s, Loss: 0.70987\n",
      "Epoch: 11, Time: 0.03196s, Loss: 0.70256\n",
      "Epoch: 12, Time: 0.03024s, Loss: 0.69659\n",
      "Epoch: 13, Time: 0.03011s, Loss: 0.69072\n",
      "Epoch: 14, Time: 0.03417s, Loss: 0.68594\n",
      "Epoch: 15, Time: 0.03265s, Loss: 0.68406\n",
      "Epoch: 16, Time: 0.02595s, Loss: 0.67902\n",
      "Epoch: 17, Time: 0.03274s, Loss: 0.67516\n",
      "Epoch: 18, Time: 0.03208s, Loss: 0.67535\n",
      "Epoch: 19, Time: 0.02853s, Loss: 0.67225\n",
      "Epoch: 20, Time: 0.03084s, Loss: 0.66854\n",
      "Epoch: 21, Time: 0.03196s, Loss: 0.66883\n",
      "Epoch: 22, Time: 0.02158s, Loss: 0.66662\n",
      "Epoch: 23, Time: 0.03247s, Loss: 0.66433\n",
      "Epoch: 24, Time: 0.03079s, Loss: 0.66664\n",
      "Epoch: 25, Time: 0.03451s, Loss: 0.66251\n",
      "Epoch: 26, Time: 0.03471s, Loss: 0.66007\n",
      "Epoch: 27, Time: 0.03253s, Loss: 0.66185\n",
      "Epoch: 28, Time: 0.02654s, Loss: 0.65787\n",
      "Epoch: 29, Time: 0.03255s, Loss: 0.65966\n",
      "Epoch: 30, Time: 0.02700s, Loss: 0.65498\n",
      "Epoch: 31, Time: 0.03060s, Loss: 0.65551\n",
      "Epoch: 32, Time: 0.02848s, Loss: 0.65611\n",
      "Epoch: 33, Time: 0.03343s, Loss: 0.65393\n",
      "Epoch: 34, Time: 0.03293s, Loss: 0.65533\n",
      "Epoch: 35, Time: 0.03148s, Loss: 0.65206\n",
      "Epoch: 36, Time: 0.03052s, Loss: 0.65296\n",
      "Epoch: 37, Time: 0.02938s, Loss: 0.65306\n",
      "Epoch: 38, Time: 0.02474s, Loss: 0.64937\n",
      "Epoch: 39, Time: 0.02834s, Loss: 0.65155\n",
      "Epoch: 40, Time: 0.03080s, Loss: 0.64839\n",
      "Epoch: 41, Time: 0.03155s, Loss: 0.64718\n",
      "Epoch: 42, Time: 0.03036s, Loss: 0.65050\n",
      "Epoch: 43, Time: 0.03356s, Loss: 0.65166\n",
      "Epoch: 44, Time: 0.03350s, Loss: 0.64235\n",
      "Epoch: 45, Time: 0.03461s, Loss: 0.64562\n",
      "Epoch: 46, Time: 0.02904s, Loss: 0.64520\n",
      "Epoch: 47, Time: 0.03267s, Loss: 0.64347\n",
      "Epoch: 48, Time: 0.03253s, Loss: 0.64809\n",
      "Epoch: 49, Time: 0.03469s, Loss: 0.64980\n",
      "Epoch: 50, Time: 0.03088s, Loss: 0.64528\n",
      "Epoch: 51, Time: 0.02875s, Loss: 0.64191\n",
      "Epoch: 52, Time: 0.03289s, Loss: 0.64347\n",
      "Epoch: 53, Time: 0.03870s, Loss: 0.63818\n",
      "Epoch: 54, Time: 0.03139s, Loss: 0.63628\n",
      "Epoch: 55, Time: 0.03474s, Loss: 0.63902\n",
      "Epoch: 56, Time: 0.03533s, Loss: 0.63920\n",
      "Epoch: 57, Time: 0.02484s, Loss: 0.63852\n",
      "Epoch: 58, Time: 0.03261s, Loss: 0.64272\n",
      "Epoch: 59, Time: 0.03191s, Loss: 0.64882\n",
      "Epoch: 60, Time: 0.03399s, Loss: 0.67225\n",
      "Epoch: 61, Time: 0.03289s, Loss: 0.65262\n",
      "Epoch: 62, Time: 0.03280s, Loss: 0.64549\n",
      "Epoch: 63, Time: 0.02988s, Loss: 0.64939\n",
      "Epoch: 64, Time: 0.03125s, Loss: 0.65025\n",
      "Epoch: 65, Time: 0.03251s, Loss: 0.64937\n",
      "Epoch: 66, Time: 0.03245s, Loss: 0.64049\n",
      "Epoch: 67, Time: 0.03278s, Loss: 0.65011\n",
      "Epoch: 68, Time: 0.02589s, Loss: 0.63996\n",
      "Epoch: 69, Time: 0.03087s, Loss: 0.64102\n",
      "Epoch: 70, Time: 0.03911s, Loss: 0.64311\n",
      "Epoch: 71, Time: 0.02776s, Loss: 0.64056\n",
      "Epoch: 72, Time: 0.03287s, Loss: 0.63827\n",
      "Epoch: 73, Time: 0.03064s, Loss: 0.66369\n",
      "Epoch: 74, Time: 0.03269s, Loss: 0.64512\n",
      "Epoch: 75, Time: 0.03496s, Loss: 0.64699\n",
      "Epoch: 76, Time: 0.02432s, Loss: 0.65347\n",
      "Epoch: 77, Time: 0.03069s, Loss: 0.64191\n",
      "Epoch: 78, Time: 0.03146s, Loss: 0.64415\n",
      "Epoch: 79, Time: 0.02792s, Loss: 0.65896\n",
      "Epoch: 80, Time: 0.03238s, Loss: 0.64794\n",
      "Epoch: 81, Time: 0.03320s, Loss: 0.64451\n",
      "Epoch: 82, Time: 0.04150s, Loss: 0.66360\n",
      "Epoch: 83, Time: 0.04021s, Loss: 0.64645\n",
      "Epoch: 84, Time: 0.03451s, Loss: 0.63768\n",
      "Epoch: 85, Time: 0.03119s, Loss: 0.65160\n",
      "Epoch: 86, Time: 0.03536s, Loss: 0.64814\n",
      "Epoch: 87, Time: 0.02524s, Loss: 0.64295\n",
      "Epoch: 88, Time: 0.03433s, Loss: 0.63882\n",
      "Epoch: 89, Time: 0.03535s, Loss: 0.64342\n",
      "Epoch: 90, Time: 0.03616s, Loss: 0.65462\n",
      "Epoch: 91, Time: 0.03350s, Loss: 0.64044\n",
      "Epoch: 92, Time: 0.02811s, Loss: 0.64655\n",
      "Epoch: 93, Time: 0.03144s, Loss: 0.64999\n",
      "Epoch: 94, Time: 0.03072s, Loss: 0.64724\n",
      "Epoch: 95, Time: 0.03694s, Loss: 0.65131\n",
      "Epoch: 96, Time: 0.03090s, Loss: 0.63788\n",
      "Epoch: 97, Time: 0.03045s, Loss: 0.64251\n",
      "Epoch: 98, Time: 0.03044s, Loss: 0.65068\n",
      "Epoch: 99, Time: 0.02524s, Loss: 0.64073\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5698, F1: 0.5698\n",
      "\n",
      "[HouseCommittees] Fold 4/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.11455s, Loss: 1.12673\n",
      "Epoch: 1, Time: 0.09954s, Loss: 1.10475\n",
      "Epoch: 2, Time: 0.09707s, Loss: 1.08582\n",
      "Epoch: 3, Time: 0.09755s, Loss: 1.06352\n",
      "Epoch: 4, Time: 0.09101s, Loss: 1.03815\n",
      "Epoch: 5, Time: 0.10217s, Loss: 1.00918\n",
      "Epoch: 6, Time: 0.09504s, Loss: 0.98027\n",
      "Epoch: 7, Time: 0.17216s, Loss: 0.95067\n",
      "Epoch: 8, Time: 0.08705s, Loss: 0.92064\n",
      "Epoch: 9, Time: 0.10053s, Loss: 0.89287\n",
      "Epoch: 10, Time: 0.09254s, Loss: 0.86848\n",
      "Epoch: 11, Time: 0.10755s, Loss: 0.84596\n",
      "Epoch: 12, Time: 0.09704s, Loss: 0.82508\n",
      "Epoch: 13, Time: 0.10756s, Loss: 0.80894\n",
      "Epoch: 14, Time: 0.08655s, Loss: 0.79386\n",
      "Epoch: 15, Time: 0.10003s, Loss: 0.78219\n",
      "Epoch: 16, Time: 0.09505s, Loss: 0.76978\n",
      "Epoch: 17, Time: 0.09856s, Loss: 0.76109\n",
      "Epoch: 18, Time: 0.09053s, Loss: 0.75367\n",
      "Epoch: 19, Time: 0.09765s, Loss: 0.74741\n",
      "Epoch: 20, Time: 0.09961s, Loss: 0.74122\n",
      "Epoch: 21, Time: 0.10256s, Loss: 0.73566\n",
      "Epoch: 22, Time: 0.09652s, Loss: 0.73232\n",
      "Epoch: 23, Time: 0.09956s, Loss: 0.72629\n",
      "Epoch: 24, Time: 0.09055s, Loss: 0.72363\n",
      "Epoch: 25, Time: 0.09656s, Loss: 0.72074\n",
      "Epoch: 26, Time: 0.09555s, Loss: 0.71721\n",
      "Epoch: 27, Time: 0.13458s, Loss: 0.71487\n",
      "Epoch: 28, Time: 0.10454s, Loss: 0.71266\n",
      "Epoch: 29, Time: 0.10209s, Loss: 0.71133\n",
      "Epoch: 30, Time: 0.10004s, Loss: 0.70959\n",
      "Epoch: 31, Time: 0.10365s, Loss: 0.70834\n",
      "Epoch: 32, Time: 0.08801s, Loss: 0.70611\n",
      "Epoch: 33, Time: 0.10606s, Loss: 0.70484\n",
      "Epoch: 34, Time: 0.09338s, Loss: 0.70394\n",
      "Epoch: 35, Time: 0.10005s, Loss: 0.70274\n",
      "Epoch: 36, Time: 0.12259s, Loss: 0.70152\n",
      "Epoch: 37, Time: 0.09503s, Loss: 0.70114\n",
      "Epoch: 38, Time: 0.09654s, Loss: 0.70095\n",
      "Epoch: 39, Time: 0.10560s, Loss: 0.70008\n",
      "Epoch: 40, Time: 0.09155s, Loss: 0.69944\n",
      "Epoch: 41, Time: 0.10355s, Loss: 0.69894\n",
      "Epoch: 42, Time: 0.09753s, Loss: 0.69814\n",
      "Epoch: 43, Time: 0.10314s, Loss: 0.69773\n",
      "Epoch: 44, Time: 0.09460s, Loss: 0.69758\n",
      "Epoch: 45, Time: 0.10604s, Loss: 0.69756\n",
      "Epoch: 46, Time: 0.08554s, Loss: 0.69728\n",
      "Epoch: 47, Time: 0.10155s, Loss: 0.69655\n",
      "Epoch: 48, Time: 0.10126s, Loss: 0.69600\n",
      "Epoch: 49, Time: 0.10853s, Loss: 0.69637\n",
      "Epoch: 50, Time: 0.09404s, Loss: 0.69591\n",
      "Epoch: 51, Time: 0.10206s, Loss: 0.69576\n",
      "Epoch: 52, Time: 0.09553s, Loss: 0.69572\n",
      "Epoch: 53, Time: 0.09929s, Loss: 0.69557\n",
      "Epoch: 54, Time: 0.08954s, Loss: 0.69491\n",
      "Epoch: 55, Time: 0.09854s, Loss: 0.69534\n",
      "Epoch: 56, Time: 0.09754s, Loss: 0.69503\n",
      "Epoch: 57, Time: 0.10807s, Loss: 0.69450\n",
      "Epoch: 58, Time: 0.10518s, Loss: 0.69450\n",
      "Epoch: 59, Time: 0.09703s, Loss: 0.69534\n",
      "Epoch: 60, Time: 0.08913s, Loss: 0.69428\n",
      "Epoch: 61, Time: 0.12155s, Loss: 0.69469\n",
      "Epoch: 62, Time: 0.09910s, Loss: 0.69455\n",
      "Epoch: 63, Time: 0.09911s, Loss: 0.69460\n",
      "Epoch: 64, Time: 0.09153s, Loss: 0.69445\n",
      "Epoch: 65, Time: 0.10155s, Loss: 0.69433\n",
      "Epoch: 66, Time: 0.09260s, Loss: 0.69413\n",
      "Epoch: 67, Time: 0.09762s, Loss: 0.69378\n",
      "Epoch: 68, Time: 0.08954s, Loss: 0.69373\n",
      "Epoch: 69, Time: 0.09854s, Loss: 0.69370\n",
      "Epoch: 70, Time: 0.09324s, Loss: 0.69419\n",
      "Epoch: 71, Time: 0.10182s, Loss: 0.69364\n",
      "Epoch: 72, Time: 0.08613s, Loss: 0.69412\n",
      "Epoch: 73, Time: 0.10455s, Loss: 0.69387\n",
      "Epoch: 74, Time: 0.09004s, Loss: 0.69337\n",
      "Epoch: 75, Time: 0.10205s, Loss: 0.69315\n",
      "Epoch: 76, Time: 0.09656s, Loss: 0.69311\n",
      "Epoch: 77, Time: 0.09760s, Loss: 0.69382\n",
      "Epoch: 78, Time: 0.09862s, Loss: 0.69391\n",
      "Epoch: 79, Time: 0.10155s, Loss: 0.69309\n",
      "Epoch: 80, Time: 0.08912s, Loss: 0.69387\n",
      "Epoch: 81, Time: 0.10103s, Loss: 0.69324\n",
      "Epoch: 82, Time: 0.10137s, Loss: 0.69354\n",
      "Epoch: 83, Time: 0.10556s, Loss: 0.69294\n",
      "Epoch: 84, Time: 0.09560s, Loss: 0.69300\n",
      "Epoch: 85, Time: 0.09454s, Loss: 0.69356\n",
      "Epoch: 86, Time: 0.08702s, Loss: 0.69339\n",
      "Epoch: 87, Time: 0.10740s, Loss: 0.69282\n",
      "Epoch: 88, Time: 0.09958s, Loss: 0.69365\n",
      "Epoch: 89, Time: 0.10254s, Loss: 0.69317\n",
      "Epoch: 90, Time: 0.09654s, Loss: 0.69294\n",
      "Epoch: 91, Time: 0.09854s, Loss: 0.69268\n",
      "Epoch: 92, Time: 0.09055s, Loss: 0.69318\n",
      "Epoch: 93, Time: 0.10354s, Loss: 0.69261\n",
      "Epoch: 94, Time: 0.10311s, Loss: 0.69254\n",
      "Epoch: 95, Time: 0.10010s, Loss: 0.69236\n",
      "Epoch: 96, Time: 0.09456s, Loss: 0.69192\n",
      "Epoch: 97, Time: 0.10856s, Loss: 0.69221\n",
      "Epoch: 98, Time: 0.09355s, Loss: 0.69254\n",
      "Epoch: 99, Time: 0.10556s, Loss: 0.69262\n",
      "    ↳ HGNN Fold Result — Acc: 0.5581, F1: 0.5581\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.01600s, Loss: 1.07254\n",
      "Epoch: 1, Time: 0.01452s, Loss: 1.02350\n",
      "Epoch: 2, Time: 0.01600s, Loss: 0.94791\n",
      "Epoch: 3, Time: 0.01451s, Loss: 0.90923\n",
      "Epoch: 4, Time: 0.01500s, Loss: 0.86709\n",
      "Epoch: 5, Time: 0.01752s, Loss: 0.83942\n",
      "Epoch: 6, Time: 0.03754s, Loss: 0.81382\n",
      "Epoch: 7, Time: 0.01700s, Loss: 0.78140\n",
      "Epoch: 8, Time: 0.01653s, Loss: 0.75988\n",
      "Epoch: 9, Time: 0.01754s, Loss: 0.73366\n",
      "Epoch: 10, Time: 0.01700s, Loss: 0.71296\n",
      "Epoch: 11, Time: 0.01400s, Loss: 0.70044\n",
      "Epoch: 12, Time: 0.01751s, Loss: 0.68789\n",
      "Epoch: 13, Time: 0.01801s, Loss: 0.67650\n",
      "Epoch: 14, Time: 0.01753s, Loss: 0.66804\n",
      "Epoch: 15, Time: 0.01600s, Loss: 0.66324\n",
      "Epoch: 16, Time: 0.01453s, Loss: 0.65736\n",
      "Epoch: 17, Time: 0.01752s, Loss: 0.65494\n",
      "Epoch: 18, Time: 0.01600s, Loss: 0.65185\n",
      "Epoch: 19, Time: 0.01700s, Loss: 0.64657\n",
      "Epoch: 20, Time: 0.01551s, Loss: 0.64378\n",
      "Epoch: 21, Time: 0.01452s, Loss: 0.64144\n",
      "Epoch: 22, Time: 0.01551s, Loss: 0.64089\n",
      "Epoch: 23, Time: 0.02100s, Loss: 0.63883\n",
      "Epoch: 24, Time: 0.01700s, Loss: 0.63639\n",
      "Epoch: 25, Time: 0.01552s, Loss: 0.63399\n",
      "Epoch: 26, Time: 0.01700s, Loss: 0.63466\n",
      "Epoch: 27, Time: 0.01600s, Loss: 0.63256\n",
      "Epoch: 28, Time: 0.01400s, Loss: 0.63112\n",
      "Epoch: 29, Time: 0.01852s, Loss: 0.63024\n",
      "Epoch: 30, Time: 0.01501s, Loss: 0.62916\n",
      "Epoch: 31, Time: 0.01700s, Loss: 0.62687\n",
      "Epoch: 32, Time: 0.01864s, Loss: 0.62658\n",
      "Epoch: 33, Time: 0.02004s, Loss: 0.62548\n",
      "Epoch: 34, Time: 0.01899s, Loss: 0.62497\n",
      "Epoch: 35, Time: 0.01601s, Loss: 0.62598\n",
      "Epoch: 36, Time: 0.01600s, Loss: 0.62275\n",
      "Epoch: 37, Time: 0.01952s, Loss: 0.62242\n",
      "Epoch: 38, Time: 0.01900s, Loss: 0.62240\n",
      "Epoch: 39, Time: 0.01652s, Loss: 0.62068\n",
      "Epoch: 40, Time: 0.01500s, Loss: 0.62151\n",
      "Epoch: 41, Time: 0.01905s, Loss: 0.62058\n",
      "Epoch: 42, Time: 0.01800s, Loss: 0.61785\n",
      "Epoch: 43, Time: 0.01700s, Loss: 0.61814\n",
      "Epoch: 44, Time: 0.01453s, Loss: 0.62068\n",
      "Epoch: 45, Time: 0.01651s, Loss: 0.62084\n",
      "Epoch: 46, Time: 0.01504s, Loss: 0.61514\n",
      "Epoch: 47, Time: 0.01755s, Loss: 0.61799\n",
      "Epoch: 48, Time: 0.01600s, Loss: 0.61683\n",
      "Epoch: 49, Time: 0.02003s, Loss: 0.61326\n",
      "Epoch: 50, Time: 0.02001s, Loss: 0.61782\n",
      "Epoch: 51, Time: 0.01501s, Loss: 0.61789\n",
      "Epoch: 52, Time: 0.01600s, Loss: 0.62794\n",
      "Epoch: 53, Time: 0.01400s, Loss: 0.62303\n",
      "Epoch: 54, Time: 0.01500s, Loss: 0.61482\n",
      "Epoch: 55, Time: 0.01651s, Loss: 0.61071\n",
      "Epoch: 56, Time: 0.01700s, Loss: 0.61833\n",
      "Epoch: 57, Time: 0.01551s, Loss: 0.61265\n",
      "Epoch: 58, Time: 0.01451s, Loss: 0.61706\n",
      "Epoch: 59, Time: 0.02052s, Loss: 0.61591\n",
      "Epoch: 60, Time: 0.02200s, Loss: 0.61820\n",
      "Epoch: 61, Time: 0.01451s, Loss: 0.61757\n",
      "Epoch: 62, Time: 0.01701s, Loss: 0.61630\n",
      "Epoch: 63, Time: 0.01650s, Loss: 0.61195\n",
      "Epoch: 64, Time: 0.01600s, Loss: 0.63247\n",
      "Epoch: 65, Time: 0.01752s, Loss: 0.62691\n",
      "Epoch: 66, Time: 0.01756s, Loss: 0.61638\n",
      "Epoch: 67, Time: 0.01952s, Loss: 0.62173\n",
      "Epoch: 68, Time: 0.02299s, Loss: 0.61551\n",
      "Epoch: 69, Time: 0.01453s, Loss: 0.61231\n",
      "Epoch: 70, Time: 0.01500s, Loss: 0.62352\n",
      "Epoch: 71, Time: 0.01755s, Loss: 0.61587\n",
      "Epoch: 72, Time: 0.01800s, Loss: 0.61555\n",
      "Epoch: 73, Time: 0.01655s, Loss: 0.61912\n",
      "Epoch: 74, Time: 0.01500s, Loss: 0.61241\n",
      "Epoch: 75, Time: 0.01401s, Loss: 0.61420\n",
      "Epoch: 76, Time: 0.01600s, Loss: 0.61759\n",
      "Epoch: 77, Time: 0.01501s, Loss: 0.60961\n",
      "Epoch: 78, Time: 0.01551s, Loss: 0.61306\n",
      "Epoch: 79, Time: 0.01652s, Loss: 0.61190\n",
      "Epoch: 80, Time: 0.01700s, Loss: 0.60683\n",
      "Epoch: 81, Time: 0.01452s, Loss: 0.61289\n",
      "Epoch: 82, Time: 0.01506s, Loss: 0.61286\n",
      "Epoch: 83, Time: 0.01600s, Loss: 0.61006\n",
      "Epoch: 84, Time: 0.01652s, Loss: 0.60782\n",
      "Epoch: 85, Time: 0.01801s, Loss: 0.61310\n",
      "Epoch: 86, Time: 0.01803s, Loss: 0.61353\n",
      "Epoch: 87, Time: 0.01953s, Loss: 0.62081\n",
      "Epoch: 88, Time: 0.01719s, Loss: 0.61621\n",
      "Epoch: 89, Time: 0.01700s, Loss: 0.61594\n",
      "Epoch: 90, Time: 0.01603s, Loss: 0.60961\n",
      "Epoch: 91, Time: 0.01501s, Loss: 0.61537\n",
      "Epoch: 92, Time: 0.01552s, Loss: 0.61230\n",
      "Epoch: 93, Time: 0.02000s, Loss: 0.61391\n",
      "Epoch: 94, Time: 0.01552s, Loss: 0.61238\n",
      "Epoch: 95, Time: 0.02052s, Loss: 0.60907\n",
      "Epoch: 96, Time: 0.01700s, Loss: 0.61662\n",
      "Epoch: 97, Time: 0.01399s, Loss: 0.61165\n",
      "Epoch: 98, Time: 0.01451s, Loss: 0.61159\n",
      "Epoch: 99, Time: 0.01700s, Loss: 0.60929\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5698, F1: 0.5698\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.02001s, Loss: 1.12764\n",
      "Epoch: 1, Time: 0.01653s, Loss: 0.94528\n",
      "Epoch: 2, Time: 0.01852s, Loss: 0.86552\n",
      "Epoch: 3, Time: 0.01852s, Loss: 0.81860\n",
      "Epoch: 4, Time: 0.01900s, Loss: 0.78120\n",
      "Epoch: 5, Time: 0.02004s, Loss: 0.75541\n",
      "Epoch: 6, Time: 0.01797s, Loss: 0.74035\n",
      "Epoch: 7, Time: 0.01800s, Loss: 0.72794\n",
      "Epoch: 8, Time: 0.01552s, Loss: 0.71642\n",
      "Epoch: 9, Time: 0.02052s, Loss: 0.70833\n",
      "Epoch: 10, Time: 0.01601s, Loss: 0.70328\n",
      "Epoch: 11, Time: 0.01801s, Loss: 0.69511\n",
      "Epoch: 12, Time: 0.01900s, Loss: 0.69218\n",
      "Epoch: 13, Time: 0.01953s, Loss: 0.68766\n",
      "Epoch: 14, Time: 0.01652s, Loss: 0.68400\n",
      "Epoch: 15, Time: 0.01700s, Loss: 0.68243\n",
      "Epoch: 16, Time: 0.01600s, Loss: 0.67826\n",
      "Epoch: 17, Time: 0.01454s, Loss: 0.67447\n",
      "Epoch: 18, Time: 0.01501s, Loss: 0.67586\n",
      "Epoch: 19, Time: 0.01600s, Loss: 0.67179\n",
      "Epoch: 20, Time: 0.01854s, Loss: 0.67360\n",
      "Epoch: 21, Time: 0.01953s, Loss: 0.66954\n",
      "Epoch: 22, Time: 0.01852s, Loss: 0.66909\n",
      "Epoch: 23, Time: 0.01600s, Loss: 0.66616\n",
      "Epoch: 24, Time: 0.01851s, Loss: 0.66598\n",
      "Epoch: 25, Time: 0.01453s, Loss: 0.66498\n",
      "Epoch: 26, Time: 0.01751s, Loss: 0.66459\n",
      "Epoch: 27, Time: 0.01800s, Loss: 0.66477\n",
      "Epoch: 28, Time: 0.01953s, Loss: 0.66159\n",
      "Epoch: 29, Time: 0.02000s, Loss: 0.66112\n",
      "Epoch: 30, Time: 0.01900s, Loss: 0.66034\n",
      "Epoch: 31, Time: 0.01600s, Loss: 0.66272\n",
      "Epoch: 32, Time: 0.01552s, Loss: 0.66102\n",
      "Epoch: 33, Time: 0.01600s, Loss: 0.66061\n",
      "Epoch: 34, Time: 0.01952s, Loss: 0.65854\n",
      "Epoch: 35, Time: 0.01900s, Loss: 0.65723\n",
      "Epoch: 36, Time: 0.01750s, Loss: 0.66412\n",
      "Epoch: 37, Time: 0.02000s, Loss: 0.66142\n",
      "Epoch: 38, Time: 0.01853s, Loss: 0.66263\n",
      "Epoch: 39, Time: 0.01800s, Loss: 0.66121\n",
      "Epoch: 40, Time: 0.01957s, Loss: 0.65953\n",
      "Epoch: 41, Time: 0.01952s, Loss: 0.65655\n",
      "Epoch: 42, Time: 0.01800s, Loss: 0.66087\n",
      "Epoch: 43, Time: 0.01654s, Loss: 0.66317\n",
      "Epoch: 44, Time: 0.01701s, Loss: 0.65486\n",
      "Epoch: 45, Time: 0.01852s, Loss: 0.65834\n",
      "Epoch: 46, Time: 0.02100s, Loss: 0.65470\n",
      "Epoch: 47, Time: 0.01753s, Loss: 0.65568\n",
      "Epoch: 48, Time: 0.01700s, Loss: 0.66358\n",
      "Epoch: 49, Time: 0.01500s, Loss: 0.65865\n",
      "Epoch: 50, Time: 0.01900s, Loss: 0.65671\n",
      "Epoch: 51, Time: 0.01803s, Loss: 0.65677\n",
      "Epoch: 52, Time: 0.01800s, Loss: 0.65092\n",
      "Epoch: 53, Time: 0.01952s, Loss: 0.65812\n",
      "Epoch: 54, Time: 0.01952s, Loss: 0.65157\n",
      "Epoch: 55, Time: 0.01751s, Loss: 0.65169\n",
      "Epoch: 56, Time: 0.01552s, Loss: 0.64994\n",
      "Epoch: 57, Time: 0.01500s, Loss: 0.65011\n",
      "Epoch: 58, Time: 0.01900s, Loss: 0.64848\n",
      "Epoch: 59, Time: 0.01703s, Loss: 0.64740\n",
      "Epoch: 60, Time: 0.01399s, Loss: 0.65352\n",
      "Epoch: 61, Time: 0.01753s, Loss: 0.64926\n",
      "Epoch: 62, Time: 0.02355s, Loss: 0.64840\n",
      "Epoch: 63, Time: 0.02052s, Loss: 0.65322\n",
      "Epoch: 64, Time: 0.02151s, Loss: 0.64950\n",
      "Epoch: 65, Time: 0.01800s, Loss: 0.64625\n",
      "Epoch: 66, Time: 0.01452s, Loss: 0.64529\n",
      "Epoch: 67, Time: 0.01700s, Loss: 0.64984\n",
      "Epoch: 68, Time: 0.01652s, Loss: 0.64535\n",
      "Epoch: 69, Time: 0.01601s, Loss: 0.64984\n",
      "Epoch: 70, Time: 0.02607s, Loss: 0.64774\n",
      "Epoch: 71, Time: 0.01801s, Loss: 0.64353\n",
      "Epoch: 72, Time: 0.01552s, Loss: 0.64413\n",
      "Epoch: 73, Time: 0.01900s, Loss: 0.64855\n",
      "Epoch: 74, Time: 0.01853s, Loss: 0.64819\n",
      "Epoch: 75, Time: 0.01600s, Loss: 0.64368\n",
      "Epoch: 76, Time: 0.01700s, Loss: 0.64950\n",
      "Epoch: 77, Time: 0.01500s, Loss: 0.64456\n",
      "Epoch: 78, Time: 0.02104s, Loss: 0.65220\n",
      "Epoch: 79, Time: 0.02252s, Loss: 0.64522\n",
      "Epoch: 80, Time: 0.01752s, Loss: 0.65263\n",
      "Epoch: 81, Time: 0.01952s, Loss: 0.65088\n",
      "Epoch: 82, Time: 0.01651s, Loss: 0.64934\n",
      "Epoch: 83, Time: 0.01600s, Loss: 0.64611\n",
      "Epoch: 84, Time: 0.01599s, Loss: 0.64871\n",
      "Epoch: 85, Time: 0.02053s, Loss: 0.64579\n",
      "Epoch: 86, Time: 0.02252s, Loss: 0.64915\n",
      "Epoch: 87, Time: 0.02000s, Loss: 0.64632\n",
      "Epoch: 88, Time: 0.01601s, Loss: 0.64515\n",
      "Epoch: 89, Time: 0.01600s, Loss: 0.64756\n",
      "Epoch: 90, Time: 0.01700s, Loss: 0.65428\n",
      "Epoch: 91, Time: 0.01800s, Loss: 0.64721\n",
      "Epoch: 92, Time: 0.02003s, Loss: 0.64700\n",
      "Epoch: 93, Time: 0.01501s, Loss: 0.64922\n",
      "Epoch: 94, Time: 0.02101s, Loss: 0.66067\n",
      "Epoch: 95, Time: 0.01952s, Loss: 0.64352\n",
      "Epoch: 96, Time: 0.01552s, Loss: 0.64933\n",
      "Epoch: 97, Time: 0.01700s, Loss: 0.64730\n",
      "Epoch: 98, Time: 0.01800s, Loss: 0.64592\n",
      "Epoch: 99, Time: 0.01551s, Loss: 0.64805\n",
      "    ↳ UniGCN Fold Result — Acc: 0.5969, F1: 0.5969\n",
      "\n",
      "[HouseCommittees] Fold 5/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.13163s, Loss: 1.10906\n",
      "Epoch: 1, Time: 0.11102s, Loss: 1.08575\n",
      "Epoch: 2, Time: 0.09356s, Loss: 1.06161\n",
      "Epoch: 3, Time: 0.10758s, Loss: 1.03613\n",
      "Epoch: 4, Time: 0.11806s, Loss: 1.00672\n",
      "Epoch: 5, Time: 0.12126s, Loss: 0.97571\n",
      "Epoch: 6, Time: 0.09513s, Loss: 0.94440\n",
      "Epoch: 7, Time: 0.10156s, Loss: 0.91125\n",
      "Epoch: 8, Time: 0.10310s, Loss: 0.88219\n",
      "Epoch: 9, Time: 0.10216s, Loss: 0.85713\n",
      "Epoch: 10, Time: 0.09315s, Loss: 0.83370\n",
      "Epoch: 11, Time: 0.11056s, Loss: 0.81441\n",
      "Epoch: 12, Time: 0.10354s, Loss: 0.79777\n",
      "Epoch: 13, Time: 0.09404s, Loss: 0.78397\n",
      "Epoch: 14, Time: 0.08955s, Loss: 0.77195\n",
      "Epoch: 15, Time: 0.10820s, Loss: 0.76281\n",
      "Epoch: 16, Time: 0.09071s, Loss: 0.75287\n",
      "Epoch: 17, Time: 0.10154s, Loss: 0.74634\n",
      "Epoch: 18, Time: 0.08854s, Loss: 0.74065\n",
      "Epoch: 19, Time: 0.10613s, Loss: 0.73558\n",
      "Epoch: 20, Time: 0.08804s, Loss: 0.73114\n",
      "Epoch: 21, Time: 0.10670s, Loss: 0.72644\n",
      "Epoch: 22, Time: 0.09255s, Loss: 0.72294\n",
      "Epoch: 23, Time: 0.10554s, Loss: 0.72090\n",
      "Epoch: 24, Time: 0.09556s, Loss: 0.71734\n",
      "Epoch: 25, Time: 0.09565s, Loss: 0.71519\n",
      "Epoch: 26, Time: 0.08555s, Loss: 0.71281\n",
      "Epoch: 27, Time: 0.10355s, Loss: 0.71039\n",
      "Epoch: 28, Time: 0.09757s, Loss: 0.70864\n",
      "Epoch: 29, Time: 0.10457s, Loss: 0.70713\n",
      "Epoch: 30, Time: 0.09453s, Loss: 0.70601\n",
      "Epoch: 31, Time: 0.12345s, Loss: 0.70475\n",
      "Epoch: 32, Time: 0.09757s, Loss: 0.70352\n",
      "Epoch: 33, Time: 0.10154s, Loss: 0.70264\n",
      "Epoch: 34, Time: 0.10655s, Loss: 0.70166\n",
      "Epoch: 35, Time: 0.10556s, Loss: 0.70066\n",
      "Epoch: 36, Time: 0.09454s, Loss: 0.70032\n",
      "Epoch: 37, Time: 0.10058s, Loss: 0.69933\n",
      "Epoch: 38, Time: 0.10408s, Loss: 0.69873\n",
      "Epoch: 39, Time: 0.10361s, Loss: 0.69884\n",
      "Epoch: 40, Time: 0.11807s, Loss: 0.69759\n",
      "Epoch: 41, Time: 0.10713s, Loss: 0.69754\n",
      "Epoch: 42, Time: 0.09655s, Loss: 0.69710\n",
      "Epoch: 43, Time: 0.10637s, Loss: 0.69668\n",
      "Epoch: 44, Time: 0.09758s, Loss: 0.69610\n",
      "Epoch: 45, Time: 0.10554s, Loss: 0.69613\n",
      "Epoch: 46, Time: 0.09955s, Loss: 0.69607\n",
      "Epoch: 47, Time: 0.10655s, Loss: 0.69564\n",
      "Epoch: 48, Time: 0.08704s, Loss: 0.69538\n",
      "Epoch: 49, Time: 0.13610s, Loss: 0.69544\n",
      "Epoch: 50, Time: 0.12715s, Loss: 0.69536\n",
      "Epoch: 51, Time: 0.11055s, Loss: 0.69444\n",
      "Epoch: 52, Time: 0.09855s, Loss: 0.69522\n",
      "Epoch: 53, Time: 0.11055s, Loss: 0.69476\n",
      "Epoch: 54, Time: 0.10374s, Loss: 0.69437\n",
      "Epoch: 55, Time: 0.10256s, Loss: 0.69450\n",
      "Epoch: 56, Time: 0.10831s, Loss: 0.69471\n",
      "Epoch: 57, Time: 0.10460s, Loss: 0.69421\n",
      "Epoch: 58, Time: 0.09657s, Loss: 0.69410\n",
      "Epoch: 59, Time: 0.10053s, Loss: 0.69422\n",
      "Epoch: 60, Time: 0.08955s, Loss: 0.69357\n",
      "Epoch: 61, Time: 0.11857s, Loss: 0.69420\n",
      "Epoch: 62, Time: 0.09255s, Loss: 0.69433\n",
      "Epoch: 63, Time: 0.10955s, Loss: 0.69360\n",
      "Epoch: 64, Time: 0.09956s, Loss: 0.69414\n",
      "Epoch: 65, Time: 0.12985s, Loss: 0.69343\n",
      "Epoch: 66, Time: 0.11014s, Loss: 0.69362\n",
      "Epoch: 67, Time: 0.10553s, Loss: 0.69359\n",
      "Epoch: 68, Time: 0.09656s, Loss: 0.69408\n",
      "Epoch: 69, Time: 0.09503s, Loss: 0.69402\n",
      "Epoch: 70, Time: 0.09153s, Loss: 0.69363\n",
      "Epoch: 71, Time: 0.10956s, Loss: 0.69349\n",
      "Epoch: 72, Time: 0.10055s, Loss: 0.69350\n",
      "Epoch: 73, Time: 0.12961s, Loss: 0.69322\n",
      "Epoch: 74, Time: 0.12057s, Loss: 0.69309\n",
      "Epoch: 75, Time: 0.10106s, Loss: 0.69352\n",
      "Epoch: 76, Time: 0.09664s, Loss: 0.69355\n",
      "Epoch: 77, Time: 0.09955s, Loss: 0.69296\n",
      "Epoch: 78, Time: 0.09754s, Loss: 0.69333\n",
      "Epoch: 79, Time: 0.10616s, Loss: 0.69303\n",
      "Epoch: 80, Time: 0.09155s, Loss: 0.69364\n",
      "Epoch: 81, Time: 0.09910s, Loss: 0.69285\n",
      "Epoch: 82, Time: 0.10354s, Loss: 0.69337\n",
      "Epoch: 83, Time: 0.09677s, Loss: 0.69334\n",
      "Epoch: 84, Time: 0.13009s, Loss: 0.69306\n",
      "Epoch: 85, Time: 0.15160s, Loss: 0.69308\n",
      "Epoch: 86, Time: 0.13008s, Loss: 0.69308\n",
      "Epoch: 87, Time: 0.19482s, Loss: 0.69328\n",
      "Epoch: 88, Time: 0.18481s, Loss: 0.69340\n",
      "Epoch: 89, Time: 0.17363s, Loss: 0.69350\n",
      "Epoch: 90, Time: 0.17939s, Loss: 0.69338\n",
      "Epoch: 91, Time: 0.12257s, Loss: 0.69353\n",
      "Epoch: 92, Time: 0.14827s, Loss: 0.69332\n",
      "Epoch: 93, Time: 0.15131s, Loss: 0.69300\n",
      "Epoch: 94, Time: 0.15080s, Loss: 0.69325\n",
      "Epoch: 95, Time: 0.15128s, Loss: 0.69291\n",
      "Epoch: 96, Time: 0.16422s, Loss: 0.69314\n",
      "Epoch: 97, Time: 0.15136s, Loss: 0.69277\n",
      "Epoch: 98, Time: 0.19211s, Loss: 0.69331\n",
      "Epoch: 99, Time: 0.14819s, Loss: 0.69285\n",
      "    ↳ HGNN Fold Result — Acc: 0.5194, F1: 0.5194\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.01900s, Loss: 1.07366\n",
      "Epoch: 1, Time: 0.01700s, Loss: 1.04640\n",
      "Epoch: 2, Time: 0.01752s, Loss: 0.93016\n",
      "Epoch: 3, Time: 0.01852s, Loss: 0.90861\n",
      "Epoch: 4, Time: 0.02400s, Loss: 0.87278\n",
      "Epoch: 5, Time: 0.01755s, Loss: 0.82988\n",
      "Epoch: 6, Time: 0.01801s, Loss: 0.81326\n",
      "Epoch: 7, Time: 0.01605s, Loss: 0.78851\n",
      "Epoch: 8, Time: 0.01852s, Loss: 0.76980\n",
      "Epoch: 9, Time: 0.01752s, Loss: 0.74911\n",
      "Epoch: 10, Time: 0.02553s, Loss: 0.73260\n",
      "Epoch: 11, Time: 0.02400s, Loss: 0.71453\n",
      "Epoch: 12, Time: 0.02346s, Loss: 0.70048\n",
      "Epoch: 13, Time: 0.01915s, Loss: 0.69105\n",
      "Epoch: 14, Time: 0.01900s, Loss: 0.67987\n",
      "Epoch: 15, Time: 0.01652s, Loss: 0.67427\n",
      "Epoch: 16, Time: 0.01600s, Loss: 0.67038\n",
      "Epoch: 17, Time: 0.01803s, Loss: 0.66296\n",
      "Epoch: 18, Time: 0.01900s, Loss: 0.65994\n",
      "Epoch: 19, Time: 0.01951s, Loss: 0.65553\n",
      "Epoch: 20, Time: 0.02201s, Loss: 0.65259\n",
      "Epoch: 21, Time: 0.01916s, Loss: 0.65078\n",
      "Epoch: 22, Time: 0.01900s, Loss: 0.64782\n",
      "Epoch: 23, Time: 0.01600s, Loss: 0.64460\n",
      "Epoch: 24, Time: 0.01712s, Loss: 0.64375\n",
      "Epoch: 25, Time: 0.02152s, Loss: 0.64318\n",
      "Epoch: 26, Time: 0.02252s, Loss: 0.63970\n",
      "Epoch: 27, Time: 0.02301s, Loss: 0.63988\n",
      "Epoch: 28, Time: 0.02602s, Loss: 0.63817\n",
      "Epoch: 29, Time: 0.02100s, Loss: 0.63605\n",
      "Epoch: 30, Time: 0.02202s, Loss: 0.63466\n",
      "Epoch: 31, Time: 0.02153s, Loss: 0.63518\n",
      "Epoch: 32, Time: 0.02200s, Loss: 0.63355\n",
      "Epoch: 33, Time: 0.02000s, Loss: 0.63249\n",
      "Epoch: 34, Time: 0.02254s, Loss: 0.63147\n",
      "Epoch: 35, Time: 0.02301s, Loss: 0.62952\n",
      "Epoch: 36, Time: 0.02111s, Loss: 0.62943\n",
      "Epoch: 37, Time: 0.01854s, Loss: 0.62980\n",
      "Epoch: 38, Time: 0.01900s, Loss: 0.62817\n",
      "Epoch: 39, Time: 0.02052s, Loss: 0.62526\n",
      "Epoch: 40, Time: 0.02301s, Loss: 0.62838\n",
      "Epoch: 41, Time: 0.02454s, Loss: 0.62430\n",
      "Epoch: 42, Time: 0.02100s, Loss: 0.63225\n",
      "Epoch: 43, Time: 0.02001s, Loss: 0.62527\n",
      "Epoch: 44, Time: 0.02102s, Loss: 0.62487\n",
      "Epoch: 45, Time: 0.02501s, Loss: 0.62659\n",
      "Epoch: 46, Time: 0.02154s, Loss: 0.62139\n",
      "Epoch: 47, Time: 0.01951s, Loss: 0.62350\n",
      "Epoch: 48, Time: 0.02400s, Loss: 0.61768\n",
      "Epoch: 49, Time: 0.02354s, Loss: 0.61924\n",
      "Epoch: 50, Time: 0.01952s, Loss: 0.61606\n",
      "Epoch: 51, Time: 0.02000s, Loss: 0.61696\n",
      "Epoch: 52, Time: 0.02352s, Loss: 0.61732\n",
      "Epoch: 53, Time: 0.02301s, Loss: 0.61426\n",
      "Epoch: 54, Time: 0.01752s, Loss: 0.61626\n",
      "Epoch: 55, Time: 0.01600s, Loss: 0.61882\n",
      "Epoch: 56, Time: 0.02300s, Loss: 0.62481\n",
      "Epoch: 57, Time: 0.01852s, Loss: 0.62325\n",
      "Epoch: 58, Time: 0.02052s, Loss: 0.61299\n",
      "Epoch: 59, Time: 0.01952s, Loss: 0.61198\n",
      "Epoch: 60, Time: 0.02101s, Loss: 0.61198\n",
      "Epoch: 61, Time: 0.02052s, Loss: 0.61535\n",
      "Epoch: 62, Time: 0.02052s, Loss: 0.62034\n",
      "Epoch: 63, Time: 0.02201s, Loss: 0.61029\n",
      "Epoch: 64, Time: 0.02403s, Loss: 0.60667\n",
      "Epoch: 65, Time: 0.03055s, Loss: 0.60969\n",
      "Epoch: 66, Time: 0.02601s, Loss: 0.60892\n",
      "Epoch: 67, Time: 0.02500s, Loss: 0.60827\n",
      "Epoch: 68, Time: 0.02152s, Loss: 0.61287\n",
      "Epoch: 69, Time: 0.02201s, Loss: 0.61303\n",
      "Epoch: 70, Time: 0.02111s, Loss: 0.61754\n",
      "Epoch: 71, Time: 0.01851s, Loss: 0.60837\n",
      "Epoch: 72, Time: 0.01600s, Loss: 0.61283\n",
      "Epoch: 73, Time: 0.01951s, Loss: 0.63285\n",
      "Epoch: 74, Time: 0.01401s, Loss: 0.61506\n",
      "Epoch: 75, Time: 0.01900s, Loss: 0.62462\n",
      "Epoch: 76, Time: 0.01800s, Loss: 0.61324\n",
      "Epoch: 77, Time: 0.02102s, Loss: 0.61707\n",
      "Epoch: 78, Time: 0.02200s, Loss: 0.60829\n",
      "Epoch: 79, Time: 0.02400s, Loss: 0.61926\n",
      "Epoch: 80, Time: 0.02052s, Loss: 0.61012\n",
      "Epoch: 81, Time: 0.02000s, Loss: 0.62474\n",
      "Epoch: 82, Time: 0.02153s, Loss: 0.60834\n",
      "Epoch: 83, Time: 0.02001s, Loss: 0.61636\n",
      "Epoch: 84, Time: 0.02400s, Loss: 0.61808\n",
      "Epoch: 85, Time: 0.02354s, Loss: 0.61468\n",
      "Epoch: 86, Time: 0.02091s, Loss: 0.61055\n",
      "Epoch: 87, Time: 0.01502s, Loss: 0.61822\n",
      "Epoch: 88, Time: 0.01953s, Loss: 0.61327\n",
      "Epoch: 89, Time: 0.01901s, Loss: 0.60597\n",
      "Epoch: 90, Time: 0.01653s, Loss: 0.61422\n",
      "Epoch: 91, Time: 0.01700s, Loss: 0.61002\n",
      "Epoch: 92, Time: 0.01953s, Loss: 0.60528\n",
      "Epoch: 93, Time: 0.02103s, Loss: 0.60705\n",
      "Epoch: 94, Time: 0.01600s, Loss: 0.60667\n",
      "Epoch: 95, Time: 0.01801s, Loss: 0.60406\n",
      "Epoch: 96, Time: 0.01753s, Loss: 0.60700\n",
      "Epoch: 97, Time: 0.01752s, Loss: 0.60253\n",
      "Epoch: 98, Time: 0.01405s, Loss: 0.60276\n",
      "Epoch: 99, Time: 0.01700s, Loss: 0.60863\n",
      "    ↳ HGNNP Fold Result — Acc: 0.5736, F1: 0.5736\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.02000s, Loss: 1.29912\n",
      "Epoch: 1, Time: 0.01952s, Loss: 1.00262\n",
      "Epoch: 2, Time: 0.01699s, Loss: 0.92044\n",
      "Epoch: 3, Time: 0.01452s, Loss: 0.87696\n",
      "Epoch: 4, Time: 0.01702s, Loss: 0.83032\n",
      "Epoch: 5, Time: 0.01919s, Loss: 0.80322\n",
      "Epoch: 6, Time: 0.01601s, Loss: 0.78298\n",
      "Epoch: 7, Time: 0.01651s, Loss: 0.76412\n",
      "Epoch: 8, Time: 0.02100s, Loss: 0.75227\n",
      "Epoch: 9, Time: 0.02155s, Loss: 0.73892\n",
      "Epoch: 10, Time: 0.02200s, Loss: 0.72557\n",
      "Epoch: 11, Time: 0.03654s, Loss: 0.71863\n",
      "Epoch: 12, Time: 0.03201s, Loss: 0.71195\n",
      "Epoch: 13, Time: 0.03655s, Loss: 0.70507\n",
      "Epoch: 14, Time: 0.03367s, Loss: 0.69775\n",
      "Epoch: 15, Time: 0.03408s, Loss: 0.69172\n",
      "Epoch: 16, Time: 0.03100s, Loss: 0.68955\n",
      "Epoch: 17, Time: 0.03402s, Loss: 0.68830\n",
      "Epoch: 18, Time: 0.02800s, Loss: 0.68390\n",
      "Epoch: 19, Time: 0.02753s, Loss: 0.68181\n",
      "Epoch: 20, Time: 0.02768s, Loss: 0.68051\n",
      "Epoch: 21, Time: 0.01900s, Loss: 0.67848\n",
      "Epoch: 22, Time: 0.02100s, Loss: 0.67601\n",
      "Epoch: 23, Time: 0.02751s, Loss: 0.67427\n",
      "Epoch: 24, Time: 0.02700s, Loss: 0.67498\n",
      "Epoch: 25, Time: 0.02801s, Loss: 0.67215\n",
      "Epoch: 26, Time: 0.03101s, Loss: 0.67043\n",
      "Epoch: 27, Time: 0.02600s, Loss: 0.66881\n",
      "Epoch: 28, Time: 0.02604s, Loss: 0.66887\n",
      "Epoch: 29, Time: 0.02101s, Loss: 0.66892\n",
      "Epoch: 30, Time: 0.02100s, Loss: 0.66697\n",
      "Epoch: 31, Time: 0.02003s, Loss: 0.66471\n",
      "Epoch: 32, Time: 0.02853s, Loss: 0.66473\n",
      "Epoch: 33, Time: 0.03001s, Loss: 0.66290\n",
      "Epoch: 34, Time: 0.02353s, Loss: 0.66330\n",
      "Epoch: 35, Time: 0.02453s, Loss: 0.66214\n",
      "Epoch: 36, Time: 0.02206s, Loss: 0.66180\n",
      "Epoch: 37, Time: 0.02500s, Loss: 0.66259\n",
      "Epoch: 38, Time: 0.02652s, Loss: 0.66048\n",
      "Epoch: 39, Time: 0.02503s, Loss: 0.66280\n",
      "Epoch: 40, Time: 0.04352s, Loss: 0.65762\n",
      "Epoch: 41, Time: 0.02453s, Loss: 0.65961\n",
      "Epoch: 42, Time: 0.03287s, Loss: 0.65817\n",
      "Epoch: 43, Time: 0.05407s, Loss: 0.65815\n",
      "Epoch: 44, Time: 0.04453s, Loss: 0.65658\n",
      "Epoch: 45, Time: 0.03453s, Loss: 0.65472\n",
      "Epoch: 46, Time: 0.04002s, Loss: 0.65680\n",
      "Epoch: 47, Time: 0.02900s, Loss: 0.65547\n",
      "Epoch: 48, Time: 0.01951s, Loss: 0.66112\n",
      "Epoch: 49, Time: 0.02104s, Loss: 0.66164\n",
      "Epoch: 50, Time: 0.02101s, Loss: 0.66507\n",
      "Epoch: 51, Time: 0.02353s, Loss: 0.65751\n",
      "Epoch: 52, Time: 0.02154s, Loss: 0.66502\n",
      "Epoch: 53, Time: 0.02200s, Loss: 0.66765\n",
      "Epoch: 54, Time: 0.02352s, Loss: 0.65902\n",
      "Epoch: 55, Time: 0.02153s, Loss: 0.66061\n",
      "Epoch: 56, Time: 0.02500s, Loss: 0.65882\n",
      "Epoch: 57, Time: 0.02353s, Loss: 0.65855\n",
      "Epoch: 58, Time: 0.02053s, Loss: 0.65880\n",
      "Epoch: 59, Time: 0.02100s, Loss: 0.65596\n",
      "Epoch: 60, Time: 0.02252s, Loss: 0.65979\n",
      "Epoch: 61, Time: 0.02153s, Loss: 0.65367\n",
      "Epoch: 62, Time: 0.02001s, Loss: 0.65439\n",
      "Epoch: 63, Time: 0.01600s, Loss: 0.66498\n",
      "Epoch: 64, Time: 0.01600s, Loss: 0.65336\n",
      "Epoch: 65, Time: 0.01653s, Loss: 0.65557\n",
      "Epoch: 66, Time: 0.01800s, Loss: 0.65347\n",
      "Epoch: 67, Time: 0.01651s, Loss: 0.65507\n",
      "Epoch: 68, Time: 0.02101s, Loss: 0.65718\n",
      "Epoch: 69, Time: 0.01603s, Loss: 0.65140\n",
      "Epoch: 70, Time: 0.01501s, Loss: 0.64873\n",
      "Epoch: 71, Time: 0.01907s, Loss: 0.65697\n",
      "Epoch: 72, Time: 0.01601s, Loss: 0.65503\n",
      "Epoch: 73, Time: 0.01904s, Loss: 0.65353\n",
      "Epoch: 74, Time: 0.01800s, Loss: 0.65807\n",
      "Epoch: 75, Time: 0.01752s, Loss: 0.66345\n",
      "Epoch: 76, Time: 0.01901s, Loss: 0.64995\n",
      "Epoch: 77, Time: 0.02153s, Loss: 0.65306\n",
      "Epoch: 78, Time: 0.01651s, Loss: 0.65066\n",
      "Epoch: 79, Time: 0.01500s, Loss: 0.65744\n",
      "Epoch: 80, Time: 0.01600s, Loss: 0.64914\n",
      "Epoch: 81, Time: 0.01752s, Loss: 0.65829\n",
      "Epoch: 82, Time: 0.01857s, Loss: 0.65438\n",
      "Epoch: 83, Time: 0.01800s, Loss: 0.65341\n",
      "Epoch: 84, Time: 0.02254s, Loss: 0.64984\n",
      "Epoch: 85, Time: 0.01901s, Loss: 0.65090\n",
      "Epoch: 86, Time: 0.01400s, Loss: 0.64790\n",
      "Epoch: 87, Time: 0.01700s, Loss: 0.64726\n",
      "Epoch: 88, Time: 0.01505s, Loss: 0.65052\n",
      "Epoch: 89, Time: 0.01401s, Loss: 0.64512\n",
      "Epoch: 90, Time: 0.01551s, Loss: 0.65507\n",
      "Epoch: 91, Time: 0.01501s, Loss: 0.64600\n",
      "Epoch: 92, Time: 0.01553s, Loss: 0.65936\n",
      "Epoch: 93, Time: 0.01600s, Loss: 0.64438\n",
      "Epoch: 94, Time: 0.01700s, Loss: 0.64471\n",
      "Epoch: 95, Time: 0.03002s, Loss: 0.64535\n",
      "Epoch: 96, Time: 0.04442s, Loss: 0.64894\n",
      "Epoch: 97, Time: 0.02274s, Loss: 0.64374\n",
      "Epoch: 98, Time: 0.02576s, Loss: 0.64589\n",
      "Epoch: 99, Time: 0.02481s, Loss: 0.64639\n",
      "    ↳ UniGCN Fold Result — Acc: 0.6047, F1: 0.6047\n"
     ]
    }
   ],
   "source": [
    "# HouseCommittees\n",
    "name = \"HouseCommittees\"\n",
    "\n",
    "print(f\"\\n==> Running {name} without Top-k\")\n",
    "results_original = run_cross_validation(dhg_datastets[name], name, use_topk=False)\n",
    "for model_name, metrics in results_original.items():\n",
    "    all_results[f\"{name}_original_{model_name}\"] = metrics\n",
    "\n",
    "print(f\"\\n==> Running {name} with Top-k\")\n",
    "results_topk = run_cross_validation(dhg_datastets[name], name, use_topk=True, k=3)\n",
    "for model_name, metrics in results_topk.items():\n",
    "    all_results[f\"{name}_topk_{model_name}\"] = metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Running CocitationCiteseer without Top-k\n",
      "\n",
      "[CocitationCiteseer] Fold 1/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.05052s, Loss: 1.78656\n",
      "Epoch: 1, Time: 0.03099s, Loss: 1.78139\n",
      "Epoch: 2, Time: 0.02900s, Loss: 1.77543\n",
      "Epoch: 3, Time: 0.03453s, Loss: 1.76766\n",
      "Epoch: 4, Time: 0.02900s, Loss: 1.75866\n",
      "Epoch: 5, Time: 0.02705s, Loss: 1.74897\n",
      "Epoch: 6, Time: 0.03394s, Loss: 1.73830\n",
      "Epoch: 7, Time: 0.02706s, Loss: 1.72693\n",
      "Epoch: 8, Time: 0.02751s, Loss: 1.71346\n",
      "Epoch: 9, Time: 0.03205s, Loss: 1.69900\n",
      "Epoch: 10, Time: 0.02900s, Loss: 1.68813\n",
      "Epoch: 11, Time: 0.03653s, Loss: 1.67084\n",
      "Epoch: 12, Time: 0.02706s, Loss: 1.65716\n",
      "Epoch: 13, Time: 0.03000s, Loss: 1.63838\n",
      "Epoch: 14, Time: 0.03752s, Loss: 1.62396\n",
      "Epoch: 15, Time: 0.02798s, Loss: 1.60838\n",
      "Epoch: 16, Time: 0.14767s, Loss: 1.59125\n",
      "Epoch: 17, Time: 0.05704s, Loss: 1.57463\n",
      "Epoch: 18, Time: 0.03552s, Loss: 1.55765\n",
      "Epoch: 19, Time: 0.03305s, Loss: 1.54014\n",
      "Epoch: 20, Time: 0.03552s, Loss: 1.52442\n",
      "Epoch: 21, Time: 0.03705s, Loss: 1.50914\n",
      "Epoch: 22, Time: 0.03401s, Loss: 1.49653\n",
      "Epoch: 23, Time: 0.03753s, Loss: 1.47943\n",
      "Epoch: 24, Time: 0.03500s, Loss: 1.46693\n",
      "Epoch: 25, Time: 0.04055s, Loss: 1.45364\n",
      "Epoch: 26, Time: 0.04007s, Loss: 1.44154\n",
      "Epoch: 27, Time: 0.03553s, Loss: 1.42761\n",
      "Epoch: 28, Time: 0.03305s, Loss: 1.42171\n",
      "Epoch: 29, Time: 0.03253s, Loss: 1.41019\n",
      "Epoch: 30, Time: 0.02806s, Loss: 1.40297\n",
      "Epoch: 31, Time: 0.02653s, Loss: 1.39007\n",
      "Epoch: 32, Time: 0.02701s, Loss: 1.38448\n",
      "Epoch: 33, Time: 0.03255s, Loss: 1.37830\n",
      "Epoch: 34, Time: 0.03053s, Loss: 1.37065\n",
      "Epoch: 35, Time: 0.02508s, Loss: 1.36291\n",
      "Epoch: 36, Time: 0.02654s, Loss: 1.35425\n",
      "Epoch: 37, Time: 0.02801s, Loss: 1.34402\n",
      "Epoch: 38, Time: 0.03053s, Loss: 1.34236\n",
      "Epoch: 39, Time: 0.03200s, Loss: 1.34066\n",
      "Epoch: 40, Time: 0.02853s, Loss: 1.33480\n",
      "Epoch: 41, Time: 0.03200s, Loss: 1.32991\n",
      "Epoch: 42, Time: 0.03104s, Loss: 1.32558\n",
      "Epoch: 43, Time: 0.03139s, Loss: 1.31762\n",
      "Epoch: 44, Time: 0.03306s, Loss: 1.32053\n",
      "Epoch: 45, Time: 0.03152s, Loss: 1.30916\n",
      "Epoch: 46, Time: 0.02853s, Loss: 1.30700\n",
      "Epoch: 47, Time: 0.03016s, Loss: 1.30519\n",
      "Epoch: 48, Time: 0.02700s, Loss: 1.30116\n",
      "Epoch: 49, Time: 0.02955s, Loss: 1.29545\n",
      "Epoch: 50, Time: 0.02853s, Loss: 1.29475\n",
      "Epoch: 51, Time: 0.03807s, Loss: 1.29472\n",
      "Epoch: 52, Time: 0.03101s, Loss: 1.28746\n",
      "Epoch: 53, Time: 0.03105s, Loss: 1.28405\n",
      "Epoch: 54, Time: 0.03253s, Loss: 1.28472\n",
      "Epoch: 55, Time: 0.03003s, Loss: 1.28299\n",
      "Epoch: 56, Time: 0.02974s, Loss: 1.27650\n",
      "Epoch: 57, Time: 0.03002s, Loss: 1.27798\n",
      "Epoch: 58, Time: 0.03252s, Loss: 1.27223\n",
      "Epoch: 59, Time: 0.03054s, Loss: 1.27297\n",
      "Epoch: 60, Time: 0.03200s, Loss: 1.27139\n",
      "Epoch: 61, Time: 0.03201s, Loss: 1.26506\n",
      "Epoch: 62, Time: 0.03006s, Loss: 1.26487\n",
      "Epoch: 63, Time: 0.03153s, Loss: 1.26401\n",
      "Epoch: 64, Time: 0.03154s, Loss: 1.26113\n",
      "Epoch: 65, Time: 0.03152s, Loss: 1.26118\n",
      "Epoch: 66, Time: 0.03304s, Loss: 1.25577\n",
      "Epoch: 67, Time: 0.03252s, Loss: 1.25534\n",
      "Epoch: 68, Time: 0.03206s, Loss: 1.25382\n",
      "Epoch: 69, Time: 0.02981s, Loss: 1.25401\n",
      "Epoch: 70, Time: 0.03251s, Loss: 1.25214\n",
      "Epoch: 71, Time: 0.03000s, Loss: 1.24970\n",
      "Epoch: 72, Time: 0.03200s, Loss: 1.24935\n",
      "Epoch: 73, Time: 0.03071s, Loss: 1.24715\n",
      "Epoch: 74, Time: 0.03231s, Loss: 1.24723\n",
      "Epoch: 75, Time: 0.03200s, Loss: 1.24351\n",
      "Epoch: 76, Time: 0.02802s, Loss: 1.24577\n",
      "Epoch: 77, Time: 0.02652s, Loss: 1.23961\n",
      "Epoch: 78, Time: 0.03253s, Loss: 1.23966\n",
      "Epoch: 79, Time: 0.02906s, Loss: 1.23926\n",
      "Epoch: 80, Time: 0.02454s, Loss: 1.23747\n",
      "Epoch: 81, Time: 0.03154s, Loss: 1.23798\n",
      "Epoch: 82, Time: 0.04952s, Loss: 1.23527\n",
      "Epoch: 83, Time: 0.05167s, Loss: 1.23687\n",
      "Epoch: 84, Time: 0.04848s, Loss: 1.23266\n",
      "Epoch: 85, Time: 0.03711s, Loss: 1.23310\n",
      "Epoch: 86, Time: 0.03307s, Loss: 1.23302\n",
      "Epoch: 87, Time: 0.03152s, Loss: 1.22688\n",
      "Epoch: 88, Time: 0.02954s, Loss: 1.22958\n",
      "Epoch: 89, Time: 0.02653s, Loss: 1.22883\n",
      "Epoch: 90, Time: 0.03152s, Loss: 1.22821\n",
      "Epoch: 91, Time: 0.03318s, Loss: 1.22696\n",
      "Epoch: 92, Time: 0.03400s, Loss: 1.22872\n",
      "Epoch: 93, Time: 0.03553s, Loss: 1.22921\n",
      "Epoch: 94, Time: 0.03001s, Loss: 1.22626\n",
      "Epoch: 95, Time: 0.02800s, Loss: 1.22340\n",
      "Epoch: 96, Time: 0.02854s, Loss: 1.22638\n",
      "Epoch: 97, Time: 0.03106s, Loss: 1.22362\n",
      "Epoch: 98, Time: 0.03353s, Loss: 1.22281\n",
      "Epoch: 99, Time: 0.03304s, Loss: 1.22404\n",
      "    ↳ HGNN Fold Result — Acc: 0.4148, F1: 0.4148\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.04914s, Loss: 1.85912\n",
      "Epoch: 1, Time: 0.04454s, Loss: 1.41539\n",
      "Epoch: 2, Time: 0.03505s, Loss: 1.26404\n",
      "Epoch: 3, Time: 0.03254s, Loss: 1.20906\n",
      "Epoch: 4, Time: 0.03205s, Loss: 1.18516\n",
      "Epoch: 5, Time: 0.03201s, Loss: 1.16473\n",
      "Epoch: 6, Time: 0.03305s, Loss: 1.15380\n",
      "Epoch: 7, Time: 0.03653s, Loss: 1.14448\n",
      "Epoch: 8, Time: 0.03305s, Loss: 1.13712\n",
      "Epoch: 9, Time: 0.03052s, Loss: 1.13089\n",
      "Epoch: 10, Time: 0.03205s, Loss: 1.12441\n",
      "Epoch: 11, Time: 0.03161s, Loss: 1.11935\n",
      "Epoch: 12, Time: 0.03303s, Loss: 1.11709\n",
      "Epoch: 13, Time: 0.03454s, Loss: 1.11415\n",
      "Epoch: 14, Time: 0.03204s, Loss: 1.11071\n",
      "Epoch: 15, Time: 0.03426s, Loss: 1.10852\n",
      "Epoch: 16, Time: 0.03810s, Loss: 1.10621\n",
      "Epoch: 17, Time: 0.03253s, Loss: 1.10047\n",
      "Epoch: 18, Time: 0.03403s, Loss: 1.09837\n",
      "Epoch: 19, Time: 0.03552s, Loss: 1.09862\n",
      "Epoch: 20, Time: 0.03407s, Loss: 1.09772\n",
      "Epoch: 21, Time: 0.03852s, Loss: 1.09620\n",
      "Epoch: 22, Time: 0.03406s, Loss: 1.09564\n",
      "Epoch: 23, Time: 0.03352s, Loss: 1.09113\n",
      "Epoch: 24, Time: 0.03305s, Loss: 1.09544\n",
      "Epoch: 25, Time: 0.03752s, Loss: 1.09466\n",
      "Epoch: 26, Time: 0.03618s, Loss: 1.09193\n",
      "Epoch: 27, Time: 0.03057s, Loss: 1.09102\n",
      "Epoch: 28, Time: 0.03004s, Loss: 1.08795\n",
      "Epoch: 29, Time: 0.03652s, Loss: 1.09203\n",
      "Epoch: 30, Time: 0.07008s, Loss: 1.08794\n",
      "Epoch: 31, Time: 0.04070s, Loss: 1.08864\n",
      "Epoch: 32, Time: 0.03752s, Loss: 1.08554\n",
      "Epoch: 33, Time: 0.04003s, Loss: 1.08759\n",
      "Epoch: 34, Time: 0.03754s, Loss: 1.08634\n",
      "Epoch: 35, Time: 0.03453s, Loss: 1.08941\n",
      "Epoch: 36, Time: 0.03300s, Loss: 1.08340\n",
      "Epoch: 37, Time: 0.03352s, Loss: 1.08321\n",
      "Epoch: 38, Time: 0.04153s, Loss: 1.08486\n",
      "Epoch: 39, Time: 0.03555s, Loss: 1.08177\n",
      "Epoch: 40, Time: 0.03452s, Loss: 1.08435\n",
      "Epoch: 41, Time: 0.02551s, Loss: 1.08226\n",
      "Epoch: 42, Time: 0.03200s, Loss: 1.08021\n",
      "Epoch: 43, Time: 0.03253s, Loss: 1.08411\n",
      "Epoch: 44, Time: 0.02700s, Loss: 1.08122\n",
      "Epoch: 45, Time: 0.02952s, Loss: 1.08152\n",
      "Epoch: 46, Time: 0.03055s, Loss: 1.08097\n",
      "Epoch: 47, Time: 0.03005s, Loss: 1.07927\n",
      "Epoch: 48, Time: 0.03152s, Loss: 1.08083\n",
      "Epoch: 49, Time: 0.03052s, Loss: 1.07931\n",
      "Epoch: 50, Time: 0.03300s, Loss: 1.07882\n",
      "Epoch: 51, Time: 0.03300s, Loss: 1.08207\n",
      "Epoch: 52, Time: 0.03167s, Loss: 1.08297\n",
      "Epoch: 53, Time: 0.03853s, Loss: 1.08234\n",
      "Epoch: 54, Time: 0.03300s, Loss: 1.08354\n",
      "Epoch: 55, Time: 0.03200s, Loss: 1.08191\n",
      "Epoch: 56, Time: 0.03154s, Loss: 1.08231\n",
      "Epoch: 57, Time: 0.03400s, Loss: 1.08206\n",
      "Epoch: 58, Time: 0.03805s, Loss: 1.07906\n",
      "Epoch: 59, Time: 0.03400s, Loss: 1.08036\n",
      "Epoch: 60, Time: 0.03054s, Loss: 1.08241\n",
      "Epoch: 61, Time: 0.03600s, Loss: 1.07668\n",
      "Epoch: 62, Time: 0.03154s, Loss: 1.08452\n",
      "Epoch: 63, Time: 0.03300s, Loss: 1.07676\n",
      "Epoch: 64, Time: 0.03553s, Loss: 1.07983\n",
      "Epoch: 65, Time: 0.03300s, Loss: 1.07801\n",
      "Epoch: 66, Time: 0.03004s, Loss: 1.07794\n",
      "Epoch: 67, Time: 0.03153s, Loss: 1.07989\n",
      "Epoch: 68, Time: 0.03107s, Loss: 1.07739\n",
      "Epoch: 69, Time: 0.02953s, Loss: 1.08021\n",
      "Epoch: 70, Time: 0.03304s, Loss: 1.07752\n",
      "Epoch: 71, Time: 0.03051s, Loss: 1.07776\n",
      "Epoch: 72, Time: 0.03154s, Loss: 1.07605\n",
      "Epoch: 73, Time: 0.03271s, Loss: 1.07638\n",
      "Epoch: 74, Time: 0.03154s, Loss: 1.07792\n",
      "Epoch: 75, Time: 0.03453s, Loss: 1.07505\n",
      "Epoch: 76, Time: 0.02802s, Loss: 1.07959\n",
      "Epoch: 77, Time: 0.02953s, Loss: 1.07811\n",
      "Epoch: 78, Time: 0.03301s, Loss: 1.07737\n",
      "Epoch: 79, Time: 0.03352s, Loss: 1.08083\n",
      "Epoch: 80, Time: 0.03100s, Loss: 1.07486\n",
      "Epoch: 81, Time: 0.02904s, Loss: 1.07910\n",
      "Epoch: 82, Time: 0.02812s, Loss: 1.07356\n",
      "Epoch: 83, Time: 0.02754s, Loss: 1.07513\n",
      "Epoch: 84, Time: 0.03300s, Loss: 1.07547\n",
      "Epoch: 85, Time: 0.03000s, Loss: 1.07968\n",
      "Epoch: 86, Time: 0.02852s, Loss: 1.07949\n",
      "Epoch: 87, Time: 0.02900s, Loss: 1.08167\n",
      "Epoch: 88, Time: 0.03152s, Loss: 1.07657\n",
      "Epoch: 89, Time: 0.03153s, Loss: 1.07640\n",
      "Epoch: 90, Time: 0.03304s, Loss: 1.07737\n",
      "Epoch: 91, Time: 0.03054s, Loss: 1.07874\n",
      "Epoch: 92, Time: 0.03005s, Loss: 1.07612\n",
      "Epoch: 93, Time: 0.03353s, Loss: 1.07501\n",
      "Epoch: 94, Time: 0.03355s, Loss: 1.07985\n",
      "Epoch: 95, Time: 0.03204s, Loss: 1.07604\n",
      "Epoch: 96, Time: 0.03100s, Loss: 1.07490\n",
      "Epoch: 97, Time: 0.02902s, Loss: 1.07919\n",
      "Epoch: 98, Time: 0.03100s, Loss: 1.07709\n",
      "Epoch: 99, Time: 0.03451s, Loss: 1.07502\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4042, F1: 0.4042\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03552s, Loss: 1.86460\n",
      "Epoch: 1, Time: 0.02952s, Loss: 1.46334\n",
      "Epoch: 2, Time: 0.03252s, Loss: 1.30470\n",
      "Epoch: 3, Time: 0.03300s, Loss: 1.25787\n",
      "Epoch: 4, Time: 0.03101s, Loss: 1.23303\n",
      "Epoch: 5, Time: 0.03253s, Loss: 1.21505\n",
      "Epoch: 6, Time: 0.03000s, Loss: 1.20223\n",
      "Epoch: 7, Time: 0.03208s, Loss: 1.19313\n",
      "Epoch: 8, Time: 0.03051s, Loss: 1.18431\n",
      "Epoch: 9, Time: 0.03404s, Loss: 1.17823\n",
      "Epoch: 10, Time: 0.03401s, Loss: 1.17389\n",
      "Epoch: 11, Time: 0.03105s, Loss: 1.16768\n",
      "Epoch: 12, Time: 0.02854s, Loss: 1.16356\n",
      "Epoch: 13, Time: 0.03804s, Loss: 1.15754\n",
      "Epoch: 14, Time: 0.03401s, Loss: 1.15343\n",
      "Epoch: 15, Time: 0.03305s, Loss: 1.15267\n",
      "Epoch: 16, Time: 0.03054s, Loss: 1.14873\n",
      "Epoch: 17, Time: 0.03460s, Loss: 1.14675\n",
      "Epoch: 18, Time: 0.03753s, Loss: 1.13836\n",
      "Epoch: 19, Time: 0.03505s, Loss: 1.14456\n",
      "Epoch: 20, Time: 0.03553s, Loss: 1.14283\n",
      "Epoch: 21, Time: 0.03253s, Loss: 1.13937\n",
      "Epoch: 22, Time: 0.03353s, Loss: 1.13611\n",
      "Epoch: 23, Time: 0.03726s, Loss: 1.13186\n",
      "Epoch: 24, Time: 0.03453s, Loss: 1.13249\n",
      "Epoch: 25, Time: 0.03404s, Loss: 1.13054\n",
      "Epoch: 26, Time: 0.03153s, Loss: 1.13370\n",
      "Epoch: 27, Time: 0.03405s, Loss: 1.13090\n",
      "Epoch: 28, Time: 0.03400s, Loss: 1.12718\n",
      "Epoch: 29, Time: 0.03267s, Loss: 1.12653\n",
      "Epoch: 30, Time: 0.03353s, Loss: 1.12513\n",
      "Epoch: 31, Time: 0.03407s, Loss: 1.12165\n",
      "Epoch: 32, Time: 0.03351s, Loss: 1.12205\n",
      "Epoch: 33, Time: 0.03253s, Loss: 1.11839\n",
      "Epoch: 34, Time: 0.03151s, Loss: 1.12161\n",
      "Epoch: 35, Time: 0.05096s, Loss: 1.12363\n",
      "Epoch: 36, Time: 0.04653s, Loss: 1.12261\n",
      "Epoch: 37, Time: 0.03504s, Loss: 1.11881\n",
      "Epoch: 38, Time: 0.03759s, Loss: 1.11935\n",
      "Epoch: 39, Time: 0.03454s, Loss: 1.11721\n",
      "Epoch: 40, Time: 0.05052s, Loss: 1.11689\n",
      "Epoch: 41, Time: 0.03660s, Loss: 1.11906\n",
      "Epoch: 42, Time: 0.03604s, Loss: 1.11835\n",
      "Epoch: 43, Time: 0.03553s, Loss: 1.11524\n",
      "Epoch: 44, Time: 0.03805s, Loss: 1.11349\n",
      "Epoch: 45, Time: 0.03400s, Loss: 1.11119\n",
      "Epoch: 46, Time: 0.03954s, Loss: 1.11458\n",
      "Epoch: 47, Time: 0.03101s, Loss: 1.11328\n",
      "Epoch: 48, Time: 0.03344s, Loss: 1.10916\n",
      "Epoch: 49, Time: 0.03859s, Loss: 1.10903\n",
      "Epoch: 50, Time: 0.03353s, Loss: 1.10890\n",
      "Epoch: 51, Time: 0.03955s, Loss: 1.10821\n",
      "Epoch: 52, Time: 0.03653s, Loss: 1.11070\n",
      "Epoch: 53, Time: 0.03154s, Loss: 1.10848\n",
      "Epoch: 54, Time: 0.03452s, Loss: 1.10806\n",
      "Epoch: 55, Time: 0.03300s, Loss: 1.10302\n",
      "Epoch: 56, Time: 0.04752s, Loss: 1.10678\n",
      "Epoch: 57, Time: 0.08608s, Loss: 1.10179\n",
      "Epoch: 58, Time: 0.05606s, Loss: 1.10573\n",
      "Epoch: 59, Time: 0.04709s, Loss: 1.10475\n",
      "Epoch: 60, Time: 0.05053s, Loss: 1.10340\n",
      "Epoch: 61, Time: 0.04554s, Loss: 1.10305\n",
      "Epoch: 62, Time: 0.04454s, Loss: 1.10166\n",
      "Epoch: 63, Time: 0.04154s, Loss: 1.10141\n",
      "Epoch: 64, Time: 0.12497s, Loss: 1.10485\n",
      "Epoch: 65, Time: 0.08237s, Loss: 1.10420\n",
      "Epoch: 66, Time: 0.04253s, Loss: 1.10007\n",
      "Epoch: 67, Time: 0.04907s, Loss: 1.10621\n",
      "Epoch: 68, Time: 0.05053s, Loss: 1.10067\n",
      "Epoch: 69, Time: 0.05206s, Loss: 1.10415\n",
      "Epoch: 70, Time: 0.05306s, Loss: 1.10715\n",
      "Epoch: 71, Time: 0.04507s, Loss: 1.10183\n",
      "Epoch: 72, Time: 0.04100s, Loss: 1.10105\n",
      "Epoch: 73, Time: 0.04406s, Loss: 1.10055\n",
      "Epoch: 74, Time: 0.04107s, Loss: 1.09897\n",
      "Epoch: 75, Time: 0.04552s, Loss: 1.10068\n",
      "Epoch: 76, Time: 0.04204s, Loss: 1.09691\n",
      "Epoch: 77, Time: 0.04955s, Loss: 1.10065\n",
      "Epoch: 78, Time: 0.04710s, Loss: 1.09795\n",
      "Epoch: 79, Time: 0.04404s, Loss: 1.10239\n",
      "Epoch: 80, Time: 0.04000s, Loss: 1.10041\n",
      "Epoch: 81, Time: 0.03351s, Loss: 1.10028\n",
      "Epoch: 82, Time: 0.03454s, Loss: 1.09957\n",
      "Epoch: 83, Time: 0.03153s, Loss: 1.09580\n",
      "Epoch: 84, Time: 0.03202s, Loss: 1.09460\n",
      "Epoch: 85, Time: 0.03053s, Loss: 1.09422\n",
      "Epoch: 86, Time: 0.03400s, Loss: 1.10020\n",
      "Epoch: 87, Time: 0.03307s, Loss: 1.09657\n",
      "Epoch: 88, Time: 0.03300s, Loss: 1.09925\n",
      "Epoch: 89, Time: 0.03253s, Loss: 1.09965\n",
      "Epoch: 90, Time: 0.02800s, Loss: 1.09219\n",
      "Epoch: 91, Time: 0.03961s, Loss: 1.10139\n",
      "Epoch: 92, Time: 0.03452s, Loss: 1.09722\n",
      "Epoch: 93, Time: 0.02852s, Loss: 1.09242\n",
      "Epoch: 94, Time: 0.03700s, Loss: 1.09381\n",
      "Epoch: 95, Time: 0.03353s, Loss: 1.09635\n",
      "Epoch: 96, Time: 0.04312s, Loss: 1.09274\n",
      "Epoch: 97, Time: 0.03610s, Loss: 1.09579\n",
      "Epoch: 98, Time: 0.03500s, Loss: 1.09506\n",
      "Epoch: 99, Time: 0.03553s, Loss: 1.09270\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4012, F1: 0.4012\n",
      "\n",
      "[CocitationCiteseer] Fold 2/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.04204s, Loss: 1.79383\n",
      "Epoch: 1, Time: 0.03035s, Loss: 1.78848\n",
      "Epoch: 2, Time: 0.03093s, Loss: 1.78200\n",
      "Epoch: 3, Time: 0.03200s, Loss: 1.77377\n",
      "Epoch: 4, Time: 0.03153s, Loss: 1.76417\n",
      "Epoch: 5, Time: 0.03101s, Loss: 1.75378\n",
      "Epoch: 6, Time: 0.02707s, Loss: 1.74310\n",
      "Epoch: 7, Time: 0.02751s, Loss: 1.73114\n",
      "Epoch: 8, Time: 0.03154s, Loss: 1.71731\n",
      "Epoch: 9, Time: 0.02952s, Loss: 1.70494\n",
      "Epoch: 10, Time: 0.02852s, Loss: 1.68971\n",
      "Epoch: 11, Time: 0.02653s, Loss: 1.67631\n",
      "Epoch: 12, Time: 0.02900s, Loss: 1.66005\n",
      "Epoch: 13, Time: 0.02753s, Loss: 1.64649\n",
      "Epoch: 14, Time: 0.02752s, Loss: 1.63036\n",
      "Epoch: 15, Time: 0.03052s, Loss: 1.61530\n",
      "Epoch: 16, Time: 0.02953s, Loss: 1.59651\n",
      "Epoch: 17, Time: 0.02801s, Loss: 1.58204\n",
      "Epoch: 18, Time: 0.03053s, Loss: 1.56562\n",
      "Epoch: 19, Time: 0.02878s, Loss: 1.55151\n",
      "Epoch: 20, Time: 0.03152s, Loss: 1.53588\n",
      "Epoch: 21, Time: 0.03101s, Loss: 1.52040\n",
      "Epoch: 22, Time: 0.02904s, Loss: 1.50506\n",
      "Epoch: 23, Time: 0.02752s, Loss: 1.49320\n",
      "Epoch: 24, Time: 0.03054s, Loss: 1.47706\n",
      "Epoch: 25, Time: 0.03353s, Loss: 1.46402\n",
      "Epoch: 26, Time: 0.02953s, Loss: 1.45491\n",
      "Epoch: 27, Time: 0.02453s, Loss: 1.44099\n",
      "Epoch: 28, Time: 0.03000s, Loss: 1.42916\n",
      "Epoch: 29, Time: 0.02855s, Loss: 1.42092\n",
      "Epoch: 30, Time: 0.02951s, Loss: 1.41282\n",
      "Epoch: 31, Time: 0.03280s, Loss: 1.40409\n",
      "Epoch: 32, Time: 0.03052s, Loss: 1.39246\n",
      "Epoch: 33, Time: 0.03253s, Loss: 1.38461\n",
      "Epoch: 34, Time: 0.03754s, Loss: 1.38171\n",
      "Epoch: 35, Time: 0.03554s, Loss: 1.37203\n",
      "Epoch: 36, Time: 0.03473s, Loss: 1.36784\n",
      "Epoch: 37, Time: 0.03153s, Loss: 1.36131\n",
      "Epoch: 38, Time: 0.03954s, Loss: 1.35662\n",
      "Epoch: 39, Time: 0.04714s, Loss: 1.35104\n",
      "Epoch: 40, Time: 0.03900s, Loss: 1.34487\n",
      "Epoch: 41, Time: 0.03454s, Loss: 1.33642\n",
      "Epoch: 42, Time: 0.03954s, Loss: 1.33330\n",
      "Epoch: 43, Time: 0.03453s, Loss: 1.32880\n",
      "Epoch: 44, Time: 0.02999s, Loss: 1.32732\n",
      "Epoch: 45, Time: 0.02852s, Loss: 1.32144\n",
      "Epoch: 46, Time: 0.03000s, Loss: 1.31735\n",
      "Epoch: 47, Time: 0.03161s, Loss: 1.31597\n",
      "Epoch: 48, Time: 0.03253s, Loss: 1.31228\n",
      "Epoch: 49, Time: 0.02907s, Loss: 1.30087\n",
      "Epoch: 50, Time: 0.03152s, Loss: 1.30048\n",
      "Epoch: 51, Time: 0.02552s, Loss: 1.29747\n",
      "Epoch: 52, Time: 0.02552s, Loss: 1.29481\n",
      "Epoch: 53, Time: 0.02800s, Loss: 1.29207\n",
      "Epoch: 54, Time: 0.03153s, Loss: 1.28590\n",
      "Epoch: 55, Time: 0.02800s, Loss: 1.28408\n",
      "Epoch: 56, Time: 0.02853s, Loss: 1.28361\n",
      "Epoch: 57, Time: 0.02853s, Loss: 1.27982\n",
      "Epoch: 58, Time: 0.02652s, Loss: 1.27968\n",
      "Epoch: 59, Time: 0.03053s, Loss: 1.27248\n",
      "Epoch: 60, Time: 0.02954s, Loss: 1.27404\n",
      "Epoch: 61, Time: 0.03008s, Loss: 1.27044\n",
      "Epoch: 62, Time: 0.02953s, Loss: 1.26988\n",
      "Epoch: 63, Time: 0.03392s, Loss: 1.27104\n",
      "Epoch: 64, Time: 0.03454s, Loss: 1.26428\n",
      "Epoch: 65, Time: 0.03154s, Loss: 1.26397\n",
      "Epoch: 66, Time: 0.03053s, Loss: 1.25977\n",
      "Epoch: 67, Time: 0.03001s, Loss: 1.25903\n",
      "Epoch: 68, Time: 0.03151s, Loss: 1.26017\n",
      "Epoch: 69, Time: 0.03000s, Loss: 1.25668\n",
      "Epoch: 70, Time: 0.02252s, Loss: 1.25608\n",
      "Epoch: 71, Time: 0.03254s, Loss: 1.25734\n",
      "Epoch: 72, Time: 0.03154s, Loss: 1.25032\n",
      "Epoch: 73, Time: 0.02857s, Loss: 1.25042\n",
      "Epoch: 74, Time: 0.02552s, Loss: 1.24816\n",
      "Epoch: 75, Time: 0.03154s, Loss: 1.24817\n",
      "Epoch: 76, Time: 0.02800s, Loss: 1.24587\n",
      "Epoch: 77, Time: 0.02506s, Loss: 1.24418\n",
      "Epoch: 78, Time: 0.03153s, Loss: 1.24468\n",
      "Epoch: 79, Time: 0.02759s, Loss: 1.23965\n",
      "Epoch: 80, Time: 0.02800s, Loss: 1.24078\n",
      "Epoch: 81, Time: 0.03000s, Loss: 1.24147\n",
      "Epoch: 82, Time: 0.02852s, Loss: 1.24075\n",
      "Epoch: 83, Time: 0.03161s, Loss: 1.23860\n",
      "Epoch: 84, Time: 0.03121s, Loss: 1.23698\n",
      "Epoch: 85, Time: 0.04152s, Loss: 1.23856\n",
      "Epoch: 86, Time: 0.03534s, Loss: 1.23394\n",
      "Epoch: 87, Time: 0.04328s, Loss: 1.23544\n",
      "Epoch: 88, Time: 0.03357s, Loss: 1.22986\n",
      "Epoch: 89, Time: 0.02653s, Loss: 1.22932\n",
      "Epoch: 90, Time: 0.05505s, Loss: 1.23353\n",
      "Epoch: 91, Time: 0.03353s, Loss: 1.23357\n",
      "Epoch: 92, Time: 0.02553s, Loss: 1.22849\n",
      "Epoch: 93, Time: 0.03052s, Loss: 1.22812\n",
      "Epoch: 94, Time: 0.03052s, Loss: 1.23177\n",
      "Epoch: 95, Time: 0.03000s, Loss: 1.22728\n",
      "Epoch: 96, Time: 0.03153s, Loss: 1.22795\n",
      "Epoch: 97, Time: 0.02601s, Loss: 1.22795\n",
      "Epoch: 98, Time: 0.03053s, Loss: 1.22254\n",
      "Epoch: 99, Time: 0.02852s, Loss: 1.22502\n",
      "    ↳ HGNN Fold Result — Acc: 0.4223, F1: 0.4223\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03425s, Loss: 1.84339\n",
      "Epoch: 1, Time: 0.02901s, Loss: 1.43081\n",
      "Epoch: 2, Time: 0.03406s, Loss: 1.27304\n",
      "Epoch: 3, Time: 0.03401s, Loss: 1.21956\n",
      "Epoch: 4, Time: 0.03352s, Loss: 1.19501\n",
      "Epoch: 5, Time: 0.03101s, Loss: 1.17841\n",
      "Epoch: 6, Time: 0.03610s, Loss: 1.16071\n",
      "Epoch: 7, Time: 0.03074s, Loss: 1.15437\n",
      "Epoch: 8, Time: 0.03354s, Loss: 1.14376\n",
      "Epoch: 9, Time: 0.03153s, Loss: 1.13481\n",
      "Epoch: 10, Time: 0.03707s, Loss: 1.12968\n",
      "Epoch: 11, Time: 0.03498s, Loss: 1.12939\n",
      "Epoch: 12, Time: 0.02553s, Loss: 1.12404\n",
      "Epoch: 13, Time: 0.03353s, Loss: 1.11875\n",
      "Epoch: 14, Time: 0.03505s, Loss: 1.11598\n",
      "Epoch: 15, Time: 0.03052s, Loss: 1.11497\n",
      "Epoch: 16, Time: 0.03153s, Loss: 1.11267\n",
      "Epoch: 17, Time: 0.03354s, Loss: 1.10876\n",
      "Epoch: 18, Time: 0.03252s, Loss: 1.10699\n",
      "Epoch: 19, Time: 0.03553s, Loss: 1.10508\n",
      "Epoch: 20, Time: 0.02655s, Loss: 1.10548\n",
      "Epoch: 21, Time: 0.03252s, Loss: 1.10256\n",
      "Epoch: 22, Time: 0.03053s, Loss: 1.10443\n",
      "Epoch: 23, Time: 0.03152s, Loss: 1.10034\n",
      "Epoch: 24, Time: 0.03353s, Loss: 1.10019\n",
      "Epoch: 25, Time: 0.03708s, Loss: 1.09615\n",
      "Epoch: 26, Time: 0.03152s, Loss: 1.09725\n",
      "Epoch: 27, Time: 0.03353s, Loss: 1.09964\n",
      "Epoch: 28, Time: 0.03652s, Loss: 1.09876\n",
      "Epoch: 29, Time: 0.03404s, Loss: 1.09515\n",
      "Epoch: 30, Time: 0.03300s, Loss: 1.09460\n",
      "Epoch: 31, Time: 0.03151s, Loss: 1.09430\n",
      "Epoch: 32, Time: 0.03300s, Loss: 1.09092\n",
      "Epoch: 33, Time: 0.03252s, Loss: 1.09171\n",
      "Epoch: 34, Time: 0.03300s, Loss: 1.09374\n",
      "Epoch: 35, Time: 0.03229s, Loss: 1.09382\n",
      "Epoch: 36, Time: 0.03601s, Loss: 1.08996\n",
      "Epoch: 37, Time: 0.03559s, Loss: 1.08926\n",
      "Epoch: 38, Time: 0.03353s, Loss: 1.09147\n",
      "Epoch: 39, Time: 0.02853s, Loss: 1.08986\n",
      "Epoch: 40, Time: 0.03053s, Loss: 1.08977\n",
      "Epoch: 41, Time: 0.03055s, Loss: 1.08669\n",
      "Epoch: 42, Time: 0.03153s, Loss: 1.08714\n",
      "Epoch: 43, Time: 0.03053s, Loss: 1.08589\n",
      "Epoch: 44, Time: 0.02951s, Loss: 1.08626\n",
      "Epoch: 45, Time: 0.03299s, Loss: 1.08971\n",
      "Epoch: 46, Time: 0.03152s, Loss: 1.08724\n",
      "Epoch: 47, Time: 0.03000s, Loss: 1.08492\n",
      "Epoch: 48, Time: 0.02552s, Loss: 1.08690\n",
      "Epoch: 49, Time: 0.03401s, Loss: 1.08603\n",
      "Epoch: 50, Time: 0.03006s, Loss: 1.08855\n",
      "Epoch: 51, Time: 0.03152s, Loss: 1.08782\n",
      "Epoch: 52, Time: 0.03353s, Loss: 1.08475\n",
      "Epoch: 53, Time: 0.03459s, Loss: 1.08601\n",
      "Epoch: 54, Time: 0.03354s, Loss: 1.08713\n",
      "Epoch: 55, Time: 0.03552s, Loss: 1.08438\n",
      "Epoch: 56, Time: 0.02854s, Loss: 1.08696\n",
      "Epoch: 57, Time: 0.03152s, Loss: 1.08528\n",
      "Epoch: 58, Time: 0.03654s, Loss: 1.08490\n",
      "Epoch: 59, Time: 0.03152s, Loss: 1.08429\n",
      "Epoch: 60, Time: 0.03051s, Loss: 1.08632\n",
      "Epoch: 61, Time: 0.03051s, Loss: 1.08664\n",
      "Epoch: 62, Time: 0.03101s, Loss: 1.08719\n",
      "Epoch: 63, Time: 0.03152s, Loss: 1.08534\n",
      "Epoch: 64, Time: 0.03222s, Loss: 1.08281\n",
      "Epoch: 65, Time: 0.02808s, Loss: 1.08480\n",
      "Epoch: 66, Time: 0.03254s, Loss: 1.08245\n",
      "Epoch: 67, Time: 0.03305s, Loss: 1.08564\n",
      "Epoch: 68, Time: 0.03001s, Loss: 1.08336\n",
      "Epoch: 69, Time: 0.03266s, Loss: 1.08510\n",
      "Epoch: 70, Time: 0.03452s, Loss: 1.08298\n",
      "Epoch: 71, Time: 0.02954s, Loss: 1.08340\n",
      "Epoch: 72, Time: 0.03453s, Loss: 1.08335\n",
      "Epoch: 73, Time: 0.03354s, Loss: 1.08494\n",
      "Epoch: 74, Time: 0.03553s, Loss: 1.08519\n",
      "Epoch: 75, Time: 0.03654s, Loss: 1.08402\n",
      "Epoch: 76, Time: 0.03254s, Loss: 1.08019\n",
      "Epoch: 77, Time: 0.03253s, Loss: 1.08155\n",
      "Epoch: 78, Time: 0.03353s, Loss: 1.07958\n",
      "Epoch: 79, Time: 0.02755s, Loss: 1.08310\n",
      "Epoch: 80, Time: 0.03154s, Loss: 1.08311\n",
      "Epoch: 81, Time: 0.03053s, Loss: 1.08276\n",
      "Epoch: 82, Time: 0.03305s, Loss: 1.08213\n",
      "Epoch: 83, Time: 0.03101s, Loss: 1.08399\n",
      "Epoch: 84, Time: 0.03404s, Loss: 1.08289\n",
      "Epoch: 85, Time: 0.03452s, Loss: 1.08157\n",
      "Epoch: 86, Time: 0.03204s, Loss: 1.07870\n",
      "Epoch: 87, Time: 0.03200s, Loss: 1.07827\n",
      "Epoch: 88, Time: 0.02951s, Loss: 1.08255\n",
      "Epoch: 89, Time: 0.03200s, Loss: 1.08117\n",
      "Epoch: 90, Time: 0.03352s, Loss: 1.08436\n",
      "Epoch: 91, Time: 0.03355s, Loss: 1.08334\n",
      "Epoch: 92, Time: 0.03505s, Loss: 1.08273\n",
      "Epoch: 93, Time: 0.03700s, Loss: 1.08000\n",
      "Epoch: 94, Time: 0.03254s, Loss: 1.07827\n",
      "Epoch: 95, Time: 0.03403s, Loss: 1.08096\n",
      "Epoch: 96, Time: 0.03254s, Loss: 1.08102\n",
      "Epoch: 97, Time: 0.03500s, Loss: 1.08351\n",
      "Epoch: 98, Time: 0.03305s, Loss: 1.08073\n",
      "Epoch: 99, Time: 0.03054s, Loss: 1.07841\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4193, F1: 0.4193\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.04227s, Loss: 1.84791\n",
      "Epoch: 1, Time: 0.03654s, Loss: 1.44673\n",
      "Epoch: 2, Time: 0.03304s, Loss: 1.29785\n",
      "Epoch: 3, Time: 0.03053s, Loss: 1.25697\n",
      "Epoch: 4, Time: 0.03304s, Loss: 1.22877\n",
      "Epoch: 5, Time: 0.03300s, Loss: 1.21668\n",
      "Epoch: 6, Time: 0.03352s, Loss: 1.19519\n",
      "Epoch: 7, Time: 0.03052s, Loss: 1.19218\n",
      "Epoch: 8, Time: 0.02753s, Loss: 1.18413\n",
      "Epoch: 9, Time: 0.02956s, Loss: 1.18108\n",
      "Epoch: 10, Time: 0.02755s, Loss: 1.17158\n",
      "Epoch: 11, Time: 0.03625s, Loss: 1.17065\n",
      "Epoch: 12, Time: 0.03455s, Loss: 1.16331\n",
      "Epoch: 13, Time: 0.03252s, Loss: 1.16029\n",
      "Epoch: 14, Time: 0.02811s, Loss: 1.15481\n",
      "Epoch: 15, Time: 0.03804s, Loss: 1.15533\n",
      "Epoch: 16, Time: 0.03655s, Loss: 1.15331\n",
      "Epoch: 17, Time: 0.03451s, Loss: 1.14584\n",
      "Epoch: 18, Time: 0.03454s, Loss: 1.14365\n",
      "Epoch: 19, Time: 0.03352s, Loss: 1.14052\n",
      "Epoch: 20, Time: 0.03458s, Loss: 1.14019\n",
      "Epoch: 21, Time: 0.03554s, Loss: 1.13688\n",
      "Epoch: 22, Time: 0.03452s, Loss: 1.13738\n",
      "Epoch: 23, Time: 0.03053s, Loss: 1.13532\n",
      "Epoch: 24, Time: 0.03254s, Loss: 1.13463\n",
      "Epoch: 25, Time: 0.03553s, Loss: 1.13167\n",
      "Epoch: 26, Time: 0.03552s, Loss: 1.13014\n",
      "Epoch: 27, Time: 0.03453s, Loss: 1.12830\n",
      "Epoch: 28, Time: 0.03404s, Loss: 1.12691\n",
      "Epoch: 29, Time: 0.03100s, Loss: 1.12502\n",
      "Epoch: 30, Time: 0.03354s, Loss: 1.12436\n",
      "Epoch: 31, Time: 0.03251s, Loss: 1.12361\n",
      "Epoch: 32, Time: 0.03654s, Loss: 1.12217\n",
      "Epoch: 33, Time: 0.03351s, Loss: 1.12254\n",
      "Epoch: 34, Time: 0.03052s, Loss: 1.12291\n",
      "Epoch: 35, Time: 0.03651s, Loss: 1.11978\n",
      "Epoch: 36, Time: 0.03256s, Loss: 1.11783\n",
      "Epoch: 37, Time: 0.03353s, Loss: 1.12182\n",
      "Epoch: 38, Time: 0.02751s, Loss: 1.11605\n",
      "Epoch: 39, Time: 0.03204s, Loss: 1.11478\n",
      "Epoch: 40, Time: 0.03351s, Loss: 1.11725\n",
      "Epoch: 41, Time: 0.03405s, Loss: 1.11798\n",
      "Epoch: 42, Time: 0.03351s, Loss: 1.11302\n",
      "Epoch: 43, Time: 0.02796s, Loss: 1.11798\n",
      "Epoch: 44, Time: 0.03200s, Loss: 1.11181\n",
      "Epoch: 45, Time: 0.03454s, Loss: 1.11346\n",
      "Epoch: 46, Time: 0.03051s, Loss: 1.10988\n",
      "Epoch: 47, Time: 0.03204s, Loss: 1.10916\n",
      "Epoch: 48, Time: 0.03408s, Loss: 1.11224\n",
      "Epoch: 49, Time: 0.03453s, Loss: 1.10986\n",
      "Epoch: 50, Time: 0.03551s, Loss: 1.11014\n",
      "Epoch: 51, Time: 0.03307s, Loss: 1.11143\n",
      "Epoch: 52, Time: 0.14512s, Loss: 1.10765\n",
      "Epoch: 53, Time: 0.06104s, Loss: 1.11089\n",
      "Epoch: 54, Time: 0.03205s, Loss: 1.11104\n",
      "Epoch: 55, Time: 0.03501s, Loss: 1.10733\n",
      "Epoch: 56, Time: 0.03354s, Loss: 1.10927\n",
      "Epoch: 57, Time: 0.03353s, Loss: 1.10543\n",
      "Epoch: 58, Time: 0.03453s, Loss: 1.10945\n",
      "Epoch: 59, Time: 0.03200s, Loss: 1.10775\n",
      "Epoch: 60, Time: 0.03308s, Loss: 1.10546\n",
      "Epoch: 61, Time: 0.03353s, Loss: 1.10759\n",
      "Epoch: 62, Time: 0.03001s, Loss: 1.10917\n",
      "Epoch: 63, Time: 0.03100s, Loss: 1.10875\n",
      "Epoch: 64, Time: 0.03552s, Loss: 1.10530\n",
      "Epoch: 65, Time: 0.03354s, Loss: 1.10554\n",
      "Epoch: 66, Time: 0.03454s, Loss: 1.10575\n",
      "Epoch: 67, Time: 0.03452s, Loss: 1.10487\n",
      "Epoch: 68, Time: 0.03479s, Loss: 1.10603\n",
      "Epoch: 69, Time: 0.03052s, Loss: 1.10637\n",
      "Epoch: 70, Time: 0.02902s, Loss: 1.10420\n",
      "Epoch: 71, Time: 0.03853s, Loss: 1.10665\n",
      "Epoch: 72, Time: 0.03355s, Loss: 1.09836\n",
      "Epoch: 73, Time: 0.03654s, Loss: 1.10257\n",
      "Epoch: 74, Time: 0.03606s, Loss: 1.10314\n",
      "Epoch: 75, Time: 0.03052s, Loss: 1.10400\n",
      "Epoch: 76, Time: 0.03354s, Loss: 1.10261\n",
      "Epoch: 77, Time: 0.03302s, Loss: 1.10207\n",
      "Epoch: 78, Time: 0.03253s, Loss: 1.10578\n",
      "Epoch: 79, Time: 0.03553s, Loss: 1.10233\n",
      "Epoch: 80, Time: 0.02855s, Loss: 1.10350\n",
      "Epoch: 81, Time: 0.03353s, Loss: 1.10629\n",
      "Epoch: 82, Time: 0.03152s, Loss: 1.10091\n",
      "Epoch: 83, Time: 0.02804s, Loss: 1.10282\n",
      "Epoch: 84, Time: 0.02953s, Loss: 1.10506\n",
      "Epoch: 85, Time: 0.03254s, Loss: 1.10117\n",
      "Epoch: 86, Time: 0.03200s, Loss: 1.10412\n",
      "Epoch: 87, Time: 0.03353s, Loss: 1.09877\n",
      "Epoch: 88, Time: 0.03099s, Loss: 1.10102\n",
      "Epoch: 89, Time: 0.03052s, Loss: 1.10696\n",
      "Epoch: 90, Time: 0.03353s, Loss: 1.10321\n",
      "Epoch: 91, Time: 0.03353s, Loss: 1.10054\n",
      "Epoch: 92, Time: 0.03301s, Loss: 1.10212\n",
      "Epoch: 93, Time: 0.03153s, Loss: 1.10017\n",
      "Epoch: 94, Time: 0.02953s, Loss: 1.09945\n",
      "Epoch: 95, Time: 0.03253s, Loss: 1.09981\n",
      "Epoch: 96, Time: 0.03552s, Loss: 1.10040\n",
      "Epoch: 97, Time: 0.03452s, Loss: 1.10185\n",
      "Epoch: 98, Time: 0.03287s, Loss: 1.10241\n",
      "Epoch: 99, Time: 0.03053s, Loss: 1.09960\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4163, F1: 0.4163\n",
      "\n",
      "[CocitationCiteseer] Fold 3/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.04252s, Loss: 1.79391\n",
      "Epoch: 1, Time: 0.02804s, Loss: 1.78871\n",
      "Epoch: 2, Time: 0.02654s, Loss: 1.78189\n",
      "Epoch: 3, Time: 0.02554s, Loss: 1.77378\n",
      "Epoch: 4, Time: 0.02601s, Loss: 1.76384\n",
      "Epoch: 5, Time: 0.02855s, Loss: 1.75398\n",
      "Epoch: 6, Time: 0.02855s, Loss: 1.74195\n",
      "Epoch: 7, Time: 0.02953s, Loss: 1.73024\n",
      "Epoch: 8, Time: 0.03008s, Loss: 1.71904\n",
      "Epoch: 9, Time: 0.02400s, Loss: 1.70556\n",
      "Epoch: 10, Time: 0.03053s, Loss: 1.69331\n",
      "Epoch: 11, Time: 0.03000s, Loss: 1.68009\n",
      "Epoch: 12, Time: 0.03253s, Loss: 1.66679\n",
      "Epoch: 13, Time: 0.02753s, Loss: 1.65208\n",
      "Epoch: 14, Time: 0.02753s, Loss: 1.63866\n",
      "Epoch: 15, Time: 0.03005s, Loss: 1.62312\n",
      "Epoch: 16, Time: 0.02901s, Loss: 1.60994\n",
      "Epoch: 17, Time: 0.02952s, Loss: 1.59512\n",
      "Epoch: 18, Time: 0.03103s, Loss: 1.58170\n",
      "Epoch: 19, Time: 0.03002s, Loss: 1.56675\n",
      "Epoch: 20, Time: 0.02752s, Loss: 1.55522\n",
      "Epoch: 21, Time: 0.02852s, Loss: 1.54050\n",
      "Epoch: 22, Time: 0.03104s, Loss: 1.52962\n",
      "Epoch: 23, Time: 0.02800s, Loss: 1.51571\n",
      "Epoch: 24, Time: 0.02955s, Loss: 1.50019\n",
      "Epoch: 25, Time: 0.02600s, Loss: 1.49232\n",
      "Epoch: 26, Time: 0.03101s, Loss: 1.47845\n",
      "Epoch: 27, Time: 0.02900s, Loss: 1.47238\n",
      "Epoch: 28, Time: 0.02752s, Loss: 1.45970\n",
      "Epoch: 29, Time: 0.02905s, Loss: 1.44744\n",
      "Epoch: 30, Time: 0.02953s, Loss: 1.43704\n",
      "Epoch: 31, Time: 0.03153s, Loss: 1.43464\n",
      "Epoch: 32, Time: 0.03157s, Loss: 1.42277\n",
      "Epoch: 33, Time: 0.02953s, Loss: 1.41852\n",
      "Epoch: 34, Time: 0.02700s, Loss: 1.40655\n",
      "Epoch: 35, Time: 0.02900s, Loss: 1.40078\n",
      "Epoch: 36, Time: 0.02707s, Loss: 1.39544\n",
      "Epoch: 37, Time: 0.03152s, Loss: 1.38905\n",
      "Epoch: 38, Time: 0.02809s, Loss: 1.38292\n",
      "Epoch: 39, Time: 0.03101s, Loss: 1.38000\n",
      "Epoch: 40, Time: 0.03604s, Loss: 1.37242\n",
      "Epoch: 41, Time: 0.02301s, Loss: 1.36563\n",
      "Epoch: 42, Time: 0.03011s, Loss: 1.36413\n",
      "Epoch: 43, Time: 0.03100s, Loss: 1.35530\n",
      "Epoch: 44, Time: 0.03054s, Loss: 1.35072\n",
      "Epoch: 45, Time: 0.03055s, Loss: 1.34422\n",
      "Epoch: 46, Time: 0.03055s, Loss: 1.34250\n",
      "Epoch: 47, Time: 0.03206s, Loss: 1.33707\n",
      "Epoch: 48, Time: 0.03051s, Loss: 1.33197\n",
      "Epoch: 49, Time: 0.03008s, Loss: 1.33320\n",
      "Epoch: 50, Time: 0.02550s, Loss: 1.32656\n",
      "Epoch: 51, Time: 0.03155s, Loss: 1.32414\n",
      "Epoch: 52, Time: 0.02700s, Loss: 1.31845\n",
      "Epoch: 53, Time: 0.02953s, Loss: 1.31977\n",
      "Epoch: 54, Time: 0.02655s, Loss: 1.31368\n",
      "Epoch: 55, Time: 0.02952s, Loss: 1.30897\n",
      "Epoch: 56, Time: 0.02902s, Loss: 1.30975\n",
      "Epoch: 57, Time: 0.02500s, Loss: 1.30350\n",
      "Epoch: 58, Time: 0.02802s, Loss: 1.30611\n",
      "Epoch: 59, Time: 0.03200s, Loss: 1.30210\n",
      "Epoch: 60, Time: 0.02752s, Loss: 1.30285\n",
      "Epoch: 61, Time: 0.03154s, Loss: 1.29677\n",
      "Epoch: 62, Time: 0.03355s, Loss: 1.29838\n",
      "Epoch: 63, Time: 0.03259s, Loss: 1.29388\n",
      "Epoch: 64, Time: 0.02954s, Loss: 1.29125\n",
      "Epoch: 65, Time: 0.03091s, Loss: 1.29221\n",
      "Epoch: 66, Time: 0.03102s, Loss: 1.28506\n",
      "Epoch: 67, Time: 0.02554s, Loss: 1.28327\n",
      "Epoch: 68, Time: 0.03000s, Loss: 1.28328\n",
      "Epoch: 69, Time: 0.03301s, Loss: 1.28476\n",
      "Epoch: 70, Time: 0.03000s, Loss: 1.27920\n",
      "Epoch: 71, Time: 0.02852s, Loss: 1.27985\n",
      "Epoch: 72, Time: 0.03106s, Loss: 1.28104\n",
      "Epoch: 73, Time: 0.02952s, Loss: 1.27871\n",
      "Epoch: 74, Time: 0.02753s, Loss: 1.27670\n",
      "Epoch: 75, Time: 0.03000s, Loss: 1.27163\n",
      "Epoch: 76, Time: 0.02801s, Loss: 1.27094\n",
      "Epoch: 77, Time: 0.02655s, Loss: 1.26901\n",
      "Epoch: 78, Time: 0.03051s, Loss: 1.27194\n",
      "Epoch: 79, Time: 0.02599s, Loss: 1.27074\n",
      "Epoch: 80, Time: 0.02801s, Loss: 1.26786\n",
      "Epoch: 81, Time: 0.05607s, Loss: 1.26747\n",
      "Epoch: 82, Time: 0.02753s, Loss: 1.26755\n",
      "Epoch: 83, Time: 0.02452s, Loss: 1.26597\n",
      "Epoch: 84, Time: 0.02900s, Loss: 1.26080\n",
      "Epoch: 85, Time: 0.03101s, Loss: 1.26149\n",
      "Epoch: 86, Time: 0.02700s, Loss: 1.26079\n",
      "Epoch: 87, Time: 0.02952s, Loss: 1.26321\n",
      "Epoch: 88, Time: 0.02505s, Loss: 1.25969\n",
      "Epoch: 89, Time: 0.02952s, Loss: 1.25964\n",
      "Epoch: 90, Time: 0.03152s, Loss: 1.25359\n",
      "Epoch: 91, Time: 0.03100s, Loss: 1.25599\n",
      "Epoch: 92, Time: 0.02954s, Loss: 1.25816\n",
      "Epoch: 93, Time: 0.03051s, Loss: 1.25400\n",
      "Epoch: 94, Time: 0.02654s, Loss: 1.25229\n",
      "Epoch: 95, Time: 0.02753s, Loss: 1.25360\n",
      "Epoch: 96, Time: 0.02553s, Loss: 1.24947\n",
      "Epoch: 97, Time: 0.02603s, Loss: 1.25149\n",
      "Epoch: 98, Time: 0.02900s, Loss: 1.25018\n",
      "Epoch: 99, Time: 0.02755s, Loss: 1.25351\n",
      "    ↳ HGNN Fold Result — Acc: 0.4577, F1: 0.4577\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03100s, Loss: 1.83279\n",
      "Epoch: 1, Time: 0.03252s, Loss: 1.45846\n",
      "Epoch: 2, Time: 0.02554s, Loss: 1.30299\n",
      "Epoch: 3, Time: 0.03354s, Loss: 1.25543\n",
      "Epoch: 4, Time: 0.03353s, Loss: 1.22458\n",
      "Epoch: 5, Time: 0.03402s, Loss: 1.20699\n",
      "Epoch: 6, Time: 0.02652s, Loss: 1.19324\n",
      "Epoch: 7, Time: 0.03152s, Loss: 1.18464\n",
      "Epoch: 8, Time: 0.03305s, Loss: 1.17508\n",
      "Epoch: 9, Time: 0.03352s, Loss: 1.16524\n",
      "Epoch: 10, Time: 0.03305s, Loss: 1.16179\n",
      "Epoch: 11, Time: 0.03053s, Loss: 1.15695\n",
      "Epoch: 12, Time: 0.02902s, Loss: 1.15476\n",
      "Epoch: 13, Time: 0.03400s, Loss: 1.14798\n",
      "Epoch: 14, Time: 0.03057s, Loss: 1.14848\n",
      "Epoch: 15, Time: 0.03051s, Loss: 1.14347\n",
      "Epoch: 16, Time: 0.03543s, Loss: 1.13932\n",
      "Epoch: 17, Time: 0.03100s, Loss: 1.13785\n",
      "Epoch: 18, Time: 0.03052s, Loss: 1.13652\n",
      "Epoch: 19, Time: 0.03251s, Loss: 1.13536\n",
      "Epoch: 20, Time: 0.03654s, Loss: 1.13195\n",
      "Epoch: 21, Time: 0.02901s, Loss: 1.12966\n",
      "Epoch: 22, Time: 0.03353s, Loss: 1.13113\n",
      "Epoch: 23, Time: 0.03200s, Loss: 1.12735\n",
      "Epoch: 24, Time: 0.03452s, Loss: 1.12921\n",
      "Epoch: 25, Time: 0.03300s, Loss: 1.12895\n",
      "Epoch: 26, Time: 0.03553s, Loss: 1.12703\n",
      "Epoch: 27, Time: 0.03400s, Loss: 1.12484\n",
      "Epoch: 28, Time: 0.03001s, Loss: 1.12479\n",
      "Epoch: 29, Time: 0.03400s, Loss: 1.12406\n",
      "Epoch: 30, Time: 0.03101s, Loss: 1.12151\n",
      "Epoch: 31, Time: 0.03455s, Loss: 1.12571\n",
      "Epoch: 32, Time: 0.03056s, Loss: 1.12347\n",
      "Epoch: 33, Time: 0.03252s, Loss: 1.11994\n",
      "Epoch: 34, Time: 0.03052s, Loss: 1.12138\n",
      "Epoch: 35, Time: 0.03504s, Loss: 1.11821\n",
      "Epoch: 36, Time: 0.03452s, Loss: 1.12143\n",
      "Epoch: 37, Time: 0.03101s, Loss: 1.12225\n",
      "Epoch: 38, Time: 0.03453s, Loss: 1.12034\n",
      "Epoch: 39, Time: 0.03406s, Loss: 1.11812\n",
      "Epoch: 40, Time: 0.03552s, Loss: 1.11783\n",
      "Epoch: 41, Time: 0.03404s, Loss: 1.11911\n",
      "Epoch: 42, Time: 0.03455s, Loss: 1.11713\n",
      "Epoch: 43, Time: 0.03203s, Loss: 1.11466\n",
      "Epoch: 44, Time: 0.03151s, Loss: 1.11818\n",
      "Epoch: 45, Time: 0.03506s, Loss: 1.12045\n",
      "Epoch: 46, Time: 0.03252s, Loss: 1.11753\n",
      "Epoch: 47, Time: 0.03463s, Loss: 1.11629\n",
      "Epoch: 48, Time: 0.03300s, Loss: 1.11739\n",
      "Epoch: 49, Time: 0.03304s, Loss: 1.11594\n",
      "Epoch: 50, Time: 0.03453s, Loss: 1.11797\n",
      "Epoch: 51, Time: 0.03603s, Loss: 1.11579\n",
      "Epoch: 52, Time: 0.03300s, Loss: 1.11889\n",
      "Epoch: 53, Time: 0.03153s, Loss: 1.11805\n",
      "Epoch: 54, Time: 0.02900s, Loss: 1.11550\n",
      "Epoch: 55, Time: 0.03401s, Loss: 1.11678\n",
      "Epoch: 56, Time: 0.03152s, Loss: 1.11617\n",
      "Epoch: 57, Time: 0.03154s, Loss: 1.11474\n",
      "Epoch: 58, Time: 0.03003s, Loss: 1.11669\n",
      "Epoch: 59, Time: 0.02952s, Loss: 1.11699\n",
      "Epoch: 60, Time: 0.02804s, Loss: 1.11777\n",
      "Epoch: 61, Time: 0.03252s, Loss: 1.11573\n",
      "Epoch: 62, Time: 0.03304s, Loss: 1.11469\n",
      "Epoch: 63, Time: 0.03400s, Loss: 1.11396\n",
      "Epoch: 64, Time: 0.02954s, Loss: 1.11650\n",
      "Epoch: 65, Time: 0.03401s, Loss: 1.11715\n",
      "Epoch: 66, Time: 0.03101s, Loss: 1.11501\n",
      "Epoch: 67, Time: 0.03701s, Loss: 1.11436\n",
      "Epoch: 68, Time: 0.03152s, Loss: 1.11467\n",
      "Epoch: 69, Time: 0.04599s, Loss: 1.11200\n",
      "Epoch: 70, Time: 0.03605s, Loss: 1.11429\n",
      "Epoch: 71, Time: 0.04053s, Loss: 1.11534\n",
      "Epoch: 72, Time: 0.03821s, Loss: 1.11756\n",
      "Epoch: 73, Time: 0.03455s, Loss: 1.11589\n",
      "Epoch: 74, Time: 0.03405s, Loss: 1.11488\n",
      "Epoch: 75, Time: 0.03754s, Loss: 1.11559\n",
      "Epoch: 76, Time: 0.03459s, Loss: 1.11528\n",
      "Epoch: 77, Time: 0.03453s, Loss: 1.11338\n",
      "Epoch: 78, Time: 0.03528s, Loss: 1.11744\n",
      "Epoch: 79, Time: 0.03654s, Loss: 1.11662\n",
      "Epoch: 80, Time: 0.03554s, Loss: 1.11369\n",
      "Epoch: 81, Time: 0.03801s, Loss: 1.11123\n",
      "Epoch: 82, Time: 0.03953s, Loss: 1.11383\n",
      "Epoch: 83, Time: 0.04060s, Loss: 1.11466\n",
      "Epoch: 84, Time: 0.03835s, Loss: 1.11286\n",
      "Epoch: 85, Time: 0.03507s, Loss: 1.10955\n",
      "Epoch: 86, Time: 0.03952s, Loss: 1.11844\n",
      "Epoch: 87, Time: 0.03489s, Loss: 1.11494\n",
      "Epoch: 88, Time: 0.03654s, Loss: 1.11364\n",
      "Epoch: 89, Time: 0.03853s, Loss: 1.11221\n",
      "Epoch: 90, Time: 0.03901s, Loss: 1.11234\n",
      "Epoch: 91, Time: 0.03500s, Loss: 1.11293\n",
      "Epoch: 92, Time: 0.03001s, Loss: 1.11351\n",
      "Epoch: 93, Time: 0.03200s, Loss: 1.11192\n",
      "Epoch: 94, Time: 0.03401s, Loss: 1.11198\n",
      "Epoch: 95, Time: 0.03253s, Loss: 1.11302\n",
      "Epoch: 96, Time: 0.03354s, Loss: 1.11092\n",
      "Epoch: 97, Time: 0.03153s, Loss: 1.11270\n",
      "Epoch: 98, Time: 0.03551s, Loss: 1.11062\n",
      "Epoch: 99, Time: 0.03305s, Loss: 1.10704\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4381, F1: 0.4381\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03855s, Loss: 1.84566\n",
      "Epoch: 1, Time: 0.03804s, Loss: 1.44402\n",
      "Epoch: 2, Time: 0.03506s, Loss: 1.31605\n",
      "Epoch: 3, Time: 0.03253s, Loss: 1.26452\n",
      "Epoch: 4, Time: 0.03130s, Loss: 1.24209\n",
      "Epoch: 5, Time: 0.03852s, Loss: 1.22858\n",
      "Epoch: 6, Time: 0.03306s, Loss: 1.21610\n",
      "Epoch: 7, Time: 0.03300s, Loss: 1.20804\n",
      "Epoch: 8, Time: 0.03052s, Loss: 1.19792\n",
      "Epoch: 9, Time: 0.03000s, Loss: 1.19752\n",
      "Epoch: 10, Time: 0.03101s, Loss: 1.19307\n",
      "Epoch: 11, Time: 0.03352s, Loss: 1.18953\n",
      "Epoch: 12, Time: 0.03501s, Loss: 1.18670\n",
      "Epoch: 13, Time: 0.03200s, Loss: 1.18195\n",
      "Epoch: 14, Time: 0.03301s, Loss: 1.17588\n",
      "Epoch: 15, Time: 0.03055s, Loss: 1.17825\n",
      "Epoch: 16, Time: 0.03355s, Loss: 1.17336\n",
      "Epoch: 17, Time: 0.03253s, Loss: 1.17355\n",
      "Epoch: 18, Time: 0.03252s, Loss: 1.16890\n",
      "Epoch: 19, Time: 0.03051s, Loss: 1.16623\n",
      "Epoch: 20, Time: 0.02854s, Loss: 1.16370\n",
      "Epoch: 21, Time: 0.03405s, Loss: 1.16193\n",
      "Epoch: 22, Time: 0.03251s, Loss: 1.15912\n",
      "Epoch: 23, Time: 0.03703s, Loss: 1.16275\n",
      "Epoch: 24, Time: 0.03252s, Loss: 1.15855\n",
      "Epoch: 25, Time: 0.03683s, Loss: 1.15632\n",
      "Epoch: 26, Time: 0.03551s, Loss: 1.15409\n",
      "Epoch: 27, Time: 0.03930s, Loss: 1.15565\n",
      "Epoch: 28, Time: 0.03602s, Loss: 1.15352\n",
      "Epoch: 29, Time: 0.03600s, Loss: 1.15160\n",
      "Epoch: 30, Time: 0.03151s, Loss: 1.15506\n",
      "Epoch: 31, Time: 0.03200s, Loss: 1.15301\n",
      "Epoch: 32, Time: 0.03516s, Loss: 1.14774\n",
      "Epoch: 33, Time: 0.04052s, Loss: 1.14568\n",
      "Epoch: 34, Time: 0.03454s, Loss: 1.15060\n",
      "Epoch: 35, Time: 0.03554s, Loss: 1.14531\n",
      "Epoch: 36, Time: 0.04308s, Loss: 1.14657\n",
      "Epoch: 37, Time: 0.04153s, Loss: 1.14197\n",
      "Epoch: 38, Time: 0.03153s, Loss: 1.14575\n",
      "Epoch: 39, Time: 0.03253s, Loss: 1.14358\n",
      "Epoch: 40, Time: 0.03561s, Loss: 1.14080\n",
      "Epoch: 41, Time: 0.03755s, Loss: 1.14405\n",
      "Epoch: 42, Time: 0.03200s, Loss: 1.14259\n",
      "Epoch: 43, Time: 0.03202s, Loss: 1.14173\n",
      "Epoch: 44, Time: 0.03152s, Loss: 1.14128\n",
      "Epoch: 45, Time: 0.03553s, Loss: 1.14101\n",
      "Epoch: 46, Time: 0.03404s, Loss: 1.14085\n",
      "Epoch: 47, Time: 0.03452s, Loss: 1.13923\n",
      "Epoch: 48, Time: 0.03104s, Loss: 1.14368\n",
      "Epoch: 49, Time: 0.03452s, Loss: 1.13885\n",
      "Epoch: 50, Time: 0.03305s, Loss: 1.13926\n",
      "Epoch: 51, Time: 0.03451s, Loss: 1.13900\n",
      "Epoch: 52, Time: 0.03104s, Loss: 1.13781\n",
      "Epoch: 53, Time: 0.03152s, Loss: 1.13630\n",
      "Epoch: 54, Time: 0.03305s, Loss: 1.13619\n",
      "Epoch: 55, Time: 0.04053s, Loss: 1.13751\n",
      "Epoch: 56, Time: 0.03107s, Loss: 1.13718\n",
      "Epoch: 57, Time: 0.03552s, Loss: 1.13603\n",
      "Epoch: 58, Time: 0.03503s, Loss: 1.13427\n",
      "Epoch: 59, Time: 0.03452s, Loss: 1.13669\n",
      "Epoch: 60, Time: 0.03504s, Loss: 1.13333\n",
      "Epoch: 61, Time: 0.03552s, Loss: 1.13687\n",
      "Epoch: 62, Time: 0.03505s, Loss: 1.13498\n",
      "Epoch: 63, Time: 0.03252s, Loss: 1.13390\n",
      "Epoch: 64, Time: 0.03405s, Loss: 1.13656\n",
      "Epoch: 65, Time: 0.03253s, Loss: 1.13240\n",
      "Epoch: 66, Time: 0.03405s, Loss: 1.13621\n",
      "Epoch: 67, Time: 0.03352s, Loss: 1.13385\n",
      "Epoch: 68, Time: 0.03778s, Loss: 1.13428\n",
      "Epoch: 69, Time: 0.03856s, Loss: 1.13309\n",
      "Epoch: 70, Time: 0.03352s, Loss: 1.13801\n",
      "Epoch: 71, Time: 0.03453s, Loss: 1.13194\n",
      "Epoch: 72, Time: 0.03700s, Loss: 1.13550\n",
      "Epoch: 73, Time: 0.03952s, Loss: 1.13214\n",
      "Epoch: 74, Time: 0.05850s, Loss: 1.13224\n",
      "Epoch: 75, Time: 0.03405s, Loss: 1.12814\n",
      "Epoch: 76, Time: 0.03551s, Loss: 1.13208\n",
      "Epoch: 77, Time: 0.03654s, Loss: 1.12889\n",
      "Epoch: 78, Time: 0.03252s, Loss: 1.13402\n",
      "Epoch: 79, Time: 0.03404s, Loss: 1.13006\n",
      "Epoch: 80, Time: 0.03451s, Loss: 1.13052\n",
      "Epoch: 81, Time: 0.04036s, Loss: 1.12810\n",
      "Epoch: 82, Time: 0.03655s, Loss: 1.13069\n",
      "Epoch: 83, Time: 0.03100s, Loss: 1.13068\n",
      "Epoch: 84, Time: 0.03501s, Loss: 1.12888\n",
      "Epoch: 85, Time: 0.03519s, Loss: 1.13235\n",
      "Epoch: 86, Time: 0.03453s, Loss: 1.13347\n",
      "Epoch: 87, Time: 0.03800s, Loss: 1.12701\n",
      "Epoch: 88, Time: 0.03155s, Loss: 1.13283\n",
      "Epoch: 89, Time: 0.03652s, Loss: 1.13140\n",
      "Epoch: 90, Time: 0.03001s, Loss: 1.12807\n",
      "Epoch: 91, Time: 0.03204s, Loss: 1.12991\n",
      "Epoch: 92, Time: 0.03152s, Loss: 1.12553\n",
      "Epoch: 93, Time: 0.03404s, Loss: 1.12758\n",
      "Epoch: 94, Time: 0.03253s, Loss: 1.13026\n",
      "Epoch: 95, Time: 0.03205s, Loss: 1.12630\n",
      "Epoch: 96, Time: 0.03151s, Loss: 1.13253\n",
      "Epoch: 97, Time: 0.03605s, Loss: 1.12643\n",
      "Epoch: 98, Time: 0.03452s, Loss: 1.12708\n",
      "Epoch: 99, Time: 0.03504s, Loss: 1.12629\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4441, F1: 0.4441\n",
      "\n",
      "[CocitationCiteseer] Fold 4/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.03401s, Loss: 1.79700\n",
      "Epoch: 1, Time: 0.03200s, Loss: 1.79264\n",
      "Epoch: 2, Time: 0.02801s, Loss: 1.78701\n",
      "Epoch: 3, Time: 0.02771s, Loss: 1.78034\n",
      "Epoch: 4, Time: 0.02653s, Loss: 1.77279\n",
      "Epoch: 5, Time: 0.03206s, Loss: 1.76409\n",
      "Epoch: 6, Time: 0.03052s, Loss: 1.75471\n",
      "Epoch: 7, Time: 0.03081s, Loss: 1.74384\n",
      "Epoch: 8, Time: 0.02900s, Loss: 1.73359\n",
      "Epoch: 9, Time: 0.02601s, Loss: 1.72075\n",
      "Epoch: 10, Time: 0.02702s, Loss: 1.70779\n",
      "Epoch: 11, Time: 0.02853s, Loss: 1.69379\n",
      "Epoch: 12, Time: 0.03106s, Loss: 1.68024\n",
      "Epoch: 13, Time: 0.02715s, Loss: 1.66659\n",
      "Epoch: 14, Time: 0.02852s, Loss: 1.65269\n",
      "Epoch: 15, Time: 0.02900s, Loss: 1.63688\n",
      "Epoch: 16, Time: 0.02153s, Loss: 1.62488\n",
      "Epoch: 17, Time: 0.03205s, Loss: 1.60627\n",
      "Epoch: 18, Time: 0.02952s, Loss: 1.59189\n",
      "Epoch: 19, Time: 0.02996s, Loss: 1.57512\n",
      "Epoch: 20, Time: 0.02851s, Loss: 1.56102\n",
      "Epoch: 21, Time: 0.02903s, Loss: 1.54157\n",
      "Epoch: 22, Time: 0.02800s, Loss: 1.53023\n",
      "Epoch: 23, Time: 0.02954s, Loss: 1.51171\n",
      "Epoch: 24, Time: 0.03203s, Loss: 1.49583\n",
      "Epoch: 25, Time: 0.02754s, Loss: 1.48268\n",
      "Epoch: 26, Time: 0.02706s, Loss: 1.47077\n",
      "Epoch: 27, Time: 0.02600s, Loss: 1.45706\n",
      "Epoch: 28, Time: 0.03424s, Loss: 1.44594\n",
      "Epoch: 29, Time: 0.03051s, Loss: 1.43581\n",
      "Epoch: 30, Time: 0.02701s, Loss: 1.42282\n",
      "Epoch: 31, Time: 0.03104s, Loss: 1.41217\n",
      "Epoch: 32, Time: 0.02552s, Loss: 1.40601\n",
      "Epoch: 33, Time: 0.02704s, Loss: 1.39227\n",
      "Epoch: 34, Time: 0.02799s, Loss: 1.38793\n",
      "Epoch: 35, Time: 0.02901s, Loss: 1.38310\n",
      "Epoch: 36, Time: 0.02900s, Loss: 1.37025\n",
      "Epoch: 37, Time: 0.02901s, Loss: 1.36761\n",
      "Epoch: 38, Time: 0.03053s, Loss: 1.35604\n",
      "Epoch: 39, Time: 0.02953s, Loss: 1.35517\n",
      "Epoch: 40, Time: 0.02807s, Loss: 1.34872\n",
      "Epoch: 41, Time: 0.03052s, Loss: 1.34290\n",
      "Epoch: 42, Time: 0.02954s, Loss: 1.33461\n",
      "Epoch: 43, Time: 0.02800s, Loss: 1.33446\n",
      "Epoch: 44, Time: 0.02900s, Loss: 1.32374\n",
      "Epoch: 45, Time: 0.03053s, Loss: 1.31715\n",
      "Epoch: 46, Time: 0.02855s, Loss: 1.31865\n",
      "Epoch: 47, Time: 0.02903s, Loss: 1.31705\n",
      "Epoch: 48, Time: 0.03152s, Loss: 1.30978\n",
      "Epoch: 49, Time: 0.02966s, Loss: 1.30766\n",
      "Epoch: 50, Time: 0.02801s, Loss: 1.30084\n",
      "Epoch: 51, Time: 0.03102s, Loss: 1.30169\n",
      "Epoch: 52, Time: 0.03200s, Loss: 1.29502\n",
      "Epoch: 53, Time: 0.02753s, Loss: 1.29338\n",
      "Epoch: 54, Time: 0.03207s, Loss: 1.29169\n",
      "Epoch: 55, Time: 0.02353s, Loss: 1.28613\n",
      "Epoch: 56, Time: 0.03153s, Loss: 1.29195\n",
      "Epoch: 57, Time: 0.02700s, Loss: 1.28445\n",
      "Epoch: 58, Time: 0.07907s, Loss: 1.28182\n",
      "Epoch: 59, Time: 0.04906s, Loss: 1.27650\n",
      "Epoch: 60, Time: 0.04743s, Loss: 1.27096\n",
      "Epoch: 61, Time: 0.02900s, Loss: 1.27209\n",
      "Epoch: 62, Time: 0.03151s, Loss: 1.26865\n",
      "Epoch: 63, Time: 0.02754s, Loss: 1.26724\n",
      "Epoch: 64, Time: 0.02753s, Loss: 1.26736\n",
      "Epoch: 65, Time: 0.02401s, Loss: 1.26643\n",
      "Epoch: 66, Time: 0.03101s, Loss: 1.26446\n",
      "Epoch: 67, Time: 0.02552s, Loss: 1.25773\n",
      "Epoch: 68, Time: 0.02806s, Loss: 1.25996\n",
      "Epoch: 69, Time: 0.02551s, Loss: 1.25696\n",
      "Epoch: 70, Time: 0.02202s, Loss: 1.25624\n",
      "Epoch: 71, Time: 0.02802s, Loss: 1.25706\n",
      "Epoch: 72, Time: 0.02953s, Loss: 1.25209\n",
      "Epoch: 73, Time: 0.02704s, Loss: 1.25092\n",
      "Epoch: 74, Time: 0.03152s, Loss: 1.24692\n",
      "Epoch: 75, Time: 0.03261s, Loss: 1.24871\n",
      "Epoch: 76, Time: 0.02853s, Loss: 1.24870\n",
      "Epoch: 77, Time: 0.02601s, Loss: 1.24633\n",
      "Epoch: 78, Time: 0.03052s, Loss: 1.24501\n",
      "Epoch: 79, Time: 0.02552s, Loss: 1.24293\n",
      "Epoch: 80, Time: 0.02506s, Loss: 1.23671\n",
      "Epoch: 81, Time: 0.03152s, Loss: 1.23906\n",
      "Epoch: 82, Time: 0.02752s, Loss: 1.23382\n",
      "Epoch: 83, Time: 0.02801s, Loss: 1.23719\n",
      "Epoch: 84, Time: 0.02900s, Loss: 1.23806\n",
      "Epoch: 85, Time: 0.03006s, Loss: 1.23760\n",
      "Epoch: 86, Time: 0.03251s, Loss: 1.23538\n",
      "Epoch: 87, Time: 0.02304s, Loss: 1.23383\n",
      "Epoch: 88, Time: 0.03152s, Loss: 1.23240\n",
      "Epoch: 89, Time: 0.02801s, Loss: 1.23368\n",
      "Epoch: 90, Time: 0.02600s, Loss: 1.23131\n",
      "Epoch: 91, Time: 0.02700s, Loss: 1.23069\n",
      "Epoch: 92, Time: 0.02854s, Loss: 1.23156\n",
      "Epoch: 93, Time: 0.03352s, Loss: 1.23045\n",
      "Epoch: 94, Time: 0.02703s, Loss: 1.22774\n",
      "Epoch: 95, Time: 0.02801s, Loss: 1.22889\n",
      "Epoch: 96, Time: 0.03001s, Loss: 1.22579\n",
      "Epoch: 97, Time: 0.03004s, Loss: 1.22369\n",
      "Epoch: 98, Time: 0.02954s, Loss: 1.22588\n",
      "Epoch: 99, Time: 0.02804s, Loss: 1.22366\n",
      "    ↳ HGNN Fold Result — Acc: 0.4275, F1: 0.4275\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03253s, Loss: 1.81880\n",
      "Epoch: 1, Time: 0.03304s, Loss: 1.41176\n",
      "Epoch: 2, Time: 0.03553s, Loss: 1.25705\n",
      "Epoch: 3, Time: 0.02854s, Loss: 1.20600\n",
      "Epoch: 4, Time: 0.03151s, Loss: 1.18110\n",
      "Epoch: 5, Time: 0.02952s, Loss: 1.16736\n",
      "Epoch: 6, Time: 0.02859s, Loss: 1.15175\n",
      "Epoch: 7, Time: 0.03454s, Loss: 1.14361\n",
      "Epoch: 8, Time: 0.03152s, Loss: 1.13908\n",
      "Epoch: 9, Time: 0.02955s, Loss: 1.13285\n",
      "Epoch: 10, Time: 0.03100s, Loss: 1.12747\n",
      "Epoch: 11, Time: 0.03002s, Loss: 1.12225\n",
      "Epoch: 12, Time: 0.03301s, Loss: 1.11723\n",
      "Epoch: 13, Time: 0.03200s, Loss: 1.11652\n",
      "Epoch: 14, Time: 0.02901s, Loss: 1.11317\n",
      "Epoch: 15, Time: 0.03251s, Loss: 1.11073\n",
      "Epoch: 16, Time: 0.03305s, Loss: 1.10851\n",
      "Epoch: 17, Time: 0.03352s, Loss: 1.10655\n",
      "Epoch: 18, Time: 0.03403s, Loss: 1.10591\n",
      "Epoch: 19, Time: 0.03353s, Loss: 1.10315\n",
      "Epoch: 20, Time: 0.03104s, Loss: 1.09932\n",
      "Epoch: 21, Time: 0.03251s, Loss: 1.09981\n",
      "Epoch: 22, Time: 0.03253s, Loss: 1.09827\n",
      "Epoch: 23, Time: 0.03183s, Loss: 1.09836\n",
      "Epoch: 24, Time: 0.03104s, Loss: 1.09867\n",
      "Epoch: 25, Time: 0.03252s, Loss: 1.09735\n",
      "Epoch: 26, Time: 0.03206s, Loss: 1.09654\n",
      "Epoch: 27, Time: 0.03453s, Loss: 1.09399\n",
      "Epoch: 28, Time: 0.03201s, Loss: 1.09328\n",
      "Epoch: 29, Time: 0.03301s, Loss: 1.09320\n",
      "Epoch: 30, Time: 0.03000s, Loss: 1.09212\n",
      "Epoch: 31, Time: 0.03100s, Loss: 1.09212\n",
      "Epoch: 32, Time: 0.03155s, Loss: 1.09236\n",
      "Epoch: 33, Time: 0.03304s, Loss: 1.09171\n",
      "Epoch: 34, Time: 0.02802s, Loss: 1.09080\n",
      "Epoch: 35, Time: 0.03230s, Loss: 1.08796\n",
      "Epoch: 36, Time: 0.03151s, Loss: 1.09073\n",
      "Epoch: 37, Time: 0.03306s, Loss: 1.08825\n",
      "Epoch: 38, Time: 0.02852s, Loss: 1.09061\n",
      "Epoch: 39, Time: 0.03505s, Loss: 1.08922\n",
      "Epoch: 40, Time: 0.03354s, Loss: 1.08815\n",
      "Epoch: 41, Time: 0.03303s, Loss: 1.08806\n",
      "Epoch: 42, Time: 0.04852s, Loss: 1.08796\n",
      "Epoch: 43, Time: 0.03304s, Loss: 1.09030\n",
      "Epoch: 44, Time: 0.02852s, Loss: 1.08834\n",
      "Epoch: 45, Time: 0.03106s, Loss: 1.08727\n",
      "Epoch: 46, Time: 0.03253s, Loss: 1.08588\n",
      "Epoch: 47, Time: 0.03507s, Loss: 1.08675\n",
      "Epoch: 48, Time: 0.03252s, Loss: 1.08892\n",
      "Epoch: 49, Time: 0.03004s, Loss: 1.08894\n",
      "Epoch: 50, Time: 0.03355s, Loss: 1.08864\n",
      "Epoch: 51, Time: 0.02752s, Loss: 1.08633\n",
      "Epoch: 52, Time: 0.03300s, Loss: 1.09074\n",
      "Epoch: 53, Time: 0.03002s, Loss: 1.08786\n",
      "Epoch: 54, Time: 0.03200s, Loss: 1.08734\n",
      "Epoch: 55, Time: 0.03000s, Loss: 1.08575\n",
      "Epoch: 56, Time: 0.03205s, Loss: 1.08862\n",
      "Epoch: 57, Time: 0.03155s, Loss: 1.08749\n",
      "Epoch: 58, Time: 0.03005s, Loss: 1.08684\n",
      "Epoch: 59, Time: 0.03052s, Loss: 1.08635\n",
      "Epoch: 60, Time: 0.02954s, Loss: 1.08714\n",
      "Epoch: 61, Time: 0.03054s, Loss: 1.08464\n",
      "Epoch: 62, Time: 0.03404s, Loss: 1.08324\n",
      "Epoch: 63, Time: 0.03352s, Loss: 1.08433\n",
      "Epoch: 64, Time: 0.03253s, Loss: 1.08686\n",
      "Epoch: 65, Time: 0.03153s, Loss: 1.08491\n",
      "Epoch: 66, Time: 0.03601s, Loss: 1.08546\n",
      "Epoch: 67, Time: 0.03500s, Loss: 1.08247\n",
      "Epoch: 68, Time: 0.03906s, Loss: 1.08556\n",
      "Epoch: 69, Time: 0.03553s, Loss: 1.08946\n",
      "Epoch: 70, Time: 0.03354s, Loss: 1.08406\n",
      "Epoch: 71, Time: 0.03453s, Loss: 1.08725\n",
      "Epoch: 72, Time: 0.03403s, Loss: 1.08453\n",
      "Epoch: 73, Time: 0.03552s, Loss: 1.08392\n",
      "Epoch: 74, Time: 0.03607s, Loss: 1.08680\n",
      "Epoch: 75, Time: 0.03252s, Loss: 1.08478\n",
      "Epoch: 76, Time: 0.03405s, Loss: 1.08564\n",
      "Epoch: 77, Time: 0.03153s, Loss: 1.08569\n",
      "Epoch: 78, Time: 0.03251s, Loss: 1.08583\n",
      "Epoch: 79, Time: 0.03119s, Loss: 1.08480\n",
      "Epoch: 80, Time: 0.03502s, Loss: 1.08266\n",
      "Epoch: 81, Time: 0.03300s, Loss: 1.08210\n",
      "Epoch: 82, Time: 0.03301s, Loss: 1.08348\n",
      "Epoch: 83, Time: 0.03452s, Loss: 1.08357\n",
      "Epoch: 84, Time: 0.03154s, Loss: 1.08541\n",
      "Epoch: 85, Time: 0.03205s, Loss: 1.08388\n",
      "Epoch: 86, Time: 0.03552s, Loss: 1.08485\n",
      "Epoch: 87, Time: 0.03105s, Loss: 1.08444\n",
      "Epoch: 88, Time: 0.03152s, Loss: 1.08253\n",
      "Epoch: 89, Time: 0.03176s, Loss: 1.08290\n",
      "Epoch: 90, Time: 0.03253s, Loss: 1.08342\n",
      "Epoch: 91, Time: 0.03453s, Loss: 1.08691\n",
      "Epoch: 92, Time: 0.03553s, Loss: 1.08332\n",
      "Epoch: 93, Time: 0.03275s, Loss: 1.08515\n",
      "Epoch: 94, Time: 0.03252s, Loss: 1.08200\n",
      "Epoch: 95, Time: 0.03106s, Loss: 1.08384\n",
      "Epoch: 96, Time: 0.03352s, Loss: 1.08496\n",
      "Epoch: 97, Time: 0.03151s, Loss: 1.08579\n",
      "Epoch: 98, Time: 0.03357s, Loss: 1.08220\n",
      "Epoch: 99, Time: 0.02952s, Loss: 1.08232\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4169, F1: 0.4169\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03553s, Loss: 1.87807\n",
      "Epoch: 1, Time: 0.04054s, Loss: 1.49016\n",
      "Epoch: 2, Time: 0.03456s, Loss: 1.32742\n",
      "Epoch: 3, Time: 0.03853s, Loss: 1.26821\n",
      "Epoch: 4, Time: 0.03554s, Loss: 1.24045\n",
      "Epoch: 5, Time: 0.03750s, Loss: 1.22496\n",
      "Epoch: 6, Time: 0.03453s, Loss: 1.20767\n",
      "Epoch: 7, Time: 0.03753s, Loss: 1.19519\n",
      "Epoch: 8, Time: 0.03605s, Loss: 1.18849\n",
      "Epoch: 9, Time: 0.03552s, Loss: 1.18274\n",
      "Epoch: 10, Time: 0.03184s, Loss: 1.17551\n",
      "Epoch: 11, Time: 0.03053s, Loss: 1.17125\n",
      "Epoch: 12, Time: 0.03253s, Loss: 1.16998\n",
      "Epoch: 13, Time: 0.03700s, Loss: 1.16163\n",
      "Epoch: 14, Time: 0.03606s, Loss: 1.16248\n",
      "Epoch: 15, Time: 0.04453s, Loss: 1.15728\n",
      "Epoch: 16, Time: 0.04354s, Loss: 1.15573\n",
      "Epoch: 17, Time: 0.04725s, Loss: 1.15375\n",
      "Epoch: 18, Time: 0.04052s, Loss: 1.15070\n",
      "Epoch: 19, Time: 0.03552s, Loss: 1.15004\n",
      "Epoch: 20, Time: 0.03756s, Loss: 1.14710\n",
      "Epoch: 21, Time: 0.03854s, Loss: 1.14647\n",
      "Epoch: 22, Time: 0.03702s, Loss: 1.14573\n",
      "Epoch: 23, Time: 0.03300s, Loss: 1.14082\n",
      "Epoch: 24, Time: 0.03200s, Loss: 1.13395\n",
      "Epoch: 25, Time: 0.03402s, Loss: 1.14009\n",
      "Epoch: 26, Time: 0.03902s, Loss: 1.13899\n",
      "Epoch: 27, Time: 0.03756s, Loss: 1.13296\n",
      "Epoch: 28, Time: 0.03451s, Loss: 1.13898\n",
      "Epoch: 29, Time: 0.03457s, Loss: 1.13393\n",
      "Epoch: 30, Time: 0.03304s, Loss: 1.13310\n",
      "Epoch: 31, Time: 0.03452s, Loss: 1.13300\n",
      "Epoch: 32, Time: 0.03507s, Loss: 1.12828\n",
      "Epoch: 33, Time: 0.04654s, Loss: 1.12610\n",
      "Epoch: 34, Time: 0.03505s, Loss: 1.12539\n",
      "Epoch: 35, Time: 0.03352s, Loss: 1.12449\n",
      "Epoch: 36, Time: 0.03005s, Loss: 1.12789\n",
      "Epoch: 37, Time: 0.03052s, Loss: 1.12069\n",
      "Epoch: 38, Time: 0.03138s, Loss: 1.12635\n",
      "Epoch: 39, Time: 0.03954s, Loss: 1.12221\n",
      "Epoch: 40, Time: 0.03503s, Loss: 1.12060\n",
      "Epoch: 41, Time: 0.03052s, Loss: 1.11823\n",
      "Epoch: 42, Time: 0.03205s, Loss: 1.12028\n",
      "Epoch: 43, Time: 0.03475s, Loss: 1.11814\n",
      "Epoch: 44, Time: 0.04104s, Loss: 1.11838\n",
      "Epoch: 45, Time: 0.04553s, Loss: 1.11867\n",
      "Epoch: 46, Time: 0.04154s, Loss: 1.11594\n",
      "Epoch: 47, Time: 0.03752s, Loss: 1.11510\n",
      "Epoch: 48, Time: 0.04454s, Loss: 1.11229\n",
      "Epoch: 49, Time: 0.03556s, Loss: 1.11343\n",
      "Epoch: 50, Time: 0.03651s, Loss: 1.11427\n",
      "Epoch: 51, Time: 0.03506s, Loss: 1.11504\n",
      "Epoch: 52, Time: 0.03754s, Loss: 1.11275\n",
      "Epoch: 53, Time: 0.03653s, Loss: 1.11388\n",
      "Epoch: 54, Time: 0.03454s, Loss: 1.11356\n",
      "Epoch: 55, Time: 0.03408s, Loss: 1.11248\n",
      "Epoch: 56, Time: 0.03100s, Loss: 1.10920\n",
      "Epoch: 57, Time: 0.03352s, Loss: 1.10554\n",
      "Epoch: 58, Time: 0.03255s, Loss: 1.11337\n",
      "Epoch: 59, Time: 0.03655s, Loss: 1.11259\n",
      "Epoch: 60, Time: 0.03800s, Loss: 1.11125\n",
      "Epoch: 61, Time: 0.05554s, Loss: 1.11028\n",
      "Epoch: 62, Time: 0.03902s, Loss: 1.10725\n",
      "Epoch: 63, Time: 0.03700s, Loss: 1.10979\n",
      "Epoch: 64, Time: 0.03511s, Loss: 1.11119\n",
      "Epoch: 65, Time: 0.03000s, Loss: 1.10705\n",
      "Epoch: 66, Time: 0.03854s, Loss: 1.11120\n",
      "Epoch: 67, Time: 0.03100s, Loss: 1.10564\n",
      "Epoch: 68, Time: 0.03809s, Loss: 1.10966\n",
      "Epoch: 69, Time: 0.03701s, Loss: 1.10447\n",
      "Epoch: 70, Time: 0.03652s, Loss: 1.10931\n",
      "Epoch: 71, Time: 0.02902s, Loss: 1.11148\n",
      "Epoch: 72, Time: 0.03452s, Loss: 1.10486\n",
      "Epoch: 73, Time: 0.03501s, Loss: 1.10527\n",
      "Epoch: 74, Time: 0.05255s, Loss: 1.11154\n",
      "Epoch: 75, Time: 0.05907s, Loss: 1.10573\n",
      "Epoch: 76, Time: 0.16534s, Loss: 1.10496\n",
      "Epoch: 77, Time: 0.03372s, Loss: 1.10756\n",
      "Epoch: 78, Time: 0.03739s, Loss: 1.10238\n",
      "Epoch: 79, Time: 0.03905s, Loss: 1.10403\n",
      "Epoch: 80, Time: 0.03654s, Loss: 1.10470\n",
      "Epoch: 81, Time: 0.03353s, Loss: 1.10961\n",
      "Epoch: 82, Time: 0.03153s, Loss: 1.10794\n",
      "Epoch: 83, Time: 0.03004s, Loss: 1.10343\n",
      "Epoch: 84, Time: 0.03553s, Loss: 1.10163\n",
      "Epoch: 85, Time: 0.03707s, Loss: 1.10194\n",
      "Epoch: 86, Time: 0.03451s, Loss: 1.10067\n",
      "Epoch: 87, Time: 0.03003s, Loss: 1.10670\n",
      "Epoch: 88, Time: 0.03584s, Loss: 1.10219\n",
      "Epoch: 89, Time: 0.03608s, Loss: 1.10621\n",
      "Epoch: 90, Time: 0.03452s, Loss: 1.10652\n",
      "Epoch: 91, Time: 0.03307s, Loss: 1.10159\n",
      "Epoch: 92, Time: 0.02953s, Loss: 1.10606\n",
      "Epoch: 93, Time: 0.03904s, Loss: 1.10305\n",
      "Epoch: 94, Time: 0.03553s, Loss: 1.10652\n",
      "Epoch: 95, Time: 0.03706s, Loss: 1.10573\n",
      "Epoch: 96, Time: 0.03754s, Loss: 1.10280\n",
      "Epoch: 97, Time: 0.03504s, Loss: 1.10187\n",
      "Epoch: 98, Time: 0.03354s, Loss: 1.10848\n",
      "Epoch: 99, Time: 0.03107s, Loss: 1.10855\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4199, F1: 0.4199\n",
      "\n",
      "[CocitationCiteseer] Fold 5/5 (Original)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.04003s, Loss: 1.79088\n",
      "Epoch: 1, Time: 0.02653s, Loss: 1.78515\n",
      "Epoch: 2, Time: 0.02552s, Loss: 1.77815\n",
      "Epoch: 3, Time: 0.02701s, Loss: 1.76927\n",
      "Epoch: 4, Time: 0.03100s, Loss: 1.76013\n",
      "Epoch: 5, Time: 0.03153s, Loss: 1.74949\n",
      "Epoch: 6, Time: 0.02954s, Loss: 1.73774\n",
      "Epoch: 7, Time: 0.03105s, Loss: 1.72623\n",
      "Epoch: 8, Time: 0.02653s, Loss: 1.71361\n",
      "Epoch: 9, Time: 0.03405s, Loss: 1.70179\n",
      "Epoch: 10, Time: 0.02952s, Loss: 1.68713\n",
      "Epoch: 11, Time: 0.06004s, Loss: 1.67534\n",
      "Epoch: 12, Time: 0.03800s, Loss: 1.65959\n",
      "Epoch: 13, Time: 0.02652s, Loss: 1.64511\n",
      "Epoch: 14, Time: 0.02852s, Loss: 1.62854\n",
      "Epoch: 15, Time: 0.02903s, Loss: 1.61719\n",
      "Epoch: 16, Time: 0.02352s, Loss: 1.59898\n",
      "Epoch: 17, Time: 0.02700s, Loss: 1.58396\n",
      "Epoch: 18, Time: 0.03152s, Loss: 1.57065\n",
      "Epoch: 19, Time: 0.03600s, Loss: 1.55477\n",
      "Epoch: 20, Time: 0.03400s, Loss: 1.54214\n",
      "Epoch: 21, Time: 0.03200s, Loss: 1.52808\n",
      "Epoch: 22, Time: 0.03323s, Loss: 1.51496\n",
      "Epoch: 23, Time: 0.04354s, Loss: 1.50024\n",
      "Epoch: 24, Time: 0.03200s, Loss: 1.48360\n",
      "Epoch: 25, Time: 0.03000s, Loss: 1.47539\n",
      "Epoch: 26, Time: 0.02953s, Loss: 1.46178\n",
      "Epoch: 27, Time: 0.02700s, Loss: 1.45079\n",
      "Epoch: 28, Time: 0.02677s, Loss: 1.44078\n",
      "Epoch: 29, Time: 0.03352s, Loss: 1.43002\n",
      "Epoch: 30, Time: 0.02652s, Loss: 1.41630\n",
      "Epoch: 31, Time: 0.02952s, Loss: 1.41477\n",
      "Epoch: 32, Time: 0.02900s, Loss: 1.40208\n",
      "Epoch: 33, Time: 0.04008s, Loss: 1.39910\n",
      "Epoch: 34, Time: 0.04207s, Loss: 1.38557\n",
      "Epoch: 35, Time: 0.03953s, Loss: 1.38346\n",
      "Epoch: 36, Time: 0.06770s, Loss: 1.37376\n",
      "Epoch: 37, Time: 0.03806s, Loss: 1.36357\n",
      "Epoch: 38, Time: 0.03552s, Loss: 1.35785\n",
      "Epoch: 39, Time: 0.03104s, Loss: 1.35280\n",
      "Epoch: 40, Time: 0.03353s, Loss: 1.34728\n",
      "Epoch: 41, Time: 0.03206s, Loss: 1.34560\n",
      "Epoch: 42, Time: 0.03452s, Loss: 1.33558\n",
      "Epoch: 43, Time: 0.03373s, Loss: 1.33709\n",
      "Epoch: 44, Time: 0.03453s, Loss: 1.32834\n",
      "Epoch: 45, Time: 0.07486s, Loss: 1.32113\n",
      "Epoch: 46, Time: 0.03507s, Loss: 1.31680\n",
      "Epoch: 47, Time: 0.03852s, Loss: 1.31487\n",
      "Epoch: 48, Time: 0.03605s, Loss: 1.31539\n",
      "Epoch: 49, Time: 0.03453s, Loss: 1.31105\n",
      "Epoch: 50, Time: 0.03353s, Loss: 1.30575\n",
      "Epoch: 51, Time: 0.03453s, Loss: 1.29721\n",
      "Epoch: 52, Time: 0.03905s, Loss: 1.29819\n",
      "Epoch: 53, Time: 0.03300s, Loss: 1.29336\n",
      "Epoch: 54, Time: 0.03605s, Loss: 1.29171\n",
      "Epoch: 55, Time: 0.03653s, Loss: 1.29385\n",
      "Epoch: 56, Time: 0.03455s, Loss: 1.28622\n",
      "Epoch: 57, Time: 0.03400s, Loss: 1.28200\n",
      "Epoch: 58, Time: 0.03853s, Loss: 1.28435\n",
      "Epoch: 59, Time: 0.03755s, Loss: 1.28158\n",
      "Epoch: 60, Time: 0.03299s, Loss: 1.27691\n",
      "Epoch: 61, Time: 0.03301s, Loss: 1.27276\n",
      "Epoch: 62, Time: 0.03350s, Loss: 1.27074\n",
      "Epoch: 63, Time: 0.03452s, Loss: 1.27094\n",
      "Epoch: 64, Time: 0.03201s, Loss: 1.27194\n",
      "Epoch: 65, Time: 0.03002s, Loss: 1.26934\n",
      "Epoch: 66, Time: 0.03213s, Loss: 1.26528\n",
      "Epoch: 67, Time: 0.03599s, Loss: 1.26533\n",
      "Epoch: 68, Time: 0.05652s, Loss: 1.26532\n",
      "Epoch: 69, Time: 0.03909s, Loss: 1.26314\n",
      "Epoch: 70, Time: 0.07965s, Loss: 1.25926\n",
      "Epoch: 71, Time: 0.04408s, Loss: 1.25367\n",
      "Epoch: 72, Time: 0.03053s, Loss: 1.25699\n",
      "Epoch: 73, Time: 0.02900s, Loss: 1.25518\n",
      "Epoch: 74, Time: 0.02702s, Loss: 1.25381\n",
      "Epoch: 75, Time: 0.02700s, Loss: 1.25259\n",
      "Epoch: 76, Time: 0.02607s, Loss: 1.25140\n",
      "Epoch: 77, Time: 0.03053s, Loss: 1.25022\n",
      "Epoch: 78, Time: 0.02953s, Loss: 1.24413\n",
      "Epoch: 79, Time: 0.02753s, Loss: 1.24948\n",
      "Epoch: 80, Time: 0.02902s, Loss: 1.24419\n",
      "Epoch: 81, Time: 0.02953s, Loss: 1.24328\n",
      "Epoch: 82, Time: 0.03054s, Loss: 1.24222\n",
      "Epoch: 83, Time: 0.03004s, Loss: 1.24184\n",
      "Epoch: 84, Time: 0.02953s, Loss: 1.24140\n",
      "Epoch: 85, Time: 0.02651s, Loss: 1.24214\n",
      "Epoch: 86, Time: 0.03401s, Loss: 1.23884\n",
      "Epoch: 87, Time: 0.03252s, Loss: 1.23692\n",
      "Epoch: 88, Time: 0.03152s, Loss: 1.23614\n",
      "Epoch: 89, Time: 0.03100s, Loss: 1.23579\n",
      "Epoch: 90, Time: 0.03152s, Loss: 1.23782\n",
      "Epoch: 91, Time: 0.03200s, Loss: 1.23576\n",
      "Epoch: 92, Time: 0.03252s, Loss: 1.23275\n",
      "Epoch: 93, Time: 0.02552s, Loss: 1.23286\n",
      "Epoch: 94, Time: 0.03003s, Loss: 1.23103\n",
      "Epoch: 95, Time: 0.02853s, Loss: 1.23280\n",
      "Epoch: 96, Time: 0.03654s, Loss: 1.23348\n",
      "Epoch: 97, Time: 0.02852s, Loss: 1.23162\n",
      "Epoch: 98, Time: 0.02751s, Loss: 1.22724\n",
      "Epoch: 99, Time: 0.02620s, Loss: 1.22815\n",
      "    ↳ HGNN Fold Result — Acc: 0.4305, F1: 0.4305\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03506s, Loss: 1.85039\n",
      "Epoch: 1, Time: 0.02952s, Loss: 1.44052\n",
      "Epoch: 2, Time: 0.03503s, Loss: 1.27757\n",
      "Epoch: 3, Time: 0.03254s, Loss: 1.22763\n",
      "Epoch: 4, Time: 0.03505s, Loss: 1.19787\n",
      "Epoch: 5, Time: 0.03552s, Loss: 1.17849\n",
      "Epoch: 6, Time: 0.03106s, Loss: 1.17053\n",
      "Epoch: 7, Time: 0.03354s, Loss: 1.15696\n",
      "Epoch: 8, Time: 0.03304s, Loss: 1.14821\n",
      "Epoch: 9, Time: 0.03251s, Loss: 1.14664\n",
      "Epoch: 10, Time: 0.03253s, Loss: 1.13600\n",
      "Epoch: 11, Time: 0.03553s, Loss: 1.13327\n",
      "Epoch: 12, Time: 0.03052s, Loss: 1.13156\n",
      "Epoch: 13, Time: 0.03353s, Loss: 1.12597\n",
      "Epoch: 14, Time: 0.03253s, Loss: 1.12504\n",
      "Epoch: 15, Time: 0.03853s, Loss: 1.12128\n",
      "Epoch: 16, Time: 0.03604s, Loss: 1.11694\n",
      "Epoch: 17, Time: 0.03252s, Loss: 1.11686\n",
      "Epoch: 18, Time: 0.03153s, Loss: 1.11461\n",
      "Epoch: 19, Time: 0.02851s, Loss: 1.11160\n",
      "Epoch: 20, Time: 0.03953s, Loss: 1.11026\n",
      "Epoch: 21, Time: 0.03252s, Loss: 1.11019\n",
      "Epoch: 22, Time: 0.07186s, Loss: 1.10900\n",
      "Epoch: 23, Time: 0.03803s, Loss: 1.10560\n",
      "Epoch: 24, Time: 0.08268s, Loss: 1.10646\n",
      "Epoch: 25, Time: 0.04754s, Loss: 1.10609\n",
      "Epoch: 26, Time: 0.03254s, Loss: 1.10299\n",
      "Epoch: 27, Time: 0.03301s, Loss: 1.10563\n",
      "Epoch: 28, Time: 0.03762s, Loss: 1.10397\n",
      "Epoch: 29, Time: 0.03500s, Loss: 1.10215\n",
      "Epoch: 30, Time: 0.03300s, Loss: 1.09894\n",
      "Epoch: 31, Time: 0.03000s, Loss: 1.09949\n",
      "Epoch: 32, Time: 0.03854s, Loss: 1.09673\n",
      "Epoch: 33, Time: 0.03652s, Loss: 1.10192\n",
      "Epoch: 34, Time: 0.03351s, Loss: 1.09768\n",
      "Epoch: 35, Time: 0.03555s, Loss: 1.09727\n",
      "Epoch: 36, Time: 0.03500s, Loss: 1.09773\n",
      "Epoch: 37, Time: 0.04052s, Loss: 1.09275\n",
      "Epoch: 38, Time: 0.03352s, Loss: 1.09491\n",
      "Epoch: 39, Time: 0.03154s, Loss: 1.09427\n",
      "Epoch: 40, Time: 0.03200s, Loss: 1.09426\n",
      "Epoch: 41, Time: 0.03553s, Loss: 1.09398\n",
      "Epoch: 42, Time: 0.03856s, Loss: 1.09450\n",
      "Epoch: 43, Time: 0.08160s, Loss: 1.09437\n",
      "Epoch: 44, Time: 0.03654s, Loss: 1.09080\n",
      "Epoch: 45, Time: 0.02952s, Loss: 1.09250\n",
      "Epoch: 46, Time: 0.03452s, Loss: 1.09124\n",
      "Epoch: 47, Time: 0.02850s, Loss: 1.09322\n",
      "Epoch: 48, Time: 0.03352s, Loss: 1.09355\n",
      "Epoch: 49, Time: 0.03301s, Loss: 1.09301\n",
      "Epoch: 50, Time: 0.03200s, Loss: 1.09341\n",
      "Epoch: 51, Time: 0.03797s, Loss: 1.09279\n",
      "Epoch: 52, Time: 0.03251s, Loss: 1.09492\n",
      "Epoch: 53, Time: 0.03152s, Loss: 1.09097\n",
      "Epoch: 54, Time: 0.03401s, Loss: 1.09210\n",
      "Epoch: 55, Time: 0.03552s, Loss: 1.09038\n",
      "Epoch: 56, Time: 0.03849s, Loss: 1.09228\n",
      "Epoch: 57, Time: 0.03252s, Loss: 1.09174\n",
      "Epoch: 58, Time: 0.02900s, Loss: 1.09203\n",
      "Epoch: 59, Time: 0.03205s, Loss: 1.08911\n",
      "Epoch: 60, Time: 0.02754s, Loss: 1.09300\n",
      "Epoch: 61, Time: 0.03405s, Loss: 1.09201\n",
      "Epoch: 62, Time: 0.03252s, Loss: 1.09153\n",
      "Epoch: 63, Time: 0.03252s, Loss: 1.08879\n",
      "Epoch: 64, Time: 0.03151s, Loss: 1.08865\n",
      "Epoch: 65, Time: 0.03654s, Loss: 1.09026\n",
      "Epoch: 66, Time: 0.03652s, Loss: 1.09362\n",
      "Epoch: 67, Time: 0.03255s, Loss: 1.09032\n",
      "Epoch: 68, Time: 0.03951s, Loss: 1.09210\n",
      "Epoch: 69, Time: 0.03506s, Loss: 1.09004\n",
      "Epoch: 70, Time: 0.03452s, Loss: 1.08641\n",
      "Epoch: 71, Time: 0.03854s, Loss: 1.09325\n",
      "Epoch: 72, Time: 0.03601s, Loss: 1.08858\n",
      "Epoch: 73, Time: 0.03703s, Loss: 1.08973\n",
      "Epoch: 74, Time: 0.02902s, Loss: 1.08557\n",
      "Epoch: 75, Time: 0.03307s, Loss: 1.08706\n",
      "Epoch: 76, Time: 0.03752s, Loss: 1.08777\n",
      "Epoch: 77, Time: 0.03678s, Loss: 1.09085\n",
      "Epoch: 78, Time: 0.08015s, Loss: 1.09103\n",
      "Epoch: 79, Time: 0.04953s, Loss: 1.08844\n",
      "Epoch: 80, Time: 0.04153s, Loss: 1.08783\n",
      "Epoch: 81, Time: 0.03704s, Loss: 1.09014\n",
      "Epoch: 82, Time: 0.03653s, Loss: 1.08911\n",
      "Epoch: 83, Time: 0.03506s, Loss: 1.08901\n",
      "Epoch: 84, Time: 0.03256s, Loss: 1.08789\n",
      "Epoch: 85, Time: 0.03706s, Loss: 1.08838\n",
      "Epoch: 86, Time: 0.03654s, Loss: 1.08856\n",
      "Epoch: 87, Time: 0.03904s, Loss: 1.08647\n",
      "Epoch: 88, Time: 0.03954s, Loss: 1.08771\n",
      "Epoch: 89, Time: 0.03352s, Loss: 1.09334\n",
      "Epoch: 90, Time: 0.03400s, Loss: 1.08635\n",
      "Epoch: 91, Time: 0.03904s, Loss: 1.08745\n",
      "Epoch: 92, Time: 0.03801s, Loss: 1.08729\n",
      "Epoch: 93, Time: 0.03906s, Loss: 1.09062\n",
      "Epoch: 94, Time: 0.03100s, Loss: 1.08963\n",
      "Epoch: 95, Time: 0.03017s, Loss: 1.09032\n",
      "Epoch: 96, Time: 0.03783s, Loss: 1.08885\n",
      "Epoch: 97, Time: 0.03652s, Loss: 1.08629\n",
      "Epoch: 98, Time: 0.03800s, Loss: 1.08960\n",
      "Epoch: 99, Time: 0.03353s, Loss: 1.08796\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4139, F1: 0.4139\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.02754s, Loss: 1.90599\n",
      "Epoch: 1, Time: 0.03905s, Loss: 1.45947\n",
      "Epoch: 2, Time: 0.03454s, Loss: 1.31057\n",
      "Epoch: 3, Time: 0.03852s, Loss: 1.25910\n",
      "Epoch: 4, Time: 0.03105s, Loss: 1.23413\n",
      "Epoch: 5, Time: 0.03551s, Loss: 1.21461\n",
      "Epoch: 6, Time: 0.04105s, Loss: 1.20344\n",
      "Epoch: 7, Time: 0.03553s, Loss: 1.19510\n",
      "Epoch: 8, Time: 0.03751s, Loss: 1.18407\n",
      "Epoch: 9, Time: 0.03553s, Loss: 1.18063\n",
      "Epoch: 10, Time: 0.03553s, Loss: 1.17517\n",
      "Epoch: 11, Time: 0.03304s, Loss: 1.16896\n",
      "Epoch: 12, Time: 0.03152s, Loss: 1.16324\n",
      "Epoch: 13, Time: 0.02909s, Loss: 1.16412\n",
      "Epoch: 14, Time: 0.03154s, Loss: 1.15790\n",
      "Epoch: 15, Time: 0.03354s, Loss: 1.15421\n",
      "Epoch: 16, Time: 0.03552s, Loss: 1.15236\n",
      "Epoch: 17, Time: 0.03101s, Loss: 1.15078\n",
      "Epoch: 18, Time: 0.03854s, Loss: 1.14836\n",
      "Epoch: 19, Time: 0.03905s, Loss: 1.14985\n",
      "Epoch: 20, Time: 0.03753s, Loss: 1.14800\n",
      "Epoch: 21, Time: 0.03704s, Loss: 1.14336\n",
      "Epoch: 22, Time: 0.03653s, Loss: 1.13944\n",
      "Epoch: 23, Time: 0.03778s, Loss: 1.14180\n",
      "Epoch: 24, Time: 0.03953s, Loss: 1.13882\n",
      "Epoch: 25, Time: 0.03553s, Loss: 1.14130\n",
      "Epoch: 26, Time: 0.03651s, Loss: 1.13545\n",
      "Epoch: 27, Time: 0.03478s, Loss: 1.13656\n",
      "Epoch: 28, Time: 0.03653s, Loss: 1.13246\n",
      "Epoch: 29, Time: 0.03752s, Loss: 1.13396\n",
      "Epoch: 30, Time: 0.06107s, Loss: 1.13211\n",
      "Epoch: 31, Time: 0.03354s, Loss: 1.13248\n",
      "Epoch: 32, Time: 0.03651s, Loss: 1.13221\n",
      "Epoch: 33, Time: 0.03154s, Loss: 1.12963\n",
      "Epoch: 34, Time: 0.02700s, Loss: 1.13239\n",
      "Epoch: 35, Time: 0.03552s, Loss: 1.12738\n",
      "Epoch: 36, Time: 0.03053s, Loss: 1.12529\n",
      "Epoch: 37, Time: 0.03300s, Loss: 1.12556\n",
      "Epoch: 38, Time: 0.03055s, Loss: 1.12323\n",
      "Epoch: 39, Time: 0.03600s, Loss: 1.12358\n",
      "Epoch: 40, Time: 0.02954s, Loss: 1.12751\n",
      "Epoch: 41, Time: 0.03353s, Loss: 1.12373\n",
      "Epoch: 42, Time: 0.03109s, Loss: 1.12155\n",
      "Epoch: 43, Time: 0.03154s, Loss: 1.11523\n",
      "Epoch: 44, Time: 0.05450s, Loss: 1.12136\n",
      "Epoch: 45, Time: 0.04487s, Loss: 1.12078\n",
      "Epoch: 46, Time: 0.04051s, Loss: 1.11532\n",
      "Epoch: 47, Time: 0.04052s, Loss: 1.11711\n",
      "Epoch: 48, Time: 0.03808s, Loss: 1.11852\n",
      "Epoch: 49, Time: 0.03352s, Loss: 1.11630\n",
      "Epoch: 50, Time: 0.02753s, Loss: 1.11853\n",
      "Epoch: 51, Time: 0.03152s, Loss: 1.11637\n",
      "Epoch: 52, Time: 0.03458s, Loss: 1.11433\n",
      "Epoch: 53, Time: 0.03453s, Loss: 1.11966\n",
      "Epoch: 54, Time: 0.03453s, Loss: 1.11635\n",
      "Epoch: 55, Time: 0.03152s, Loss: 1.11463\n",
      "Epoch: 56, Time: 0.03252s, Loss: 1.11439\n",
      "Epoch: 57, Time: 0.02852s, Loss: 1.11337\n",
      "Epoch: 58, Time: 0.03200s, Loss: 1.10691\n",
      "Epoch: 59, Time: 0.03055s, Loss: 1.11016\n",
      "Epoch: 60, Time: 0.03300s, Loss: 1.10881\n",
      "Epoch: 61, Time: 0.02904s, Loss: 1.11440\n",
      "Epoch: 62, Time: 0.03053s, Loss: 1.11500\n",
      "Epoch: 63, Time: 0.03406s, Loss: 1.10964\n",
      "Epoch: 64, Time: 0.03100s, Loss: 1.11155\n",
      "Epoch: 65, Time: 0.03008s, Loss: 1.10816\n",
      "Epoch: 66, Time: 0.03352s, Loss: 1.11416\n",
      "Epoch: 67, Time: 0.02953s, Loss: 1.11039\n",
      "Epoch: 68, Time: 0.03652s, Loss: 1.11359\n",
      "Epoch: 69, Time: 0.04410s, Loss: 1.11082\n",
      "Epoch: 70, Time: 0.10560s, Loss: 1.10800\n",
      "Epoch: 71, Time: 0.02652s, Loss: 1.10785\n",
      "Epoch: 72, Time: 0.03401s, Loss: 1.11139\n",
      "Epoch: 73, Time: 0.03354s, Loss: 1.10765\n",
      "Epoch: 74, Time: 0.02900s, Loss: 1.11131\n",
      "Epoch: 75, Time: 0.03452s, Loss: 1.11089\n",
      "Epoch: 76, Time: 0.02907s, Loss: 1.10686\n",
      "Epoch: 77, Time: 0.03205s, Loss: 1.11177\n",
      "Epoch: 78, Time: 0.03653s, Loss: 1.11070\n",
      "Epoch: 79, Time: 0.02952s, Loss: 1.10961\n",
      "Epoch: 80, Time: 0.03800s, Loss: 1.10823\n",
      "Epoch: 81, Time: 0.03006s, Loss: 1.10665\n",
      "Epoch: 82, Time: 0.03152s, Loss: 1.10663\n",
      "Epoch: 83, Time: 0.03254s, Loss: 1.11143\n",
      "Epoch: 84, Time: 0.02952s, Loss: 1.11059\n",
      "Epoch: 85, Time: 0.03054s, Loss: 1.11430\n",
      "Epoch: 86, Time: 0.03051s, Loss: 1.10579\n",
      "Epoch: 87, Time: 0.03152s, Loss: 1.11382\n",
      "Epoch: 88, Time: 0.02952s, Loss: 1.10568\n",
      "Epoch: 89, Time: 0.03000s, Loss: 1.10963\n",
      "Epoch: 90, Time: 0.03404s, Loss: 1.11572\n",
      "Epoch: 91, Time: 0.03200s, Loss: 1.11163\n",
      "Epoch: 92, Time: 0.03153s, Loss: 1.10999\n",
      "Epoch: 93, Time: 0.03401s, Loss: 1.10804\n",
      "Epoch: 94, Time: 0.03152s, Loss: 1.11044\n",
      "Epoch: 95, Time: 0.03200s, Loss: 1.11100\n",
      "Epoch: 96, Time: 0.03208s, Loss: 1.10405\n",
      "Epoch: 97, Time: 0.02754s, Loss: 1.10723\n",
      "Epoch: 98, Time: 0.03452s, Loss: 1.11170\n",
      "Epoch: 99, Time: 0.03451s, Loss: 1.10419\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4290, F1: 0.4290\n",
      "\n",
      "==> Running CocitationCiteseer with Top-k\n",
      "\n",
      "[CocitationCiteseer] Fold 1/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.05996s, Loss: 1.79136\n",
      "Epoch: 1, Time: 0.02469s, Loss: 1.78682\n",
      "Epoch: 2, Time: 0.02711s, Loss: 1.78137\n",
      "Epoch: 3, Time: 0.02719s, Loss: 1.77476\n",
      "Epoch: 4, Time: 0.02777s, Loss: 1.76741\n",
      "Epoch: 5, Time: 0.02364s, Loss: 1.75897\n",
      "Epoch: 6, Time: 0.02755s, Loss: 1.74979\n",
      "Epoch: 7, Time: 0.02763s, Loss: 1.73928\n",
      "Epoch: 8, Time: 0.02815s, Loss: 1.72945\n",
      "Epoch: 9, Time: 0.02763s, Loss: 1.71621\n",
      "Epoch: 10, Time: 0.02720s, Loss: 1.70692\n",
      "Epoch: 11, Time: 0.02976s, Loss: 1.69374\n",
      "Epoch: 12, Time: 0.03622s, Loss: 1.68222\n",
      "Epoch: 13, Time: 0.02865s, Loss: 1.66939\n",
      "Epoch: 14, Time: 0.03014s, Loss: 1.65803\n",
      "Epoch: 15, Time: 0.02368s, Loss: 1.64444\n",
      "Epoch: 16, Time: 0.02669s, Loss: 1.62907\n",
      "Epoch: 17, Time: 0.02722s, Loss: 1.61705\n",
      "Epoch: 18, Time: 0.02612s, Loss: 1.60184\n",
      "Epoch: 19, Time: 0.03090s, Loss: 1.58609\n",
      "Epoch: 20, Time: 0.02828s, Loss: 1.57467\n",
      "Epoch: 21, Time: 0.02419s, Loss: 1.56183\n",
      "Epoch: 22, Time: 0.02723s, Loss: 1.54843\n",
      "Epoch: 23, Time: 0.03122s, Loss: 1.53355\n",
      "Epoch: 24, Time: 0.02562s, Loss: 1.52706\n",
      "Epoch: 25, Time: 0.03645s, Loss: 1.50664\n",
      "Epoch: 26, Time: 0.03171s, Loss: 1.50298\n",
      "Epoch: 27, Time: 0.03007s, Loss: 1.48939\n",
      "Epoch: 28, Time: 0.03236s, Loss: 1.47919\n",
      "Epoch: 29, Time: 0.03182s, Loss: 1.47295\n",
      "Epoch: 30, Time: 0.03015s, Loss: 1.46311\n",
      "Epoch: 31, Time: 0.03333s, Loss: 1.45341\n",
      "Epoch: 32, Time: 0.03169s, Loss: 1.44566\n",
      "Epoch: 33, Time: 0.03326s, Loss: 1.43745\n",
      "Epoch: 34, Time: 0.03228s, Loss: 1.43041\n",
      "Epoch: 35, Time: 0.03265s, Loss: 1.42504\n",
      "Epoch: 36, Time: 0.03032s, Loss: 1.41739\n",
      "Epoch: 37, Time: 0.02968s, Loss: 1.41625\n",
      "Epoch: 38, Time: 0.03220s, Loss: 1.41022\n",
      "Epoch: 39, Time: 0.03116s, Loss: 1.40273\n",
      "Epoch: 40, Time: 0.03289s, Loss: 1.40297\n",
      "Epoch: 41, Time: 0.03168s, Loss: 1.39693\n",
      "Epoch: 42, Time: 0.03049s, Loss: 1.39488\n",
      "Epoch: 43, Time: 0.03075s, Loss: 1.38575\n",
      "Epoch: 44, Time: 0.03222s, Loss: 1.38408\n",
      "Epoch: 45, Time: 0.02770s, Loss: 1.38144\n",
      "Epoch: 46, Time: 0.02281s, Loss: 1.37678\n",
      "Epoch: 47, Time: 0.03167s, Loss: 1.37024\n",
      "Epoch: 48, Time: 0.03529s, Loss: 1.37045\n",
      "Epoch: 49, Time: 0.02520s, Loss: 1.37032\n",
      "Epoch: 50, Time: 0.03020s, Loss: 1.36595\n",
      "Epoch: 51, Time: 0.02981s, Loss: 1.36477\n",
      "Epoch: 52, Time: 0.03226s, Loss: 1.36205\n",
      "Epoch: 53, Time: 0.03117s, Loss: 1.35960\n",
      "Epoch: 54, Time: 0.03175s, Loss: 1.36127\n",
      "Epoch: 55, Time: 0.03287s, Loss: 1.35149\n",
      "Epoch: 56, Time: 0.03175s, Loss: 1.34961\n",
      "Epoch: 57, Time: 0.03245s, Loss: 1.34904\n",
      "Epoch: 58, Time: 0.02869s, Loss: 1.34582\n",
      "Epoch: 59, Time: 0.03125s, Loss: 1.34680\n",
      "Epoch: 60, Time: 0.04093s, Loss: 1.34299\n",
      "Epoch: 61, Time: 0.03099s, Loss: 1.34597\n",
      "Epoch: 62, Time: 0.03131s, Loss: 1.34313\n",
      "Epoch: 63, Time: 0.02815s, Loss: 1.33821\n",
      "Epoch: 64, Time: 0.03008s, Loss: 1.33862\n",
      "Epoch: 65, Time: 0.03373s, Loss: 1.33608\n",
      "Epoch: 66, Time: 0.02717s, Loss: 1.33704\n",
      "Epoch: 67, Time: 0.02668s, Loss: 1.33653\n",
      "Epoch: 68, Time: 0.03268s, Loss: 1.33234\n",
      "Epoch: 69, Time: 0.02463s, Loss: 1.33440\n",
      "Epoch: 70, Time: 0.02313s, Loss: 1.33440\n",
      "Epoch: 71, Time: 0.03827s, Loss: 1.32812\n",
      "Epoch: 72, Time: 0.04096s, Loss: 1.32569\n",
      "Epoch: 73, Time: 0.02858s, Loss: 1.32791\n",
      "Epoch: 74, Time: 0.03073s, Loss: 1.32568\n",
      "Epoch: 75, Time: 0.03214s, Loss: 1.32273\n",
      "Epoch: 76, Time: 0.03586s, Loss: 1.32525\n",
      "Epoch: 77, Time: 0.04414s, Loss: 1.32280\n",
      "Epoch: 78, Time: 0.03369s, Loss: 1.32353\n",
      "Epoch: 79, Time: 0.03974s, Loss: 1.32070\n",
      "Epoch: 80, Time: 0.03865s, Loss: 1.32115\n",
      "Epoch: 81, Time: 0.05170s, Loss: 1.31572\n",
      "Epoch: 82, Time: 0.03510s, Loss: 1.31737\n",
      "Epoch: 83, Time: 0.03524s, Loss: 1.31882\n",
      "Epoch: 84, Time: 0.03592s, Loss: 1.31451\n",
      "Epoch: 85, Time: 0.19973s, Loss: 1.31638\n",
      "Epoch: 86, Time: 0.05664s, Loss: 1.31814\n",
      "Epoch: 87, Time: 0.05914s, Loss: 1.31521\n",
      "Epoch: 88, Time: 0.04166s, Loss: 1.31297\n",
      "Epoch: 89, Time: 0.04205s, Loss: 1.31233\n",
      "Epoch: 90, Time: 0.06450s, Loss: 1.31379\n",
      "Epoch: 91, Time: 0.03931s, Loss: 1.31347\n",
      "Epoch: 92, Time: 0.03526s, Loss: 1.31052\n",
      "Epoch: 93, Time: 0.04699s, Loss: 1.30805\n",
      "Epoch: 94, Time: 0.04080s, Loss: 1.30916\n",
      "Epoch: 95, Time: 0.03781s, Loss: 1.30709\n",
      "Epoch: 96, Time: 0.03633s, Loss: 1.30742\n",
      "Epoch: 97, Time: 0.03470s, Loss: 1.30725\n",
      "Epoch: 98, Time: 0.04349s, Loss: 1.30971\n",
      "Epoch: 99, Time: 0.03843s, Loss: 1.30760\n",
      "    ↳ HGNN Fold Result — Acc: 0.3801, F1: 0.3801\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.04426s, Loss: 1.83440\n",
      "Epoch: 1, Time: 0.04407s, Loss: 1.47344\n",
      "Epoch: 2, Time: 0.04491s, Loss: 1.33481\n",
      "Epoch: 3, Time: 0.03790s, Loss: 1.29017\n",
      "Epoch: 4, Time: 0.03842s, Loss: 1.27380\n",
      "Epoch: 5, Time: 0.03831s, Loss: 1.25621\n",
      "Epoch: 6, Time: 0.04439s, Loss: 1.24868\n",
      "Epoch: 7, Time: 0.03279s, Loss: 1.24197\n",
      "Epoch: 8, Time: 0.03988s, Loss: 1.23644\n",
      "Epoch: 9, Time: 0.03719s, Loss: 1.23090\n",
      "Epoch: 10, Time: 0.03650s, Loss: 1.22891\n",
      "Epoch: 11, Time: 0.04130s, Loss: 1.22620\n",
      "Epoch: 12, Time: 0.03577s, Loss: 1.22343\n",
      "Epoch: 13, Time: 0.03279s, Loss: 1.22238\n",
      "Epoch: 14, Time: 0.03653s, Loss: 1.22048\n",
      "Epoch: 15, Time: 0.03978s, Loss: 1.21912\n",
      "Epoch: 16, Time: 0.04543s, Loss: 1.21863\n",
      "Epoch: 17, Time: 0.03597s, Loss: 1.21555\n",
      "Epoch: 18, Time: 0.03878s, Loss: 1.21490\n",
      "Epoch: 19, Time: 0.03623s, Loss: 1.21218\n",
      "Epoch: 20, Time: 0.03905s, Loss: 1.21241\n",
      "Epoch: 21, Time: 0.07371s, Loss: 1.21242\n",
      "Epoch: 22, Time: 0.04184s, Loss: 1.21194\n",
      "Epoch: 23, Time: 0.03422s, Loss: 1.21103\n",
      "Epoch: 24, Time: 0.03754s, Loss: 1.20754\n",
      "Epoch: 25, Time: 0.04129s, Loss: 1.21049\n",
      "Epoch: 26, Time: 0.03985s, Loss: 1.20585\n",
      "Epoch: 27, Time: 0.03644s, Loss: 1.20497\n",
      "Epoch: 28, Time: 0.03281s, Loss: 1.20612\n",
      "Epoch: 29, Time: 0.04761s, Loss: 1.20375\n",
      "Epoch: 30, Time: 0.04435s, Loss: 1.20469\n",
      "Epoch: 31, Time: 0.03774s, Loss: 1.20475\n",
      "Epoch: 32, Time: 0.03134s, Loss: 1.20521\n",
      "Epoch: 33, Time: 0.04204s, Loss: 1.20303\n",
      "Epoch: 34, Time: 0.04791s, Loss: 1.20377\n",
      "Epoch: 35, Time: 0.03822s, Loss: 1.20338\n",
      "Epoch: 36, Time: 0.03352s, Loss: 1.20272\n",
      "Epoch: 37, Time: 0.03469s, Loss: 1.20318\n",
      "Epoch: 38, Time: 0.03982s, Loss: 1.20123\n",
      "Epoch: 39, Time: 0.04334s, Loss: 1.20211\n",
      "Epoch: 40, Time: 0.03518s, Loss: 1.20275\n",
      "Epoch: 41, Time: 0.04024s, Loss: 1.20375\n",
      "Epoch: 42, Time: 0.03986s, Loss: 1.20186\n",
      "Epoch: 43, Time: 0.04881s, Loss: 1.20469\n",
      "Epoch: 44, Time: 0.03735s, Loss: 1.20037\n",
      "Epoch: 45, Time: 0.03439s, Loss: 1.19849\n",
      "Epoch: 46, Time: 0.03577s, Loss: 1.20218\n",
      "Epoch: 47, Time: 0.03291s, Loss: 1.20104\n",
      "Epoch: 48, Time: 0.04398s, Loss: 1.20164\n",
      "Epoch: 49, Time: 0.03602s, Loss: 1.19917\n",
      "Epoch: 50, Time: 0.03532s, Loss: 1.19802\n",
      "Epoch: 51, Time: 0.03571s, Loss: 1.19780\n",
      "Epoch: 52, Time: 0.03266s, Loss: 1.19888\n",
      "Epoch: 53, Time: 0.03976s, Loss: 1.19914\n",
      "Epoch: 54, Time: 0.04699s, Loss: 1.19760\n",
      "Epoch: 55, Time: 0.03716s, Loss: 1.19913\n",
      "Epoch: 56, Time: 0.04261s, Loss: 1.19852\n",
      "Epoch: 57, Time: 0.04291s, Loss: 1.19748\n",
      "Epoch: 58, Time: 0.03999s, Loss: 1.19702\n",
      "Epoch: 59, Time: 0.03830s, Loss: 1.19741\n",
      "Epoch: 60, Time: 0.03874s, Loss: 1.19708\n",
      "Epoch: 61, Time: 0.03690s, Loss: 1.19709\n",
      "Epoch: 62, Time: 0.05203s, Loss: 1.20029\n",
      "Epoch: 63, Time: 0.03573s, Loss: 1.20016\n",
      "Epoch: 64, Time: 0.03907s, Loss: 1.19630\n",
      "Epoch: 65, Time: 0.04185s, Loss: 1.19547\n",
      "Epoch: 66, Time: 0.03771s, Loss: 1.19986\n",
      "Epoch: 67, Time: 0.04436s, Loss: 1.19571\n",
      "Epoch: 68, Time: 0.04246s, Loss: 1.19876\n",
      "Epoch: 69, Time: 0.03890s, Loss: 1.19907\n",
      "Epoch: 70, Time: 0.03685s, Loss: 1.19635\n",
      "Epoch: 71, Time: 0.05693s, Loss: 1.19920\n",
      "Epoch: 72, Time: 0.04645s, Loss: 1.20083\n",
      "Epoch: 73, Time: 0.04127s, Loss: 1.19984\n",
      "Epoch: 74, Time: 0.04128s, Loss: 1.19769\n",
      "Epoch: 75, Time: 0.05091s, Loss: 1.19693\n",
      "Epoch: 76, Time: 0.03927s, Loss: 1.20137\n",
      "Epoch: 77, Time: 0.04066s, Loss: 1.19668\n",
      "Epoch: 78, Time: 0.03723s, Loss: 1.20099\n",
      "Epoch: 79, Time: 0.03993s, Loss: 1.19916\n",
      "Epoch: 80, Time: 0.05236s, Loss: 1.19840\n",
      "Epoch: 81, Time: 0.04839s, Loss: 1.19864\n",
      "Epoch: 82, Time: 0.04136s, Loss: 1.19677\n",
      "Epoch: 83, Time: 0.04233s, Loss: 1.19844\n",
      "Epoch: 84, Time: 0.05198s, Loss: 1.20087\n",
      "Epoch: 85, Time: 0.03831s, Loss: 1.19742\n",
      "Epoch: 86, Time: 0.03924s, Loss: 1.19754\n",
      "Epoch: 87, Time: 0.03417s, Loss: 1.20121\n",
      "Epoch: 88, Time: 0.04239s, Loss: 1.19971\n",
      "Epoch: 89, Time: 0.03627s, Loss: 1.19854\n",
      "Epoch: 90, Time: 0.03371s, Loss: 1.19833\n",
      "Epoch: 91, Time: 0.03418s, Loss: 1.19839\n",
      "Epoch: 92, Time: 0.03217s, Loss: 1.19806\n",
      "Epoch: 93, Time: 0.04137s, Loss: 1.19662\n",
      "Epoch: 94, Time: 0.04792s, Loss: 1.19578\n",
      "Epoch: 95, Time: 0.04132s, Loss: 1.19893\n",
      "Epoch: 96, Time: 0.03730s, Loss: 1.19844\n",
      "Epoch: 97, Time: 0.04646s, Loss: 1.20001\n",
      "Epoch: 98, Time: 0.04290s, Loss: 1.19679\n",
      "Epoch: 99, Time: 0.04090s, Loss: 1.19804\n",
      "    ↳ HGNNP Fold Result — Acc: 0.3710, F1: 0.3710\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.10448s, Loss: 1.85557\n",
      "Epoch: 1, Time: 0.07460s, Loss: 1.48426\n",
      "Epoch: 2, Time: 0.04542s, Loss: 1.35582\n",
      "Epoch: 3, Time: 0.05051s, Loss: 1.31349\n",
      "Epoch: 4, Time: 0.03928s, Loss: 1.29736\n",
      "Epoch: 5, Time: 0.03794s, Loss: 1.28788\n",
      "Epoch: 6, Time: 0.04227s, Loss: 1.27718\n",
      "Epoch: 7, Time: 0.04529s, Loss: 1.27239\n",
      "Epoch: 8, Time: 0.04386s, Loss: 1.26261\n",
      "Epoch: 9, Time: 0.04128s, Loss: 1.25886\n",
      "Epoch: 10, Time: 0.04743s, Loss: 1.25179\n",
      "Epoch: 11, Time: 0.06256s, Loss: 1.25113\n",
      "Epoch: 12, Time: 0.03726s, Loss: 1.24856\n",
      "Epoch: 13, Time: 0.03671s, Loss: 1.24757\n",
      "Epoch: 14, Time: 0.04146s, Loss: 1.24497\n",
      "Epoch: 15, Time: 0.04247s, Loss: 1.24149\n",
      "Epoch: 16, Time: 0.04334s, Loss: 1.24065\n",
      "Epoch: 17, Time: 0.04079s, Loss: 1.23828\n",
      "Epoch: 18, Time: 0.04868s, Loss: 1.23683\n",
      "Epoch: 19, Time: 0.04385s, Loss: 1.23204\n",
      "Epoch: 20, Time: 0.06049s, Loss: 1.23448\n",
      "Epoch: 21, Time: 0.05447s, Loss: 1.23025\n",
      "Epoch: 22, Time: 0.06101s, Loss: 1.23247\n",
      "Epoch: 23, Time: 0.04184s, Loss: 1.23012\n",
      "Epoch: 24, Time: 0.04121s, Loss: 1.22566\n",
      "Epoch: 25, Time: 0.04281s, Loss: 1.23114\n",
      "Epoch: 26, Time: 0.04316s, Loss: 1.22998\n",
      "Epoch: 27, Time: 0.04384s, Loss: 1.22541\n",
      "Epoch: 28, Time: 0.03829s, Loss: 1.22524\n",
      "Epoch: 29, Time: 0.04286s, Loss: 1.22876\n",
      "Epoch: 30, Time: 0.03928s, Loss: 1.22213\n",
      "Epoch: 31, Time: 0.03719s, Loss: 1.22168\n",
      "Epoch: 32, Time: 0.04232s, Loss: 1.21957\n",
      "Epoch: 33, Time: 0.03879s, Loss: 1.22195\n",
      "Epoch: 34, Time: 0.04633s, Loss: 1.22213\n",
      "Epoch: 35, Time: 0.04486s, Loss: 1.22114\n",
      "Epoch: 36, Time: 0.04033s, Loss: 1.22083\n",
      "Epoch: 37, Time: 0.03473s, Loss: 1.21640\n",
      "Epoch: 38, Time: 0.04699s, Loss: 1.21939\n",
      "Epoch: 39, Time: 0.03723s, Loss: 1.22005\n",
      "Epoch: 40, Time: 0.04003s, Loss: 1.22033\n",
      "Epoch: 41, Time: 0.03821s, Loss: 1.21914\n",
      "Epoch: 42, Time: 0.04486s, Loss: 1.21705\n",
      "Epoch: 43, Time: 0.04234s, Loss: 1.21388\n",
      "Epoch: 44, Time: 0.03219s, Loss: 1.21099\n",
      "Epoch: 45, Time: 0.03725s, Loss: 1.21718\n",
      "Epoch: 46, Time: 0.03496s, Loss: 1.21354\n",
      "Epoch: 47, Time: 0.03686s, Loss: 1.21598\n",
      "Epoch: 48, Time: 0.03320s, Loss: 1.21522\n",
      "Epoch: 49, Time: 0.03228s, Loss: 1.21544\n",
      "Epoch: 50, Time: 0.03789s, Loss: 1.21263\n",
      "Epoch: 51, Time: 0.03674s, Loss: 1.21300\n",
      "Epoch: 52, Time: 0.04046s, Loss: 1.21515\n",
      "Epoch: 53, Time: 0.03730s, Loss: 1.21489\n",
      "Epoch: 54, Time: 0.03583s, Loss: 1.21176\n",
      "Epoch: 55, Time: 0.03431s, Loss: 1.21027\n",
      "Epoch: 56, Time: 0.03328s, Loss: 1.21404\n",
      "Epoch: 57, Time: 0.04420s, Loss: 1.21453\n",
      "Epoch: 58, Time: 0.03932s, Loss: 1.21515\n",
      "Epoch: 59, Time: 0.03825s, Loss: 1.21408\n",
      "Epoch: 60, Time: 0.03916s, Loss: 1.21357\n",
      "Epoch: 61, Time: 0.03472s, Loss: 1.21211\n",
      "Epoch: 62, Time: 0.04798s, Loss: 1.21363\n",
      "Epoch: 63, Time: 0.04082s, Loss: 1.21398\n",
      "Epoch: 64, Time: 0.02978s, Loss: 1.20903\n",
      "Epoch: 65, Time: 0.04028s, Loss: 1.21284\n",
      "Epoch: 66, Time: 0.05101s, Loss: 1.21177\n",
      "Epoch: 67, Time: 0.04109s, Loss: 1.21080\n",
      "Epoch: 68, Time: 0.04198s, Loss: 1.21128\n",
      "Epoch: 69, Time: 0.04739s, Loss: 1.21174\n",
      "Epoch: 70, Time: 0.06118s, Loss: 1.21361\n",
      "Epoch: 71, Time: 0.04101s, Loss: 1.21197\n",
      "Epoch: 72, Time: 0.04334s, Loss: 1.21237\n",
      "Epoch: 73, Time: 0.03728s, Loss: 1.20928\n",
      "Epoch: 74, Time: 0.04233s, Loss: 1.20995\n",
      "Epoch: 75, Time: 0.03571s, Loss: 1.20946\n",
      "Epoch: 76, Time: 0.03878s, Loss: 1.20757\n",
      "Epoch: 77, Time: 0.03518s, Loss: 1.21002\n",
      "Epoch: 78, Time: 0.03619s, Loss: 1.21217\n",
      "Epoch: 79, Time: 0.03672s, Loss: 1.20694\n",
      "Epoch: 80, Time: 0.03873s, Loss: 1.20697\n",
      "Epoch: 81, Time: 0.03776s, Loss: 1.20873\n",
      "Epoch: 82, Time: 0.03677s, Loss: 1.20935\n",
      "Epoch: 83, Time: 0.03865s, Loss: 1.20774\n",
      "Epoch: 84, Time: 0.04602s, Loss: 1.21129\n",
      "Epoch: 85, Time: 0.03394s, Loss: 1.21122\n",
      "Epoch: 86, Time: 0.03317s, Loss: 1.20868\n",
      "Epoch: 87, Time: 0.04118s, Loss: 1.20828\n",
      "Epoch: 88, Time: 0.03644s, Loss: 1.20538\n",
      "Epoch: 89, Time: 0.04433s, Loss: 1.20891\n",
      "Epoch: 90, Time: 0.03887s, Loss: 1.20490\n",
      "Epoch: 91, Time: 0.03985s, Loss: 1.20840\n",
      "Epoch: 92, Time: 0.03788s, Loss: 1.21133\n",
      "Epoch: 93, Time: 0.03695s, Loss: 1.20740\n",
      "Epoch: 94, Time: 0.03924s, Loss: 1.21048\n",
      "Epoch: 95, Time: 0.03673s, Loss: 1.21101\n",
      "Epoch: 96, Time: 0.04693s, Loss: 1.20506\n",
      "Epoch: 97, Time: 0.03976s, Loss: 1.20453\n",
      "Epoch: 98, Time: 0.06329s, Loss: 1.20841\n",
      "Epoch: 99, Time: 0.03994s, Loss: 1.20633\n",
      "    ↳ UniGCN Fold Result — Acc: 0.3801, F1: 0.3801\n",
      "\n",
      "[CocitationCiteseer] Fold 2/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.03148s, Loss: 1.79929\n",
      "Epoch: 1, Time: 0.02253s, Loss: 1.79473\n",
      "Epoch: 2, Time: 0.10709s, Loss: 1.78915\n",
      "Epoch: 3, Time: 0.04409s, Loss: 1.78202\n",
      "Epoch: 4, Time: 0.02900s, Loss: 1.77463\n",
      "Epoch: 5, Time: 0.02854s, Loss: 1.76545\n",
      "Epoch: 6, Time: 0.02552s, Loss: 1.75603\n",
      "Epoch: 7, Time: 0.02869s, Loss: 1.74553\n",
      "Epoch: 8, Time: 0.02805s, Loss: 1.73488\n",
      "Epoch: 9, Time: 0.02501s, Loss: 1.72355\n",
      "Epoch: 10, Time: 0.02954s, Loss: 1.71286\n",
      "Epoch: 11, Time: 0.02952s, Loss: 1.69851\n",
      "Epoch: 12, Time: 0.02553s, Loss: 1.68764\n",
      "Epoch: 13, Time: 0.02653s, Loss: 1.67405\n",
      "Epoch: 14, Time: 0.02953s, Loss: 1.66438\n",
      "Epoch: 15, Time: 0.02954s, Loss: 1.64950\n",
      "Epoch: 16, Time: 0.02753s, Loss: 1.63470\n",
      "Epoch: 17, Time: 0.02952s, Loss: 1.62267\n",
      "Epoch: 18, Time: 0.02451s, Loss: 1.61108\n",
      "Epoch: 19, Time: 0.02899s, Loss: 1.59800\n",
      "Epoch: 20, Time: 0.03054s, Loss: 1.58164\n",
      "Epoch: 21, Time: 0.02600s, Loss: 1.56970\n",
      "Epoch: 22, Time: 0.02952s, Loss: 1.55800\n",
      "Epoch: 23, Time: 0.03006s, Loss: 1.54682\n",
      "Epoch: 24, Time: 0.02552s, Loss: 1.53310\n",
      "Epoch: 25, Time: 0.03106s, Loss: 1.51871\n",
      "Epoch: 26, Time: 0.03000s, Loss: 1.51082\n",
      "Epoch: 27, Time: 0.03055s, Loss: 1.50081\n",
      "Epoch: 28, Time: 0.02254s, Loss: 1.49258\n",
      "Epoch: 29, Time: 0.02352s, Loss: 1.48052\n",
      "Epoch: 30, Time: 0.02654s, Loss: 1.47151\n",
      "Epoch: 31, Time: 0.02753s, Loss: 1.46672\n",
      "Epoch: 32, Time: 0.03223s, Loss: 1.45722\n",
      "Epoch: 33, Time: 0.02852s, Loss: 1.45103\n",
      "Epoch: 34, Time: 0.02953s, Loss: 1.44422\n",
      "Epoch: 35, Time: 0.02752s, Loss: 1.43649\n",
      "Epoch: 36, Time: 0.03100s, Loss: 1.42971\n",
      "Epoch: 37, Time: 0.02902s, Loss: 1.42763\n",
      "Epoch: 38, Time: 0.02601s, Loss: 1.41878\n",
      "Epoch: 39, Time: 0.02953s, Loss: 1.41560\n",
      "Epoch: 40, Time: 0.03105s, Loss: 1.40958\n",
      "Epoch: 41, Time: 0.02853s, Loss: 1.40348\n",
      "Epoch: 42, Time: 0.03184s, Loss: 1.40156\n",
      "Epoch: 43, Time: 0.02900s, Loss: 1.39545\n",
      "Epoch: 44, Time: 0.02854s, Loss: 1.39344\n",
      "Epoch: 45, Time: 0.03052s, Loss: 1.38735\n",
      "Epoch: 46, Time: 0.02953s, Loss: 1.38593\n",
      "Epoch: 47, Time: 0.02704s, Loss: 1.37780\n",
      "Epoch: 48, Time: 0.03152s, Loss: 1.38215\n",
      "Epoch: 49, Time: 0.02739s, Loss: 1.37307\n",
      "Epoch: 50, Time: 0.02601s, Loss: 1.36926\n",
      "Epoch: 51, Time: 0.03001s, Loss: 1.36950\n",
      "Epoch: 52, Time: 0.03459s, Loss: 1.36342\n",
      "Epoch: 53, Time: 0.03152s, Loss: 1.36128\n",
      "Epoch: 54, Time: 0.03260s, Loss: 1.36074\n",
      "Epoch: 55, Time: 0.03052s, Loss: 1.35403\n",
      "Epoch: 56, Time: 0.03161s, Loss: 1.35600\n",
      "Epoch: 57, Time: 0.02953s, Loss: 1.35480\n",
      "Epoch: 58, Time: 0.03254s, Loss: 1.34922\n",
      "Epoch: 59, Time: 0.03000s, Loss: 1.34907\n",
      "Epoch: 60, Time: 0.03053s, Loss: 1.34546\n",
      "Epoch: 61, Time: 0.03262s, Loss: 1.34612\n",
      "Epoch: 62, Time: 0.03251s, Loss: 1.34208\n",
      "Epoch: 63, Time: 0.03353s, Loss: 1.34256\n",
      "Epoch: 64, Time: 0.03253s, Loss: 1.34088\n",
      "Epoch: 65, Time: 0.03053s, Loss: 1.34183\n",
      "Epoch: 66, Time: 0.03171s, Loss: 1.33585\n",
      "Epoch: 67, Time: 0.02306s, Loss: 1.33411\n",
      "Epoch: 68, Time: 0.03052s, Loss: 1.33267\n",
      "Epoch: 69, Time: 0.03306s, Loss: 1.33321\n",
      "Epoch: 70, Time: 0.03200s, Loss: 1.33681\n",
      "Epoch: 71, Time: 0.02752s, Loss: 1.32943\n",
      "Epoch: 72, Time: 0.02801s, Loss: 1.32914\n",
      "Epoch: 73, Time: 0.03355s, Loss: 1.32788\n",
      "Epoch: 74, Time: 0.02800s, Loss: 1.32804\n",
      "Epoch: 75, Time: 0.03053s, Loss: 1.32528\n",
      "Epoch: 76, Time: 0.03104s, Loss: 1.32449\n",
      "Epoch: 77, Time: 0.02854s, Loss: 1.32711\n",
      "Epoch: 78, Time: 0.02802s, Loss: 1.32274\n",
      "Epoch: 79, Time: 0.02854s, Loss: 1.32188\n",
      "Epoch: 80, Time: 0.02853s, Loss: 1.32196\n",
      "Epoch: 81, Time: 0.03204s, Loss: 1.32059\n",
      "Epoch: 82, Time: 0.03152s, Loss: 1.31930\n",
      "Epoch: 83, Time: 0.02602s, Loss: 1.31703\n",
      "Epoch: 84, Time: 0.03153s, Loss: 1.31628\n",
      "Epoch: 85, Time: 0.03055s, Loss: 1.31274\n",
      "Epoch: 86, Time: 0.03102s, Loss: 1.31720\n",
      "Epoch: 87, Time: 0.03176s, Loss: 1.31556\n",
      "Epoch: 88, Time: 0.03353s, Loss: 1.31375\n",
      "Epoch: 89, Time: 0.03401s, Loss: 1.31403\n",
      "Epoch: 90, Time: 0.03153s, Loss: 1.31445\n",
      "Epoch: 91, Time: 0.03154s, Loss: 1.31332\n",
      "Epoch: 92, Time: 0.03152s, Loss: 1.31337\n",
      "Epoch: 93, Time: 0.03455s, Loss: 1.31258\n",
      "Epoch: 94, Time: 0.03103s, Loss: 1.31158\n",
      "Epoch: 95, Time: 0.03352s, Loss: 1.31158\n",
      "Epoch: 96, Time: 0.03506s, Loss: 1.30731\n",
      "Epoch: 97, Time: 0.03552s, Loss: 1.30693\n",
      "Epoch: 98, Time: 0.04703s, Loss: 1.30772\n",
      "Epoch: 99, Time: 0.03054s, Loss: 1.30722\n",
      "    ↳ HGNN Fold Result — Acc: 0.3725, F1: 0.3725\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03400s, Loss: 1.81715\n",
      "Epoch: 1, Time: 0.03354s, Loss: 1.47129\n",
      "Epoch: 2, Time: 0.03201s, Loss: 1.33836\n",
      "Epoch: 3, Time: 0.03601s, Loss: 1.29496\n",
      "Epoch: 4, Time: 0.03304s, Loss: 1.27196\n",
      "Epoch: 5, Time: 0.02554s, Loss: 1.25512\n",
      "Epoch: 6, Time: 0.03454s, Loss: 1.25018\n",
      "Epoch: 7, Time: 0.03153s, Loss: 1.24057\n",
      "Epoch: 8, Time: 0.03253s, Loss: 1.23475\n",
      "Epoch: 9, Time: 0.03152s, Loss: 1.23211\n",
      "Epoch: 10, Time: 0.04428s, Loss: 1.22591\n",
      "Epoch: 11, Time: 0.03053s, Loss: 1.22182\n",
      "Epoch: 12, Time: 0.03550s, Loss: 1.22469\n",
      "Epoch: 13, Time: 0.03401s, Loss: 1.21686\n",
      "Epoch: 14, Time: 0.03351s, Loss: 1.21742\n",
      "Epoch: 15, Time: 0.03454s, Loss: 1.21666\n",
      "Epoch: 16, Time: 0.03200s, Loss: 1.21491\n",
      "Epoch: 17, Time: 0.02755s, Loss: 1.21594\n",
      "Epoch: 18, Time: 0.03256s, Loss: 1.21029\n",
      "Epoch: 19, Time: 0.03255s, Loss: 1.20818\n",
      "Epoch: 20, Time: 0.03381s, Loss: 1.20889\n",
      "Epoch: 21, Time: 0.02751s, Loss: 1.20962\n",
      "Epoch: 22, Time: 0.03107s, Loss: 1.20715\n",
      "Epoch: 23, Time: 0.03452s, Loss: 1.20635\n",
      "Epoch: 24, Time: 0.03353s, Loss: 1.20514\n",
      "Epoch: 25, Time: 0.03000s, Loss: 1.20804\n",
      "Epoch: 26, Time: 0.03153s, Loss: 1.20457\n",
      "Epoch: 27, Time: 0.03600s, Loss: 1.20257\n",
      "Epoch: 28, Time: 0.03101s, Loss: 1.20586\n",
      "Epoch: 29, Time: 0.03501s, Loss: 1.20229\n",
      "Epoch: 30, Time: 0.03341s, Loss: 1.20222\n",
      "Epoch: 31, Time: 0.03400s, Loss: 1.20450\n",
      "Epoch: 32, Time: 0.03554s, Loss: 1.20427\n",
      "Epoch: 33, Time: 0.03408s, Loss: 1.20365\n",
      "Epoch: 34, Time: 0.03502s, Loss: 1.20043\n",
      "Epoch: 35, Time: 0.02752s, Loss: 1.20305\n",
      "Epoch: 36, Time: 0.02853s, Loss: 1.20136\n",
      "Epoch: 37, Time: 0.03104s, Loss: 1.19992\n",
      "Epoch: 38, Time: 0.03200s, Loss: 1.19928\n",
      "Epoch: 39, Time: 0.03653s, Loss: 1.20076\n",
      "Epoch: 40, Time: 0.02900s, Loss: 1.19931\n",
      "Epoch: 41, Time: 0.03354s, Loss: 1.20275\n",
      "Epoch: 42, Time: 0.02800s, Loss: 1.19803\n",
      "Epoch: 43, Time: 0.03107s, Loss: 1.19950\n",
      "Epoch: 44, Time: 0.03107s, Loss: 1.19913\n",
      "Epoch: 45, Time: 0.03351s, Loss: 1.19760\n",
      "Epoch: 46, Time: 0.03205s, Loss: 1.19918\n",
      "Epoch: 47, Time: 0.03354s, Loss: 1.20115\n",
      "Epoch: 48, Time: 0.03309s, Loss: 1.19869\n",
      "Epoch: 49, Time: 0.03300s, Loss: 1.19805\n",
      "Epoch: 50, Time: 0.03052s, Loss: 1.19900\n",
      "Epoch: 51, Time: 0.03000s, Loss: 1.19737\n",
      "Epoch: 52, Time: 0.03502s, Loss: 1.19982\n",
      "Epoch: 53, Time: 0.03001s, Loss: 1.19681\n",
      "Epoch: 54, Time: 0.03352s, Loss: 1.19804\n",
      "Epoch: 55, Time: 0.03053s, Loss: 1.19581\n",
      "Epoch: 56, Time: 0.03252s, Loss: 1.19704\n",
      "Epoch: 57, Time: 0.03005s, Loss: 1.19803\n",
      "Epoch: 58, Time: 0.03452s, Loss: 1.19813\n",
      "Epoch: 59, Time: 0.02898s, Loss: 1.19863\n",
      "Epoch: 60, Time: 0.03500s, Loss: 1.19953\n",
      "Epoch: 61, Time: 0.03454s, Loss: 1.19835\n",
      "Epoch: 62, Time: 0.03200s, Loss: 1.19627\n",
      "Epoch: 63, Time: 0.02900s, Loss: 1.19629\n",
      "Epoch: 64, Time: 0.03500s, Loss: 1.19960\n",
      "Epoch: 65, Time: 0.03154s, Loss: 1.19611\n",
      "Epoch: 66, Time: 0.03401s, Loss: 1.19995\n",
      "Epoch: 67, Time: 0.03502s, Loss: 1.19691\n",
      "Epoch: 68, Time: 0.03753s, Loss: 1.19665\n",
      "Epoch: 69, Time: 0.03456s, Loss: 1.19592\n",
      "Epoch: 70, Time: 0.03601s, Loss: 1.19516\n",
      "Epoch: 71, Time: 0.03553s, Loss: 1.19708\n",
      "Epoch: 72, Time: 0.03452s, Loss: 1.19713\n",
      "Epoch: 73, Time: 0.02654s, Loss: 1.19647\n",
      "Epoch: 74, Time: 0.03505s, Loss: 1.19856\n",
      "Epoch: 75, Time: 0.03352s, Loss: 1.19433\n",
      "Epoch: 76, Time: 0.03106s, Loss: 1.19570\n",
      "Epoch: 77, Time: 0.03552s, Loss: 1.19562\n",
      "Epoch: 78, Time: 0.03304s, Loss: 1.19524\n",
      "Epoch: 79, Time: 0.03157s, Loss: 1.19615\n",
      "Epoch: 80, Time: 0.03306s, Loss: 1.19498\n",
      "Epoch: 81, Time: 0.02900s, Loss: 1.19495\n",
      "Epoch: 82, Time: 0.03254s, Loss: 1.19506\n",
      "Epoch: 83, Time: 0.03100s, Loss: 1.19537\n",
      "Epoch: 84, Time: 0.03000s, Loss: 1.19277\n",
      "Epoch: 85, Time: 0.03200s, Loss: 1.19452\n",
      "Epoch: 86, Time: 0.03154s, Loss: 1.19579\n",
      "Epoch: 87, Time: 0.04753s, Loss: 1.19356\n",
      "Epoch: 88, Time: 0.02852s, Loss: 1.19765\n",
      "Epoch: 89, Time: 0.03200s, Loss: 1.19746\n",
      "Epoch: 90, Time: 0.03102s, Loss: 1.19734\n",
      "Epoch: 91, Time: 0.03500s, Loss: 1.19380\n",
      "Epoch: 92, Time: 0.03654s, Loss: 1.19386\n",
      "Epoch: 93, Time: 0.03501s, Loss: 1.19504\n",
      "Epoch: 94, Time: 0.03301s, Loss: 1.19374\n",
      "Epoch: 95, Time: 0.03452s, Loss: 1.19436\n",
      "Epoch: 96, Time: 0.03201s, Loss: 1.19843\n",
      "Epoch: 97, Time: 0.03501s, Loss: 1.19727\n",
      "Epoch: 98, Time: 0.03401s, Loss: 1.19626\n",
      "Epoch: 99, Time: 0.03453s, Loss: 1.19615\n",
      "    ↳ HGNNP Fold Result — Acc: 0.3620, F1: 0.3620\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03105s, Loss: 1.87183\n",
      "Epoch: 1, Time: 0.03100s, Loss: 1.50139\n",
      "Epoch: 2, Time: 0.03253s, Loss: 1.36480\n",
      "Epoch: 3, Time: 0.03000s, Loss: 1.32477\n",
      "Epoch: 4, Time: 0.03152s, Loss: 1.29950\n",
      "Epoch: 5, Time: 0.03201s, Loss: 1.28679\n",
      "Epoch: 6, Time: 0.03151s, Loss: 1.27438\n",
      "Epoch: 7, Time: 0.02669s, Loss: 1.26721\n",
      "Epoch: 8, Time: 0.02700s, Loss: 1.25951\n",
      "Epoch: 9, Time: 0.03053s, Loss: 1.25640\n",
      "Epoch: 10, Time: 0.03358s, Loss: 1.25235\n",
      "Epoch: 11, Time: 0.03206s, Loss: 1.24453\n",
      "Epoch: 12, Time: 0.03251s, Loss: 1.24706\n",
      "Epoch: 13, Time: 0.03405s, Loss: 1.24244\n",
      "Epoch: 14, Time: 0.02552s, Loss: 1.23723\n",
      "Epoch: 15, Time: 0.03004s, Loss: 1.23379\n",
      "Epoch: 16, Time: 0.03153s, Loss: 1.23843\n",
      "Epoch: 17, Time: 0.03205s, Loss: 1.23516\n",
      "Epoch: 18, Time: 0.03200s, Loss: 1.23427\n",
      "Epoch: 19, Time: 0.03678s, Loss: 1.22710\n",
      "Epoch: 20, Time: 0.03353s, Loss: 1.23059\n",
      "Epoch: 21, Time: 0.03904s, Loss: 1.22686\n",
      "Epoch: 22, Time: 0.03100s, Loss: 1.22778\n",
      "Epoch: 23, Time: 0.03279s, Loss: 1.22505\n",
      "Epoch: 24, Time: 0.03552s, Loss: 1.22244\n",
      "Epoch: 25, Time: 0.02954s, Loss: 1.22197\n",
      "Epoch: 26, Time: 0.03200s, Loss: 1.22602\n",
      "Epoch: 27, Time: 0.03504s, Loss: 1.22260\n",
      "Epoch: 28, Time: 0.03401s, Loss: 1.22164\n",
      "Epoch: 29, Time: 0.03153s, Loss: 1.22064\n",
      "Epoch: 30, Time: 0.03031s, Loss: 1.21816\n",
      "Epoch: 31, Time: 0.03302s, Loss: 1.21907\n",
      "Epoch: 32, Time: 0.03201s, Loss: 1.21773\n",
      "Epoch: 33, Time: 0.03300s, Loss: 1.21690\n",
      "Epoch: 34, Time: 0.03600s, Loss: 1.21792\n",
      "Epoch: 35, Time: 0.03401s, Loss: 1.21590\n",
      "Epoch: 36, Time: 0.03453s, Loss: 1.21578\n",
      "Epoch: 37, Time: 0.03501s, Loss: 1.21556\n",
      "Epoch: 38, Time: 0.03400s, Loss: 1.21604\n",
      "Epoch: 39, Time: 0.03354s, Loss: 1.21545\n",
      "Epoch: 40, Time: 0.03353s, Loss: 1.21448\n",
      "Epoch: 41, Time: 0.03252s, Loss: 1.21375\n",
      "Epoch: 42, Time: 0.03403s, Loss: 1.21451\n",
      "Epoch: 43, Time: 0.03152s, Loss: 1.21258\n",
      "Epoch: 44, Time: 0.03206s, Loss: 1.21297\n",
      "Epoch: 45, Time: 0.03352s, Loss: 1.21470\n",
      "Epoch: 46, Time: 0.03211s, Loss: 1.21468\n",
      "Epoch: 47, Time: 0.03452s, Loss: 1.21214\n",
      "Epoch: 48, Time: 0.03105s, Loss: 1.20942\n",
      "Epoch: 49, Time: 0.03052s, Loss: 1.21556\n",
      "Epoch: 50, Time: 0.03504s, Loss: 1.21207\n",
      "Epoch: 51, Time: 0.03452s, Loss: 1.21145\n",
      "Epoch: 52, Time: 0.03153s, Loss: 1.21078\n",
      "Epoch: 53, Time: 0.03152s, Loss: 1.20873\n",
      "Epoch: 54, Time: 0.03554s, Loss: 1.21127\n",
      "Epoch: 55, Time: 0.02800s, Loss: 1.21245\n",
      "Epoch: 56, Time: 0.03202s, Loss: 1.21256\n",
      "Epoch: 57, Time: 0.03200s, Loss: 1.20547\n",
      "Epoch: 58, Time: 0.03201s, Loss: 1.20942\n",
      "Epoch: 59, Time: 0.02900s, Loss: 1.20677\n",
      "Epoch: 60, Time: 0.02953s, Loss: 1.20730\n",
      "Epoch: 61, Time: 0.03405s, Loss: 1.20671\n",
      "Epoch: 62, Time: 0.03253s, Loss: 1.20866\n",
      "Epoch: 63, Time: 0.03203s, Loss: 1.20632\n",
      "Epoch: 64, Time: 0.03152s, Loss: 1.21097\n",
      "Epoch: 65, Time: 0.03104s, Loss: 1.20999\n",
      "Epoch: 66, Time: 0.02900s, Loss: 1.20854\n",
      "Epoch: 67, Time: 0.03252s, Loss: 1.20489\n",
      "Epoch: 68, Time: 0.03653s, Loss: 1.20550\n",
      "Epoch: 69, Time: 0.03554s, Loss: 1.20709\n",
      "Epoch: 70, Time: 0.03352s, Loss: 1.20597\n",
      "Epoch: 71, Time: 0.05263s, Loss: 1.20815\n",
      "Epoch: 72, Time: 0.03059s, Loss: 1.20577\n",
      "Epoch: 73, Time: 0.02894s, Loss: 1.20493\n",
      "Epoch: 74, Time: 0.03152s, Loss: 1.20511\n",
      "Epoch: 75, Time: 0.03594s, Loss: 1.20827\n",
      "Epoch: 76, Time: 0.03354s, Loss: 1.20686\n",
      "Epoch: 77, Time: 0.04407s, Loss: 1.20686\n",
      "Epoch: 78, Time: 0.03453s, Loss: 1.20926\n",
      "Epoch: 79, Time: 0.03208s, Loss: 1.20535\n",
      "Epoch: 80, Time: 0.03552s, Loss: 1.20755\n",
      "Epoch: 81, Time: 0.03406s, Loss: 1.20994\n",
      "Epoch: 82, Time: 0.03252s, Loss: 1.20673\n",
      "Epoch: 83, Time: 0.02703s, Loss: 1.20751\n",
      "Epoch: 84, Time: 0.03553s, Loss: 1.20721\n",
      "Epoch: 85, Time: 0.03306s, Loss: 1.20712\n",
      "Epoch: 86, Time: 0.03452s, Loss: 1.20402\n",
      "Epoch: 87, Time: 0.03405s, Loss: 1.20848\n",
      "Epoch: 88, Time: 0.02653s, Loss: 1.20519\n",
      "Epoch: 89, Time: 0.03553s, Loss: 1.20393\n",
      "Epoch: 90, Time: 0.02971s, Loss: 1.20659\n",
      "Epoch: 91, Time: 0.03204s, Loss: 1.20296\n",
      "Epoch: 92, Time: 0.03400s, Loss: 1.20419\n",
      "Epoch: 93, Time: 0.03101s, Loss: 1.20390\n",
      "Epoch: 94, Time: 0.03100s, Loss: 1.20475\n",
      "Epoch: 95, Time: 0.03801s, Loss: 1.20589\n",
      "Epoch: 96, Time: 0.03401s, Loss: 1.20402\n",
      "Epoch: 97, Time: 0.03501s, Loss: 1.20317\n",
      "Epoch: 98, Time: 0.03004s, Loss: 1.20482\n",
      "Epoch: 99, Time: 0.03655s, Loss: 1.20554\n",
      "    ↳ UniGCN Fold Result — Acc: 0.3620, F1: 0.3620\n",
      "\n",
      "[CocitationCiteseer] Fold 3/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.02969s, Loss: 1.79072\n",
      "Epoch: 1, Time: 0.02297s, Loss: 1.78653\n",
      "Epoch: 2, Time: 0.02053s, Loss: 1.78122\n",
      "Epoch: 3, Time: 0.02402s, Loss: 1.77427\n",
      "Epoch: 4, Time: 0.02304s, Loss: 1.76668\n",
      "Epoch: 5, Time: 0.02054s, Loss: 1.75986\n",
      "Epoch: 6, Time: 0.02201s, Loss: 1.74995\n",
      "Epoch: 7, Time: 0.02197s, Loss: 1.74056\n",
      "Epoch: 8, Time: 0.02553s, Loss: 1.72949\n",
      "Epoch: 9, Time: 0.02552s, Loss: 1.71797\n",
      "Epoch: 10, Time: 0.02651s, Loss: 1.70622\n",
      "Epoch: 11, Time: 0.02353s, Loss: 1.69256\n",
      "Epoch: 12, Time: 0.02553s, Loss: 1.68271\n",
      "Epoch: 13, Time: 0.02660s, Loss: 1.67018\n",
      "Epoch: 14, Time: 0.02059s, Loss: 1.65849\n",
      "Epoch: 15, Time: 0.02796s, Loss: 1.64695\n",
      "Epoch: 16, Time: 0.02457s, Loss: 1.63282\n",
      "Epoch: 17, Time: 0.02553s, Loss: 1.62203\n",
      "Epoch: 18, Time: 0.02566s, Loss: 1.60727\n",
      "Epoch: 19, Time: 0.02725s, Loss: 1.59388\n",
      "Epoch: 20, Time: 0.02500s, Loss: 1.58256\n",
      "Epoch: 21, Time: 0.02862s, Loss: 1.56761\n",
      "Epoch: 22, Time: 0.02849s, Loss: 1.55803\n",
      "Epoch: 23, Time: 0.02597s, Loss: 1.54526\n",
      "Epoch: 24, Time: 0.03065s, Loss: 1.53430\n",
      "Epoch: 25, Time: 0.02453s, Loss: 1.52308\n",
      "Epoch: 26, Time: 0.02957s, Loss: 1.51413\n",
      "Epoch: 27, Time: 0.02859s, Loss: 1.50493\n",
      "Epoch: 28, Time: 0.02854s, Loss: 1.49625\n",
      "Epoch: 29, Time: 0.02740s, Loss: 1.48948\n",
      "Epoch: 30, Time: 0.02853s, Loss: 1.48000\n",
      "Epoch: 31, Time: 0.02863s, Loss: 1.47502\n",
      "Epoch: 32, Time: 0.02253s, Loss: 1.46219\n",
      "Epoch: 33, Time: 0.02300s, Loss: 1.46068\n",
      "Epoch: 34, Time: 0.02689s, Loss: 1.45106\n",
      "Epoch: 35, Time: 0.02353s, Loss: 1.45122\n",
      "Epoch: 36, Time: 0.02553s, Loss: 1.44222\n",
      "Epoch: 37, Time: 0.03203s, Loss: 1.43767\n",
      "Epoch: 38, Time: 0.03201s, Loss: 1.43795\n",
      "Epoch: 39, Time: 0.02952s, Loss: 1.43181\n",
      "Epoch: 40, Time: 0.03197s, Loss: 1.42975\n",
      "Epoch: 41, Time: 0.03354s, Loss: 1.42211\n",
      "Epoch: 42, Time: 0.03052s, Loss: 1.42094\n",
      "Epoch: 43, Time: 0.02553s, Loss: 1.41550\n",
      "Epoch: 44, Time: 0.02752s, Loss: 1.41307\n",
      "Epoch: 45, Time: 0.03053s, Loss: 1.40883\n",
      "Epoch: 46, Time: 0.03235s, Loss: 1.40431\n",
      "Epoch: 47, Time: 0.03000s, Loss: 1.40646\n",
      "Epoch: 48, Time: 0.02251s, Loss: 1.39909\n",
      "Epoch: 49, Time: 0.03154s, Loss: 1.39378\n",
      "Epoch: 50, Time: 0.03051s, Loss: 1.39559\n",
      "Epoch: 51, Time: 0.02656s, Loss: 1.38988\n",
      "Epoch: 52, Time: 0.02600s, Loss: 1.38851\n",
      "Epoch: 53, Time: 0.03155s, Loss: 1.38151\n",
      "Epoch: 54, Time: 0.03200s, Loss: 1.38589\n",
      "Epoch: 55, Time: 0.02909s, Loss: 1.38246\n",
      "Epoch: 56, Time: 0.02853s, Loss: 1.38075\n",
      "Epoch: 57, Time: 0.03053s, Loss: 1.37552\n",
      "Epoch: 58, Time: 0.03051s, Loss: 1.37688\n",
      "Epoch: 59, Time: 0.02401s, Loss: 1.37313\n",
      "Epoch: 60, Time: 0.03210s, Loss: 1.37402\n",
      "Epoch: 61, Time: 0.02553s, Loss: 1.37018\n",
      "Epoch: 62, Time: 0.02661s, Loss: 1.36944\n",
      "Epoch: 63, Time: 0.02600s, Loss: 1.37175\n",
      "Epoch: 64, Time: 0.02901s, Loss: 1.36678\n",
      "Epoch: 65, Time: 0.03012s, Loss: 1.36501\n",
      "Epoch: 66, Time: 0.02855s, Loss: 1.36324\n",
      "Epoch: 67, Time: 0.03060s, Loss: 1.36297\n",
      "Epoch: 68, Time: 0.02300s, Loss: 1.36158\n",
      "Epoch: 69, Time: 0.02601s, Loss: 1.35984\n",
      "Epoch: 70, Time: 0.02957s, Loss: 1.35869\n",
      "Epoch: 71, Time: 0.03004s, Loss: 1.35593\n",
      "Epoch: 72, Time: 0.02903s, Loss: 1.35362\n",
      "Epoch: 73, Time: 0.02452s, Loss: 1.35264\n",
      "Epoch: 74, Time: 0.02299s, Loss: 1.35574\n",
      "Epoch: 75, Time: 0.02554s, Loss: 1.35101\n",
      "Epoch: 76, Time: 0.02904s, Loss: 1.35169\n",
      "Epoch: 77, Time: 0.02354s, Loss: 1.34910\n",
      "Epoch: 78, Time: 0.02952s, Loss: 1.35028\n",
      "Epoch: 79, Time: 0.02704s, Loss: 1.34847\n",
      "Epoch: 80, Time: 0.02454s, Loss: 1.34576\n",
      "Epoch: 81, Time: 0.02760s, Loss: 1.34610\n",
      "Epoch: 82, Time: 0.02951s, Loss: 1.34508\n",
      "Epoch: 83, Time: 0.02553s, Loss: 1.34395\n",
      "Epoch: 84, Time: 0.02953s, Loss: 1.34445\n",
      "Epoch: 85, Time: 0.02352s, Loss: 1.34277\n",
      "Epoch: 86, Time: 0.02753s, Loss: 1.34515\n",
      "Epoch: 87, Time: 0.02653s, Loss: 1.34003\n",
      "Epoch: 88, Time: 0.03053s, Loss: 1.34183\n",
      "Epoch: 89, Time: 0.03302s, Loss: 1.34107\n",
      "Epoch: 90, Time: 0.02954s, Loss: 1.33879\n",
      "Epoch: 91, Time: 0.02900s, Loss: 1.33926\n",
      "Epoch: 92, Time: 0.02455s, Loss: 1.33761\n",
      "Epoch: 93, Time: 0.02554s, Loss: 1.33648\n",
      "Epoch: 94, Time: 0.02701s, Loss: 1.33827\n",
      "Epoch: 95, Time: 0.03161s, Loss: 1.33541\n",
      "Epoch: 96, Time: 0.02800s, Loss: 1.33489\n",
      "Epoch: 97, Time: 0.02253s, Loss: 1.33823\n",
      "Epoch: 98, Time: 0.03064s, Loss: 1.33601\n",
      "Epoch: 99, Time: 0.02454s, Loss: 1.33507\n",
      "    ↳ HGNN Fold Result — Acc: 0.4063, F1: 0.4063\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03356s, Loss: 1.81313\n",
      "Epoch: 1, Time: 0.03450s, Loss: 1.48767\n",
      "Epoch: 2, Time: 0.02963s, Loss: 1.36960\n",
      "Epoch: 3, Time: 0.03063s, Loss: 1.32920\n",
      "Epoch: 4, Time: 0.02877s, Loss: 1.30482\n",
      "Epoch: 5, Time: 0.02897s, Loss: 1.29134\n",
      "Epoch: 6, Time: 0.03454s, Loss: 1.27664\n",
      "Epoch: 7, Time: 0.03447s, Loss: 1.27418\n",
      "Epoch: 8, Time: 0.02856s, Loss: 1.26755\n",
      "Epoch: 9, Time: 0.03100s, Loss: 1.26135\n",
      "Epoch: 10, Time: 0.03052s, Loss: 1.25913\n",
      "Epoch: 11, Time: 0.02900s, Loss: 1.25584\n",
      "Epoch: 12, Time: 0.03253s, Loss: 1.25406\n",
      "Epoch: 13, Time: 0.03352s, Loss: 1.24859\n",
      "Epoch: 14, Time: 0.03205s, Loss: 1.24811\n",
      "Epoch: 15, Time: 0.03155s, Loss: 1.24775\n",
      "Epoch: 16, Time: 0.03252s, Loss: 1.24637\n",
      "Epoch: 17, Time: 0.03153s, Loss: 1.24264\n",
      "Epoch: 18, Time: 0.02954s, Loss: 1.24142\n",
      "Epoch: 19, Time: 0.03504s, Loss: 1.24154\n",
      "Epoch: 20, Time: 0.03200s, Loss: 1.24182\n",
      "Epoch: 21, Time: 0.03251s, Loss: 1.23986\n",
      "Epoch: 22, Time: 0.03100s, Loss: 1.23992\n",
      "Epoch: 23, Time: 0.03454s, Loss: 1.24098\n",
      "Epoch: 24, Time: 0.02901s, Loss: 1.23844\n",
      "Epoch: 25, Time: 0.03005s, Loss: 1.24036\n",
      "Epoch: 26, Time: 0.03253s, Loss: 1.23792\n",
      "Epoch: 27, Time: 0.03459s, Loss: 1.23665\n",
      "Epoch: 28, Time: 0.02951s, Loss: 1.23948\n",
      "Epoch: 29, Time: 0.03055s, Loss: 1.23645\n",
      "Epoch: 30, Time: 0.03502s, Loss: 1.23861\n",
      "Epoch: 31, Time: 0.02753s, Loss: 1.23513\n",
      "Epoch: 32, Time: 0.03352s, Loss: 1.23301\n",
      "Epoch: 33, Time: 0.02800s, Loss: 1.23644\n",
      "Epoch: 34, Time: 0.02755s, Loss: 1.23445\n",
      "Epoch: 35, Time: 0.02654s, Loss: 1.23305\n",
      "Epoch: 36, Time: 0.03502s, Loss: 1.23338\n",
      "Epoch: 37, Time: 0.03455s, Loss: 1.23448\n",
      "Epoch: 38, Time: 0.03252s, Loss: 1.23294\n",
      "Epoch: 39, Time: 0.03343s, Loss: 1.23288\n",
      "Epoch: 40, Time: 0.03252s, Loss: 1.23302\n",
      "Epoch: 41, Time: 0.03052s, Loss: 1.23304\n",
      "Epoch: 42, Time: 0.03048s, Loss: 1.23407\n",
      "Epoch: 43, Time: 0.03453s, Loss: 1.23120\n",
      "Epoch: 44, Time: 0.03501s, Loss: 1.23102\n",
      "Epoch: 45, Time: 0.04604s, Loss: 1.23293\n",
      "Epoch: 46, Time: 0.03553s, Loss: 1.23347\n",
      "Epoch: 47, Time: 0.03353s, Loss: 1.23202\n",
      "Epoch: 48, Time: 0.02900s, Loss: 1.23446\n",
      "Epoch: 49, Time: 0.03252s, Loss: 1.23252\n",
      "Epoch: 50, Time: 0.03401s, Loss: 1.23125\n",
      "Epoch: 51, Time: 0.03007s, Loss: 1.23021\n",
      "Epoch: 52, Time: 0.03153s, Loss: 1.23237\n",
      "Epoch: 53, Time: 0.03154s, Loss: 1.23023\n",
      "Epoch: 54, Time: 0.03253s, Loss: 1.23120\n",
      "Epoch: 55, Time: 0.03553s, Loss: 1.22997\n",
      "Epoch: 56, Time: 0.02682s, Loss: 1.23134\n",
      "Epoch: 57, Time: 0.03360s, Loss: 1.22878\n",
      "Epoch: 58, Time: 0.03506s, Loss: 1.22953\n",
      "Epoch: 59, Time: 0.02996s, Loss: 1.23054\n",
      "Epoch: 60, Time: 0.02756s, Loss: 1.23057\n",
      "Epoch: 61, Time: 0.03300s, Loss: 1.22968\n",
      "Epoch: 62, Time: 0.03153s, Loss: 1.22868\n",
      "Epoch: 63, Time: 0.03753s, Loss: 1.23009\n",
      "Epoch: 64, Time: 0.02952s, Loss: 1.23087\n",
      "Epoch: 65, Time: 0.03452s, Loss: 1.23018\n",
      "Epoch: 66, Time: 0.03354s, Loss: 1.23010\n",
      "Epoch: 67, Time: 0.03253s, Loss: 1.23015\n",
      "Epoch: 68, Time: 0.03354s, Loss: 1.23110\n",
      "Epoch: 69, Time: 0.02757s, Loss: 1.22976\n",
      "Epoch: 70, Time: 0.02860s, Loss: 1.23123\n",
      "Epoch: 71, Time: 0.03007s, Loss: 1.22835\n",
      "Epoch: 72, Time: 0.02500s, Loss: 1.22970\n",
      "Epoch: 73, Time: 0.03354s, Loss: 1.22930\n",
      "Epoch: 74, Time: 0.03100s, Loss: 1.22899\n",
      "Epoch: 75, Time: 0.03201s, Loss: 1.22981\n",
      "Epoch: 76, Time: 0.02754s, Loss: 1.23161\n",
      "Epoch: 77, Time: 0.03454s, Loss: 1.23046\n",
      "Epoch: 78, Time: 0.02952s, Loss: 1.23045\n",
      "Epoch: 79, Time: 0.03153s, Loss: 1.23009\n",
      "Epoch: 80, Time: 0.02705s, Loss: 1.23023\n",
      "Epoch: 81, Time: 0.02800s, Loss: 1.22997\n",
      "Epoch: 82, Time: 0.03055s, Loss: 1.22782\n",
      "Epoch: 83, Time: 0.02853s, Loss: 1.23040\n",
      "Epoch: 84, Time: 0.03154s, Loss: 1.23119\n",
      "Epoch: 85, Time: 0.03252s, Loss: 1.22893\n",
      "Epoch: 86, Time: 0.02853s, Loss: 1.22872\n",
      "Epoch: 87, Time: 0.02752s, Loss: 1.22981\n",
      "Epoch: 88, Time: 0.03052s, Loss: 1.22783\n",
      "Epoch: 89, Time: 0.03323s, Loss: 1.22683\n",
      "Epoch: 90, Time: 0.02997s, Loss: 1.22923\n",
      "Epoch: 91, Time: 0.02653s, Loss: 1.22877\n",
      "Epoch: 92, Time: 0.02959s, Loss: 1.22743\n",
      "Epoch: 93, Time: 0.03453s, Loss: 1.22813\n",
      "Epoch: 94, Time: 0.03073s, Loss: 1.23126\n",
      "Epoch: 95, Time: 0.03150s, Loss: 1.22988\n",
      "Epoch: 96, Time: 0.03212s, Loss: 1.22713\n",
      "Epoch: 97, Time: 0.03353s, Loss: 1.22973\n",
      "Epoch: 98, Time: 0.02855s, Loss: 1.22692\n",
      "Epoch: 99, Time: 0.03100s, Loss: 1.22804\n",
      "    ↳ HGNNP Fold Result — Acc: 0.4003, F1: 0.4003\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.03305s, Loss: 1.86637\n",
      "Epoch: 1, Time: 0.03252s, Loss: 1.49560\n",
      "Epoch: 2, Time: 0.02552s, Loss: 1.37466\n",
      "Epoch: 3, Time: 0.02701s, Loss: 1.33756\n",
      "Epoch: 4, Time: 0.03454s, Loss: 1.31979\n",
      "Epoch: 5, Time: 0.03001s, Loss: 1.30795\n",
      "Epoch: 6, Time: 0.03277s, Loss: 1.29903\n",
      "Epoch: 7, Time: 0.03500s, Loss: 1.29503\n",
      "Epoch: 8, Time: 0.02953s, Loss: 1.28923\n",
      "Epoch: 9, Time: 0.03100s, Loss: 1.28219\n",
      "Epoch: 10, Time: 0.03205s, Loss: 1.28088\n",
      "Epoch: 11, Time: 0.03453s, Loss: 1.27796\n",
      "Epoch: 12, Time: 0.03001s, Loss: 1.27137\n",
      "Epoch: 13, Time: 0.02960s, Loss: 1.26841\n",
      "Epoch: 14, Time: 0.03061s, Loss: 1.26500\n",
      "Epoch: 15, Time: 0.03351s, Loss: 1.26472\n",
      "Epoch: 16, Time: 0.02508s, Loss: 1.26314\n",
      "Epoch: 17, Time: 0.03252s, Loss: 1.26054\n",
      "Epoch: 18, Time: 0.03453s, Loss: 1.25939\n",
      "Epoch: 19, Time: 0.02853s, Loss: 1.25738\n",
      "Epoch: 20, Time: 0.02900s, Loss: 1.25597\n",
      "Epoch: 21, Time: 0.03481s, Loss: 1.25831\n",
      "Epoch: 22, Time: 0.03457s, Loss: 1.25423\n",
      "Epoch: 23, Time: 0.03054s, Loss: 1.25691\n",
      "Epoch: 24, Time: 0.03360s, Loss: 1.25064\n",
      "Epoch: 25, Time: 0.02860s, Loss: 1.25050\n",
      "Epoch: 26, Time: 0.03161s, Loss: 1.25215\n",
      "Epoch: 27, Time: 0.03553s, Loss: 1.24758\n",
      "Epoch: 28, Time: 0.03459s, Loss: 1.24980\n",
      "Epoch: 29, Time: 0.03354s, Loss: 1.24938\n",
      "Epoch: 30, Time: 0.03152s, Loss: 1.24604\n",
      "Epoch: 31, Time: 0.03159s, Loss: 1.24978\n",
      "Epoch: 32, Time: 0.03152s, Loss: 1.24759\n",
      "Epoch: 33, Time: 0.03253s, Loss: 1.24664\n",
      "Epoch: 34, Time: 0.03405s, Loss: 1.24498\n",
      "Epoch: 35, Time: 0.03355s, Loss: 1.24548\n",
      "Epoch: 36, Time: 0.03226s, Loss: 1.24689\n",
      "Epoch: 37, Time: 0.02800s, Loss: 1.24338\n",
      "Epoch: 38, Time: 0.03184s, Loss: 1.24364\n",
      "Epoch: 39, Time: 0.03100s, Loss: 1.24451\n",
      "Epoch: 40, Time: 0.03606s, Loss: 1.24220\n",
      "Epoch: 41, Time: 0.03356s, Loss: 1.24631\n",
      "Epoch: 42, Time: 0.03452s, Loss: 1.24401\n",
      "Epoch: 43, Time: 0.03300s, Loss: 1.24188\n",
      "Epoch: 44, Time: 0.03295s, Loss: 1.24110\n",
      "Epoch: 45, Time: 0.03153s, Loss: 1.23987\n",
      "Epoch: 46, Time: 0.02954s, Loss: 1.24348\n",
      "Epoch: 47, Time: 0.03157s, Loss: 1.24228\n",
      "Epoch: 48, Time: 0.02500s, Loss: 1.24313\n",
      "Epoch: 49, Time: 0.03211s, Loss: 1.24231\n",
      "Epoch: 50, Time: 0.02901s, Loss: 1.24031\n",
      "Epoch: 51, Time: 0.03052s, Loss: 1.24125\n",
      "Epoch: 52, Time: 0.03354s, Loss: 1.24501\n",
      "Epoch: 53, Time: 0.03411s, Loss: 1.24032\n",
      "Epoch: 54, Time: 0.03401s, Loss: 1.24273\n",
      "Epoch: 55, Time: 0.03412s, Loss: 1.23903\n",
      "Epoch: 56, Time: 0.03352s, Loss: 1.23502\n",
      "Epoch: 57, Time: 0.02857s, Loss: 1.23909\n",
      "Epoch: 58, Time: 0.02953s, Loss: 1.24297\n",
      "Epoch: 59, Time: 0.03653s, Loss: 1.23898\n",
      "Epoch: 60, Time: 0.02852s, Loss: 1.23752\n",
      "Epoch: 61, Time: 0.03252s, Loss: 1.23787\n",
      "Epoch: 62, Time: 0.03304s, Loss: 1.23916\n",
      "Epoch: 63, Time: 0.03551s, Loss: 1.24152\n",
      "Epoch: 64, Time: 0.03205s, Loss: 1.23802\n",
      "Epoch: 65, Time: 0.03252s, Loss: 1.23888\n",
      "Epoch: 66, Time: 0.03152s, Loss: 1.23765\n",
      "Epoch: 67, Time: 0.02800s, Loss: 1.23925\n",
      "Epoch: 68, Time: 0.03161s, Loss: 1.23710\n",
      "Epoch: 69, Time: 0.03500s, Loss: 1.23664\n",
      "Epoch: 70, Time: 0.03409s, Loss: 1.23830\n",
      "Epoch: 71, Time: 0.03352s, Loss: 1.23985\n",
      "Epoch: 72, Time: 0.02801s, Loss: 1.24102\n",
      "Epoch: 73, Time: 0.03401s, Loss: 1.23808\n",
      "Epoch: 74, Time: 0.02854s, Loss: 1.23726\n",
      "Epoch: 75, Time: 0.03252s, Loss: 1.23934\n",
      "Epoch: 76, Time: 0.02753s, Loss: 1.23870\n",
      "Epoch: 77, Time: 0.03305s, Loss: 1.23638\n",
      "Epoch: 78, Time: 0.03252s, Loss: 1.23987\n",
      "Epoch: 79, Time: 0.03065s, Loss: 1.23681\n",
      "Epoch: 80, Time: 0.03552s, Loss: 1.24102\n",
      "Epoch: 81, Time: 0.03505s, Loss: 1.23719\n",
      "Epoch: 82, Time: 0.03256s, Loss: 1.24088\n",
      "Epoch: 83, Time: 0.03104s, Loss: 1.23624\n",
      "Epoch: 84, Time: 0.03552s, Loss: 1.23612\n",
      "Epoch: 85, Time: 0.03138s, Loss: 1.23805\n",
      "Epoch: 86, Time: 0.03654s, Loss: 1.24655\n",
      "Epoch: 87, Time: 0.03004s, Loss: 1.23884\n",
      "Epoch: 88, Time: 0.03000s, Loss: 1.24217\n",
      "Epoch: 89, Time: 0.03107s, Loss: 1.23914\n",
      "Epoch: 90, Time: 0.02954s, Loss: 1.23708\n",
      "Epoch: 91, Time: 0.03356s, Loss: 1.23837\n",
      "Epoch: 92, Time: 0.03554s, Loss: 1.23881\n",
      "Epoch: 93, Time: 0.03055s, Loss: 1.23842\n",
      "Epoch: 94, Time: 0.02852s, Loss: 1.23429\n",
      "Epoch: 95, Time: 0.02951s, Loss: 1.23694\n",
      "Epoch: 96, Time: 0.03305s, Loss: 1.24130\n",
      "Epoch: 97, Time: 0.03253s, Loss: 1.23784\n",
      "Epoch: 98, Time: 0.03276s, Loss: 1.23770\n",
      "Epoch: 99, Time: 0.03553s, Loss: 1.23620\n",
      "    ↳ UniGCN Fold Result — Acc: 0.4048, F1: 0.4048\n",
      "\n",
      "[CocitationCiteseer] Fold 4/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.04348s, Loss: 1.78559\n",
      "Epoch: 1, Time: 0.02256s, Loss: 1.78149\n",
      "Epoch: 2, Time: 0.02500s, Loss: 1.77626\n",
      "Epoch: 3, Time: 0.02300s, Loss: 1.76952\n",
      "Epoch: 4, Time: 0.02504s, Loss: 1.76221\n",
      "Epoch: 5, Time: 0.02251s, Loss: 1.75436\n",
      "Epoch: 6, Time: 0.02652s, Loss: 1.74601\n",
      "Epoch: 7, Time: 0.02752s, Loss: 1.73560\n",
      "Epoch: 8, Time: 0.02553s, Loss: 1.72645\n",
      "Epoch: 9, Time: 0.02652s, Loss: 1.71542\n",
      "Epoch: 10, Time: 0.02454s, Loss: 1.70281\n",
      "Epoch: 11, Time: 0.02700s, Loss: 1.69155\n",
      "Epoch: 12, Time: 0.02059s, Loss: 1.67802\n",
      "Epoch: 13, Time: 0.02551s, Loss: 1.66481\n",
      "Epoch: 14, Time: 0.02653s, Loss: 1.65267\n",
      "Epoch: 15, Time: 0.02501s, Loss: 1.64224\n",
      "Epoch: 16, Time: 0.02501s, Loss: 1.62591\n",
      "Epoch: 17, Time: 0.02804s, Loss: 1.61266\n",
      "Epoch: 18, Time: 0.02653s, Loss: 1.59684\n",
      "Epoch: 19, Time: 0.02955s, Loss: 1.58557\n",
      "Epoch: 20, Time: 0.03252s, Loss: 1.56710\n",
      "Epoch: 21, Time: 0.02901s, Loss: 1.55772\n",
      "Epoch: 22, Time: 0.02352s, Loss: 1.54046\n",
      "Epoch: 23, Time: 0.02453s, Loss: 1.52852\n",
      "Epoch: 24, Time: 0.02953s, Loss: 1.51671\n",
      "Epoch: 25, Time: 0.02865s, Loss: 1.49943\n",
      "Epoch: 26, Time: 0.05553s, Loss: 1.49397\n",
      "Epoch: 27, Time: 0.04497s, Loss: 1.48230\n",
      "Epoch: 28, Time: 0.03353s, Loss: 1.47341\n",
      "Epoch: 29, Time: 0.03458s, Loss: 1.46048\n",
      "Epoch: 30, Time: 0.03670s, Loss: 1.45870\n",
      "Epoch: 31, Time: 0.03152s, Loss: 1.45017\n",
      "Epoch: 32, Time: 0.03154s, Loss: 1.44091\n",
      "Epoch: 33, Time: 0.03254s, Loss: 1.43163\n",
      "Epoch: 34, Time: 0.03254s, Loss: 1.43019\n",
      "Epoch: 35, Time: 0.03300s, Loss: 1.42033\n",
      "Epoch: 36, Time: 0.03405s, Loss: 1.41313\n",
      "Epoch: 37, Time: 0.03499s, Loss: 1.40984\n",
      "Epoch: 38, Time: 0.03451s, Loss: 1.40833\n",
      "Epoch: 39, Time: 0.03500s, Loss: 1.40339\n",
      "Epoch: 40, Time: 0.03635s, Loss: 1.39916\n",
      "Epoch: 41, Time: 0.03400s, Loss: 1.39251\n",
      "Epoch: 42, Time: 0.03555s, Loss: 1.39375\n",
      "Epoch: 43, Time: 0.03853s, Loss: 1.38808\n",
      "Epoch: 44, Time: 0.03553s, Loss: 1.38272\n",
      "Epoch: 45, Time: 0.03700s, Loss: 1.38147\n",
      "Epoch: 46, Time: 0.04205s, Loss: 1.37729\n",
      "Epoch: 47, Time: 0.03907s, Loss: 1.37366\n",
      "Epoch: 48, Time: 0.04122s, Loss: 1.36859\n",
      "Epoch: 49, Time: 0.04607s, Loss: 1.37201\n",
      "Epoch: 50, Time: 0.04000s, Loss: 1.36815\n",
      "Epoch: 51, Time: 0.04002s, Loss: 1.36484\n",
      "Epoch: 52, Time: 0.02300s, Loss: 1.36451\n",
      "Epoch: 53, Time: 0.02855s, Loss: 1.35881\n",
      "Epoch: 54, Time: 0.02901s, Loss: 1.36047\n",
      "Epoch: 55, Time: 0.03504s, Loss: 1.35786\n",
      "Epoch: 56, Time: 0.03361s, Loss: 1.35504\n",
      "Epoch: 57, Time: 0.03254s, Loss: 1.35505\n",
      "Epoch: 58, Time: 0.03200s, Loss: 1.35036\n",
      "Epoch: 59, Time: 0.03703s, Loss: 1.35004\n",
      "Epoch: 60, Time: 0.03353s, Loss: 1.34759\n",
      "Epoch: 61, Time: 0.03107s, Loss: 1.34775\n",
      "Epoch: 62, Time: 0.03153s, Loss: 1.34459\n",
      "Epoch: 63, Time: 0.03153s, Loss: 1.34219\n",
      "Epoch: 64, Time: 0.03053s, Loss: 1.34351\n",
      "Epoch: 65, Time: 0.03054s, Loss: 1.33887\n",
      "Epoch: 66, Time: 0.03254s, Loss: 1.33533\n",
      "Epoch: 67, Time: 0.03400s, Loss: 1.33662\n",
      "Epoch: 68, Time: 0.03791s, Loss: 1.33533\n",
      "Epoch: 69, Time: 0.03100s, Loss: 1.33510\n",
      "Epoch: 70, Time: 0.03251s, Loss: 1.33446\n",
      "Epoch: 71, Time: 0.02800s, Loss: 1.33194\n",
      "Epoch: 72, Time: 0.03489s, Loss: 1.32936\n",
      "Epoch: 73, Time: 0.03462s, Loss: 1.33079\n",
      "Epoch: 74, Time: 0.03453s, Loss: 1.32597\n",
      "Epoch: 75, Time: 0.03099s, Loss: 1.32863\n",
      "Epoch: 76, Time: 0.03410s, Loss: 1.32911\n",
      "Epoch: 77, Time: 0.03053s, Loss: 1.32526\n",
      "Epoch: 78, Time: 0.03007s, Loss: 1.32598\n",
      "Epoch: 79, Time: 0.03053s, Loss: 1.32561\n",
      "Epoch: 80, Time: 0.03054s, Loss: 1.32663\n",
      "Epoch: 81, Time: 0.02633s, Loss: 1.32566\n",
      "Epoch: 82, Time: 0.03354s, Loss: 1.32270\n",
      "Epoch: 83, Time: 0.02554s, Loss: 1.32130\n",
      "Epoch: 84, Time: 0.02901s, Loss: 1.32166\n",
      "Epoch: 85, Time: 0.03304s, Loss: 1.31893\n",
      "Epoch: 86, Time: 0.03452s, Loss: 1.31618\n",
      "Epoch: 87, Time: 0.05826s, Loss: 1.31870\n",
      "Epoch: 88, Time: 0.03552s, Loss: 1.31665\n",
      "Epoch: 89, Time: 0.03266s, Loss: 1.31770\n",
      "Epoch: 90, Time: 0.03054s, Loss: 1.31650\n",
      "Epoch: 91, Time: 0.02854s, Loss: 1.31513\n",
      "Epoch: 92, Time: 0.02802s, Loss: 1.31529\n",
      "Epoch: 93, Time: 0.03306s, Loss: 1.31561\n",
      "Epoch: 94, Time: 0.03253s, Loss: 1.31735\n",
      "Epoch: 95, Time: 0.03106s, Loss: 1.31539\n",
      "Epoch: 96, Time: 0.03253s, Loss: 1.31442\n",
      "Epoch: 97, Time: 0.02752s, Loss: 1.31459\n",
      "Epoch: 98, Time: 0.02951s, Loss: 1.31604\n",
      "Epoch: 99, Time: 0.02302s, Loss: 1.30958\n",
      "    ↳ HGNN Fold Result — Acc: 0.3897, F1: 0.3897\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.03853s, Loss: 1.81901\n",
      "Epoch: 1, Time: 0.02755s, Loss: 1.50842\n",
      "Epoch: 2, Time: 0.03685s, Loss: 1.36235\n",
      "Epoch: 3, Time: 0.02800s, Loss: 1.31112\n",
      "Epoch: 4, Time: 0.03252s, Loss: 1.28469\n",
      "Epoch: 5, Time: 0.04256s, Loss: 1.26782\n",
      "Epoch: 6, Time: 0.03453s, Loss: 1.25655\n",
      "Epoch: 7, Time: 0.02852s, Loss: 1.24909\n",
      "Epoch: 8, Time: 0.03353s, Loss: 1.24411\n",
      "Epoch: 9, Time: 0.03352s, Loss: 1.23589\n",
      "Epoch: 10, Time: 0.03151s, Loss: 1.23324\n",
      "Epoch: 11, Time: 0.02700s, Loss: 1.22866\n",
      "Epoch: 12, Time: 0.03719s, Loss: 1.22946\n",
      "Epoch: 13, Time: 0.04705s, Loss: 1.22428\n",
      "Epoch: 14, Time: 0.03753s, Loss: 1.22212\n",
      "Epoch: 15, Time: 0.03308s, Loss: 1.22101\n",
      "Epoch: 16, Time: 0.03554s, Loss: 1.21875\n",
      "Epoch: 17, Time: 0.02903s, Loss: 1.21894\n",
      "Epoch: 18, Time: 0.03041s, Loss: 1.21615\n",
      "Epoch: 19, Time: 0.02952s, Loss: 1.21821\n",
      "Epoch: 20, Time: 0.02655s, Loss: 1.21467\n",
      "Epoch: 21, Time: 0.03000s, Loss: 1.21369\n",
      "Epoch: 22, Time: 0.03603s, Loss: 1.21157\n",
      "Epoch: 23, Time: 0.03252s, Loss: 1.21484\n",
      "Epoch: 24, Time: 0.03154s, Loss: 1.21629\n",
      "Epoch: 25, Time: 0.03454s, Loss: 1.21156\n",
      "Epoch: 26, Time: 0.03552s, Loss: 1.21090\n",
      "Epoch: 27, Time: 0.03300s, Loss: 1.21025\n",
      "Epoch: 28, Time: 0.03553s, Loss: 1.21024\n",
      "Epoch: 29, Time: 0.03706s, Loss: 1.20787\n",
      "Epoch: 30, Time: 0.03253s, Loss: 1.20844\n",
      "Epoch: 31, Time: 0.03300s, Loss: 1.20745\n",
      "Epoch: 32, Time: 0.02751s, Loss: 1.20664\n",
      "Epoch: 33, Time: 0.03001s, Loss: 1.20927\n",
      "Epoch: 34, Time: 0.03153s, Loss: 1.20598\n",
      "Epoch: 35, Time: 0.03453s, Loss: 1.20785\n",
      "Epoch: 36, Time: 0.03304s, Loss: 1.20740\n",
      "Epoch: 37, Time: 0.03053s, Loss: 1.20603\n",
      "Epoch: 38, Time: 0.03125s, Loss: 1.20340\n",
      "Epoch: 39, Time: 0.03000s, Loss: 1.20601\n",
      "Epoch: 40, Time: 0.03333s, Loss: 1.20468\n",
      "Epoch: 41, Time: 0.03305s, Loss: 1.20587\n",
      "Epoch: 42, Time: 0.02952s, Loss: 1.20624\n",
      "Epoch: 43, Time: 0.02853s, Loss: 1.20378\n",
      "Epoch: 44, Time: 0.02752s, Loss: 1.20626\n",
      "Epoch: 45, Time: 0.03306s, Loss: 1.20432\n",
      "Epoch: 46, Time: 0.03353s, Loss: 1.20382\n",
      "Epoch: 47, Time: 0.03252s, Loss: 1.20203\n",
      "Epoch: 48, Time: 0.02852s, Loss: 1.20419\n",
      "Epoch: 49, Time: 0.02852s, Loss: 1.20324\n",
      "Epoch: 50, Time: 0.02904s, Loss: 1.20581\n",
      "Epoch: 51, Time: 0.03000s, Loss: 1.20350\n",
      "Epoch: 52, Time: 0.03006s, Loss: 1.20453\n",
      "Epoch: 53, Time: 0.02752s, Loss: 1.20403\n",
      "Epoch: 54, Time: 0.02554s, Loss: 1.20147\n",
      "Epoch: 55, Time: 0.03053s, Loss: 1.20161\n",
      "Epoch: 56, Time: 0.02952s, Loss: 1.20306\n",
      "Epoch: 57, Time: 0.03452s, Loss: 1.20110\n",
      "Epoch: 58, Time: 0.03153s, Loss: 1.20128\n",
      "Epoch: 59, Time: 0.03152s, Loss: 1.20197\n",
      "Epoch: 60, Time: 0.03469s, Loss: 1.20347\n",
      "Epoch: 61, Time: 0.02954s, Loss: 1.20167\n",
      "Epoch: 62, Time: 0.03559s, Loss: 1.20169\n",
      "Epoch: 63, Time: 0.02851s, Loss: 1.20247\n",
      "Epoch: 64, Time: 0.02852s, Loss: 1.20371\n",
      "Epoch: 65, Time: 0.02911s, Loss: 1.20163\n",
      "Epoch: 66, Time: 0.03354s, Loss: 1.20274\n",
      "Epoch: 67, Time: 0.03153s, Loss: 1.20122\n",
      "Epoch: 68, Time: 0.02852s, Loss: 1.20292\n",
      "Epoch: 69, Time: 0.03354s, Loss: 1.20121\n",
      "Epoch: 70, Time: 0.03363s, Loss: 1.20195\n",
      "Epoch: 71, Time: 0.02901s, Loss: 1.20086\n",
      "Epoch: 72, Time: 0.03252s, Loss: 1.20368\n",
      "Epoch: 73, Time: 0.03501s, Loss: 1.19890\n",
      "Epoch: 74, Time: 0.03258s, Loss: 1.20103\n",
      "Epoch: 75, Time: 0.02752s, Loss: 1.20249\n",
      "Epoch: 76, Time: 0.03506s, Loss: 1.20220\n",
      "Epoch: 77, Time: 0.02752s, Loss: 1.20192\n",
      "Epoch: 78, Time: 0.02956s, Loss: 1.20210\n",
      "Epoch: 79, Time: 0.03352s, Loss: 1.20062\n",
      "Epoch: 80, Time: 0.03353s, Loss: 1.20213\n",
      "Epoch: 81, Time: 0.02452s, Loss: 1.20450\n",
      "Epoch: 82, Time: 0.03153s, Loss: 1.20370\n",
      "Epoch: 83, Time: 0.03152s, Loss: 1.20069\n",
      "Epoch: 84, Time: 0.02901s, Loss: 1.20272\n",
      "Epoch: 85, Time: 0.02953s, Loss: 1.20322\n",
      "Epoch: 86, Time: 0.02801s, Loss: 1.20021\n",
      "Epoch: 87, Time: 0.03554s, Loss: 1.20185\n",
      "Epoch: 88, Time: 0.03452s, Loss: 1.20348\n",
      "Epoch: 89, Time: 0.03154s, Loss: 1.20121\n",
      "Epoch: 90, Time: 0.03053s, Loss: 1.20149\n",
      "Epoch: 91, Time: 0.03054s, Loss: 1.20057\n",
      "Epoch: 92, Time: 0.02653s, Loss: 1.20116\n",
      "Epoch: 93, Time: 0.03100s, Loss: 1.20314\n",
      "Epoch: 94, Time: 0.03355s, Loss: 1.20085\n",
      "Epoch: 95, Time: 0.03600s, Loss: 1.20279\n",
      "Epoch: 96, Time: 0.03357s, Loss: 1.20042\n",
      "Epoch: 97, Time: 0.03300s, Loss: 1.20099\n",
      "Epoch: 98, Time: 0.03055s, Loss: 1.20287\n",
      "Epoch: 99, Time: 0.02853s, Loss: 1.20445\n",
      "    ↳ HGNNP Fold Result — Acc: 0.3867, F1: 0.3867\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.04424s, Loss: 1.87239\n",
      "Epoch: 1, Time: 0.03000s, Loss: 1.48126\n",
      "Epoch: 2, Time: 0.03454s, Loss: 1.35474\n",
      "Epoch: 3, Time: 0.03000s, Loss: 1.31580\n",
      "Epoch: 4, Time: 0.02954s, Loss: 1.29529\n",
      "Epoch: 5, Time: 0.02900s, Loss: 1.28436\n",
      "Epoch: 6, Time: 0.03408s, Loss: 1.27958\n",
      "Epoch: 7, Time: 0.03453s, Loss: 1.27017\n",
      "Epoch: 8, Time: 0.03355s, Loss: 1.26575\n",
      "Epoch: 9, Time: 0.03000s, Loss: 1.26373\n",
      "Epoch: 10, Time: 0.03206s, Loss: 1.25800\n",
      "Epoch: 11, Time: 0.03354s, Loss: 1.25236\n",
      "Epoch: 12, Time: 0.03156s, Loss: 1.25016\n",
      "Epoch: 13, Time: 0.02953s, Loss: 1.24980\n",
      "Epoch: 14, Time: 0.03253s, Loss: 1.24654\n",
      "Epoch: 15, Time: 0.03153s, Loss: 1.24541\n",
      "Epoch: 16, Time: 0.02558s, Loss: 1.24416\n",
      "Epoch: 17, Time: 0.03652s, Loss: 1.24263\n",
      "Epoch: 18, Time: 0.03327s, Loss: 1.24033\n",
      "Epoch: 19, Time: 0.03153s, Loss: 1.24048\n",
      "Epoch: 20, Time: 0.02951s, Loss: 1.23477\n",
      "Epoch: 21, Time: 0.03505s, Loss: 1.23456\n",
      "Epoch: 22, Time: 0.03451s, Loss: 1.23327\n",
      "Epoch: 23, Time: 0.03309s, Loss: 1.23553\n",
      "Epoch: 24, Time: 0.03000s, Loss: 1.23187\n",
      "Epoch: 25, Time: 0.03204s, Loss: 1.23155\n",
      "Epoch: 26, Time: 0.03453s, Loss: 1.23029\n",
      "Epoch: 27, Time: 0.03055s, Loss: 1.23102\n",
      "Epoch: 28, Time: 0.03002s, Loss: 1.22803\n",
      "Epoch: 29, Time: 0.03454s, Loss: 1.22862\n",
      "Epoch: 30, Time: 0.03200s, Loss: 1.22651\n",
      "Epoch: 31, Time: 0.02954s, Loss: 1.22495\n",
      "Epoch: 32, Time: 0.03154s, Loss: 1.22648\n",
      "Epoch: 33, Time: 0.03306s, Loss: 1.22458\n",
      "Epoch: 34, Time: 0.03653s, Loss: 1.22252\n",
      "Epoch: 35, Time: 0.03254s, Loss: 1.22693\n",
      "Epoch: 36, Time: 0.03353s, Loss: 1.22326\n",
      "Epoch: 37, Time: 0.03405s, Loss: 1.22377\n",
      "Epoch: 38, Time: 0.03154s, Loss: 1.22250\n",
      "Epoch: 39, Time: 0.02953s, Loss: 1.22236\n",
      "Epoch: 40, Time: 0.03753s, Loss: 1.22643\n",
      "Epoch: 41, Time: 0.03055s, Loss: 1.22116\n",
      "Epoch: 42, Time: 0.03153s, Loss: 1.21917\n",
      "Epoch: 43, Time: 0.03354s, Loss: 1.22204\n",
      "Epoch: 44, Time: 0.03506s, Loss: 1.22159\n",
      "Epoch: 45, Time: 0.03352s, Loss: 1.22002\n",
      "Epoch: 46, Time: 0.03353s, Loss: 1.21969\n",
      "Epoch: 47, Time: 0.03219s, Loss: 1.22071\n",
      "Epoch: 48, Time: 0.03151s, Loss: 1.21891\n",
      "Epoch: 49, Time: 0.03352s, Loss: 1.21615\n",
      "Epoch: 50, Time: 0.03354s, Loss: 1.21591\n",
      "Epoch: 51, Time: 0.02900s, Loss: 1.22087\n",
      "Epoch: 52, Time: 0.02652s, Loss: 1.21741\n",
      "Epoch: 53, Time: 0.02653s, Loss: 1.21590\n",
      "Epoch: 54, Time: 0.02954s, Loss: 1.21689\n",
      "Epoch: 55, Time: 0.03353s, Loss: 1.21446\n",
      "Epoch: 56, Time: 0.03151s, Loss: 1.21583\n",
      "Epoch: 57, Time: 0.03139s, Loss: 1.21529\n",
      "Epoch: 58, Time: 0.03200s, Loss: 1.21728\n",
      "Epoch: 59, Time: 0.03740s, Loss: 1.21320\n",
      "Epoch: 60, Time: 0.03753s, Loss: 1.21421\n",
      "Epoch: 61, Time: 0.03511s, Loss: 1.21437\n",
      "Epoch: 62, Time: 0.03206s, Loss: 1.21665\n",
      "Epoch: 63, Time: 0.03606s, Loss: 1.21025\n",
      "Epoch: 64, Time: 0.03252s, Loss: 1.21375\n",
      "Epoch: 65, Time: 0.03152s, Loss: 1.21291\n",
      "Epoch: 66, Time: 0.03201s, Loss: 1.21320\n",
      "Epoch: 67, Time: 0.03254s, Loss: 1.21437\n",
      "Epoch: 68, Time: 0.03400s, Loss: 1.21273\n",
      "Epoch: 69, Time: 0.03406s, Loss: 1.21398\n",
      "Epoch: 70, Time: 0.03153s, Loss: 1.21394\n",
      "Epoch: 71, Time: 0.02754s, Loss: 1.21238\n",
      "Epoch: 72, Time: 0.03252s, Loss: 1.21219\n",
      "Epoch: 73, Time: 0.03053s, Loss: 1.21255\n",
      "Epoch: 74, Time: 0.03052s, Loss: 1.21244\n",
      "Epoch: 75, Time: 0.03452s, Loss: 1.20981\n",
      "Epoch: 76, Time: 0.03239s, Loss: 1.21456\n",
      "Epoch: 77, Time: 0.03451s, Loss: 1.21225\n",
      "Epoch: 78, Time: 0.03504s, Loss: 1.21040\n",
      "Epoch: 79, Time: 0.02951s, Loss: 1.21077\n",
      "Epoch: 80, Time: 0.03353s, Loss: 1.21304\n",
      "Epoch: 81, Time: 0.03637s, Loss: 1.21269\n",
      "Epoch: 82, Time: 0.03504s, Loss: 1.21182\n",
      "Epoch: 83, Time: 0.03300s, Loss: 1.21146\n",
      "Epoch: 84, Time: 0.03252s, Loss: 1.21141\n",
      "Epoch: 85, Time: 0.03452s, Loss: 1.21153\n",
      "Epoch: 86, Time: 0.03205s, Loss: 1.20970\n",
      "Epoch: 87, Time: 0.03301s, Loss: 1.21172\n",
      "Epoch: 88, Time: 0.03553s, Loss: 1.21207\n",
      "Epoch: 89, Time: 0.03352s, Loss: 1.21211\n",
      "Epoch: 90, Time: 0.03205s, Loss: 1.21340\n",
      "Epoch: 91, Time: 0.03300s, Loss: 1.21072\n",
      "Epoch: 92, Time: 0.02753s, Loss: 1.20936\n",
      "Epoch: 93, Time: 0.03052s, Loss: 1.21352\n",
      "Epoch: 94, Time: 0.03253s, Loss: 1.20887\n",
      "Epoch: 95, Time: 0.03154s, Loss: 1.21091\n",
      "Epoch: 96, Time: 0.03653s, Loss: 1.21355\n",
      "Epoch: 97, Time: 0.02852s, Loss: 1.20906\n",
      "Epoch: 98, Time: 0.03351s, Loss: 1.20816\n",
      "Epoch: 99, Time: 0.03054s, Loss: 1.20893\n",
      "    ↳ UniGCN Fold Result — Acc: 0.3867, F1: 0.3867\n",
      "\n",
      "[CocitationCiteseer] Fold 5/5 (Top-k)\n",
      "  → Training model: HGNN\n",
      "Epoch: 0, Time: 0.12336s, Loss: 1.79469\n",
      "Epoch: 1, Time: 0.05220s, Loss: 1.78950\n",
      "Epoch: 2, Time: 0.04744s, Loss: 1.78321\n",
      "Epoch: 3, Time: 0.04066s, Loss: 1.77554\n",
      "Epoch: 4, Time: 0.03505s, Loss: 1.76696\n",
      "Epoch: 5, Time: 0.03818s, Loss: 1.75716\n",
      "Epoch: 6, Time: 0.04091s, Loss: 1.74613\n",
      "Epoch: 7, Time: 0.04366s, Loss: 1.73512\n",
      "Epoch: 8, Time: 0.03686s, Loss: 1.72225\n",
      "Epoch: 9, Time: 0.03876s, Loss: 1.70876\n",
      "Epoch: 10, Time: 0.04080s, Loss: 1.69610\n",
      "Epoch: 11, Time: 0.04299s, Loss: 1.68272\n",
      "Epoch: 12, Time: 0.04380s, Loss: 1.66826\n",
      "Epoch: 13, Time: 0.03836s, Loss: 1.65612\n",
      "Epoch: 14, Time: 0.03590s, Loss: 1.64156\n",
      "Epoch: 15, Time: 0.03426s, Loss: 1.62490\n",
      "Epoch: 16, Time: 0.02452s, Loss: 1.60981\n",
      "Epoch: 17, Time: 0.02431s, Loss: 1.59859\n",
      "Epoch: 18, Time: 0.02471s, Loss: 1.57897\n",
      "Epoch: 19, Time: 0.02413s, Loss: 1.56901\n",
      "Epoch: 20, Time: 0.02205s, Loss: 1.55333\n",
      "Epoch: 21, Time: 0.02632s, Loss: 1.54050\n",
      "Epoch: 22, Time: 0.02338s, Loss: 1.52744\n",
      "Epoch: 23, Time: 0.02269s, Loss: 1.51534\n",
      "Epoch: 24, Time: 0.02435s, Loss: 1.50601\n",
      "Epoch: 25, Time: 0.03281s, Loss: 1.49446\n",
      "Epoch: 26, Time: 0.02313s, Loss: 1.47792\n",
      "Epoch: 27, Time: 0.03236s, Loss: 1.47315\n",
      "Epoch: 28, Time: 0.03263s, Loss: 1.45913\n",
      "Epoch: 29, Time: 0.03225s, Loss: 1.45479\n",
      "Epoch: 30, Time: 0.02980s, Loss: 1.44762\n",
      "Epoch: 31, Time: 0.04098s, Loss: 1.44110\n",
      "Epoch: 32, Time: 0.03542s, Loss: 1.43413\n",
      "Epoch: 33, Time: 0.04639s, Loss: 1.42699\n",
      "Epoch: 34, Time: 0.03638s, Loss: 1.42631\n",
      "Epoch: 35, Time: 0.03929s, Loss: 1.42162\n",
      "Epoch: 36, Time: 0.03835s, Loss: 1.41135\n",
      "Epoch: 37, Time: 0.03777s, Loss: 1.40822\n",
      "Epoch: 38, Time: 0.03573s, Loss: 1.40579\n",
      "Epoch: 39, Time: 0.04040s, Loss: 1.39805\n",
      "Epoch: 40, Time: 0.04200s, Loss: 1.39310\n",
      "Epoch: 41, Time: 0.04074s, Loss: 1.38840\n",
      "Epoch: 42, Time: 0.04694s, Loss: 1.38682\n",
      "Epoch: 43, Time: 0.04079s, Loss: 1.38168\n",
      "Epoch: 44, Time: 0.04017s, Loss: 1.38140\n",
      "Epoch: 45, Time: 0.03081s, Loss: 1.37736\n",
      "Epoch: 46, Time: 0.03904s, Loss: 1.37508\n",
      "Epoch: 47, Time: 0.03274s, Loss: 1.37410\n",
      "Epoch: 48, Time: 0.03558s, Loss: 1.37021\n",
      "Epoch: 49, Time: 0.03028s, Loss: 1.36868\n",
      "Epoch: 50, Time: 0.03450s, Loss: 1.36260\n",
      "Epoch: 51, Time: 0.02913s, Loss: 1.35951\n",
      "Epoch: 52, Time: 0.02621s, Loss: 1.35593\n",
      "Epoch: 53, Time: 0.02789s, Loss: 1.35546\n",
      "Epoch: 54, Time: 0.03226s, Loss: 1.35372\n",
      "Epoch: 55, Time: 0.04117s, Loss: 1.35411\n",
      "Epoch: 56, Time: 0.04054s, Loss: 1.34752\n",
      "Epoch: 57, Time: 0.04408s, Loss: 1.34220\n",
      "Epoch: 58, Time: 0.04267s, Loss: 1.34709\n",
      "Epoch: 59, Time: 0.04457s, Loss: 1.34576\n",
      "Epoch: 60, Time: 0.04638s, Loss: 1.34347\n",
      "Epoch: 61, Time: 0.04688s, Loss: 1.34368\n",
      "Epoch: 62, Time: 0.04403s, Loss: 1.33729\n",
      "Epoch: 63, Time: 0.04369s, Loss: 1.33779\n",
      "Epoch: 64, Time: 0.03981s, Loss: 1.33513\n",
      "Epoch: 65, Time: 0.04225s, Loss: 1.33792\n",
      "Epoch: 66, Time: 0.04293s, Loss: 1.33350\n",
      "Epoch: 67, Time: 0.02616s, Loss: 1.33373\n",
      "Epoch: 68, Time: 0.03825s, Loss: 1.33171\n",
      "Epoch: 69, Time: 0.04089s, Loss: 1.33152\n",
      "Epoch: 70, Time: 0.04251s, Loss: 1.32998\n",
      "Epoch: 71, Time: 0.04283s, Loss: 1.32954\n",
      "Epoch: 72, Time: 0.04817s, Loss: 1.32732\n",
      "Epoch: 73, Time: 0.03903s, Loss: 1.32846\n",
      "Epoch: 74, Time: 0.04602s, Loss: 1.32805\n",
      "Epoch: 75, Time: 0.04650s, Loss: 1.32490\n",
      "Epoch: 76, Time: 0.03012s, Loss: 1.32482\n",
      "Epoch: 77, Time: 0.03256s, Loss: 1.32035\n",
      "Epoch: 78, Time: 0.02842s, Loss: 1.32077\n",
      "Epoch: 79, Time: 0.03442s, Loss: 1.31982\n",
      "Epoch: 80, Time: 0.04138s, Loss: 1.31847\n",
      "Epoch: 81, Time: 0.04195s, Loss: 1.31838\n",
      "Epoch: 82, Time: 0.04291s, Loss: 1.31831\n",
      "Epoch: 83, Time: 0.04201s, Loss: 1.31632\n",
      "Epoch: 84, Time: 0.04055s, Loss: 1.31709\n",
      "Epoch: 85, Time: 0.04104s, Loss: 1.31569\n",
      "Epoch: 86, Time: 0.04953s, Loss: 1.31409\n",
      "Epoch: 87, Time: 0.04376s, Loss: 1.31827\n",
      "Epoch: 88, Time: 0.04689s, Loss: 1.31283\n",
      "Epoch: 89, Time: 0.04907s, Loss: 1.31356\n",
      "Epoch: 90, Time: 0.04837s, Loss: 1.31279\n",
      "Epoch: 91, Time: 0.04244s, Loss: 1.31469\n",
      "Epoch: 92, Time: 0.04087s, Loss: 1.31297\n",
      "Epoch: 93, Time: 0.04756s, Loss: 1.31204\n",
      "Epoch: 94, Time: 0.03605s, Loss: 1.31283\n",
      "Epoch: 95, Time: 0.04169s, Loss: 1.30954\n",
      "Epoch: 96, Time: 0.04323s, Loss: 1.30719\n",
      "Epoch: 97, Time: 0.05028s, Loss: 1.30964\n",
      "Epoch: 98, Time: 0.04031s, Loss: 1.31003\n",
      "Epoch: 99, Time: 0.04170s, Loss: 1.30592\n",
      "    ↳ HGNN Fold Result — Acc: 0.3761, F1: 0.3761\n",
      "  → Training model: HGNNP\n",
      "Epoch: 0, Time: 0.05883s, Loss: 1.84750\n",
      "Epoch: 1, Time: 0.04343s, Loss: 1.47826\n",
      "Epoch: 2, Time: 0.04388s, Loss: 1.34584\n",
      "Epoch: 3, Time: 0.04233s, Loss: 1.29078\n",
      "Epoch: 4, Time: 0.04532s, Loss: 1.26963\n",
      "Epoch: 5, Time: 0.04449s, Loss: 1.25792\n",
      "Epoch: 6, Time: 0.05279s, Loss: 1.24932\n",
      "Epoch: 7, Time: 0.05114s, Loss: 1.24366\n",
      "Epoch: 8, Time: 0.04865s, Loss: 1.23499\n",
      "Epoch: 9, Time: 0.04569s, Loss: 1.23095\n",
      "Epoch: 10, Time: 0.04839s, Loss: 1.22825\n",
      "Epoch: 11, Time: 0.04081s, Loss: 1.22868\n",
      "Epoch: 12, Time: 0.04495s, Loss: 1.22299\n",
      "Epoch: 13, Time: 0.05295s, Loss: 1.22254\n",
      "Epoch: 14, Time: 0.05228s, Loss: 1.21962\n",
      "Epoch: 15, Time: 0.04236s, Loss: 1.21943\n",
      "Epoch: 16, Time: 0.04879s, Loss: 1.21761\n",
      "Epoch: 17, Time: 0.03874s, Loss: 1.21877\n",
      "Epoch: 18, Time: 0.03219s, Loss: 1.21419\n",
      "Epoch: 19, Time: 0.03952s, Loss: 1.21252\n",
      "Epoch: 20, Time: 0.04055s, Loss: 1.21105\n",
      "Epoch: 21, Time: 0.04215s, Loss: 1.21263\n",
      "Epoch: 22, Time: 0.03907s, Loss: 1.20978\n",
      "Epoch: 23, Time: 0.03884s, Loss: 1.21107\n",
      "Epoch: 24, Time: 0.03218s, Loss: 1.20982\n",
      "Epoch: 25, Time: 0.04016s, Loss: 1.20876\n",
      "Epoch: 26, Time: 0.04034s, Loss: 1.20768\n",
      "Epoch: 27, Time: 0.05408s, Loss: 1.20647\n",
      "Epoch: 28, Time: 0.04878s, Loss: 1.20591\n",
      "Epoch: 29, Time: 0.05288s, Loss: 1.20436\n",
      "Epoch: 30, Time: 0.03511s, Loss: 1.20607\n",
      "Epoch: 31, Time: 0.04704s, Loss: 1.20613\n",
      "Epoch: 32, Time: 0.04572s, Loss: 1.20575\n",
      "Epoch: 33, Time: 0.02686s, Loss: 1.20433\n",
      "Epoch: 34, Time: 0.02866s, Loss: 1.20491\n",
      "Epoch: 35, Time: 0.03278s, Loss: 1.20521\n",
      "Epoch: 36, Time: 0.04995s, Loss: 1.20495\n",
      "Epoch: 37, Time: 0.05049s, Loss: 1.20354\n",
      "Epoch: 38, Time: 0.04920s, Loss: 1.20446\n",
      "Epoch: 39, Time: 0.05264s, Loss: 1.20426\n",
      "Epoch: 40, Time: 0.05005s, Loss: 1.20364\n",
      "Epoch: 41, Time: 0.04701s, Loss: 1.20097\n",
      "Epoch: 42, Time: 0.05048s, Loss: 1.20274\n",
      "Epoch: 43, Time: 0.05279s, Loss: 1.20139\n",
      "Epoch: 44, Time: 0.05062s, Loss: 1.20349\n",
      "Epoch: 45, Time: 0.05747s, Loss: 1.20078\n",
      "Epoch: 46, Time: 0.04466s, Loss: 1.20274\n",
      "Epoch: 47, Time: 0.05274s, Loss: 1.20159\n",
      "Epoch: 48, Time: 0.04775s, Loss: 1.20038\n",
      "Epoch: 49, Time: 0.05244s, Loss: 1.20264\n",
      "Epoch: 50, Time: 0.04460s, Loss: 1.19868\n",
      "Epoch: 51, Time: 0.04960s, Loss: 1.20078\n",
      "Epoch: 52, Time: 0.04904s, Loss: 1.19965\n",
      "Epoch: 53, Time: 0.04697s, Loss: 1.19913\n",
      "Epoch: 54, Time: 0.05491s, Loss: 1.20013\n",
      "Epoch: 55, Time: 0.04886s, Loss: 1.20132\n",
      "Epoch: 56, Time: 0.04421s, Loss: 1.20003\n",
      "Epoch: 57, Time: 0.04758s, Loss: 1.19874\n",
      "Epoch: 58, Time: 0.04173s, Loss: 1.19967\n",
      "Epoch: 59, Time: 0.05596s, Loss: 1.19823\n",
      "Epoch: 60, Time: 0.05088s, Loss: 1.19873\n",
      "Epoch: 61, Time: 0.05159s, Loss: 1.19879\n",
      "Epoch: 62, Time: 0.05118s, Loss: 1.19843\n",
      "Epoch: 63, Time: 0.05065s, Loss: 1.19737\n",
      "Epoch: 64, Time: 0.05239s, Loss: 1.19797\n",
      "Epoch: 65, Time: 0.05280s, Loss: 1.19769\n",
      "Epoch: 66, Time: 0.05560s, Loss: 1.19915\n",
      "Epoch: 67, Time: 0.05134s, Loss: 1.19851\n",
      "Epoch: 68, Time: 0.04796s, Loss: 1.19719\n",
      "Epoch: 69, Time: 0.04087s, Loss: 1.19883\n",
      "Epoch: 70, Time: 0.03655s, Loss: 1.19924\n",
      "Epoch: 71, Time: 0.04085s, Loss: 1.19907\n",
      "Epoch: 72, Time: 0.04789s, Loss: 1.19691\n",
      "Epoch: 73, Time: 0.04971s, Loss: 1.19669\n",
      "Epoch: 74, Time: 0.04319s, Loss: 1.19904\n",
      "Epoch: 75, Time: 0.04996s, Loss: 1.20020\n",
      "Epoch: 76, Time: 0.05192s, Loss: 1.19814\n",
      "Epoch: 77, Time: 0.05096s, Loss: 1.19933\n",
      "Epoch: 78, Time: 0.03693s, Loss: 1.19835\n",
      "Epoch: 79, Time: 0.04975s, Loss: 1.19751\n",
      "Epoch: 80, Time: 0.04260s, Loss: 1.20009\n",
      "Epoch: 81, Time: 0.05677s, Loss: 1.19812\n",
      "Epoch: 82, Time: 0.05017s, Loss: 1.19832\n",
      "Epoch: 83, Time: 0.04601s, Loss: 1.20113\n",
      "Epoch: 84, Time: 0.05047s, Loss: 1.19664\n",
      "Epoch: 85, Time: 0.04033s, Loss: 1.19722\n",
      "Epoch: 86, Time: 0.04566s, Loss: 1.20050\n",
      "Epoch: 87, Time: 0.05534s, Loss: 1.19845\n",
      "Epoch: 88, Time: 0.05048s, Loss: 1.19686\n",
      "Epoch: 89, Time: 0.04921s, Loss: 1.19951\n",
      "Epoch: 90, Time: 0.04939s, Loss: 1.19628\n",
      "Epoch: 91, Time: 0.05303s, Loss: 1.19644\n",
      "Epoch: 92, Time: 0.05081s, Loss: 1.19625\n",
      "Epoch: 93, Time: 0.04941s, Loss: 1.19900\n",
      "Epoch: 94, Time: 0.04856s, Loss: 1.19880\n",
      "Epoch: 95, Time: 0.04487s, Loss: 1.19640\n",
      "Epoch: 96, Time: 0.04642s, Loss: 1.19939\n",
      "Epoch: 97, Time: 0.05503s, Loss: 1.19836\n",
      "Epoch: 98, Time: 0.04409s, Loss: 1.19480\n",
      "Epoch: 99, Time: 0.04368s, Loss: 1.19705\n",
      "    ↳ HGNNP Fold Result — Acc: 0.3671, F1: 0.3671\n",
      "  → Training model: UniGCN\n",
      "Epoch: 0, Time: 0.05692s, Loss: 1.87045\n",
      "Epoch: 1, Time: 0.05391s, Loss: 1.49166\n",
      "Epoch: 2, Time: 0.05241s, Loss: 1.35804\n",
      "Epoch: 3, Time: 0.05433s, Loss: 1.31508\n",
      "Epoch: 4, Time: 0.05133s, Loss: 1.29159\n",
      "Epoch: 5, Time: 0.04855s, Loss: 1.28504\n",
      "Epoch: 6, Time: 0.05001s, Loss: 1.27466\n",
      "Epoch: 7, Time: 0.05531s, Loss: 1.26736\n",
      "Epoch: 8, Time: 0.05301s, Loss: 1.26142\n",
      "Epoch: 9, Time: 0.05824s, Loss: 1.25938\n",
      "Epoch: 10, Time: 0.05306s, Loss: 1.25688\n",
      "Epoch: 11, Time: 0.05195s, Loss: 1.25116\n",
      "Epoch: 12, Time: 0.05097s, Loss: 1.24769\n",
      "Epoch: 13, Time: 0.05660s, Loss: 1.24523\n",
      "Epoch: 14, Time: 0.04767s, Loss: 1.24425\n",
      "Epoch: 15, Time: 0.05466s, Loss: 1.24262\n",
      "Epoch: 16, Time: 0.04990s, Loss: 1.24143\n",
      "Epoch: 17, Time: 0.05167s, Loss: 1.23929\n",
      "Epoch: 18, Time: 0.04643s, Loss: 1.23809\n",
      "Epoch: 19, Time: 0.04901s, Loss: 1.23603\n",
      "Epoch: 20, Time: 0.04949s, Loss: 1.23433\n",
      "Epoch: 21, Time: 0.05591s, Loss: 1.23367\n",
      "Epoch: 22, Time: 0.04712s, Loss: 1.23567\n",
      "Epoch: 23, Time: 0.04676s, Loss: 1.23132\n",
      "Epoch: 24, Time: 0.05624s, Loss: 1.22920\n",
      "Epoch: 25, Time: 0.04976s, Loss: 1.22812\n",
      "Epoch: 26, Time: 0.05269s, Loss: 1.22816\n",
      "Epoch: 27, Time: 0.05044s, Loss: 1.22585\n",
      "Epoch: 28, Time: 0.04939s, Loss: 1.22509\n",
      "Epoch: 29, Time: 0.04669s, Loss: 1.22569\n",
      "Epoch: 30, Time: 0.04788s, Loss: 1.22594\n",
      "Epoch: 31, Time: 0.05008s, Loss: 1.22209\n",
      "Epoch: 32, Time: 0.05101s, Loss: 1.22196\n",
      "Epoch: 33, Time: 0.05257s, Loss: 1.22346\n",
      "Epoch: 34, Time: 0.05333s, Loss: 1.22013\n",
      "Epoch: 35, Time: 0.05015s, Loss: 1.22271\n",
      "Epoch: 36, Time: 0.05630s, Loss: 1.21826\n",
      "Epoch: 37, Time: 0.05331s, Loss: 1.22061\n",
      "Epoch: 38, Time: 0.06147s, Loss: 1.21837\n",
      "Epoch: 39, Time: 0.05166s, Loss: 1.21805\n",
      "Epoch: 40, Time: 0.04889s, Loss: 1.22105\n",
      "Epoch: 41, Time: 0.05761s, Loss: 1.22054\n",
      "Epoch: 42, Time: 0.05690s, Loss: 1.21728\n",
      "Epoch: 43, Time: 0.04202s, Loss: 1.21816\n",
      "Epoch: 44, Time: 0.04835s, Loss: 1.21794\n",
      "Epoch: 45, Time: 0.05070s, Loss: 1.21501\n",
      "Epoch: 46, Time: 0.05082s, Loss: 1.21685\n",
      "Epoch: 47, Time: 0.04965s, Loss: 1.22112\n",
      "Epoch: 48, Time: 0.05309s, Loss: 1.21679\n",
      "Epoch: 49, Time: 0.05619s, Loss: 1.21349\n",
      "Epoch: 50, Time: 0.05789s, Loss: 1.21246\n",
      "Epoch: 51, Time: 0.05231s, Loss: 1.21454\n",
      "Epoch: 52, Time: 0.04735s, Loss: 1.21293\n",
      "Epoch: 53, Time: 0.05017s, Loss: 1.21240\n",
      "Epoch: 54, Time: 0.05826s, Loss: 1.21553\n",
      "Epoch: 55, Time: 0.05679s, Loss: 1.21197\n",
      "Epoch: 56, Time: 0.05510s, Loss: 1.21291\n",
      "Epoch: 57, Time: 0.05058s, Loss: 1.21188\n",
      "Epoch: 58, Time: 0.04580s, Loss: 1.20987\n",
      "Epoch: 59, Time: 0.05221s, Loss: 1.21335\n",
      "Epoch: 60, Time: 0.04743s, Loss: 1.20901\n",
      "Epoch: 61, Time: 0.04815s, Loss: 1.21195\n",
      "Epoch: 62, Time: 0.04710s, Loss: 1.21222\n",
      "Epoch: 63, Time: 0.04871s, Loss: 1.21369\n",
      "Epoch: 64, Time: 0.04609s, Loss: 1.21250\n",
      "Epoch: 65, Time: 0.05167s, Loss: 1.21151\n",
      "Epoch: 66, Time: 0.04396s, Loss: 1.21035\n",
      "Epoch: 67, Time: 0.04705s, Loss: 1.20693\n",
      "Epoch: 68, Time: 0.05482s, Loss: 1.21027\n",
      "Epoch: 69, Time: 0.04906s, Loss: 1.21082\n",
      "Epoch: 70, Time: 0.05417s, Loss: 1.21080\n",
      "Epoch: 71, Time: 0.05749s, Loss: 1.20672\n",
      "Epoch: 72, Time: 0.05431s, Loss: 1.21081\n",
      "Epoch: 73, Time: 0.05554s, Loss: 1.21192\n",
      "Epoch: 74, Time: 0.04453s, Loss: 1.20961\n",
      "Epoch: 75, Time: 0.05324s, Loss: 1.20729\n",
      "Epoch: 76, Time: 0.04774s, Loss: 1.20972\n",
      "Epoch: 77, Time: 0.04925s, Loss: 1.20948\n",
      "Epoch: 78, Time: 0.04876s, Loss: 1.20722\n",
      "Epoch: 79, Time: 0.05138s, Loss: 1.20933\n",
      "Epoch: 80, Time: 0.05101s, Loss: 1.20773\n",
      "Epoch: 81, Time: 0.05201s, Loss: 1.21079\n",
      "Epoch: 82, Time: 0.05114s, Loss: 1.20846\n",
      "Epoch: 83, Time: 0.04979s, Loss: 1.20934\n",
      "Epoch: 84, Time: 0.03065s, Loss: 1.20597\n",
      "Epoch: 85, Time: 0.03657s, Loss: 1.20473\n",
      "Epoch: 86, Time: 0.03305s, Loss: 1.20869\n",
      "Epoch: 87, Time: 0.04828s, Loss: 1.20561\n",
      "Epoch: 88, Time: 0.05353s, Loss: 1.20700\n",
      "Epoch: 89, Time: 0.04878s, Loss: 1.20583\n",
      "Epoch: 90, Time: 0.04481s, Loss: 1.20558\n",
      "Epoch: 91, Time: 0.04634s, Loss: 1.20649\n",
      "Epoch: 92, Time: 0.04627s, Loss: 1.20947\n",
      "Epoch: 93, Time: 0.04475s, Loss: 1.20607\n",
      "Epoch: 94, Time: 0.04832s, Loss: 1.20560\n",
      "Epoch: 95, Time: 0.04164s, Loss: 1.20693\n",
      "Epoch: 96, Time: 0.05418s, Loss: 1.21057\n",
      "Epoch: 97, Time: 0.05528s, Loss: 1.20457\n",
      "Epoch: 98, Time: 0.04598s, Loss: 1.20499\n",
      "Epoch: 99, Time: 0.05125s, Loss: 1.21259\n",
      "    ↳ UniGCN Fold Result — Acc: 0.3656, F1: 0.3656\n"
     ]
    }
   ],
   "source": [
    "# CocitationCiteseer\n",
    "name = \"CocitationCiteseer\"\n",
    "\n",
    "print(f\"\\n==> Running {name} without Top-k\")\n",
    "results_original = run_cross_validation(dhg_datastets[name], name, use_topk=False)\n",
    "for model_name, metrics in results_original.items():\n",
    "    all_results[f\"{name}_original_{model_name}\"] = metrics\n",
    "\n",
    "print(f\"\\n==> Running {name} with Top-k\")\n",
    "results_topk = run_cross_validation(dhg_datastets[name], name, use_topk=True, k=3)\n",
    "for model_name, metrics in results_topk.items():\n",
    "    all_results[f\"{name}_topk_{model_name}\"] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HouseCommittees_original_HGNN\n",
      "accuracy mean 0.5209302325581395\n",
      "accuracy std 0.0031007751937984548\n",
      "f1 mean 0.5209302325581395\n",
      "f1 std 0.0031007751937984548\n",
      "==============================\n",
      "\n",
      "\n",
      "HouseCommittees_original_HGNNP\n",
      "accuracy mean 0.575968992248062\n",
      "accuracy std 0.027906976744186015\n",
      "f1 mean 0.575968992248062\n",
      "f1 std 0.027906976744186015\n",
      "==============================\n",
      "\n",
      "\n",
      "HouseCommittees_original_UniGCN\n",
      "accuracy mean 0.5852713178294573\n",
      "accuracy std 0.014082094670220879\n",
      "f1 mean 0.5852713178294573\n",
      "f1 std 0.014082094670220879\n",
      "==============================\n",
      "\n",
      "\n",
      "HouseCommittees_topk_HGNN\n",
      "accuracy mean 0.5271317829457365\n",
      "accuracy std 0.01550387596899223\n",
      "f1 mean 0.5271317829457365\n",
      "f1 std 0.01550387596899223\n",
      "==============================\n",
      "\n",
      "\n",
      "HouseCommittees_topk_HGNNP\n",
      "accuracy mean 0.5697674418604651\n",
      "accuracy std 0.015696478086291918\n",
      "f1 mean 0.5697674418604651\n",
      "f1 std 0.015696478086291918\n",
      "==============================\n",
      "\n",
      "\n",
      "HouseCommittees_topk_UniGCN\n",
      "accuracy mean 0.5767441860465118\n",
      "accuracy std 0.030949557794848242\n",
      "f1 mean 0.5767441860465118\n",
      "f1 std 0.030949557794848242\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_original_HGNN\n",
      "accuracy mean 0.4305628084373419\n",
      "accuracy std 0.014580974794410641\n",
      "f1 mean 0.4305628084373419\n",
      "f1 std 0.014580974794410645\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_original_HGNNP\n",
      "accuracy mean 0.4184823173982585\n",
      "accuracy std 0.01105447915602654\n",
      "f1 mean 0.41848231739825836\n",
      "f1 std 0.011054479156026543\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_original_UniGCN\n",
      "accuracy mean 0.4221095177555103\n",
      "accuracy std 0.014191530722621791\n",
      "f1 mean 0.4221095177555103\n",
      "f1 std 0.014191530722621797\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_topk_HGNN\n",
      "accuracy mean 0.384968991082373\n",
      "accuracy std 0.012130037201397501\n",
      "f1 mean 0.384968991082373\n",
      "f1 std 0.012130037201397501\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_topk_HGNNP\n",
      "accuracy mean 0.3774220448114175\n",
      "accuracy std 0.014111856314112953\n",
      "f1 mean 0.3774220448114175\n",
      "f1 std 0.014111856314112946\n",
      "==============================\n",
      "\n",
      "\n",
      "CocitationCiteseer_topk_UniGCN\n",
      "accuracy mean 0.3798362291697994\n",
      "accuracy std 0.015456051137214426\n",
      "f1 mean 0.3798362291697994\n",
      "f1 std 0.01545605113721444\n",
      "==============================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, value in all_results.items():\n",
    "    print(name)\n",
    "    print(\"accuracy mean\", np.array(value['accuracy']).mean())\n",
    "    print(\"accuracy std\", np.array(value['accuracy']).std())\n",
    "    print(\"f1 mean\", np.array(value['f1']).mean())\n",
    "    print(\"f1 std\", np.array(value['f1']).std())\n",
    "    print(\"===\"*10)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, dataset in datasets.items():\n",
    "#     print(f\"\\n==> Running {name} without Top-k\")\n",
    "#     accs, f1s = run_cross_validation(dataset, name, use_topk=False)\n",
    "#     all_results[f\"{name}_original\"] = {\"accuracy\": accs, \"f1\": f1s}\n",
    "\n",
    "#     print(f\"\\n==> Running {name} with Top-k\")\n",
    "#     accs_topk, f1s_topk = run_cross_validation(dataset, name, use_topk=True, k=3)\n",
    "#     all_results[f\"{name}_topk\"] = {\"accuracy\": accs_topk, \"f1\": f1s_topk}\n",
    "\n",
    "# Plotting all\n",
    "plot_results(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.11)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/utkarshx27/movies-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.13M/5.13M [00:00<00:00, 23.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n",
      "Path to dataset files: C:\\Users\\rustem_izmailov\\.cache\\kagglehub\\datasets\\utkarshx27\\movies-dataset\\versions\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment to use kagglehub for downloading datasets\n",
    "\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"utkarshx27/movies-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(movie_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4803 entries, 0 to 4802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 4803 non-null   int64  \n",
      " 1   budget                4803 non-null   int64  \n",
      " 2   genres                4775 non-null   object \n",
      " 3   homepage              1712 non-null   object \n",
      " 4   id                    4803 non-null   int64  \n",
      " 5   keywords              4391 non-null   object \n",
      " 6   original_language     4803 non-null   object \n",
      " 7   original_title        4803 non-null   object \n",
      " 8   overview              4800 non-null   object \n",
      " 9   popularity            4803 non-null   float64\n",
      " 10  production_companies  4803 non-null   object \n",
      " 11  production_countries  4803 non-null   object \n",
      " 12  release_date          4802 non-null   object \n",
      " 13  revenue               4803 non-null   int64  \n",
      " 14  runtime               4801 non-null   float64\n",
      " 15  spoken_languages      4803 non-null   object \n",
      " 16  status                4803 non-null   object \n",
      " 17  tagline               3959 non-null   object \n",
      " 18  title                 4803 non-null   object \n",
      " 19  vote_average          4803 non-null   float64\n",
      " 20  vote_count            4803 non-null   int64  \n",
      " 21  cast                  4760 non-null   object \n",
      " 22  crew                  4803 non-null   object \n",
      " 23  director              4773 non-null   object \n",
      "dtypes: float64(3), int64(5), object(16)\n",
      "memory usage: 900.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profitabile\n",
       "1    2585\n",
       "0    2218\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['profitabile'] = df['revenue'] - df['budget']\n",
    "df['profitabile'] = df['profitabile'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "df['profitabile'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Action Adventure Fantasy Science Fiction\n",
       "1                    Adventure Fantasy Action\n",
       "2                      Action Adventure Crime\n",
       "3                 Action Crime Drama Thriller\n",
       "4            Action Adventure Science Fiction\n",
       "Name: genres, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genres.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphExperiment:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.results = []\n",
    "        self.losses = {}\n",
    "        \n",
    "    def run_experiments(self, n_samples=500):\n",
    "        \"\"\"Запускает все варианты экспериментов\"\"\"\n",
    "        # 1. Базовый гиперграф (без плотных подграфов)\n",
    "        print(\"\\n=== Experiment 1: Base Hypergraph ===\")\n",
    "        base_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=False\n",
    "        )\n",
    "        base_results = self._train_and_evaluate(base_data)\n",
    "        self.results.append((\"Base\", base_results))\n",
    "        \n",
    "        # 2. С топ-k плотными подграфами\n",
    "        print(\"\\n=== Experiment 2: With Densest Subgraphs ===\")\n",
    "        dense_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=True,\n",
    "            k=5\n",
    "        )\n",
    "        dense_results = self._train_and_evaluate(dense_data)\n",
    "        self.results.append((\"With Densest Subgraphs\", dense_results))\n",
    "        \n",
    "        # # 3. Только жанровые гиперребра\n",
    "        # print(\"\\n=== Experiment 3: Genre-Only Hypergraph ===\")\n",
    "        # genre_data = MovieHypergraphDataset(\n",
    "        #     data_root=self.data_path,\n",
    "        #     n_samples=n_samples,\n",
    "        #     hyperedge_types=[\"genre\"]\n",
    "        # )\n",
    "        # genre_results = self._train_and_evaluate(genre_data)\n",
    "        # self.results.append((\"Genre-Only\", genre_results))\n",
    "        \n",
    "        self._visualize_loss()\n",
    "        \n",
    "        \n",
    "        # Визуализация результатов\n",
    "        self._visualize_results()\n",
    "\n",
    "    def _train_and_evaluate(self, data) -> Dict[str, float]:\n",
    "        \"\"\"Обучает модели и возвращает результаты\"\"\"\n",
    "        # Get all the data we need\n",
    "        X = data[\"features\"]\n",
    "        lbl = data[\"labels\"]\n",
    "        edge_list = data[\"edge_list\"]\n",
    "        num_vertices = data[\"num_vertices\"]\n",
    "        train_mask = data[\"train_mask\"]\n",
    "        val_mask = data[\"val_mask\"]\n",
    "        test_mask = data[\"test_mask\"]\n",
    "        num_classes = data[\"num_classes\"]\n",
    "        \n",
    "        # Check if edge_list is empty\n",
    "        if not edge_list:\n",
    "            print(\"Warning: Empty edge list! Creating a fallback edge list based on nearest neighbors.\")\n",
    "            # Create a fallback edge list using k-nearest neighbors\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X.numpy())\n",
    "            _, indices = knn.kneighbors(X.numpy())\n",
    "            edge_list = [list(idx) for idx in indices]\n",
    "        \n",
    "        # Now use the potentially updated edge_list\n",
    "        hg = Hypergraph(num_vertices, edge_list)\n",
    "        masks = {\n",
    "            \"train\": train_mask,\n",
    "            \"val\": val_mask,\n",
    "            \"test\": test_mask\n",
    "        }\n",
    "        \n",
    "        models = {\n",
    "            \"HGNN\": HGNN(X.shape[1], 64, num_classes),\n",
    "            \"HGNNP\": HGNNP(X.shape[1], 64, num_classes, use_bn=True),\n",
    "            \"UniGCN\": UniGCN(X.shape[1], 64, num_classes, use_bn=True),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model = model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            \n",
    "            best_val_accuracy = 0\n",
    "            best_val_f1 = 0\n",
    "            losses = []\n",
    "            for epoch in range(100):\n",
    "                loss = train(model, X, hg, lbl, masks[\"train\"], optimizer, epoch)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    val_res = infer(model, X, hg, lbl, masks[\"val\"])\n",
    "                    # Extract the accuracy value from the dictionary\n",
    "                    val_accuracy = val_res.get('accuracy', 0)\n",
    "                    val_f1 = val_res.get('f1_score', 0)\n",
    "                    if val_accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = val_accuracy\n",
    "                    if val_f1 > best_val_f1:\n",
    "                        best_val_f1 = val_f1\n",
    "                        best_state = deepcopy(model.state_dict())\n",
    "            self.losses[name] = losses\n",
    "            \n",
    "            test_res = infer(model, X, hg, lbl, masks[\"test\"], test=True)\n",
    "            results[name] = {\n",
    "                \"val_accuracy\": best_val_accuracy,\n",
    "                \"val_f1\": best_val_f1,\n",
    "                \"test_accuracy\": test_res.get('accuracy', 0),\n",
    "                \"test_f1\": test_res.get('f1_score', 0)\n",
    "            }\n",
    "                  \n",
    "        return results\n",
    "\n",
    "\n",
    "    def _visualize_loss(self):\n",
    "        for name, losses in self.losses.items():\n",
    "            plt.plot(losses, label=name)\n",
    "        \n",
    "        plt.title(\"Losses\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"./images/losses.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _visualize_results(self):\n",
    "        print(self.results)\n",
    "        \"\"\"Визуализирует сравнение экспериментов\"\"\"\n",
    "        df_val = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        df_test = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            # \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_val.plot(\n",
    "            x=\"Experiment\",\n",
    "            # y=[\"HGNN Val\", \"HGNN Test\", \"HGNNP Val\", \"HGNNP Test\", \"UniGCN Val\", \"UniGCN Test\"],\n",
    "            y=[\"HGNN Val\", \"HGNNP Val\", \"UniGCN Val\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Val)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./images/hypergraph_comparison_val.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_test.plot(\n",
    "            x=\"Experiment\",\n",
    "            y=[\"HGNN Test\", \"HGNNP Test\", \"UniGCN Test\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Test)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./images/hypergraph_comparison_test.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Results Summary Val ===\")\n",
    "        print(df_val.to_string(index=False))\n",
    "        print()\n",
    "        print(\"\\n=== Results Summary Test ===\")\n",
    "        print(df_test.to_string(index=False))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieHypergraphDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        n_samples: Optional[int] = None,\n",
    "        use_densest_subgraphs: bool = False,\n",
    "        k: int = 3,\n",
    "        hyperedge_types: List[str] = [\"genre\", \"director\", \"numerical\"]\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.n_samples = n_samples\n",
    "        self.use_densest_subgraphs = use_densest_subgraphs\n",
    "        self.k = k\n",
    "        self.hyperedge_types = hyperedge_types\n",
    "        self._content = self._build_dataset()\n",
    "\n",
    "    def _build_dataset(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build the complete dataset dictionary\"\"\"\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(self.data_root)\n",
    "        if self.n_samples:\n",
    "            df = df.sample(min(self.n_samples, len(df)))\n",
    "\n",
    "        df = self._preprocess_data(df)\n",
    "        \n",
    "        # Create features and labels\n",
    "        features = self._create_features(df)\n",
    "        labels = (df['revenue'] > df['budget']).astype(int).values\n",
    "        \n",
    "        # Create splits\n",
    "        train_mask, val_mask, test_mask = self._create_splits(labels)\n",
    "        \n",
    "        # Create hyperedges\n",
    "        edge_list = self._create_hyperedges(df)\n",
    "        \n",
    "        # Add densest subgraphs if enabled\n",
    "        if self.use_densest_subgraphs and len(edge_list) > 0:\n",
    "            temp_hg = Hypergraph(len(df), edge_list)\n",
    "            top_k_subgraphs = self._get_top_k_densest_subgraphs(temp_hg, k=self.k)\n",
    "            edge_list.extend([list(subset) for subset, _ in top_k_subgraphs])\n",
    "        \n",
    "        return {\n",
    "            \"num_classes\": 2,\n",
    "            \"num_vertices\": len(df),\n",
    "            \"num_edges\": len(edge_list),\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"labels\": torch.LongTensor(labels),\n",
    "            \"edge_list\": edge_list,\n",
    "            \"train_mask\": train_mask,\n",
    "            \"val_mask\": val_mask,\n",
    "            \"test_mask\": test_mask,\n",
    "        }\n",
    "\n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
    "        # Convert stringified lists to actual lists\n",
    "        for col in ['genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: literal_eval(x) \n",
    "                    if pd.notna(x) and isinstance(x, str) and x.startswith('[') \n",
    "                    else []\n",
    "                )\n",
    "        \n",
    "        # Handle genres specially if they're space-separated strings\n",
    "        if 'genres' in df.columns:\n",
    "            df['genres'] = df['genres'].apply(\n",
    "                lambda x: [{'name': g.strip()} for g in x.split()] \n",
    "                if pd.notna(x) and isinstance(x, str) and not x.startswith('[')\n",
    "                else x\n",
    "            )\n",
    "        \n",
    "        # Fill missing values\n",
    "        text_cols = ['overview', 'tagline', 'director']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "        \n",
    "        num_cols = ['runtime', 'budget', 'revenue', 'popularity', 'vote_average', 'vote_count']\n",
    "        for col in num_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _create_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create feature matrix combining numerical and text features\"\"\"\n",
    "        # Numerical features\n",
    "        num_features = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
    "        X_num = StandardScaler().fit_transform(df[num_features].values) if num_features else np.zeros((len(df), 0))\n",
    "        \n",
    "        # Text features from overview\n",
    "        tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "        X_text = tfidf.fit_transform(df['overview']).toarray() if 'overview' in df.columns else np.zeros((len(df), 0))\n",
    "        \n",
    "        return np.concatenate([X_num, X_text], axis=1)\n",
    "\n",
    "    def _create_splits(self, labels: np.ndarray) -> tuple:\n",
    "        \"\"\"Create train/val/test splits with stratification\"\"\"\n",
    "        indices = np.arange(len(labels))\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=0.3, stratify=labels)\n",
    "        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=labels[test_idx])\n",
    "        \n",
    "        train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        \n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "        \n",
    "        return train_mask, val_mask, test_mask\n",
    "\n",
    "    def _create_hyperedges(self, df: pd.DataFrame) -> list:\n",
    "        \"\"\"Create hyperedges based on specified types\"\"\"\n",
    "        edge_list = []\n",
    "        \n",
    "        if \"genre\" in self.hyperedge_types and 'genres' in df.columns:\n",
    "            genre_to_movies = {}\n",
    "            for idx, genres in enumerate(df['genres']):\n",
    "                if isinstance(genres, list):\n",
    "                    for genre in genres:\n",
    "                        name = genre['name'] if isinstance(genre, dict) else genre\n",
    "                        if name not in genre_to_movies:\n",
    "                            genre_to_movies[name] = []\n",
    "                        genre_to_movies[name].append(idx)\n",
    "            edge_list.extend(list(genre_to_movies.values()))\n",
    "        \n",
    "        if \"director\" in self.hyperedge_types and 'director' in df.columns:\n",
    "            director_to_movies = {}\n",
    "            for idx, director in enumerate(df['director']):\n",
    "                if pd.notna(director):\n",
    "                    if director not in director_to_movies:\n",
    "                        director_to_movies[director] = []\n",
    "                    director_to_movies[director].append(idx)\n",
    "            edge_list.extend(list(director_to_movies.values()))\n",
    "        \n",
    "        if \"numerical\" in self.hyperedge_types:\n",
    "            numerical_features = ['budget', 'popularity', 'runtime', 'vote_average']\n",
    "            X_num = df[numerical_features].values\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X_num)\n",
    "            _, indices = knn.kneighbors(X_num)\n",
    "            edge_list.extend([list(idx) for idx in indices])\n",
    "        \n",
    "        return edge_list\n",
    "\n",
    "    def _get_top_k_densest_subgraphs(self, hg: Hypergraph, k: int = 3) -> list:\n",
    "        \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "        nodes = set(range(hg.num_v))\n",
    "        subgraphs = []\n",
    "        \n",
    "        for _ in range(k):\n",
    "            if len(nodes) < 3:  # min_size\n",
    "                break\n",
    "                \n",
    "            current_nodes = set(nodes)\n",
    "            best_subset = None\n",
    "            best_density = -1\n",
    "            \n",
    "            while len(current_nodes) >= 3:\n",
    "                edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "                density = edge_count / len(current_nodes)\n",
    "                \n",
    "                if density > best_density:\n",
    "                    best_density = density\n",
    "                    best_subset = set(current_nodes)\n",
    "                \n",
    "                # Remove node with lowest degree\n",
    "                degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "                node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "                current_nodes.remove(node_to_remove)\n",
    "            \n",
    "            if best_subset:\n",
    "                subgraphs.append((best_subset, best_density))\n",
    "                nodes -= best_subset\n",
    "        \n",
    "        return subgraphs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._content[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    \n",
    "    # Handle both cases: when evaluator returns dict or float\n",
    "    if isinstance(res, dict):\n",
    "        return res\n",
    "    else:\n",
    "        return {'accuracy': res, 'f1_score': res}  \n",
    "\n",
    "\n",
    "def evaluate_model(model, hypergraph_type, X, hg, labels, masks):\n",
    "    results = {}\n",
    "    for name, mask in masks.items():\n",
    "        res = infer(model, X, hg, labels, mask, test=(name == \"test\"))\n",
    "        results[f\"{hypergraph_type}_{name}\"] = res\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1: Base Hypergraph ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69275\n",
      "Epoch: 1, Loss: 0.66751\n",
      "Epoch: 2, Loss: 0.64399\n",
      "Epoch: 3, Loss: 0.62005\n",
      "Epoch: 4, Loss: 0.59748\n",
      "Epoch: 5, Loss: 0.57722\n",
      "Epoch: 6, Loss: 0.56126\n",
      "Epoch: 7, Loss: 0.55088\n",
      "Epoch: 8, Loss: 0.54155\n",
      "Epoch: 9, Loss: 0.53810\n",
      "Epoch: 10, Loss: 0.53098\n",
      "Epoch: 11, Loss: 0.52912\n",
      "Epoch: 12, Loss: 0.52214\n",
      "Epoch: 13, Loss: 0.51526\n",
      "Epoch: 14, Loss: 0.51033\n",
      "Epoch: 15, Loss: 0.50401\n",
      "Epoch: 16, Loss: 0.49680\n",
      "Epoch: 17, Loss: 0.49455\n",
      "Epoch: 18, Loss: 0.49382\n",
      "Epoch: 19, Loss: 0.48841\n",
      "Epoch: 20, Loss: 0.48594\n",
      "Epoch: 21, Loss: 0.48648\n",
      "Epoch: 22, Loss: 0.48898\n",
      "Epoch: 23, Loss: 0.48500\n",
      "Epoch: 24, Loss: 0.48525\n",
      "Epoch: 25, Loss: 0.48145\n",
      "Epoch: 26, Loss: 0.48317\n",
      "Epoch: 27, Loss: 0.47578\n",
      "Epoch: 28, Loss: 0.47731\n",
      "Epoch: 29, Loss: 0.47532\n",
      "Epoch: 30, Loss: 0.47534\n",
      "Epoch: 31, Loss: 0.47260\n",
      "Epoch: 32, Loss: 0.47090\n",
      "Epoch: 33, Loss: 0.47327\n",
      "Epoch: 34, Loss: 0.46989\n",
      "Epoch: 35, Loss: 0.46892\n",
      "Epoch: 36, Loss: 0.46661\n",
      "Epoch: 37, Loss: 0.46509\n",
      "Epoch: 38, Loss: 0.46514\n",
      "Epoch: 39, Loss: 0.46445\n",
      "Epoch: 40, Loss: 0.46416\n",
      "Epoch: 41, Loss: 0.46074\n",
      "Epoch: 42, Loss: 0.46219\n",
      "Epoch: 43, Loss: 0.46195\n",
      "Epoch: 44, Loss: 0.46066\n",
      "Epoch: 45, Loss: 0.45743\n",
      "Epoch: 46, Loss: 0.45811\n",
      "Epoch: 47, Loss: 0.45799\n",
      "Epoch: 48, Loss: 0.45602\n",
      "Epoch: 49, Loss: 0.45415\n",
      "Epoch: 50, Loss: 0.45412\n",
      "Epoch: 51, Loss: 0.45352\n",
      "Epoch: 52, Loss: 0.45250\n",
      "Epoch: 53, Loss: 0.45074\n",
      "Epoch: 54, Loss: 0.45281\n",
      "Epoch: 55, Loss: 0.44824\n",
      "Epoch: 56, Loss: 0.44892\n",
      "Epoch: 57, Loss: 0.44966\n",
      "Epoch: 58, Loss: 0.44648\n",
      "Epoch: 59, Loss: 0.44867\n",
      "Epoch: 60, Loss: 0.44496\n",
      "Epoch: 61, Loss: 0.44345\n",
      "Epoch: 62, Loss: 0.44587\n",
      "Epoch: 63, Loss: 0.44373\n",
      "Epoch: 64, Loss: 0.44034\n",
      "Epoch: 65, Loss: 0.44206\n",
      "Epoch: 66, Loss: 0.44259\n",
      "Epoch: 67, Loss: 0.44357\n",
      "Epoch: 68, Loss: 0.44092\n",
      "Epoch: 69, Loss: 0.44260\n",
      "Epoch: 70, Loss: 0.44011\n",
      "Epoch: 71, Loss: 0.43777\n",
      "Epoch: 72, Loss: 0.43630\n",
      "Epoch: 73, Loss: 0.43591\n",
      "Epoch: 74, Loss: 0.43466\n",
      "Epoch: 75, Loss: 0.43439\n",
      "Epoch: 76, Loss: 0.43969\n",
      "Epoch: 77, Loss: 0.43926\n",
      "Epoch: 78, Loss: 0.43069\n",
      "Epoch: 79, Loss: 0.43515\n",
      "Epoch: 80, Loss: 0.42936\n",
      "Epoch: 81, Loss: 0.43134\n",
      "Epoch: 82, Loss: 0.42951\n",
      "Epoch: 83, Loss: 0.43099\n",
      "Epoch: 84, Loss: 0.42979\n",
      "Epoch: 85, Loss: 0.42477\n",
      "Epoch: 86, Loss: 0.42727\n",
      "Epoch: 87, Loss: 0.42785\n",
      "Epoch: 88, Loss: 0.42589\n",
      "Epoch: 89, Loss: 0.42931\n",
      "Epoch: 90, Loss: 0.42743\n",
      "Epoch: 91, Loss: 0.42040\n",
      "Epoch: 92, Loss: 0.42554\n",
      "Epoch: 93, Loss: 0.42511\n",
      "Epoch: 94, Loss: 0.42653\n",
      "Epoch: 95, Loss: 0.42301\n",
      "Epoch: 96, Loss: 0.42423\n",
      "Epoch: 97, Loss: 0.42115\n",
      "Epoch: 98, Loss: 0.42043\n",
      "Epoch: 99, Loss: 0.42081\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.77593\n",
      "Epoch: 1, Loss: 0.54626\n",
      "Epoch: 2, Loss: 0.53010\n",
      "Epoch: 3, Loss: 0.51273\n",
      "Epoch: 4, Loss: 0.49577\n",
      "Epoch: 5, Loss: 0.48369\n",
      "Epoch: 6, Loss: 0.47760\n",
      "Epoch: 7, Loss: 0.47689\n",
      "Epoch: 8, Loss: 0.47129\n",
      "Epoch: 9, Loss: 0.46308\n",
      "Epoch: 10, Loss: 0.45997\n",
      "Epoch: 11, Loss: 0.45179\n",
      "Epoch: 12, Loss: 0.44965\n",
      "Epoch: 13, Loss: 0.44638\n",
      "Epoch: 14, Loss: 0.44659\n",
      "Epoch: 15, Loss: 0.43888\n",
      "Epoch: 16, Loss: 0.43881\n",
      "Epoch: 17, Loss: 0.43651\n",
      "Epoch: 18, Loss: 0.42904\n",
      "Epoch: 19, Loss: 0.42516\n",
      "Epoch: 20, Loss: 0.42388\n",
      "Epoch: 21, Loss: 0.42166\n",
      "Epoch: 22, Loss: 0.41792\n",
      "Epoch: 23, Loss: 0.41549\n",
      "Epoch: 24, Loss: 0.41303\n",
      "Epoch: 25, Loss: 0.41139\n",
      "Epoch: 26, Loss: 0.40291\n",
      "Epoch: 27, Loss: 0.40190\n",
      "Epoch: 28, Loss: 0.39669\n",
      "Epoch: 29, Loss: 0.39806\n",
      "Epoch: 30, Loss: 0.39179\n",
      "Epoch: 31, Loss: 0.38685\n",
      "Epoch: 32, Loss: 0.38570\n",
      "Epoch: 33, Loss: 0.38610\n",
      "Epoch: 34, Loss: 0.38129\n",
      "Epoch: 35, Loss: 0.37927\n",
      "Epoch: 36, Loss: 0.37303\n",
      "Epoch: 37, Loss: 0.37624\n",
      "Epoch: 38, Loss: 0.36491\n",
      "Epoch: 39, Loss: 0.36688\n",
      "Epoch: 40, Loss: 0.35823\n",
      "Epoch: 41, Loss: 0.35776\n",
      "Epoch: 42, Loss: 0.35323\n",
      "Epoch: 43, Loss: 0.35156\n",
      "Epoch: 44, Loss: 0.35066\n",
      "Epoch: 45, Loss: 0.34579\n",
      "Epoch: 46, Loss: 0.34292\n",
      "Epoch: 47, Loss: 0.33437\n",
      "Epoch: 48, Loss: 0.33539\n",
      "Epoch: 49, Loss: 0.33279\n",
      "Epoch: 50, Loss: 0.32982\n",
      "Epoch: 51, Loss: 0.32197\n",
      "Epoch: 52, Loss: 0.33249\n",
      "Epoch: 53, Loss: 0.32492\n",
      "Epoch: 54, Loss: 0.31782\n",
      "Epoch: 55, Loss: 0.31889\n",
      "Epoch: 56, Loss: 0.31668\n",
      "Epoch: 57, Loss: 0.30872\n",
      "Epoch: 58, Loss: 0.30653\n",
      "Epoch: 59, Loss: 0.31057\n",
      "Epoch: 60, Loss: 0.31115\n",
      "Epoch: 61, Loss: 0.29970\n",
      "Epoch: 62, Loss: 0.30851\n",
      "Epoch: 63, Loss: 0.30161\n",
      "Epoch: 64, Loss: 0.30142\n",
      "Epoch: 65, Loss: 0.29599\n",
      "Epoch: 66, Loss: 0.29206\n",
      "Epoch: 67, Loss: 0.29337\n",
      "Epoch: 68, Loss: 0.30119\n",
      "Epoch: 69, Loss: 0.29898\n",
      "Epoch: 70, Loss: 0.28486\n",
      "Epoch: 71, Loss: 0.29513\n",
      "Epoch: 72, Loss: 0.28461\n",
      "Epoch: 73, Loss: 0.27863\n",
      "Epoch: 74, Loss: 0.28640\n",
      "Epoch: 75, Loss: 0.28297\n",
      "Epoch: 76, Loss: 0.29273\n",
      "Epoch: 77, Loss: 0.27982\n",
      "Epoch: 78, Loss: 0.28075\n",
      "Epoch: 79, Loss: 0.28065\n",
      "Epoch: 80, Loss: 0.28025\n",
      "Epoch: 81, Loss: 0.26988\n",
      "Epoch: 82, Loss: 0.27705\n",
      "Epoch: 83, Loss: 0.27636\n",
      "Epoch: 84, Loss: 0.27104\n",
      "Epoch: 85, Loss: 0.26795\n",
      "Epoch: 86, Loss: 0.27578\n",
      "Epoch: 87, Loss: 0.26968\n",
      "Epoch: 88, Loss: 0.26599\n",
      "Epoch: 89, Loss: 0.26709\n",
      "Epoch: 90, Loss: 0.26729\n",
      "Epoch: 91, Loss: 0.26948\n",
      "Epoch: 92, Loss: 0.27705\n",
      "Epoch: 93, Loss: 0.26136\n",
      "Epoch: 94, Loss: 0.26204\n",
      "Epoch: 95, Loss: 0.26939\n",
      "Epoch: 96, Loss: 0.26389\n",
      "Epoch: 97, Loss: 0.26466\n",
      "Epoch: 98, Loss: 0.25619\n",
      "Epoch: 99, Loss: 0.25468\n",
      "\n",
      "Training UniGCN...\n",
      "Epoch: 0, Loss: 0.87361\n",
      "Epoch: 1, Loss: 0.55460\n",
      "Epoch: 2, Loss: 0.52932\n",
      "Epoch: 3, Loss: 0.50185\n",
      "Epoch: 4, Loss: 0.48618\n",
      "Epoch: 5, Loss: 0.48152\n",
      "Epoch: 6, Loss: 0.47448\n",
      "Epoch: 7, Loss: 0.47205\n",
      "Epoch: 8, Loss: 0.46349\n",
      "Epoch: 9, Loss: 0.45203\n",
      "Epoch: 10, Loss: 0.45348\n",
      "Epoch: 11, Loss: 0.44363\n",
      "Epoch: 12, Loss: 0.44014\n",
      "Epoch: 13, Loss: 0.44013\n",
      "Epoch: 14, Loss: 0.43754\n",
      "Epoch: 15, Loss: 0.43594\n",
      "Epoch: 16, Loss: 0.42976\n",
      "Epoch: 17, Loss: 0.42355\n",
      "Epoch: 18, Loss: 0.41828\n",
      "Epoch: 19, Loss: 0.41597\n",
      "Epoch: 20, Loss: 0.41544\n",
      "Epoch: 21, Loss: 0.40776\n",
      "Epoch: 22, Loss: 0.40770\n",
      "Epoch: 23, Loss: 0.39922\n",
      "Epoch: 24, Loss: 0.39903\n",
      "Epoch: 25, Loss: 0.39756\n",
      "Epoch: 26, Loss: 0.39307\n",
      "Epoch: 27, Loss: 0.38976\n",
      "Epoch: 28, Loss: 0.38254\n",
      "Epoch: 29, Loss: 0.37883\n",
      "Epoch: 30, Loss: 0.37601\n",
      "Epoch: 31, Loss: 0.37324\n",
      "Epoch: 32, Loss: 0.37644\n",
      "Epoch: 33, Loss: 0.36725\n",
      "Epoch: 34, Loss: 0.36471\n",
      "Epoch: 35, Loss: 0.35826\n",
      "Epoch: 36, Loss: 0.35596\n",
      "Epoch: 37, Loss: 0.35122\n",
      "Epoch: 38, Loss: 0.35162\n",
      "Epoch: 39, Loss: 0.34917\n",
      "Epoch: 40, Loss: 0.34001\n",
      "Epoch: 41, Loss: 0.34370\n",
      "Epoch: 42, Loss: 0.33658\n",
      "Epoch: 43, Loss: 0.33254\n",
      "Epoch: 44, Loss: 0.33267\n",
      "Epoch: 45, Loss: 0.32898\n",
      "Epoch: 46, Loss: 0.32405\n",
      "Epoch: 47, Loss: 0.32216\n",
      "Epoch: 48, Loss: 0.31701\n",
      "Epoch: 49, Loss: 0.31136\n",
      "Epoch: 50, Loss: 0.32024\n",
      "Epoch: 51, Loss: 0.32043\n",
      "Epoch: 52, Loss: 0.31340\n",
      "Epoch: 53, Loss: 0.30724\n",
      "Epoch: 54, Loss: 0.31035\n",
      "Epoch: 55, Loss: 0.30884\n",
      "Epoch: 56, Loss: 0.30704\n",
      "Epoch: 57, Loss: 0.29378\n",
      "Epoch: 58, Loss: 0.29450\n",
      "Epoch: 59, Loss: 0.29468\n",
      "Epoch: 60, Loss: 0.29437\n",
      "Epoch: 61, Loss: 0.29425\n",
      "Epoch: 62, Loss: 0.29893\n",
      "Epoch: 63, Loss: 0.28973\n",
      "Epoch: 64, Loss: 0.28572\n",
      "Epoch: 65, Loss: 0.28711\n",
      "Epoch: 66, Loss: 0.29316\n",
      "Epoch: 67, Loss: 0.28117\n",
      "Epoch: 68, Loss: 0.28749\n",
      "Epoch: 69, Loss: 0.28553\n",
      "Epoch: 70, Loss: 0.28448\n",
      "Epoch: 71, Loss: 0.28617\n",
      "Epoch: 72, Loss: 0.28147\n",
      "Epoch: 73, Loss: 0.28613\n",
      "Epoch: 74, Loss: 0.27334\n",
      "Epoch: 75, Loss: 0.28171\n",
      "Epoch: 76, Loss: 0.28077\n",
      "Epoch: 77, Loss: 0.27724\n",
      "Epoch: 78, Loss: 0.27138\n",
      "Epoch: 79, Loss: 0.27402\n",
      "Epoch: 80, Loss: 0.27417\n",
      "Epoch: 81, Loss: 0.26695\n",
      "Epoch: 82, Loss: 0.26503\n",
      "Epoch: 83, Loss: 0.26273\n",
      "Epoch: 84, Loss: 0.26329\n",
      "Epoch: 85, Loss: 0.26976\n",
      "Epoch: 86, Loss: 0.26135\n",
      "Epoch: 87, Loss: 0.25190\n",
      "Epoch: 88, Loss: 0.25407\n",
      "Epoch: 89, Loss: 0.25985\n",
      "Epoch: 90, Loss: 0.26440\n",
      "Epoch: 91, Loss: 0.26333\n",
      "Epoch: 92, Loss: 0.25466\n",
      "Epoch: 93, Loss: 0.25587\n",
      "Epoch: 94, Loss: 0.24817\n",
      "Epoch: 95, Loss: 0.26755\n",
      "Epoch: 96, Loss: 0.25613\n",
      "Epoch: 97, Loss: 0.26318\n",
      "Epoch: 98, Loss: 0.25815\n",
      "Epoch: 99, Loss: 0.24713\n",
      "\n",
      "=== Experiment 2: With Densest Subgraphs ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69168\n",
      "Epoch: 1, Loss: 0.67197\n",
      "Epoch: 2, Loss: 0.65242\n",
      "Epoch: 3, Loss: 0.63067\n",
      "Epoch: 4, Loss: 0.60857\n",
      "Epoch: 5, Loss: 0.58721\n",
      "Epoch: 6, Loss: 0.56868\n",
      "Epoch: 7, Loss: 0.55719\n",
      "Epoch: 8, Loss: 0.54810\n",
      "Epoch: 9, Loss: 0.54321\n",
      "Epoch: 10, Loss: 0.53915\n",
      "Epoch: 11, Loss: 0.53586\n",
      "Epoch: 12, Loss: 0.53405\n",
      "Epoch: 13, Loss: 0.52944\n",
      "Epoch: 14, Loss: 0.52470\n",
      "Epoch: 15, Loss: 0.52325\n",
      "Epoch: 16, Loss: 0.51746\n",
      "Epoch: 17, Loss: 0.51156\n",
      "Epoch: 18, Loss: 0.50772\n",
      "Epoch: 19, Loss: 0.50256\n",
      "Epoch: 20, Loss: 0.49888\n",
      "Epoch: 21, Loss: 0.49936\n",
      "Epoch: 22, Loss: 0.49864\n",
      "Epoch: 23, Loss: 0.49495\n",
      "Epoch: 24, Loss: 0.49521\n",
      "Epoch: 25, Loss: 0.49526\n",
      "Epoch: 26, Loss: 0.49436\n",
      "Epoch: 27, Loss: 0.49310\n",
      "Epoch: 28, Loss: 0.49370\n",
      "Epoch: 29, Loss: 0.48978\n",
      "Epoch: 30, Loss: 0.48882\n",
      "Epoch: 31, Loss: 0.49005\n",
      "Epoch: 32, Loss: 0.48666\n",
      "Epoch: 33, Loss: 0.48927\n",
      "Epoch: 34, Loss: 0.48835\n",
      "Epoch: 35, Loss: 0.48466\n",
      "Epoch: 36, Loss: 0.48386\n",
      "Epoch: 37, Loss: 0.48472\n",
      "Epoch: 38, Loss: 0.48246\n",
      "Epoch: 39, Loss: 0.48591\n",
      "Epoch: 40, Loss: 0.48153\n",
      "Epoch: 41, Loss: 0.48163\n",
      "Epoch: 42, Loss: 0.48191\n",
      "Epoch: 43, Loss: 0.47820\n",
      "Epoch: 44, Loss: 0.48001\n",
      "Epoch: 45, Loss: 0.47716\n",
      "Epoch: 46, Loss: 0.47940\n",
      "Epoch: 47, Loss: 0.48126\n",
      "Epoch: 48, Loss: 0.48072\n",
      "Epoch: 49, Loss: 0.47758\n",
      "Epoch: 50, Loss: 0.47719\n",
      "Epoch: 51, Loss: 0.47809\n",
      "Epoch: 52, Loss: 0.47561\n",
      "Epoch: 53, Loss: 0.47559\n",
      "Epoch: 54, Loss: 0.47582\n",
      "Epoch: 55, Loss: 0.47663\n",
      "Epoch: 56, Loss: 0.47586\n",
      "Epoch: 57, Loss: 0.47103\n",
      "Epoch: 58, Loss: 0.47190\n",
      "Epoch: 59, Loss: 0.47253\n",
      "Epoch: 60, Loss: 0.47173\n",
      "Epoch: 61, Loss: 0.47173\n",
      "Epoch: 62, Loss: 0.47157\n",
      "Epoch: 63, Loss: 0.47219\n",
      "Epoch: 64, Loss: 0.47008\n",
      "Epoch: 65, Loss: 0.46780\n",
      "Epoch: 66, Loss: 0.46959\n",
      "Epoch: 67, Loss: 0.46874\n",
      "Epoch: 68, Loss: 0.47156\n",
      "Epoch: 69, Loss: 0.46932\n",
      "Epoch: 70, Loss: 0.46638\n",
      "Epoch: 71, Loss: 0.46657\n",
      "Epoch: 72, Loss: 0.46581\n",
      "Epoch: 73, Loss: 0.46702\n",
      "Epoch: 74, Loss: 0.46843\n",
      "Epoch: 75, Loss: 0.46381\n",
      "Epoch: 76, Loss: 0.46541\n",
      "Epoch: 77, Loss: 0.46631\n",
      "Epoch: 78, Loss: 0.46251\n",
      "Epoch: 79, Loss: 0.46361\n",
      "Epoch: 80, Loss: 0.46306\n",
      "Epoch: 81, Loss: 0.46198\n",
      "Epoch: 82, Loss: 0.46073\n",
      "Epoch: 83, Loss: 0.46083\n",
      "Epoch: 84, Loss: 0.46093\n",
      "Epoch: 85, Loss: 0.45918\n",
      "Epoch: 86, Loss: 0.45805\n",
      "Epoch: 87, Loss: 0.45806\n",
      "Epoch: 88, Loss: 0.45875\n",
      "Epoch: 89, Loss: 0.45809\n",
      "Epoch: 90, Loss: 0.45563\n",
      "Epoch: 91, Loss: 0.45641\n",
      "Epoch: 92, Loss: 0.45291\n",
      "Epoch: 93, Loss: 0.45401\n",
      "Epoch: 94, Loss: 0.45513\n",
      "Epoch: 95, Loss: 0.45391\n",
      "Epoch: 96, Loss: 0.45561\n",
      "Epoch: 97, Loss: 0.45443\n",
      "Epoch: 98, Loss: 0.45414\n",
      "Epoch: 99, Loss: 0.45369\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.86697\n",
      "Epoch: 1, Loss: 0.57491\n",
      "Epoch: 2, Loss: 0.54418\n",
      "Epoch: 3, Loss: 0.52872\n",
      "Epoch: 4, Loss: 0.51192\n",
      "Epoch: 5, Loss: 0.49929\n",
      "Epoch: 6, Loss: 0.49316\n",
      "Epoch: 7, Loss: 0.48828\n",
      "Epoch: 8, Loss: 0.48544\n",
      "Epoch: 9, Loss: 0.48197\n",
      "Epoch: 10, Loss: 0.47659\n",
      "Epoch: 11, Loss: 0.47066\n",
      "Epoch: 12, Loss: 0.46814\n",
      "Epoch: 13, Loss: 0.46125\n",
      "Epoch: 14, Loss: 0.45895\n",
      "Epoch: 15, Loss: 0.45638\n",
      "Epoch: 16, Loss: 0.45102\n",
      "Epoch: 17, Loss: 0.44922\n",
      "Epoch: 18, Loss: 0.44840\n",
      "Epoch: 19, Loss: 0.44491\n",
      "Epoch: 20, Loss: 0.44287\n",
      "Epoch: 21, Loss: 0.43918\n",
      "Epoch: 22, Loss: 0.43758\n",
      "Epoch: 23, Loss: 0.43318\n",
      "Epoch: 24, Loss: 0.43459\n",
      "Epoch: 25, Loss: 0.42861\n",
      "Epoch: 26, Loss: 0.42497\n",
      "Epoch: 27, Loss: 0.42159\n",
      "Epoch: 28, Loss: 0.41565\n",
      "Epoch: 29, Loss: 0.41288\n",
      "Epoch: 30, Loss: 0.41071\n",
      "Epoch: 31, Loss: 0.41074\n",
      "Epoch: 32, Loss: 0.40714\n",
      "Epoch: 33, Loss: 0.40435\n",
      "Epoch: 34, Loss: 0.40050\n",
      "Epoch: 35, Loss: 0.39859\n",
      "Epoch: 36, Loss: 0.39597\n",
      "Epoch: 37, Loss: 0.39365\n",
      "Epoch: 38, Loss: 0.38638\n",
      "Epoch: 39, Loss: 0.38319\n",
      "Epoch: 40, Loss: 0.38060\n",
      "Epoch: 41, Loss: 0.37526\n",
      "Epoch: 42, Loss: 0.37884\n",
      "Epoch: 43, Loss: 0.37304\n",
      "Epoch: 44, Loss: 0.36885\n",
      "Epoch: 45, Loss: 0.36348\n",
      "Epoch: 46, Loss: 0.36035\n",
      "Epoch: 47, Loss: 0.36138\n",
      "Epoch: 48, Loss: 0.36463\n",
      "Epoch: 49, Loss: 0.35810\n",
      "Epoch: 50, Loss: 0.35605\n",
      "Epoch: 51, Loss: 0.35158\n",
      "Epoch: 52, Loss: 0.35194\n",
      "Epoch: 53, Loss: 0.34486\n",
      "Epoch: 54, Loss: 0.34611\n",
      "Epoch: 55, Loss: 0.33884\n",
      "Epoch: 56, Loss: 0.34151\n",
      "Epoch: 57, Loss: 0.34323\n",
      "Epoch: 58, Loss: 0.33583\n",
      "Epoch: 59, Loss: 0.33655\n",
      "Epoch: 60, Loss: 0.32305\n",
      "Epoch: 61, Loss: 0.32743\n",
      "Epoch: 62, Loss: 0.32513\n",
      "Epoch: 63, Loss: 0.32505\n",
      "Epoch: 64, Loss: 0.32167\n",
      "Epoch: 65, Loss: 0.32333\n",
      "Epoch: 66, Loss: 0.32400\n",
      "Epoch: 67, Loss: 0.31874\n",
      "Epoch: 68, Loss: 0.31370\n",
      "Epoch: 69, Loss: 0.32307\n",
      "Epoch: 70, Loss: 0.31871\n",
      "Epoch: 71, Loss: 0.31466\n",
      "Epoch: 72, Loss: 0.30678\n",
      "Epoch: 73, Loss: 0.31415\n",
      "Epoch: 74, Loss: 0.30812\n",
      "Epoch: 75, Loss: 0.31053\n",
      "Epoch: 76, Loss: 0.30770\n",
      "Epoch: 77, Loss: 0.31165\n",
      "Epoch: 78, Loss: 0.29440\n",
      "Epoch: 79, Loss: 0.29969\n",
      "Epoch: 80, Loss: 0.30853\n",
      "Epoch: 81, Loss: 0.29815\n",
      "Epoch: 82, Loss: 0.29585\n",
      "Epoch: 83, Loss: 0.30468\n",
      "Epoch: 84, Loss: 0.30009\n",
      "Epoch: 85, Loss: 0.29974\n",
      "Epoch: 86, Loss: 0.29755\n",
      "Epoch: 87, Loss: 0.29657\n",
      "Epoch: 88, Loss: 0.30796\n",
      "Epoch: 89, Loss: 0.28761\n",
      "Epoch: 90, Loss: 0.30456\n",
      "Epoch: 91, Loss: 0.29823\n",
      "Epoch: 92, Loss: 0.29320\n",
      "Epoch: 93, Loss: 0.28756\n",
      "Epoch: 94, Loss: 0.28488\n",
      "Epoch: 95, Loss: 0.29081\n",
      "Epoch: 96, Loss: 0.28888\n",
      "Epoch: 97, Loss: 0.28546\n",
      "Epoch: 98, Loss: 0.28309\n",
      "Epoch: 99, Loss: 0.29036\n",
      "\n",
      "Training UniGCN...\n",
      "Epoch: 0, Loss: 0.86491\n",
      "Epoch: 1, Loss: 0.55466\n",
      "Epoch: 2, Loss: 0.53538\n",
      "Epoch: 3, Loss: 0.52491\n",
      "Epoch: 4, Loss: 0.50506\n",
      "Epoch: 5, Loss: 0.49835\n",
      "Epoch: 6, Loss: 0.48896\n",
      "Epoch: 7, Loss: 0.48592\n",
      "Epoch: 8, Loss: 0.48697\n",
      "Epoch: 9, Loss: 0.48420\n",
      "Epoch: 10, Loss: 0.47598\n",
      "Epoch: 11, Loss: 0.46876\n",
      "Epoch: 12, Loss: 0.46563\n",
      "Epoch: 13, Loss: 0.46139\n",
      "Epoch: 14, Loss: 0.45938\n",
      "Epoch: 15, Loss: 0.45898\n",
      "Epoch: 16, Loss: 0.45801\n",
      "Epoch: 17, Loss: 0.45508\n",
      "Epoch: 18, Loss: 0.45370\n",
      "Epoch: 19, Loss: 0.44705\n",
      "Epoch: 20, Loss: 0.44491\n",
      "Epoch: 21, Loss: 0.44155\n",
      "Epoch: 22, Loss: 0.43873\n",
      "Epoch: 23, Loss: 0.43602\n",
      "Epoch: 24, Loss: 0.43491\n",
      "Epoch: 25, Loss: 0.43244\n",
      "Epoch: 26, Loss: 0.43144\n",
      "Epoch: 27, Loss: 0.42513\n",
      "Epoch: 28, Loss: 0.42167\n",
      "Epoch: 29, Loss: 0.42086\n",
      "Epoch: 30, Loss: 0.41895\n",
      "Epoch: 31, Loss: 0.41429\n",
      "Epoch: 32, Loss: 0.40954\n",
      "Epoch: 33, Loss: 0.40707\n",
      "Epoch: 34, Loss: 0.40185\n",
      "Epoch: 35, Loss: 0.40402\n",
      "Epoch: 36, Loss: 0.40063\n",
      "Epoch: 37, Loss: 0.39653\n",
      "Epoch: 38, Loss: 0.39408\n",
      "Epoch: 39, Loss: 0.39561\n",
      "Epoch: 40, Loss: 0.38719\n",
      "Epoch: 41, Loss: 0.38654\n",
      "Epoch: 42, Loss: 0.38494\n",
      "Epoch: 43, Loss: 0.38576\n",
      "Epoch: 44, Loss: 0.37879\n",
      "Epoch: 45, Loss: 0.37577\n",
      "Epoch: 46, Loss: 0.37184\n",
      "Epoch: 47, Loss: 0.36372\n",
      "Epoch: 48, Loss: 0.36362\n",
      "Epoch: 49, Loss: 0.36285\n",
      "Epoch: 50, Loss: 0.35801\n",
      "Epoch: 51, Loss: 0.35645\n",
      "Epoch: 52, Loss: 0.35101\n",
      "Epoch: 53, Loss: 0.34799\n",
      "Epoch: 54, Loss: 0.34412\n",
      "Epoch: 55, Loss: 0.34837\n",
      "Epoch: 56, Loss: 0.33830\n",
      "Epoch: 57, Loss: 0.33852\n",
      "Epoch: 58, Loss: 0.33575\n",
      "Epoch: 59, Loss: 0.32815\n",
      "Epoch: 60, Loss: 0.33015\n",
      "Epoch: 61, Loss: 0.32819\n",
      "Epoch: 62, Loss: 0.32709\n",
      "Epoch: 63, Loss: 0.32295\n",
      "Epoch: 64, Loss: 0.31662\n",
      "Epoch: 65, Loss: 0.31551\n",
      "Epoch: 66, Loss: 0.31977\n",
      "Epoch: 67, Loss: 0.31829\n",
      "Epoch: 68, Loss: 0.31640\n",
      "Epoch: 69, Loss: 0.31465\n",
      "Epoch: 70, Loss: 0.31083\n",
      "Epoch: 71, Loss: 0.31192\n",
      "Epoch: 72, Loss: 0.29797\n",
      "Epoch: 73, Loss: 0.30580\n",
      "Epoch: 74, Loss: 0.30536\n",
      "Epoch: 75, Loss: 0.29687\n",
      "Epoch: 76, Loss: 0.29680\n",
      "Epoch: 77, Loss: 0.29156\n",
      "Epoch: 78, Loss: 0.29379\n",
      "Epoch: 79, Loss: 0.29244\n",
      "Epoch: 80, Loss: 0.28980\n",
      "Epoch: 81, Loss: 0.29544\n",
      "Epoch: 82, Loss: 0.29559\n",
      "Epoch: 83, Loss: 0.29034\n",
      "Epoch: 84, Loss: 0.28886\n",
      "Epoch: 85, Loss: 0.29354\n",
      "Epoch: 86, Loss: 0.29205\n",
      "Epoch: 87, Loss: 0.28781\n",
      "Epoch: 88, Loss: 0.27918\n",
      "Epoch: 89, Loss: 0.27970\n",
      "Epoch: 90, Loss: 0.28025\n",
      "Epoch: 91, Loss: 0.28906\n",
      "Epoch: 92, Loss: 0.28118\n",
      "Epoch: 93, Loss: 0.27925\n",
      "Epoch: 94, Loss: 0.28856\n",
      "Epoch: 95, Loss: 0.28057\n",
      "Epoch: 96, Loss: 0.27497\n",
      "Epoch: 97, Loss: 0.27683\n",
      "Epoch: 98, Loss: 0.27684\n",
      "Epoch: 99, Loss: 0.27674\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Base', {'HGNN': {'val_accuracy': 0.742222249507904, 'val_f1': 0.742222249507904, 'test_accuracy': 0.7333333492279053, 'test_f1': 0.7318022886204705}, 'HGNNP': {'val_accuracy': 0.7400000095367432, 'val_f1': 0.7400000095367432, 'test_accuracy': 0.699999988079071, 'test_f1': 0.6979580472428417}, 'UniGCN': {'val_accuracy': 0.7333333492279053, 'val_f1': 0.7333333492279053, 'test_accuracy': 0.7288888692855835, 'test_f1': 0.7279808149674966}}), ('With Densest Subgraphs', {'HGNN': {'val_accuracy': 0.753333330154419, 'val_f1': 0.753333330154419, 'test_accuracy': 0.7711111307144165, 'test_f1': 0.7671076630874438}, 'HGNNP': {'val_accuracy': 0.7377777695655823, 'val_f1': 0.7377777695655823, 'test_accuracy': 0.7400000095367432, 'test_f1': 0.7373737373737373}, 'UniGCN': {'val_accuracy': 0.742222249507904, 'val_f1': 0.742222249507904, 'test_accuracy': 0.742222249507904, 'test_f1': 0.7354497354497354}})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAByBElEQVR4nO3dd1hT598G8DuEDTJkqwgoiuJARUXcA0XF1dZtZThaByri1iquinsUB4p7U62rrqq46p5oXVitW3EhoGyS5/3Dl/yMgAKigXB/ritXmyfPOed7kkO8c8ZzJEIIASIiIiIq9DRUXQARERER5Q8GOyIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsq9CQSCSZOnKjqMr7YunXrUKFCBWhpacHExETV5RRZ9vb2aNOmjarLKLJ8fX1hb2+v6jK+qfv370MikWD27NlffVkTJ06ERCLJt/m9e/cOlpaW2LBhQ77NMz907doVnTt3VnUZKsFgpwbu3r2Ln3/+GWXKlIGuri6MjIxQr149LFiwAElJSaouj3Lg1q1b8PX1RdmyZREWFoZly5Zl2zfji/nVq1dZvs5gonrJycmYN28e3NzcYGxsDF1dXZQvXx7+/v64ffu2Smt7+vQpJk6ciMjIyCJdw8cywpVEIsHUqVOz7NOjRw9IJBIYGhrmaRl79+5Vix+hH1qwYAGKFSuGrl27Kr2Hn3vcv3//i5f9qe1o1KhR+OOPP3DlypUvXk5ho6nqAujL7NmzB506dYKOjg68vb1RuXJlpKam4sSJExgxYgSuX7/+yZCgDpKSkqCpWbg35aNHj0Iul2PBggVwdHRUdTn0BV69eoWWLVvi4sWLaNOmDbp37w5DQ0NERUVh8+bNWLZsGVJTU1VW39OnTzFp0iTY29ujWrVqBa6GsLAwyOVyldQFALq6uti0aRN++eUXpfaEhATs3LkTurq6eZ733r17sWjRIrUJd2lpaViwYAGGDh0KqVQKCwsLrFu3TqnPnDlz8PjxY8ybN0+p3cLC4ouX/6ntqHr16qhZsybmzJmDtWvXfvGyCpPC/a9hEXfv3j107doVdnZ2OHz4MGxsbBSvDRw4EHfu3MGePXtUWOHXI5fLkZqaCl1d3S/6oi0oXrx4AQBF4hBsQkICDAwMVD6Pr8XX1xeXL1/G1q1b8cMPPyi9NmXKFIwbN05FleVNYmIi9PX1v9nytLS0vtmystK6dWts27YNV65cgYuLi6J9586dSE1NRcuWLXH48GEVVlhw7N69Gy9fvlQc8jQwMMCPP/6o1Gfz5s148+ZNpvZvoXPnzggKCsLixYvzvJe1MOKh2EJs5syZePfuHVasWKEU6jI4OjpiyJAhiufp6emYMmUKypYtCx0dHdjb22Ps2LFISUlRmi7jUN7Ro0dRs2ZN6OnpoUqVKjh69CgAYNu2bahSpQp0dXXh6uqKy5cvK03v6+sLQ0ND/Pfff/D09ISBgQFKlCiByZMnQwih1Hf27NmoW7cuzMzMoKenB1dXV2zdujXTukgkEvj7+2PDhg2oVKkSdHR0sH//fsVrH/4Cfvv2LQICAmBvbw8dHR1YWlqiefPmuHTpktI8t2zZAldXV+jp6cHc3Bw//vgjnjx5kuW6PHnyBB06dIChoSEsLCwwfPhwyGSybD4ZZYsXL1bUXKJECQwcOBCxsbFK73dQUBCA979i8/OcQSEE7O3t0b59+0yvJScnw9jYGD///DOA93sNJRIJwsPDMXbsWFhbW8PAwADt2rXDo0ePMk1/9uxZtGzZEsbGxtDX10ejRo1w8uRJpT4Zh41v3LiB7t27w9TUFPXr1wfwPpxPnDgRJUqUgL6+Ppo0aYIbN27A3t4evr6+inmsXr0aEokEx44dw4ABA2BpaYlSpUoBAB48eIABAwbAyckJenp6MDMzQ6dOnTId5smYx/Hjx/Hzzz/DzMwMRkZG8Pb2xps3b7J8706cOIHatWtDV1cXZcqUydGv/rNnz2LPnj3o3bt3plAHADo6OpnOozp8+DAaNGgAAwMDmJiYoH379rh582aW7+OdO3fg6+sLExMTGBsbw8/PD4mJiUp9Dx48iPr168PExASGhoZwcnLC2LFjAbz/jGvVqgUA8PPzUxwWW716NQCgcePGqFy5Mi5evIiGDRtCX19fMW122+XHnxcAxMbGYujQoYq/wVKlSsHb2xuvXr36bA1ZnWOXkJCAYcOGwdbWFjo6OnBycsLs2bMzfZ9kfE/s2LEDlStXho6ODipVqqT4rsgJd3d3ODg4YOPGjUrtGzZsQMuWLVG8ePEsp9u3b5/icyxWrBi8vLxw/fp1xeu+vr5YtGiRos6Mx8eWLVum+I6uVasWzp8/n6lPTrYZ4P02XKtWLejq6qJs2bJYunRplrV/apv5lB07dsDe3h5ly5b9bN8PpaSkICgoCI6OjtDR0YGtrS1GjhyZ6d+iL9mWAaB58+ZISEjAwYMHc1VfoSeo0CpZsqQoU6ZMjvv7+PgIAKJjx45i0aJFwtvbWwAQHTp0UOpnZ2cnnJychI2NjZg4caKYN2+eKFmypDA0NBTr168XpUuXFtOnTxfTp08XxsbGwtHRUchkMqXl6OrqinLlyomePXuKhQsXijZt2ggAYvz48UrLKlWqlBgwYIBYuHChmDt3rqhdu7YAIHbv3q3UD4CoWLGisLCwEJMmTRKLFi0Sly9fVrwWFBSk6Nu9e3ehra0tAgMDxfLly8WMGTNE27Ztxfr16xV9Vq1aJQCIWrVqiXnz5onRo0cLPT09YW9vL968eZNpXSpVqiR69eollixZIn744QcBQCxevPiz73lQUJAAIDw8PERISIjw9/cXUqlU1KpVS6SmpgohhNi+fbv47rvvBACxZMkSsW7dOnHlypXPzjMqKkq8fPky08PW1lZ4eXkp+o8bN05oaWmJ169fK83n999/FwDE8ePHhRBCHDlyRAAQVapUEVWrVhVz584Vo0ePFrq6uqJ8+fIiMTFRMW1ERITQ1tYW7u7uYs6cOWLevHmiatWqQltbW5w9ezZTrc7OzqJ9+/Zi8eLFYtGiRUIIIUaOHCkAiLZt24qFCxeKvn37ilKlSglzc3Ph4+OT6bNydnYWjRo1EiEhIWL69OlCCCG2bNkiXFxcxIQJE8SyZcvE2LFjhampqbCzsxMJCQmZ5lGlShXRoEED8dtvv4mBAwcKDQ0N0bBhQyGXyxV9M7Z/KysrMXbsWLFw4UJRo0YNIZFIxLVr1z75eY8dO1bpPf2cgwcPCk1NTVG+fHkxc+ZMMWnSJGFubi5MTU3FvXv3Mr2P1atXF99//71YvHix6NOnjwAgRo4cqeh37do1oa2tLWrWrCkWLFggQkNDxfDhw0XDhg2FEEJER0eLyZMnCwDip59+EuvWrRPr1q0Td+/eFUII0ahRI2FtbS0sLCzEoEGDxNKlS8WOHTuEEJn/zj58vz78vN6+fSsqV64spFKp6Nu3r1iyZImYMmWKqFWrlrh8+fJna/Dx8RF2dnaK+cnlctG0aVMhkUhEnz59xMKFC0Xbtm0FABEQEKBUCwDh4uIibGxsxJQpU8T8+fNFmTJlhL6+vnj16tUnP4t79+4JAGLWrFli7NixonTp0ort4uXLl0JTU1Ns2rRJ+Pj4CAMDA6Vp165dKyQSiWjZsqUICQkRM2bMEPb29sLExETxOZ46dUo0b95cAFCs87p165SWXb16deHo6ChmzJghZs6cKczNzUWpUqUU3xW52WauXr0q9PT0ROnSpUVwcLCYMmWKsLKyElWrVhUf/tP/uW3mUxwdHcX333//yT5eXl5Kn6dMJhMtWrQQ+vr6IiAgQCxdulT4+/sLTU1N0b59+xzX9bntSAgh0tLShJ6enhg2bNhn10WdMNgVUnFxcQKA0h/Cp0RGRgoAok+fPkrtw4cPFwDE4cOHFW12dnYCgDh16pSi7a+//hIAhJ6ennjw4IGifenSpQKAOHLkiKItI0AOGjRI0SaXy4WXl5fQ1tYWL1++VLR/GBaEECI1NVVUrlxZNG3aVKkdgNDQ0BDXr1/PtG4f/4NjbGwsBg4cmO17kZqaKiwtLUXlypVFUlKSon337t0CgJgwYUKmdZk8ebLSPKpXry5cXV2zXYYQQrx48UJoa2uLFi1aKAXfhQsXCgBi5cqViraMf7g/fG+yk9H3U48Pg11UVJQiNH6oXbt2wt7eXvGPV0awK1mypIiPj1f0ywiACxYsEEK8/yzLlSsnPD09lQJRYmKicHBwEM2bN89Ua7du3ZSWHR0dLTQ1NTP9qJg4caIAkGWwq1+/vkhPT1fq//H2I4QQp0+fFgDE2rVrM83D1dVV6R/JmTNnCgBi586diraM7f/DcPbixQuho6Pz2X8gMgL6hz8OPqVatWrC0tJSKXRfuXJFaGhoCG9vb0VbxvvYq1evTMszMzNTPJ83b95nt6Pz588LAGLVqlWZXmvUqJEAIEJDQzO9ltNgN2HCBAFAbNu2LVPfjO3lUzV8HOx27NghAIipU6cq9evYsaOQSCTizp07SjVqa2srtV25ckUAECEhIZmW9aEPg921a9cEAPH3338LIYRYtGiRMDQ0FAkJCZmC3du3b4WJiYno27ev0vyio6OFsbGxUvvAgQOVQtXHyzYzMxMxMTGK9p07dwoA4s8//1S05XSb6dChg9DV1VX6vr5x44aQSqVKNeRkm8lKWlqakEgkn/2b+DjYrVu3TmhoaCje2wyhoaECgDh58mSO6/rUdpShfPnyolWrVp9fITXCQ7GFVHx8PACgWLFiOeq/d+9eAEBgYKBS+7BhwwAg07l4zs7OcHd3Vzx3c3MDADRt2hSlS5fO1P7ff/9lWqa/v7/i/zMOkaSmpuLQoUOKdj09PcX/v3nzBnFxcWjQoEGmw6YA0KhRIzg7O39mTd+fp3b27Fk8ffo0y9cvXLiAFy9eYMCAAUrn53l5eaFChQpZnpfYr18/pecNGjTIcp0/dOjQIaSmpiIgIAAaGv/7U+vbty+MjIy++PzHP/74AwcPHsz0sLKyUupXvnx5uLm5KQ1HEBMTg3379imu8vuQt7e30nbVsWNH2NjYKLahyMhI/Pvvv+jevTtev36NV69e4dWrV0hISECzZs1w/PjxTCe/f/z+RUREID09HQMGDFBqHzRoULbr27dvX0ilUqW2D7eftLQ0vH79Go6OjjAxMclyG/rpp5+UzuHq378/NDU1FeuWwdnZGQ0aNFA8t7CwgJOT02c/89z8XT579gyRkZHw9fVVOrxXtWpVNG/ePFNNQNbb4evXrxXLzThHc+fOnXm+AEFHRwd+fn55mhZ4v126uLjgu+++y/RaXobZ2Lt3L6RSKQYPHqzUPmzYMAghsG/fPqV2Dw8PpUODVatWhZGR0Wc/uw9VqlQJVatWxaZNmwAAGzduRPv27bM81/DgwYOIjY1Ft27dFH8Lr169glQqhZubG44cOZLj5Xbp0gWmpqaK5xnbYEbtOd1mZDIZ/vrrL3To0EHp+7pixYrw9PRUWmZet5mYmBgIIZTqzYktW7agYsWKqFChgtL71bRpUwBQvF/5sS0DgKmpabYjCKgrBrtCysjICMD788ly4sGDB9DQ0Mh0xaW1tTVMTEzw4MEDpfYPvwwAwNjYGABga2ubZfvH5ylpaGigTJkySm3ly5cHAKXzn3bv3o06depAV1cXxYsXh4WFBZYsWYK4uLhM6+Dg4PC51QTw/tzDa9euwdbWFrVr18bEiROVvtQz1tXJySnTtBUqVMj0Xujq6ma6gsvU1DTbc7M+txxtbW2UKVMm03Jyq2HDhvDw8Mj0yOpiEm9vb5w8eVKxzC1btiAtLQ09e/bM1LdcuXJKzyUSCRwdHRWf27///gsA8PHxgYWFhdJj+fLlSElJyfT5ffzZZdTx8fZYvHjxbP+hyOrzT0pKwoQJExTnXpmbm8PCwgKxsbFZbkMfr5uhoSFsbGwynZP38fYP5Owzz83f5ae2w4oVKyrC8qfqynivMurq0qUL6tWrhz59+sDKygpdu3bF77//nqt/GEuWLAltbe0c9//Y3bt3Ubly5TxP/7EHDx6gRIkSmcJyxYoVFa9/KK+f3ce6d++OLVu24M6dOzh16hS6d++eZb+Mv4emTZtm+ns4cOCA4sKonPjc55vTbebly5dISkrKtL1nNe2XbjPio/McP+fff//F9evXM71XGf8+ZLxf+bEtZ9SXn+P2FQa8KraQMjIyQokSJXDt2rVcTZfTDfzjPSOfa8/tHzcA/P3332jXrh0aNmyIxYsXw8bGBlpaWli1alWmE5cB5b0zn9K5c2c0aNAA27dvx4EDBzBr1izMmDED27ZtQ6tWrXJdZ3brXJh07doVQ4cOxYYNGzB27FisX78eNWvWzPIfiM/J+GKdNWtWtsNlfHwFWk4/u0/Jah6DBg3CqlWrEBAQAHd3dxgbG0MikaBr165f9Cs/r9t5hQoVAAD//POP0h6//PK5uvT09HD8+HEcOXIEe/bswf79+xEeHo6mTZviwIEDOdqWc/tZ5fQiom8lv76junXrhjFjxqBv374wMzNDixYtsuyXsZ2tW7cO1tbWmV7PzVBM+fn9mlN53WaKFy8OiUSS68Asl8tRpUoVzJ07N8vXM3Ye5Me2DLwPxVkFXHXGPXaFWJs2bXD37l2cPn36s33t7Owgl8sVvy4zPH/+HLGxsbCzs8vX2uRyeaZDHxkDs2Zc8fbHH39AV1cXf/31F3r16oVWrVrBw8MjX5ZvY2ODAQMGYMeOHbh37x7MzMzw66+/AoBiXaOiojJNFxUVlW/vRXbLSU1Nxb179/L9Pf+U4sWLw8vLCxs2bMCDBw9w8uTJLPfWAci0jQghcOfOHcXnlnGYy8jIKMs9hh4eHp8dsiJj3e/cuaPU/vr161z9Q7F161b4+Phgzpw56NixI5o3b4769esrXXX8qXV79+4dnj17lm93Omjbti0AYP369Z/t+6nt8NatWzA3N8/TkC4aGhpo1qwZ5s6dixs3buDXX3/F4cOHFYe48rr3wtTUNNP7mpqaimfPnim1lS1b9rM/OHNTg52dHZ4+fZppL+itW7cUr38NpUuXRr169XD06FF06tQp24CW8fdgaWmZ5d9C48aNFX2/dM9RTrcZCwsL6OnpZdres5v2c9tMVjQ1NVG2bFncu3cvV+tQtmxZxMTEoFmzZlm+Xx/+2PzSbTk9PR2PHj1S7N0tKhjsCrGRI0fCwMAAffr0wfPnzzO9fvfuXSxYsADA+7GZAGD+/PlKfTJ+NXl5eeV7fQsXLlT8vxACCxcuhJaWFpo1awbg/a9TiUSi9Iv//v372LFjR56XKZPJMh2Cs7S0RIkSJRSX0tesWROWlpYIDQ1Vurx+3759uHnzZr69Fx4eHtDW1sZvv/2m9It7xYoViIuL+yrv+af07NkTN27cwIgRIyCVStG1a9cs+61du1bpH9GtW7fi2bNnir2drq6uKFu2LGbPno13795lmv7ly5efraVZs2bQ1NTEkiVLlNo/3GZyQiqVZtqbERISku1epGXLliEtLU3xfMmSJUhPT8/TntysuLu7o2XLlli+fHmW23FqaiqGDx8O4P2Pj2rVqmHNmjVKgenatWs4cOCA4m82N2JiYjK1ZexVzdjWM8JiduE3O2XLlsXx48eV2pYtW5bpvf7hhx9w5coVbN++PdM8Mj6r3NTQunVryGSyTNvGvHnzIJFI8u2zy8rUqVMRFBT0yXM/PT09YWRkhGnTpiltWxk+/HvI63ufIafbjFQqhaenJ3bs2IGHDx8q+t28eRN//fWX0jxzss1kx93dHRcuXMjVOnTu3BlPnjxBWFhYpteSkpIUpx/kx7Z848YNJCcno27durmqsbDjodhCrGzZsti4cSO6dOmCihUrKt154tSpU9iyZYtifCkXFxf4+Phg2bJliI2NRaNGjXDu3DmsWbMGHTp0QJMmTfK1Nl1dXezfvx8+Pj5wc3PDvn37sGfPHowdO1ZxvpqXlxfmzp2Lli1bonv37njx4gUWLVoER0dHXL16NU/Lffv2LUqVKoWOHTvCxcUFhoaGOHToEM6fP485c+YAeD8A6owZM+Dn54dGjRqhW7dueP78ORYsWAB7e3sMHTo0X94DCwsLjBkzBpMmTULLli3Rrl07REVFYfHixahVq9Y3H7DTy8sLZmZm2LJlC1q1agVLS8ss+xUvXhz169eHn58fnj9/jvnz58PR0RF9+/YF8P5X9PLly9GqVStUqlQJfn5+KFmyJJ48eYIjR47AyMgIf/755ydrsbKywpAhQzBnzhy0a9cOLVu2xJUrV7Bv3z6Ym5vneM9GmzZtsG7dOhgbG8PZ2RmnT5/GoUOHYGZmlmX/1NRUNGvWDJ07d1Z8FvXr10e7du1ytLycWLt2LVq0aIHvv/8ebdu2RbNmzWBgYIB///0XmzdvxrNnzxRj2c2aNQutWrWCu7s7evfujaSkJISEhMDY2DhPYxlOnjwZx48fh5eXF+zs7PDixQssXrwYpUqVUowfWLZsWZiYmCA0NBTFihWDgYEB3NzcPnsOa58+fdCvXz/88MMPaN68Oa5cuYK//voL5ubmSv1GjBiBrVu3olOnTujVqxdcXV0RExODXbt2ITQ0FC4uLrmqoW3btmjSpAnGjRuH+/fvw8XFBQcOHMDOnTsREBCQ6zHUcqNRo0Zo1KjRJ/sYGRlhyZIl6NmzJ2rUqIGuXbvCwsICDx8+xJ49e1CvXj1FKHV1dQUADB48GJ6enp/8gZWdnG4zkyZNwv79+9GgQQMMGDAA6enpCAkJQaVKlZS+X3OyzWSnffv2WLduHW7fvq04R+5zevbsid9//x39+vXDkSNHUK9ePchkMty6dQu///47/vrrL9SsWTNftuWDBw9CX18fzZs3z9V7XOip4lJcyl+3b98Wffv2Ffb29kJbW1sUK1ZM1KtXT4SEhIjk5GRFv7S0NDFp0iTh4OAgtLS0hK2trRgzZoxSHyHeD1/w4XAZGQBkGkbkwyECMmQMB3D37l3FeEVWVlYiKChIadgPIYRYsWKFKFeunNDR0REVKlQQq1atUgzt8Lllf/haxjAMKSkpYsSIEcLFxUUUK1ZMGBgYCBcXlyzHnAsPDxfVq1cXOjo6onjx4qJHjx7i8ePHSn2yGrNKCJFljdlZuHChqFChgtDS0hJWVlaif//+mYbDyMtwJ9n1ze7zE0KIAQMGCABi48aNmV7LGO5k06ZNYsyYMcLS0lLo6ekJLy8vpSETMly+fFl8//33wszMTOjo6Ag7OzvRuXNnERERkaNa09PTxfjx44W1tbXQ09MTTZs2FTdv3hRmZmaiX79+in4ZQ5WcP38+0zzevHkj/Pz8hLm5uTA0NBSenp7i1q1bmYbgyJjHsWPHxE8//SRMTU2FoaGh6NGjR6bx/bJ7/xo1aiQaNWqU5fv6scTERDF79mxRq1YtYWhoKLS1tUW5cuXEoEGDlIbiEEKIQ4cOiXr16gk9PT1hZGQk2rZtK27cuKHUJ7v3MWO9MsYvi4iIEO3btxclSpQQ2traokSJEqJbt27i9u3bStPt3LlTODs7C01NTaXhIho1aiQqVaqU5TrJZDIxatQoYW5uLvT19YWnp6e4c+dOpvdaCCFev34t/P39RcmSJYW2trYoVaqU8PHxURpLLrsaPh7uRIj3Q4oMHTpUlChRQmhpaYly5cqJWbNmKQ23I0T23xNZ1fixrL7LspLdd8KRI0eEp6enMDY2Frq6uqJs2bLC19dXXLhwQdEnPT1dDBo0SFhYWAiJRKL4DvnUsj/8fsuQk21GCCGOHTsmXF1dhba2tihTpowIDQ3N9N2V020mKykpKcLc3FxMmTIl2z4fD3cixPshp2bMmCEqVaokdHR0hKmpqXB1dRWTJk0ScXFxuaoru+1ICCHc3NzEjz/++Nn1UDcMdpTvsvviI9ULCAgQxYoVUxq8N0NGsNuyZYsKKnvvzZs3WY5Z9qU+FQ6JKO8mT54sHBwcMo0xqWqXL18WEolEMZB9UcJz7IiKiOTkZKxfvx4//PDDN733Z3aSkpIytWWcA/rhCedEVHANHToU7969w+bNm1VdipLp06ejY8eO2V65r854jh2Rmnvx4gUOHTqErVu34vXr10r3D1al8PBwrF69Gq1bt4ahoSFOnDiBTZs2oUWLFqhXr56qyyOiHDA0NMzVWH3fSkELmt8Sgx2Rmrtx4wZ69OgBS0tL/PbbbwXmF2zVqlWhqamJmTNnIj4+XnFBxdSpU1VdGhFRoSUR4iuOfPgZx48fx6xZs3Dx4kU8e/YM27dvR4cOHT45zdGjRxEYGIjr16/D1tYWv/zyi+LKTyIiIqKiTKXn2CUkJMDFxQWLFi3KUf979+7By8sLTZo0QWRkJAICAtCnT59M4/IQERERFUUq3WP3IYlE8tk9dqNGjcKePXuURjXv2rUrYmNjsX///m9QJREREVHBVajOsTt9+nSmW055enoiICAg22lSUlKURs+Wy+WIiYmBmZlZkbsxMBERERU+Qgi8ffsWJUqUgIbGpw+2FqpgFx0dDSsrK6U2KysrxMfHIykpKcubVwcHB2PSpEnfqkQiIiKir+LRo0coVarUJ/sUqmCXF2PGjEFgYKDieVxcHEqXLo1Hjx7ByMhIhZURERERfV58fDxsbW1RrFixz/YtVMHO2to6083unz9/DiMjoyz31gGAjo4OdHR0MrUbGRkx2BEREVGhkZNTyArVnSfc3d0RERGh1Hbw4EG4u7urqCIiIiKigkOlwe7du3eIjIxEZGQkgPfDmURGRuLhw4cA3h9G9fb2VvTv168f/vvvP4wcORK3bt3C4sWL8fvvv2Po0KGqKJ+IiIioQFFpsLtw4QKqV6+O6tWrAwACAwNRvXp1TJgwAQDw7NkzRcgDAAcHB+zZswcHDx6Ei4sL5syZg+XLl8PT01Ml9RMREREVJAVmHLtvJT4+HsbGxoiLi+M5dkRERFTg5Sa7FKpz7IiIiIgoewx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQmGOyIiIiI1ASDHREREZGaYLAjIiIiUhMMdkRERERqgsGOiIiISE0w2BERERGpCQY7IiIiIjXBYEdERESkJhjsiIiIiNQEgx0RERGRmmCwIyIiIlITDHZEREREaoLBjoiIiEhNMNgRERERqQkGOyIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQmGOyIiIiI1ASDHREREZGaYLAjIiIiUhMMdkRERERqgsGOiIiISE0w2BERERGpCQY7IiIiIjXBYEdERESkJhjsiIiIiNQEgx0RERGRmmCwIyIiIlITDHZEREREakLlwW7RokWwt7eHrq4u3NzccO7cuU/2nz9/PpycnKCnpwdbW1sMHToUycnJ36haIiIiooJLpcEuPDwcgYGBCAoKwqVLl+Di4gJPT0+8ePEiy/4bN27E6NGjERQUhJs3b2LFihUIDw/H2LFjv3HlRERERAWPSoPd3Llz0bdvX/j5+cHZ2RmhoaHQ19fHypUrs+x/6tQp1KtXD927d4e9vT1atGiBbt26fXYvHxEREVFRoLJgl5qaiosXL8LDw+N/xWhowMPDA6dPn85ymrp16+LixYuKIPfff/9h7969aN269TepmYiIiKgg01TVgl+9egWZTAYrKyuldisrK9y6dSvLabp3745Xr16hfv36EEIgPT0d/fr1++Sh2JSUFKSkpCiex8fH588KEBERERUwKr94IjeOHj2KadOmYfHixbh06RK2bduGPXv2YMqUKdlOExwcDGNjY8XD1tb2G1ZMRERE9O1IhBBCFQtOTU2Fvr4+tm7dig4dOijafXx8EBsbi507d2aapkGDBqhTpw5mzZqlaFu/fj1++uknvHv3DhoamXNqVnvsbG1tERcXByMjo/xdKSIiIqJ8Fh8fD2Nj4xxlF5XtsdPW1oarqysiIiIUbXK5HBEREXB3d89ymsTExEzhTSqVAgCyy6c6OjowMjJSehARERGpI5WdYwcAgYGB8PHxQc2aNVG7dm3Mnz8fCQkJ8PPzAwB4e3ujZMmSCA4OBgC0bdsWc+fORfXq1eHm5oY7d+5g/PjxaNu2rSLgERERERVVKg12Xbp0wcuXLzFhwgRER0ejWrVq2L9/v+KCiocPHyrtofvll18gkUjwyy+/4MmTJ7CwsEDbtm3x66+/qmoViIiIiAoMlZ1jpyq5OU5NREREpGqF4hw7IiIiIspfDHZEREREakKl59gREZH6sx+9R9Ul5Nn96V6qLoEoV7jHjoiIiEhNMNgRERERqQkGOyIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZrgcCdERERqqMqaKqouIU/+8flH1SUUagx2RERE2ZlorOoK8s6htKorIBXgoVgiIiIiNcE9dqTA0eGJiIgKNwY7IhUqrOfAADwPhoioIOKhWCIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITXAcO1IPhfW2P7zlDxER5SPusSMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQmGOyIiIiI1ASDHREREZGaYLAjIiIiUhMMdkRERERqgsGOiIiISE0w2BERERGpCQY7IiIiIjXBYEdERESkJhjsiIiIiNQEgx0RERGRmmCwIyIiIlITDHZEREREaoLBjoiIiEhNMNgRERERqQkGOyIiIiI1ofJgt2jRItjb20NXVxdubm44d+7cJ/vHxsZi4MCBsLGxgY6ODsqXL4+9e/d+o2qJiIiICi5NVS48PDwcgYGBCA0NhZubG+bPnw9PT09ERUXB0tIyU//U1FQ0b94clpaW2Lp1K0qWLIkHDx7AxMTk2xdPREREVMDkeo+dvb09Jk+ejIcPH37xwufOnYu+ffvCz88Pzs7OCA0Nhb6+PlauXJll/5UrVyImJgY7duxAvXr1YG9vj0aNGsHFxeWLayEiIiIq7HId7AICArBt2zaUKVMGzZs3x+bNm5GSkpLrBaempuLixYvw8PD4XzEaGvDw8MDp06eznGbXrl1wd3fHwIEDYWVlhcqVK2PatGmQyWTZLiclJQXx8fFKDyIiIiJ1lKdgFxkZiXPnzqFixYoYNGgQbGxs4O/vj0uXLuV4Pq9evYJMJoOVlZVSu5WVFaKjo7Oc5r///sPWrVshk8mwd+9ejB8/HnPmzMHUqVOzXU5wcDCMjY0VD1tb2xzXSERERFSY5PniiRo1auC3337D06dPERQUhOXLl6NWrVqoVq0aVq5cCSFEftYJAJDL5bC0tMSyZcvg6uqKLl26YNy4cQgNDc12mjFjxiAuLk7xePToUb7XRURERFQQ5PniibS0NGzfvh2rVq3CwYMHUadOHfTu3RuPHz/G2LFjcejQIWzcuDHb6c3NzSGVSvH8+XOl9ufPn8Pa2jrLaWxsbKClpQWpVKpoq1ixIqKjo5Gamgptbe1M0+jo6EBHRyePa0lERERUeOQ62F26dAmrVq3Cpk2boKGhAW9vb8ybNw8VKlRQ9Pnuu+9Qq1atT85HW1sbrq6uiIiIQIcOHQC83yMXEREBf3//LKepV68eNm7cCLlcDg2N9zsbb9++DRsbmyxDHREREVFRkutDsbVq1cK///6LJUuW4MmTJ5g9e7ZSqAMABwcHdO3a9bPzCgwMRFhYGNasWYObN2+if//+SEhIgJ+fHwDA29sbY8aMUfTv378/YmJiMGTIENy+fRt79uzBtGnTMHDgwNyuBhEREZHayfUeu//++w92dnaf7GNgYIBVq1Z9dl5dunTBy5cvMWHCBERHR6NatWrYv3+/4oKKhw8fKvbMAYCtrS3++usvDB06FFWrVkXJkiUxZMgQjBo1KrerQURERKR2ch3sXrx4gejoaLi5uSm1nz17FlKpFDVr1szV/Pz9/bM99Hr06NFMbe7u7jhz5kyulkFERERUFOT6UOzAgQOzvLL0yZMnPCRKREREpEK5DnY3btxAjRo1MrVXr14dN27cyJeiiIiIiCj3ch3sdHR0Mg1RAgDPnj2DpqZKbz1LREREVKTlOti1aNFCMehvhtjYWIwdOxbNmzfP1+KIiIiIKOdyvYtt9uzZaNiwIezs7FC9enUAQGRkJKysrLBu3bp8L5CIiIiIcibXwa5kyZK4evUqNmzYgCtXrkBPTw9+fn7o1q0btLS0vkaNRERERJQDeTopzsDAAD/99FN+10JEREREXyDPVzvcuHEDDx8+RGpqqlJ7u3btvrgoIiIiIsq9PN154rvvvsM///wDiUQCIQQAQCKRAABkMln+VkhEREREOZLrq2KHDBkCBwcHvHjxAvr6+rh+/TqOHz+OmjVrZnmnCCIiIiL6NnK9x+706dM4fPgwzM3NoaGhAQ0NDdSvXx/BwcEYPHgwLl++/DXqJCIiIqLPyPUeO5lMhmLFigEAzM3N8fTpUwCAnZ0doqKi8rc6IiIiIsqxXO+xq1y5Mq5cuQIHBwe4ublh5syZ0NbWxrJly1CmTJmvUSMRERER5UCug90vv/yChIQEAMDkyZPRpk0bNGjQAGZmZggPD8/3AomIiIgoZ3Id7Dw9PRX/7+joiFu3biEmJgampqaKK2OJiIiI6NvL1Tl2aWlp0NTUxLVr15TaixcvzlBHREREpGK5CnZaWlooXbo0x6ojIiIiKoByfVXsuHHjMHbsWMTExHyNeoiIiIgoj3J9jt3ChQtx584dlChRAnZ2djAwMFB6/dKlS/lWHBERERHlXK6DXYcOHb5CGURERET0pXId7IKCgr5GHURERET0hXJ9jh0RERERFUy53mOnoaHxyaFNeMUsERERkWrkOtht375d6XlaWhouX76MNWvWYNKkSflWGBERERHlTq6DXfv27TO1dezYEZUqVUJ4eDh69+6dL4URERERUe7k2zl2derUQURERH7NjoiIiIhyKV+CXVJSEn777TeULFkyP2ZHRERERHmQ60OxpqamShdPCCHw9u1b6OvrY/369flaHBERERHlXK6D3bx585SCnYaGBiwsLODm5gZTU9N8LY6IiIiIci7Xwc7X1/crlEFEREREXyrX59itWrUKW7ZsydS+ZcsWrFmzJl+KIiIiIqLcy3WwCw4Ohrm5eaZ2S0tLTJs2LV+KIiIiIqLcy3Wwe/jwIRwcHDK129nZ4eHDh/lSFBERERHlXq6DnaWlJa5evZqp/cqVKzAzM8uXooiIiIgo93Id7Lp164bBgwfjyJEjkMlkkMlkOHz4MIYMGYKuXbt+jRqJiIiIKAdyfVXslClTcP/+fTRr1gyamu8nl8vl8Pb25jl2RERERCqU62Cnra2N8PBwTJ06FZGRkdDT00OVKlVgZ2f3NeojIiIiohzKdbDLUK5cOZQrVy4/ayEiIiKiL5Drc+x++OEHzJgxI1P7zJkz0alTp3wpioiIiIhyL9fB7vjx42jdunWm9latWuH48eP5UhQRERER5V6ug927d++gra2dqV1LSwvx8fH5UhQRERER5V6ug12VKlUQHh6eqX3z5s1wdnbOl6KIiIiIKPdyHezGjx+PKVOmwMfHB2vWrMGaNWvg7e2NqVOnYvz48XkqYtGiRbC3t4euri7c3Nxw7ty5HE23efNmSCQSdOjQIU/LJSIiIlInuQ52bdu2xY4dO3Dnzh0MGDAAw4YNw5MnT3D48GE4OjrmuoDw8HAEBgYiKCgIly5dgouLCzw9PfHixYtPTnf//n0MHz4cDRo0yPUyiYiIiNRRroMdAHh5eeHkyZNISEjAf//9h86dO2P48OFwcXHJ9bzmzp2Lvn37ws/PD87OzggNDYW+vj5WrlyZ7TQymQw9evTApEmTUKZMmbysAhEREZHayVOwA95fHevj44MSJUpgzpw5aNq0Kc6cOZOreaSmpuLixYvw8PD4X0EaGvDw8MDp06eznW7y5MmwtLRE796981o+ERERkdrJ1QDF0dHRWL16NVasWIH4+Hh07twZKSkp2LFjR54unHj16hVkMhmsrKyU2q2srHDr1q0spzlx4gRWrFiByMjIHC0jJSUFKSkpiue8cpeIiIjUVY732LVt2xZOTk64evUq5s+fj6dPnyIkJORr1pbJ27dv0bNnT4SFhcHc3DxH0wQHB8PY2FjxsLW1/cpVEhEREalGjvfY7du3D4MHD0b//v3z7VZi5ubmkEqleP78uVL78+fPYW1tnan/3bt3cf/+fbRt21bRJpfLAQCampqIiopC2bJllaYZM2YMAgMDFc/j4+MZ7oiIiEgt5XiP3YkTJ/D27Vu4urrCzc0NCxcuxKtXr75o4dra2nB1dUVERISiTS6XIyIiAu7u7pn6V6hQAf/88w8iIyMVj3bt2qFJkyaIjIzMMrDp6OjAyMhI6UFERESkjnIc7OrUqYOwsDA8e/YMP//8MzZv3owSJUpALpfj4MGDePv2bZ4KCAwMRFhYGNasWYObN2+if//+SEhIgJ+fHwDA29sbY8aMAQDo6uqicuXKSg8TExMUK1YMlStXzvKOGERERERFRa6vijUwMECvXr1w4sQJ/PPPPxg2bBimT58OS0tLtGvXLtcFdOnSBbNnz8aECRNQrVo1REZGYv/+/YoLKh4+fIhnz57ler5ERERERY1ECCG+dCYymQx//vknVq5ciV27duVHXV9NfHw8jI2NERcXx8OyH7EfvUfVJeTZfd3uqi4hT6o4lFZ1CXn2j88/qi6BCgl+t6hGYf1+4XdLZrnJLnkex+5DUqkUHTp0KPChjoiIiEid5UuwIyIiIiLVY7AjIiIiUhMMdkRERERqgsGOiIiISE0w2BERERGpCQY7IiIiIjXBYEdERESkJhjsiIiIiNQEgx0RERGRmmCwIyIiIlITDHZEREREaoLBjoiIiEhNMNgRERERqQkGOyIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQmGOyIiIiI1ASDHREREZGa0FR1AQWVTCZDWlqaqsv4pkoWk6q6hDxL1rH9ejMXcmgnPoeGSP96yyAiIsoHDHYfEUIgOjoasbGxqi7lm5vYxFLVJeTZPcmcrzh3AY2kGDicGw/t5FdfcTlERERfhsHuIxmhztLSEvr6+pBIJKou6ZtJ1YtXdQl55vAVTyqQC+DpGyM8q9ALpSNnQQLx9RZGRET0BRjsPiCTyRShzszMTNXlfHMSzWRVl5BnuhpfN4BbGOviqUU1pGsbQSs17qsui4iIKK948cQHMs6p09fXV3ElVNBoawDQ0IRMq5iqSyEiIsoWg10WitLhV8oZxSbBbYOIiAowBjsiIiIiNcFgR0RERKQmePFEDtmP3vNNl3d/uleu+vv6+iI2NhY7duxQaj969CiaNGmCN2/ewMTEBMD7IV2WL1+OlStX4vr165DL5bCzs0O1Og3QzfcnlHYoAwBYMnc6QufNQMcffTE+eJ5inreu/4MuLRti76krKGlbGk8ePUTrui4wNTPHnhOXYGD4v/PQOns2QBNPL/QPHJ2p5t6d2uDCmZPZrlPNOvWwYsvuXL0PGRp37ItqzuUxf/KIPE1PRERUGHGPXREjhED37t0xePBgtG7dGgcOHMCNGzewYsUKaOvoIOy32Ur9dXR0sWPzejy4d/ez80589w5rli7McS1zl61DxMVbiLh4Cxv+jAAALNu0Q9E2d9m63K0cERFREcc9dkVMeHg4Nm/ejJ07d6Jdu3aK9tKlS0O/VAUIoTxGm31ZR5iaWWDhzKmYtWTVJ+fdza8v1oUtRhefPjAzt/hsLcampor/T0lJ+f+24jC3tAIAXDp3Gr9Nn4wbVyNhUrw4mrZsg8GjJ0Bf3+D9uqxZjvXLlyD62ROYFDNAg9rVsTVsFnwDgnDs9EUcO30RC1ZsAgDcO7Mb9rYlcvAOERERFV7cY1fEbNq0CU5OTkqh7kNZXREcMCYIh/buwvUrlz8575btO8LW3gFL58/84jof3b+HAT07waN1O2w5eAIzF6/E5fNnEPzLSADA9SuXMSNoNAYMG4OdR89h/4aFaFinBgBgweThcHetir49vsOzywfw7PIB2Jaw+uKaiIiICjoGOzWye/duGBoaKj1atWql1Of27dtwcnJSagsICIChoSHqOJVC81qVMs23YhUXtGjTAfODJ35y+RKJBENGB+GPjWvw6P69L1qXFYvmofV3HfFjn/6wcyiLajXdMGrSdOz+YzNSkpPx7Olj6Onro6GHJ0qUKo3qlStgcO9uAABjo2LQ1taCvq4urC3NYW1pDqm08N4Hl4iIKKcY7NRIkyZNEBkZqfRYvnz5Z6cbN24cIiMj8XPACCQlvsuyj/+IX3Dp3GmcOnb4k/Oq17gZqteqg0Wzf83TOmS4feMadm3ZhDpOpRSP/j92hFwux5NHD+DeoDFsStrCq151jB3yMzZs24vEpKQvWiYREVFhx3Ps1IiBgQEcHR2V2h4/fqz0vFy5coiKilJqs7CwgIWFBYqbZX9enK29A37o5o0F0ydh4qyQT9YxZHQQvDu0gE+/wblcg/9JTExAxx6+6O73c6bXbEqWgpa2NjbvO4YLp0/g9PHDmDA7FBPnLMX5vethYsy7QxARUdHEPXZFTLdu3RAVFYWdO3fmetqfA0biwX93sX/XH5/sV6W6K5q1aosFwZPyWiYqVq6K//6NQmmHMpkeWtraAABNTU3UadAYQ8dNxtVD4bj/+BkOnzwHANDW0oJMLs/z8omIiAoj7rErYrp27Ypt27aha9euGDNmDDw9PWFlZYUHDx7grz+3QUMj+3PRzCws0bPvAKwJ/fQeOwDwH/kLfmjmDqk0b5uY34Ah6NmuBab9MgLfd/OGnr4+/rsdhdN/H8HYqbNw7NB+PH74AK5udWFkbIyTR3ZDLpfDqaw9AMDe1gZnL1/D/UdPYWigh+ImxtDQ4O8YIiJSbwXiX7pFixbB3t4eurq6cHNzw7lz57LtGxYWhgYNGsDU1BSmpqbw8PD4ZH9SJpFIEB4ejvnz52Pv3r1o1qwZnJyc0KtXL1iXKIXV2/Z9cnqfn/2hb2Dw2eXYl3FE+y49kJKSnKc6y1esjBVbduPBf3fh90NrdGnZCIvnTIOllTUAoJiRMQ7v+xN9u7TDd03qIHTdVmxaNA2VnMoCAIb/7A2phgacG3eERZVmePgkOk91EBERFSYS8fHAZd9YeHg4vL29ERoaCjc3N8yfPx9btmxBVFQULC0tM/Xv0aMH6tWrh7p160JXVxczZszA9u3bcf36dZQsWfKzy4uPj4exsTHi4uJgZGSk9FpycjLu3bsHBwcH6Orq5ts6FhZXH8equoQ8q6rxZVfhfk5yusC9Jy/hcHIYdN89yrf5VnEonW/z+tb+8flH1SVQIfGt79yTn+7rdld1CXlWWL9f+N2S2aeyy8dUvsdu7ty56Nu3L/z8/ODs7IzQ0FDo6+tj5cqVWfbfsGEDBgwYgGrVqqFChQpYvnw55HI5IiIivnHlRERERAWLSoNdamoqLl68CA8PD0WbhoYGPDw8cPr06RzNIzExEWlpaShevHiWr6ekpCA+Pl7pQURERKSOVBrsXr16BZlMBisr5bsCWFlZITo6Z+dEjRo1CiVKlFAKhx8KDg6GsbGx4mFra/vFdRMREREVRCo/FPslpk+fjs2bN2P79u3ZnhM3ZswYxMXFKR6PHuXf+VFEREREBYlKhzsxN39/q6fnz58rtT9//hzW1tafnHb27NmYPn06Dh06hKpVq2bbT0dHBzo6OvlSLxEREVFBptI9dtra2nB1dVW68CHjQgh3d/dsp5s5cyamTJmC/fv3o2bNmt+iVCIiIqICT+UDFAcGBsLHxwc1a9ZE7dq1MX/+fCQkJMDPzw8A4O3tjZIlSyI4OBgAMGPGDEyYMAEbN26Evb294ly8jJveExERERVVKg92Xbp0wcuXLzFhwgRER0ejWrVq2L9/v+KCiocPHyrdMWDJkiVITU1Fx44dleYTFBSEiRMnfsvSiYiIiAoUlQc7APD394e/v3+Wrx09elTp+f37979+QURERESFUKG+Kpb+x9fXFx06dMjUfvToUUgkEsTGxirahBAICwuDu7s7jIyMYGhoiEqVKmFG0Gg8vPefot+SudPhYmuKKWOGKs3z1vV/4GJriiePHgIAnjx6CBdbUzSuVg4J794q9e3s2QBL5k5XPO/dqQ1cbE3hYmuKWo7W+K5pHYSvWZ7lOu38faOib3aPjBpya3X4LphUbJinaYmIiAqqArHHrlCYaPyNlxf3VWYrhED37t2xY8cOjB07FvPmzUOJEiXw9OlTLF27CWG/zcaUeYsV/XV0dLFj83p4/+QPO4eyn5x34rt3WLN0IQYMG/PJfj9098GAYWOQnJSEP//YjGm/jICRsQladVA+vO7Z9jvUa9xM8Tzwp55wdHJWmr+pmXluVp+IiEitMdgVMeHh4di8eTN27tyJdu3aKdpLly4N/VIV8PGtg+3LOsLUzAILZ07FrCWrPjnvbn59sS5sMbr49IGZuUW2/XT19GBu+f4cyv6Bo7F3x1YcPbgvU7DT1dODrp6e4rmWlrbStPFxcZgyeiiOHtiL9NRk1KxaEfMmDodLpfIAgCvXbyMgaDYuXL0BiUSCcg62WDrjF7xLSIRf4EQAgKRkDQBAUOBPmDis3yfXj4iIqKDjodgiZtOmTXByclIKdR+SSCSZ2gLGBOHQ3l24fuXyJ+fdsn1H2No7YOn8mbmqSVdXF2lpabmaBgBG9PdFzKuXWLR2Cy7u24AaVSqiWZd+iHnzfm9nj0HjUMrGEuf3rsPFfRsweqAftDQ1UbemC+ZPGg6jYoZ4dvkAnl0+gOH9vHO9fCIiooKGwU6N7N69WzHsS8ajVatWSn1u374NJycnpbaAgAAYGhqijlMpNK9VKdN8K1ZxQYs2HTA/eOInly+RSDBkdBD+2LgGj+7f+2y9MpkMu7eF4/bN66hdt8HnV/ADl86dxrXIi5gduhqVXKqjXJnSmD1hKEyMDbF1zyEAwMMn0fBo4IYKjg4oV6Y0OrVtDpdK5aGtrQXjYoaQSABrS3NYW5rD0EA/V8snIiIqiBjs1EiTJk0QGRmp9Fi+POsLEz40btw4REZG4ueAEUhKfJdlH/8Rv+DSudM4dezwJ+dVr3EzVK9VB4tm/5ptn/C1K1DHqRRql7PB5JEB+LHPAHT27v3ZOj90+8Y1JCYkoGHVsqjjVAqG5erBsFw93Hv4FHcfPAYABP7UA31GTIFHl36YvnAV7t7n7eSIiEi98Rw7NWJgYABHR0eltsePHys9L1euHKKiopTaLCwsYGFhgeJm2Z8XZ2vvgB+6eWPB9EmYOCvkk3UMGR0E7w4t4NNvcJavt+7QCX0HDYOOri4srKyVxinMqcTEBJhbWmPF738CACpo/C+0mRgXAwBMHNYP3Tu0wp6Iv7HvyCkEzQnF5sXB+K5V01wvj4iIqDDgHrsiplu3boiKisLOnTtzPe3PASPx4L+72L/rj0/2q1LdFc1atcWC4ElZvl7MyAilHcrAyqZEnkIdAFSs7ILXL59DqqmJ0g5l4OhQWvEwL26q6Fe+rB2G/vQjDmxajO9bNcWq8F0AAG1tLchk8jwtm4iIqKBisCtiunbtio4dO6Jr166YPHkyzp49i/v37+PYsWP4689t0NCQZjutmYUlevYdgE0rl312Of4jf8H5U8dx/+6d/CxfoU6DxqhaoxaG9umBU8cO4/6jpzh1/grGTV+IC1duICkpGf7jpuPoqQt48PgpTp6PxPkr11GxnAMAwL5UCbxLSETE32fxKuYNEpOSvkqdRERE3xKDXREjkUgQHh6O+fPnY+/evWjWrBmcnJzQq1cvWJcohdXb9n1yep+f/aFvYPDZ5diXcUT7Lj2QkpKcX6UrkUgkWLT2d9Rwq4sJw/xRvkEHdB0wBg+ePIOVeXFIpVK8fhMH7yETUL7Bd+jcbxRaNamHSf8/pEndWi7o17MjuvQfA4sqzTBz8ZqvUicREdG3JBEfD1ym5uLj42FsbIy4uDgYGRkpvZacnIx79+7BwcEBurq6KqpQda4+jlV1CXlWVePzV+F+ieR0gXtPXsLh5DDovsu/izCqOJTOt3l9a//4/KPqEqiQsB+9R9Ul5Nl93e6qLiHPCuv3C79bMvtUdvkY99gRERERqQkGOyIiIiI1wWBHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjvKpJV7VaxfvkTVZRAREVEuMdipicaNGyMgICBT++rVq2FiYpKreW3YfRg/9PBRart57SpG9O+FZq4VULOsFVrWqQJ/3y44enAfPr4r3aG9u9C7UxvUcy6NOk6l0LF5PYTOn4m4N28AADt/3wgXW1P0/7Gj0nTxcXFwsTXF+dMnsqzLxdY024ekZA1MnBOaq/X8kKRkDezYfyTP0xMRERUEmqouoLCosqbKN12eKu+VV9zMXOn5kb/2YsQAP9Sp3whT5i5GafsySE1NQeTFc1g061fUqF0XRsbGAICQGVOwaskC/NinPwaNGg8LKxs8vHcXW9avwu5t4ejRux8AQFNTE2dPHMW5U3+jdt0GOaor4uItxf//9ed2LJ4zDTuPngcAOGs8hKGBfn6sPhERUaHFYFfE+Pr6IjY2FvXr18ecOXOQmpqKrl27Yv78+Yo+rdyrokfv/vixT38kJiZg4ohBaNC0BeaFrVOaV5lyTvi+a0/FHrt/Ll/E8oVzMXJisCLAAUBJ29Jwb9gE8XFxijY9fX20aPMdFgRPwoY/D+WodnNLK8X/GxYzgkQiUbRZayRi+cbtmLN0He49egr7UiUwuFdXDPDtDABITU1D4KQ5+GPvYbyJi4eVeXH069kRYwb1gr2bFwDgu97DAAB2pWxw/2zhvWk5EREVXQx2RdCRI0dgY2ODI0eO4M6dO+jSpQuqVasGt1adMvU9fewIYt/EwK//4GznJ5FIAAB7d2yBvoEhOnv3zrJfxl69DP2GjkLbBq44uGcnmnu1/4I1AjZs24sJs5dg4dRRqF65Ai5fu4W+I6bCQF8PPp3b4reVm7DrwHH8HjodpUta49HT53j09DkA4Pze9bCs2gyr5k5EyyZ1IZVKv6gWIiIiVWGwK4JMTU2xcOFCSKVSVKhQAV5eXoiIiMgy2D24dwcAYF+mnKLtWuQl9OnSTvF8xqLlaOTREg/v3UWp0nbQ0tLKUR2W1jbo3vtnhMyciiaeXl+0TkFzQjFnQiC+b90MAOBQuiRu3L6Hpev/gE/ntnj4JBrlHGxRv3Z1SCQS2JUqoZjWwswUAGBiXAzWluZZzp+IiKgw4MUTRVClSpWU9krZ2NjgxYsXOZ6+fMVK+H3/cfy+/ziSEhMgS5cBQKaLKHLCr38A3rx+hR3h63M9bYbExATcvf8YvYdNhmG5eorH1N+W4+6DxwAA385tEXn9NpwafIfB42fiwLHTeV4eERFRQcU9dmrCyMgIcR+cw5YhNjYWxh8dAv14j5pEIoFcLs9yvqUdygIA7v/3L6rWqAUA0NbRQWmHMpn62pVxxOXzZ5GWlpbjvXZGxsbo7T8UofNmomEzzxxN87GkhAQAQNisX+BWvbLSaxkBtkaVirh35k/sO3wSh06cQ+d+o+BR3w1bw2blaZlEREQFEffYqQknJydcunQpU/ulS5dQvnz5PM+3bsMmMDYxxcrFCz7bt1WHjkhMeIff167I8vX4LIInAHTz/QkaGhJsWJm34UrMLCxRwtoC/z14AkeH0koPh9IlFf2MihmiS3tPhM0aj/Al0/HH3gjEvHlfk5aWJmSyrMMtERFRYcE9dmqif//+WLhwIQYPHow+ffpAR0cHe/bswaZNm/Dnn3/meb76BoYImvkbRg7sBX+fzujm9zPsHMoiMfEdTh6NAABoSN//PqhavSZ8+w/GnCm/4EX0UzRt2QYWVtZ4dP8etqxfheq16ihdLZtBR1cX/QPHIPiXEXmuc9Kwfhg8fhaMjQzRsnFdpKSm4sLVG3gT+xaBP/+IuUvXw8bKHNUrO0FDooEtuw/B2tIcJsbFAAD2pUog4sQ51KvlAh1tbZiaGOW5FiIiIlVhsFMTZcqUwfHjxzFu3Dh4eHggNTUVFSpUwJYtW9CyZcsvmnezVm2wdvtfWLVkAX4Z2h/xsW9gWMwIzlWrY8aiFWjk8b/5Dx07Cc5VqiF8zXJsWb8acrkctnb28GjdHm07dst2Ge06dcPasEX47/atbPt8Sp/u30FfTxezlqzFiKnzYaCvhyoVHBHQpzsAoJihPmYuXoN/7z2EVCpFLRdn7F33GzQ03ofSOROGInDSXIRt3I6S1hYc7oSIiAolicjLGe+FWHx8PIyNjREXFwcjI+W9MsnJybh37x4cHBygq6urogpV5+rjWFWXkGdVNe591fknpwvce/ISDieHQffdo3ybbxWH0vk2r29NlYNoU+FiP7rw/lC6r9td1SXkWWH9fuF3S2afyi4f4zl2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQmGOyyUMQuFKYcUGwS3DaIiKgAY7D7QMZtsBITE1VcCRU0qXIA8nRI096quhQiIqJscYDiD0ilUpiYmODFixcAAH19fUgkEhVX9e2I9FRVl5BnyRpfb0+aXAAv45Kh/+ISNFPjv9pyiIiIvhSD3Uesra0BQBHuipIXb5JUXUKeaUtefsW5C2gkxaB01GpIwEOxRERUcDHYfUQikcDGxgaWlpZIS0tTdTnfVJ9tR1VdQp5F6Az/ejOXy6Cd9AIaIv3rLYOIiCgfFIhgt2jRIsyaNQvR0dFwcXFBSEgIateunW3/LVu2YPz48bh//z7KlSuHGTNmoHXr1vlak1QqhVQqzdd5FnRP3spUXUKe6abl322+iIiICiuVXzwRHh6OwMBABAUF4dKlS3BxcYGnp2e2h0JPnTqFbt26oXfv3rh8+TI6dOiADh064Nq1a9+4ciIiIqKCReXBbu7cuejbty/8/Pzg7OyM0NBQ6OvrY+XKlVn2X7BgAVq2bIkRI0agYsWKmDJlCmrUqIGFCxd+48qJiIiIChaVBrvU1FRcvHgRHh4eijYNDQ14eHjg9OnTWU5z+vRppf4A4OnpmW1/IiIioqJCpefYvXr1CjKZDFZWVkrtVlZWuHXrVpbTREdHZ9k/Ojo6y/4pKSlISUlRPI+LiwMAxMdz2IqPyVMK7/h98ZLCebWqLKnwntfIvyHKKX63qEZh/X7hd0tmGe9JTm6gUCAunviagoODMWnSpEzttra2KqiGvhZjVReQZzdVXUCeGfcvvO86UU4V7q28cH6/8Lsle2/fvoWx8affH5UGO3Nzc0ilUjx//lyp/fnz54rx5D5mbW2dq/5jxoxBYGCg4rlcLkdMTAzMzMyK1ODDlDfx8fGwtbXFo0ePYGRkpOpyiEhN8LuFckMIgbdv36JEiRKf7avSYKetrQ1XV1dERESgQ4cOAN4Hr4iICPj7+2c5jbu7OyIiIhAQEKBoO3jwINzd3bPsr6OjAx0dHaU2ExOT/CifihAjIyN++RJRvuN3C+XU5/bUZVD5odjAwED4+PigZs2aqF27NubPn4+EhAT4+fkBALy9vVGyZEkEBwcDAIYMGYJGjRphzpw58PLywubNm3HhwgUsW7ZMlatBREREpHIqD3ZdunTBy5cvMWHCBERHR6NatWrYv3+/4gKJhw8fQkPjfxfv1q1bFxs3bsQvv/yCsWPHoly5ctixYwcqV66sqlUgIiIiKhAkIieXWBAVUSkpKQgODsaYMWMyHdInIsorfrfQ18JgR0RERKQmVH7nCSIiIiLKHwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiLKN3K5XOk5r9H8thjsiIiIKF/I5XLF2LOHDx9GUlISb9/5jTHYERER0RcTQihC3S+//IJ+/fph9erVkMvl3Gv3Dan8zhNEhY0QAhKJBNHR0dDS0kJCQgJKly6t6rKIiFQqY8/c+PHjsXTpUuzYsQMVKlRQunsUfX18t4lyISPU7dq1C99//z0aNWoET09PzJw5k79IiajIu3fvHv766y9s2LAB9erVg1wuR2RkJMaNG4djx44hISFB1SWqPQY7olyQSCTYv38/unTpgh49emDTpk3w8fHB6NGjcfToUVWXR0SkUhoaGrh9+zZev36Ny5cvY/To0ejZsye2bt2KFi1a4NSpU6ouUe0x2BHlghAC27dvx/DhwzFw4EAYGxtj+fLl+Omnn9CkSRNVl0dE9E0IITJd/QoAdnZ28PPzQ//+/VG/fn0UK1YM06ZNQ1RUFGrXro1Dhw6poNqihefYEeVCamoqzpw5g6FDhyI+Ph5169aFl5cXlixZAgBYsmQJqlatinr16qm4UiKiryM5ORm6urqKc+q2bduG6OhoODk5wd3dHfPmzUPnzp2hra0NV1dXAIBMJoNEIkHJkiVVWXqRwGBH9AkZ59QlJydDR0cHOjo6aN++PY4cOYJx48ahXbt2WLRoESQSCZKSknDmzBnExcWhTp06kEqlqi6fiChfjRkzBk+ePMGSJUtgYGCAYcOGYf369dDX14euri7q1q2LX3/9Fe7u7gCAxMRE3LlzB+PGjUN8fDwGDBig4jVQfzwUS5SNjFC3f/9+jB07FtevXwcAODk54fDhw7C1tcW4ceOgoaGB9PR0TJ06FcePH0enTp0Y6ohI7chkMgghcOfOHYwdOxbnz5/HrVu3sH//fvzzzz/w9/fH7du34e/vj+fPnwMA9u7di1GjRuHdu3c4f/48NDU1IZPJVLwm6k0ieCkfUba2bdsGPz8/DBw4EL6+vihfvjwAYObMmViyZAnKli2LEiVKIDExEUePHsXBgwdRvXp1FVdNRJS/Mn7opqWlYdasWThw4ACKFy8OTU1NbNiwAVpaWgCAFStWYPXq1bCxsUFoaCi0tbVx8uRJeHh4QCqVIj09HZqaPFj4NTHYEWUjMjISnp6emD59Ovz8/BTtb968gampKQ4ePIiIiAhcv34drq6u6NatG5ycnFRYMRHR15NxV4nU1FTMmDED69evhxACt27dUhqrbuXKlVizZg20tLSwdetWmJiYKE1PXxdjM1E2nj9/jnLlyqFTp0549+4dtm7dig0bNuDp06do0KABZs6ciebNm6u6TCKiryojkGWEMm1tbYwaNQra2tpYtmwZ/P39MWPGDBQrVgwA0KtXLyQkJODGjRswMjJSzIeh7tvgHjuiD2QcbgCA3bt3o0OHDhg9ejR2796N0qVLw97eHjY2NggLC8Py5cvRtGlTFVdMRPT1fLiX7dKlS9DV1QUAODs7Iy0tDbNnz8bOnTtRq1YtBAcHw9DQUDFtxvcp99R9Wwx2RPjfF9CHwQ4AgoODcebMGTg6OsLPzw+VK1dGWloaateujRkzZqBFixYqrJqI6NsYMWIE1q9frzgU279/f0yYMAEAMGPGDOzevRtubm6YMmWKYs8dgEzfqfT18VAsFXkZXzzHjx/Hzp07kZ6ejvLly2PgwIEYM2YMYmNjFeeIAMCkSZPw9u1bODs7q65oIqKv6MNAdvz4cWzevBmbNm2CpqYmbt++jX79+uHZs2cICwvDiBEjAACrVq2CnZ0dhg4dqpgPQ923xz12RAC2b98OPz8/tG3bFunp6bh27Rrc3NywfPlyAO8PR6xZswanTp3Cjh07cODAAV79SkRqb82aNThz5gxMTEwQHBysaD906BBatGiBkJAQDBw4ECkpKdi8eTN+/PFHDvekYjzoTUXehQsXEBgYiBkzZmDdunUICgrC8+fPsW7dOnTq1AnA+5N+5XI5Xr9+jWPHjjHUEZHae/jwIcLDw7F+/Xq8efMGwPsfuWlpafDw8MDQoUMRHh6OuLg46OjowMfHB1KplOPUqRiDHRU5H9/j8ObNm2jRogV+/vlnPHz4EK1bt0abNm2wePFi7N69G3379gUA9O7dG+vWreMhWCIqEkqXLo0RI0agSZMm2LBhA44fPw4NDQ3FOHTGxsaQy+VKF0wA4B47FeOhWCpSbt++jZCQEDx58gR169bF8OHDAQDnz59HjRo10KZNG1haWmLNmjV49eoV6tatizt37qBr167YuHEjTwQmIrX0qStX//77b8yePRtRUVFYunQp6tevj8TERLRv3x6mpqbYunUrvxcLEF48QUXGlStX0Lx5c9SrVw+6uroYO3YsZDIZRo0ahVq1auHBgwd49OgRRo8eDeD94Vc3NzdMmDAB9erVA8ATgYlI/XwY6lauXIlTp05BV1cXNWrUQK9evdCgQQOkp6dj1qxZaNq0KSpUqIBatWrh7du32L9/f5YjCpDqMNhRkXD16lW4u7tj6NCh+PXXXyGXy2Fubo7o6GgkJydDV1cXurq6SElJwdatW1GtWjXMmjULUVFRmDt3LiwsLFS9CkREX0VGqBs1ahTWrVuHtm3bIiUlBUFBQXj06BGCgoLQpEkTaGpqQk9PD5GRkWjSpAlWr14NAEhLS1PcUoxUj4diSe09evQINWrUQJMmTfD7778r2rt27YqoqCgkJyfD3t4e33//PRISEjBr1ixIpVKkpqZi3759vFCCiNTeqlWr8Ouvv2LDhg1wc3PDpk2b4OfnB4lEgv79+2Pu3LkA3l8NGxYWhtu3b2PZsmWoVasW99YVMLx4gtSeTCaDg4MDUlJScPLkSQDA9OnT8eeff+KHH37A8OHDcf/+fSxatAiurq44dOgQFi5ciPPnzzPUEZFa+vACMgCIiYlBr1694Obmhj///BMDBgzAr7/+igkTJmD+/PmYNGkSAMDDwwMDBw5U3G7x/PnzDHUFDPfYUZHw77//YvDgwdDW1oalpSV27dqFdevWKe4c8eDBAzg4OGDp0qWKq2CJiNTdrFmz4OzsDA8PDzx58gS6urrw9PSEj48Phg8fjgsXLsDDwwPx8fGYNWsWhg0bBgCIiIjA2rVrERQUhDJlyqh4LehD3GNHRUK5cuWwYMECJCUlYcOGDRg5ciRatGgBIQTS0tKgqamJKlWqwNTUFMD7IVGIiNTNh3vqVq5cifnz58Pc3Bw6OjooU6YMbt26hfT0dHTv3h0AoK2tjTZt2uDPP/9EQECAYtpmzZohNDSUoa4AYrCjIqN8+fJYsmQJGjRogIiICPz999+QSCTQ0tLC0qVL8fbtW7i5uQHg1a9EpJ4yLpQ4e/Ysrl69iilTpsDNzU3xY9bY2BhPnjxBeHg4Hj9+jNGjR0MIgdatWysGH87oq6enp7L1oOzxUCwVORmHZYUQCA4OxsGDBxEUFIRTp07xnDoiUmtCCFy8eBH169cHAMycORODBw9WvB4XF4eZM2fit99+g7m5OUxNTXH27FloaWnxIolCgsGOiqR///0XgYGBOHfuHN68eYPTp0/D1dVV1WUREeW7rALZqlWrMGzYMDRs2BAzZ85E+fLlFa/FxcXhyZMnePLkCZo2bQqpVIr09HTFHSeoYGOwoyIrKioKI0eOxLRp01CpUiVVl0NElO8+DHXh4eGIiYlB//79AQBhYWEICgrCjz/+iP79+8PBwSHTNMD7kQV4m7DCg/GbiiwnJyds3bqVA2sSkVr68I4S165dQ3BwMPT09GBqaoquXbuib9++SE9Px9SpUxXj1dnb22fau8dQV7gw2FGRxlBHROoqI9SNHDkSDx48gI6ODm7cuIFff/0Vqamp8Pb2Rv/+/SGRSBAcHIy4uDgEBQXBxsZGxZXTl2CwIyIiUlMrVqzAsmXLEBERAXt7e7x9+xZ+fn4ICwuDVCpFjx490K9fP7x79w4nTpyAtbW1qkumL8Rz7IiIiNTUyJEjceHCBRw+fFhx7tx///2HH374AampqRg7dix69OgB4H/n0vHq18KN49gRERGpGZlMBgDQ0dFBUlISUlNTIZFIkJ6ejjJlymDatGm4f/8+1q5diz/++AMAGOrUBIMdERFRISaEyHTv14wLHlq3bo2zZ89iwYIFAKAYskQmk8HT0xPJyclYtWqVIggy1BV+PMeOiIiokEpKSoKenp4ikG3ZsgVPnjyBsbExPDw84O7ujpCQEAQEBCAhIQHfffcdTE1NERoaikaNGqFZs2aoWbMmTp48iYYNG6p4bSg/8Bw7IiKiQmjMmDF49OgRQkNDYWhoiKFDh2Lt2rWwsbGBTCbD06dPsXXrVjRv3hyrV69GYGAgDAwMAABmZmY4c+YMnjx5gpYtW2Lnzp1wdnZW8RpRfuAeOyIiokJGLpcrLoQYM2YMunXrhqtXr+LAgQNwdnbG69evMXnyZHz33Xc4cOAAfH19Ub9+fTx79gxpaWlo3LgxNDQ0EBYWBm1tbZiZmal6lSifcI8dERFRIZJxgUN6ejrmzJmDffv2wdjYGAkJCdi1axf09fUBAGlpafD19cWFCxdw+vRpFC9eXDGP69evY8aMGdizZw8iIiJQrVo1Fa0N5TdePEFERFSICCEghICmpiYCAwPRrFkz3L59Gzdu3FCca5eeng4tLS306NEDycnJeP36tWL6tLQ0JCUlwcTEBMeOHWOoUzMMdkRERIWIhoYGJBIJbt68CS0tLYwePRq9e/eGpqYm+vXrh5iYGMXVrzY2NhBCID4+XjG9lpYWXF1dMWfOHFSuXFlVq0FfCYMdERFRIbNv3z64urri999/h5aWFoYMGYJ+/frh5s2b8PPzw5UrV3DixAmMHTsWNjY2qF69utL0EomEt1RUU7x4goiIqJApVaoUunfvjlGjRkFDQwMdO3bEiBEjIJVKMW/ePDRq1AiNGzeGvb09du3aBQ0NDcWdJUi9MdgREREVYFndDaJKlSoYNmwYpFIphg4dCgDo2LEjAgMDIZVKERoainr16mH48OGKCy0yDs+SeuOnTEREVIBlhLoVK1agTJkyaNKkCQCgYsWKCAgIAAAEBARAR0cHbdu2xeDBg2FmZgYfHx9IJBLFhRZUNHC4EyIiogLo6dOnKFGiBADg0aNH6NOnD548eYKlS5eiXr16in5XrlzBjz/+iNevX2POnDno1q2b4jUefi16ePEEERFRAbNt2zZ069YNISEhAABbW1uMGzcOlStXxsCBA3HixAlFXxcXF1SoUAHGxsbYunUrgPeHbwEw1BVBDHZEREQFyIoVK9CnTx94eXmhfPnyivaGDRtiwIABcHR0xKBBg3DmzBkAQEJCAvT09DBt2jRFsPv4nDwqOngoloiIqIDYs2cPfH19ERoaih9++CHLPufOncOsWbNw8OBBtGvXDrdu3QIAnD59GlKpFHK5HBoa3G9TVDHYERERFRAjR45EQkICQkJCFOHsypUrOH/+PG7cuIEWLVrAw8MDL1++xIYNG3D8+HHY2tpi/vz50NLSYqgjBjsiIqKCQC6Xo0WLFjA2NsYff/wBAJg8eTKOHz+OGzduQEdHB3K5HGPGjEG/fv0AvL89WMZAwxzShACeY0dERFQgaGho4Mcff8SZM2fg7e0NNzc3rF69Gh4eHjh58iTu3buHatWqYe3atUhJSQEARajjkCaUgVsBERFRAeHh4YG4uDgcPHgQjo6O2LRpE2xsbKCnpwcAqFu3Lg4dOgS5XK40HS+WoAw8FEtERFQIJCUloUOHDihfvrxiGBSijzHYERERfWMfX+Tw4fOMW4hl/Dc5ORnPnj3DgAEDEB0djfPnz0NTUzPLW40R8Rw7IiKibyg+Pl4R4vbv3w8ASiEvI6xJJBK8e/cOU6ZMgY+PD5KTk3Hu3DloampCJpMx1FGWGOyIiIi+ke3bt6Nr16549+4dhg4diu7duyM6Ojrb/rGxsShfvjx69OiBQ4cOQUtLC+np6byjBGWLh2KJiIi+kZs3b6Jq1apwdHTEs2fPcPz4cVStWvWTh1U/vN8r7/1Kn8M9dkRERN9Aeno6KlasiJ49eyIqKgo1atSAjY0NACjOqcvKh0GOoY4+h8GOiIjoK8oYmiRjnDkvLy9s3boVFy9eRJ8+fXDv3j0AmYcs+XhIE6Kc4KFYIiKir+TDq12XLFmCd+/eoXfv3ihevDiuXbuGunXromnTpliwYAHs7OwAABs3bkT37t1VWTYVYtxjR0RE9JVkhLqRI0di8uTJsLCwwNu3bwEAlStXxsmTJ3HkyBH4+/tj27ZtaNu2LSZNmsS9dZRn3GNHRET0FS1fvhwTJkzArl27ULNmTUX7q1evYG5ujmvXrqFTp04wNDSErq4uDh8+DC0tLY5TR3nCW4oRERF9RVeuXEHz5s1Rs2ZNREVF4cSJE1i2bBni4+MxY8YMtGvXDidOnEBcXBzs7e2hoaGB9PR03vuV8oRbDRERUT7Jai+blZUVDh8+jMDAQJw4cQK2traoX78+3r59Cx8fH0RFRcHS0hJmZmYA3p+Xx1BHecUth4iIKB98eKFEbGwstLW1oaenh+7duyMmJgaHDh1Cr1690Lx5c1SqVAm7d+/GnTt3oKurqzSfD+9CQZRbPMeOiIjoC324py44OBh///037t27hzp16mDAgAGoVasW3r59i2LFigF4P6Zd+/btoaWlhe3bt/NcOso3DHZERET5ZNy4cVi6dCkWL14MAJg3bx7u37+PK1euwNLSEu/evcOhQ4ewcOFCvHz5EhcuXOCFEpSvuL+XiIgoH9y9excHDx7EH3/8gc6dO8PIyAg3btzA5MmTYWlpCSEEYmNjceLECZQpUwYXL15U3PuVoY7yC/fYERER5YPr16+jefPmuHnzJo4dO4YePXpg1qxZ6NevH5KSkrB+/Xp07doVMpkMxsbGkEgkvPcr5TvusSMiIsqlrAYQNjQ0RMWKFbFkyRJ4e3srQh0A3Lx5EwcOHMD169dhYmKiuDcsQx3lNwY7IiKiXPjw6teFCxdixYoVAAA7OztYWlpi7Nix8Pf3V4S6xMREjB8/HomJiahdu7ZiPjz8Sl8DhzshIiLKhQ9vE7Zx40YMHDgQz549g42NDTZt2oSXL19i9erVSE1NhY6ODk6ePIkXL17g8uXL0NDQUAqGRPmN59gRERHl0uLFixEUFIRDhw7BxcUFAJCWlgYtLS3I5XKMGzcOkZGRkEqlqFixIoKDg6Gpqck7StBXx62LiIgoF2QyGa5fv46+ffvCxcUFUVFROHPmDEJCQmBjY4PBgwcjODgYaWlp0NTUVBxylclkDHX01XELIyIi+oSPx5iTSqVISkrC77//DkdHR4SFhaF48eJo0qQJTp06hUmTJqFhw4bQ0dFRmgcvlKBvgYdiiYiIsvHh+XBJSUnQ1dWFRCJBbGwsfHx8cPPmTfTq1QstW7ZEtWrVsG/fPvz666/YtWsXihcvruLqqShisCMiIvqMmTNnYt++fbC2tkazZs3Qp08fAMCrV69gbm4O4P2hVi8vLxgbG2Pz5s286pVUgpflEBERfeTDcermzp2LGTNmoE6dOnj79i1mz56NkSNHAgDMzc0RFxeHTZs2wcvLC8+ePcP69esV49QRfWs8x46IiOgjGYdfT506heTkZGzatAktWrTAixcvsHbtWixcuBAaGhqYPn064uLicOnSJRQvXhy7d+/m1a+kUtzqiIiIsnDkyBH06NEDAPDnn38CACwtLeHr6wsNDQ2EhIRAU1MTU6dOxcSJE6Gvr6+4TRhDHakKD8USERFloUSJEujevTvevn2Lffv2KdrNzc3h7e2NIUOGYNasWViyZAkMDAx4mzAqEPiTgoiIiryP7wYhhICTkxMCAwMhhMDKlSthaGiIgIAAAO/DXffu3WFtbY1OnToppuMFE6RqvCqWiIiKtA9D3ZIlS/Dvv//i+vXrGDx4MOrWrQuZTIaZM2di165d6N+/P4YMGZJpHjKZjHvqqEDgoVgiIirSMkLdqFGjMHnyZOjo6MDBwQHe3t6YPHkyzM3N0b9/f7Rv3x5Lly7F1KlTM82DoY4KCh6KJSKiIu/QoUP4/fffsXfvXlSvXh1nz57FsmXLULt2bQCAg4MD/P39ERsbi+vXr2e6GwVRQcFgR0RERc7HwSwxMRFlypRB9erVsWnTJvz8889YtGgRunXrhrdv3+L27dtwdXXFxIkTYW1trbhQguGOChoeiiUioiLn40D28uVLxMbG4tChQ+jXrx+mT5+O/v37AwD279+P0NBQvHjxAjY2Ngx1VKDx4gkiIioy7ty5g7t37+LEiROoVKkSnJ2dUbVqVSQlJcHd3R1Xr17F4sWL0a9fPwBASkoKOnbsCFNTU6xZs4Zhjgo8BjsiIioSwsPDsXDhQrx48QLp6em4f/8+KlWqhD59+mDw4MHYtm0bxo8fj9KlS2Py5Ml4+PAhli9fjsePH+Py5cvQ1NTknjoq8HiOHRERqb1ly5Zh+PDhmDFjBurXr48qVarg0KFDmDVrFqZOnQqpVIqBAwdCKpUiODgYLVu2RJkyZWBvb49Lly5BU1OTQ5pQocA9dkREpNZWrlyJn3/+Gdu3b0ebNm2UXvvnn38wfvx4XLlyBevXr0e9evUAvD9ka2FhASMjI0gkEt77lQoNXjxBRERq68KFCxg0aBB69OihCHVyuRwZ+zSqVKmCMWPG4M2bN0q3DStbtiyMjY0hkUggl8sZ6qjQYLAjIiK1ZWZmhu+//x7379/HwoULAbwfkDgj2MlkMri5uaFt27Y4ceIE5HI55HK50nl0H95qjKig49ZKRERqSQgBBwcHTJo0CY6OjtiwYQMWLVoE4H1Yk8vlkEqlSE1NxePHj+Hk5AQNDQ0GOSrUuPUSEZFayhhvrkyZMhg7diwqVaqE9evXK4U7AHj06BE0NTXRsGFDAABPPafCjBdPEBGRWssYouS///7DtGnTcP36dfTo0QP+/v4AAC8vLyQnJ+PAgQO86pUKPQY7IiJSex+Hu5s3b+LHH3/E/v37cfv2bVy9ehVaWloc0oQKPQY7IiIq9D4eODirgYQ/DHfTp0/HunXr4ODggCtXrkBLS4tDmpBaYLAjIqJC7cO9bK9evYKBgQH09PSy7JsR7u7cuYM//vgDw4YNg6amJkMdqQ0GOyIiKpR27NiB2rVro0SJEgCAiRMn4vDhw3jx4gVGjhyJFi1aoFSpUpmm+3hvHkMdqRNeFUtERIXOxo0b0aVLF2zYsAHv3r3DypUrsXjxYnTu3Blubm4ICgrC/Pnzcffu3UzTfnyIlqGO1Am3ZiIiKnS6d++OmzdvYtGiRdDT08Pdu3cRFhaG9u3bAwAWLlyIRYsWQQiBAQMGoGzZsiqumOjbYLAjIqJCJTk5Gbq6upgyZQo0NDQwffp0pKamKu7zCgD+/v6QSCRYuHAhNDQ00KdPHzg5OamwaqJvg8GOiIgKDblcDl1dXQDA+fPnMWnSJBgYGGDChAk4fvw4GjRoACsrKwDAwIEDoaGhgbFjx6J06dIMdlQkMNgREVGhsHfvXsycORNHjx5FYGAgjh07hqNHj2LkyJFITEzEihUrYGtrCx8fH1haWgIA+vfvDysrK8UhWiJ1x2BHREQFXnp6OuRyOR4/fozy5cvj5cuXuHDhAooVKwbg/RWxMpkMISEhEELA19dXEe6+//57AODgw1Qk8KpYIiIqsBo1aoQTJ05AU1MTbdq0Qa1atXDnzh1UrFhRcUFEcnIyAGDKlCnw8fFBaGgoQkJC8ObNG6V5MdRRUcBgR0REBdLbt2/h6emJWrVqKdpat26NkJAQxMXFoXnz5gAAXV1dJCYmAngf7jp27Ih//vkHJiYmqiibSKU4QDERERVIb968gampKQBg2rRpqFKlCtq2bQuZTIa9e/dixIgRsLW1xcGDBxXTHDp0CB4eHopBiLO6tRiROuMeOyIiKnCOHTuGcuXK4fXr1wCAa9euoX379ti3bx+kUimaN2+O2bNn4/Hjx2jYsCFu3ryJFi1aYObMmQx1VKQx2BERUYFjY2OD4sWLY/z48ZDJZAgLC8OAAQPQvn177N27F7q6uvDw8EBISAhiY2PRsmVLJCYmYs+ePQx1VKTxUCwRERU46enpmDJlCnbs2IGQkBA0bNgQMTExGD9+PMLCwrBjxw60bt0acrkcKSkpuH79OmrUqAENDQ3e+5WKNAY7IiIqEG7duoUKFSoonsfGxqJ27dqoVKkStm/fDgB4/fo1JkyYgOXLl2PHjh1o1aqV0jzkcjk0NHgwiooubv1ERKRyf/75J5ydneHl5YUHDx4gLi4OJiYmWLZsGf766y/89ttvAAAzMzNMnToVP/30E7y8vHDmzBml+TDUUVHHPXZERKRyV69ehZeXF+Li4tCgQQPUq1cPrVu3RrVq1dC/f3/cuHEDCxYsQLVq1QAAMTExWLVqFYYMGcLDrkQfYLAjIiKVyDhsmp6eDplMhgULFiA+Ph7GxsZ4+PAhIiIiMHPmTOjo6KBv374YPHgwAgMDM10YwXPqiP6H+6yJiEglnjx5AgDQ1NSEjo4OqlWrhhMnTqBWrVoICQlBQEAA+vTpg8jISFhbW2PatGmIiorKdLUrQx3R/zDYERHRN3f+/HnY2dlhxIgRiIqKAgC0aNECDRo0QLdu3fDs2TP89NNP2LlzJx4/fgw9PT3ExMRgyZIlKq6cqGDjoVgiIvrmYmNjsW7dOkyePBnOzs7w9PTE2LFjAQC+vr4wMDDA9OnTUaxYMcTExODu3btYu3Yt5s2bxz10RJ/AYEdERCpz+/ZtBAcH49ixY7C2tkZISAgiIyPx999/o1+/fqhTpw7PqSPKBQY7IiJSqbi4OERGRmL06NF4+fIlWrdujf3798PDwwOLFy9WdXlEhQqDHRERFRjjxo3DtWvXcPz4ccTFxWHbtm3o0KGDqssiKjQY7IiISOU+vGPEuXPnsHv3bhw8eBB///03D7sS5QKDHRERFQgfn0uXgefUEeUcgx0RERVY2YU9Isoax7EjIqICi6GOKHcY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0SUj3x9fXlvUyJSGQY7Iip0fH19IZFIMj1atmyp6tKwYMECrF69WtVlAHg/uO+OHTtUXQYRfUO8+R4RFUotW7bEqlWrlNp0dHRUVA0gk8kgkUhgbGysshqIiLjHjogKJR0dHVhbWys9TE1NcfToUWhra+Pvv/9W9J05cyYsLS3x/PlzAEDjxo3h7+8Pf39/GBsbw9zcHOPHj8eHt85OSUnB8OHDUbJkSRgYGMDNzQ1Hjx5VvL569WqYmJhg165dcHZ2ho6ODh4+fJjpUGzjxo0xaNAgBAQEwNTUFFZWVggLC0NCQgL8/PxQrFgxODo6Yt++fUrrd+3aNbRq1QqGhoawsrJCz5498erVK6X5Dh48GCNHjkTx4sVhbW2NiRMnKl63t7cHAHz33XeQSCSK50Sk3hjsiEitNG7cGAEBAejZsyfi4uJw+fJljB8/HsuXL4eVlZWi35o1a6CpqYlz585hwYIFmDt3LpYvX6543d/fH6dPn8bmzZtx9epVdOrUCS1btsS///6r6JOYmIgZM2Zg+fLluH79OiwtLbOsac2aNTA3N8e5c+cwaNAg9O/fH506dULdunVx6dIltGjRAj179kRiYiIAIDY2Fk2bNkX16tVx4cIF7N+/H8+fP0fnzp0zzdfAwABnz57FzJkzMXnyZBw8eBAAcP78eQDAqlWr8OzZM8VzIlJzgoiokPHx8RFSqVQYGBgoPX799VchhBApKSmiWrVqonPnzsLZ2Vn07dtXafpGjRqJihUrCrlcrmgbNWqUqFixohBCiAcPHgipVCqePHmiNF2zZs3EmDFjhBBCrFq1SgAQkZGRmWpr37690rLq16+veJ6eni4MDAxEz549FW3Pnj0TAMTp06eFEEJMmTJFtGjRQmm+jx49EgBEVFRUlvMVQohatWqJUaNGKZ4DENu3b8/mXSQidcRz7IioUGrSpAmWLFmi1Fa8eHEAgLa2NjZs2ICqVavCzs4O8+bNyzR9nTp1IJFIFM/d3d0xZ84cyGQy/PPPP5DJZChfvrzSNCkpKTAzM1M819bWRtWqVT9b64d9pFIpzMzMUKVKFUVbxp7EFy9eAACuXLmCI0eOwNDQMNO87t69q6jr42Xb2Ngo5kFERRODHREVSgYGBnB0dMz29VOnTgEAYmJiEBMTAwMDgxzP+927d5BKpbh48SKkUqnSax+GLT09PaVwmB0tLS2l5xKJRKktYx5yuVyx/LZt22LGjBmZ5mVjY/PJ+WbMg4iKJgY7IlI7d+/exdChQxEWFobw8HD4+Pjg0KFD0ND432nFZ8+eVZrmzJkzKFeuHKRSKapXrw6ZTIYXL16gQYMG37p81KhRA3/88Qfs7e2hqZn3r2ktLS3IZLJ8rIyICjpePEFEhVJKSgqio6OVHq9evYJMJsOPP/4IT09P+Pn5YdWqVbh69SrmzJmjNP3Dhw8RGBiIqKgobNq0CSEhIRgyZAgAoHz58ujRowe8vb2xbds23Lt3D+fOnUNwcDD27Nnz1ddt4MCBiImJQbdu3XD+/HncvXsXf/31F/z8/HIV1Ozt7REREYHo6Gi8efPmK1ZMRAUF99gRUaG0f/9+pcOSAODk5ITu3bvjwYMH2L17N4D3hy6XLVuGbt26oUWLFnBxcQEAeHt7IykpCbVr14ZUKsWQIUPw008/Kea1atUqTJ06FcOGDcOTJ09gbm6OOnXqoE2bNl993UqUKIGTJ09i1KhRaNGiBVJSUmBnZ4eWLVsq7XX8nDlz5iAwMBBhYWEoWbIk7t+///WKJqICQSLEBwM3EREVAY0bN0a1atUwf/58VZdCRJSveCiWiIiISE0w2BERERGpCR6KJSIiIlIT3GNHREREpCYY7IiIiIjUBIMdERERkZpgsCMiIiJSEwx2RERERGqCwY6IiIhITTDYEREREakJBjsiIiIiNcFgR0RERKQm/g8ZLDa0r5PelAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results Summary Val ===\n",
      "            Experiment  HGNN Val  HGNNP Val  UniGCN Val\n",
      "                  Base  0.742222   0.740000    0.733333\n",
      "With Densest Subgraphs  0.753333   0.737778    0.742222\n",
      "\n",
      "\n",
      "=== Results Summary Test ===\n",
      "            Experiment  HGNN Test  HGNNP Test  UniGCN Test\n",
      "                  Base   0.733333        0.70     0.728889\n",
      "With Densest Subgraphs   0.771111        0.74     0.742222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Base',\n",
       "  {'HGNN': {'val_accuracy': 0.742222249507904,\n",
       "    'val_f1': 0.742222249507904,\n",
       "    'test_accuracy': 0.7333333492279053,\n",
       "    'test_f1': 0.7318022886204705},\n",
       "   'HGNNP': {'val_accuracy': 0.7400000095367432,\n",
       "    'val_f1': 0.7400000095367432,\n",
       "    'test_accuracy': 0.699999988079071,\n",
       "    'test_f1': 0.6979580472428417},\n",
       "   'UniGCN': {'val_accuracy': 0.7333333492279053,\n",
       "    'val_f1': 0.7333333492279053,\n",
       "    'test_accuracy': 0.7288888692855835,\n",
       "    'test_f1': 0.7279808149674966}}),\n",
       " ('With Densest Subgraphs',\n",
       "  {'HGNN': {'val_accuracy': 0.753333330154419,\n",
       "    'val_f1': 0.753333330154419,\n",
       "    'test_accuracy': 0.7711111307144165,\n",
       "    'test_f1': 0.7671076630874438},\n",
       "   'HGNNP': {'val_accuracy': 0.7377777695655823,\n",
       "    'val_f1': 0.7377777695655823,\n",
       "    'test_accuracy': 0.7400000095367432,\n",
       "    'test_f1': 0.7373737373737373},\n",
       "   'UniGCN': {'val_accuracy': 0.742222249507904,\n",
       "    'val_f1': 0.742222249507904,\n",
       "    'test_accuracy': 0.742222249507904,\n",
       "    'test_f1': 0.7354497354497354}})]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "experiment = HypergraphExperiment(\n",
    "    data_path=movie_path\n",
    ")\n",
    "experiment.run_experiments(n_samples=3000)\n",
    "\n",
    "experiment.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grouped bar chart for Movie dataset\n",
    "def plot_movie_metrics(accuracy_data, f1_data):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    metrics = [(\"Test Accuracy\", accuracy_data), (\"F1 Score\", f1_data)]\n",
    "    groups = ['HGNN', 'HGNNP', 'UniGCN']\n",
    "    variations = ['Base', 'With Densest Subgraphs']\n",
    "    x = np.arange(len(groups))\n",
    "    width = 0.35\n",
    "\n",
    "    for ax, (metric_name, metric_data) in zip(axes, metrics):\n",
    "        bars1 = ax.bar(x - width/2, [metric_data[model][0] for model in groups],\n",
    "                       width=width, label='Without Top-k')\n",
    "        bars2 = ax.bar(x + width/2, [metric_data[model][1] for model in groups],\n",
    "                       width=width, label='With Top-k')\n",
    "\n",
    "        # Annotate bar values\n",
    "        for bars in [bars1, bars2]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                        f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(groups)\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.set_title(f'Movie Dataset - {metric_name}')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    axes[1].legend(loc='lower center', bbox_to_anchor=(0.5, -0.35), ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_movie_metrics(accuracy_data, f1_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
