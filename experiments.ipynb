{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Hypergraph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import dhg\n",
    "from dhg import Graph, Hypergraph\n",
    "from dhg.data import Cooking200, News20\n",
    "from dhg.models import GCN, HGNN, HGNNP, UniGCN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.utils import split_by_ratio\n",
    "\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\.dhg\n",
      "d:\\Rustem\\2_Education\\9_UWindsor_CSS\\COMP_8720-Topics_in_AI\\project\\hyper_modeling\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(dhg.CACHE_ROOT)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=dhg.data.Cooking200()\n",
    "# for key in d.content:\n",
    "#     d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooking200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = Cooking200(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 27.05090s, Loss: 3.00726\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.11877s, Loss: 2.87365\n",
      "Epoch: 2, Time: 0.97288s, Loss: 2.63956\n",
      "update best: 0.09500\n",
      "Epoch: 3, Time: 0.97195s, Loss: 2.46788\n",
      "Epoch: 4, Time: 0.95451s, Loss: 2.36260\n",
      "Epoch: 5, Time: 0.97938s, Loss: 2.28780\n",
      "update best: 0.13000\n",
      "Epoch: 6, Time: 1.09381s, Loss: 2.21204\n",
      "update best: 0.13500\n",
      "Epoch: 7, Time: 0.93865s, Loss: 2.13388\n",
      "Epoch: 8, Time: 1.13131s, Loss: 2.06262\n",
      "update best: 0.14000\n",
      "Epoch: 9, Time: 1.40688s, Loss: 1.99684\n",
      "Epoch: 10, Time: 1.38883s, Loss: 1.91773\n",
      "Epoch: 11, Time: 1.27973s, Loss: 1.84279\n",
      "Epoch: 12, Time: 1.31780s, Loss: 1.76692\n",
      "Epoch: 13, Time: 1.21678s, Loss: 1.69607\n",
      "Epoch: 14, Time: 1.50236s, Loss: 1.62226\n",
      "Epoch: 15, Time: 1.37457s, Loss: 1.55436\n",
      "Epoch: 16, Time: 1.23136s, Loss: 1.48475\n",
      "Epoch: 17, Time: 1.41042s, Loss: 1.41734\n",
      "Epoch: 18, Time: 1.25632s, Loss: 1.34714\n",
      "Epoch: 19, Time: 1.30601s, Loss: 1.27656\n",
      "update best: 0.16500\n",
      "Epoch: 20, Time: 1.20952s, Loss: 1.20234\n",
      "Epoch: 21, Time: 1.19049s, Loss: 1.14656\n",
      "Epoch: 22, Time: 1.15300s, Loss: 1.08389\n",
      "Epoch: 23, Time: 1.26429s, Loss: 1.02032\n",
      "Epoch: 24, Time: 1.30182s, Loss: 0.95723\n",
      "Epoch: 25, Time: 1.42021s, Loss: 0.90450\n",
      "Epoch: 26, Time: 1.22268s, Loss: 0.84604\n",
      "Epoch: 27, Time: 1.20182s, Loss: 0.79651\n",
      "Epoch: 28, Time: 1.27569s, Loss: 0.74074\n",
      "Epoch: 29, Time: 1.19896s, Loss: 0.69620\n",
      "Epoch: 30, Time: 1.54059s, Loss: 0.66053\n",
      "Epoch: 31, Time: 1.35103s, Loss: 0.61037\n",
      "Epoch: 32, Time: 1.32461s, Loss: 0.56589\n",
      "update best: 0.17000\n",
      "Epoch: 33, Time: 1.33072s, Loss: 0.53298\n",
      "update best: 0.19000\n",
      "Epoch: 34, Time: 1.29055s, Loss: 0.49662\n",
      "update best: 0.21000\n",
      "Epoch: 35, Time: 1.22395s, Loss: 0.46070\n",
      "update best: 0.22500\n",
      "Epoch: 36, Time: 1.18261s, Loss: 0.43163\n",
      "Epoch: 37, Time: 1.18048s, Loss: 0.40783\n",
      "Epoch: 38, Time: 1.32629s, Loss: 0.37449\n",
      "Epoch: 39, Time: 1.18640s, Loss: 0.34452\n",
      "Epoch: 40, Time: 1.17261s, Loss: 0.32959\n",
      "Epoch: 41, Time: 1.35494s, Loss: 0.30877\n",
      "Epoch: 42, Time: 1.29722s, Loss: 0.29094\n",
      "update best: 0.23000\n",
      "Epoch: 43, Time: 1.23423s, Loss: 0.27251\n",
      "update best: 0.23500\n",
      "Epoch: 44, Time: 1.29604s, Loss: 0.25644\n",
      "Epoch: 45, Time: 1.26459s, Loss: 0.24776\n",
      "update best: 0.24500\n",
      "Epoch: 46, Time: 1.37385s, Loss: 0.23207\n",
      "Epoch: 47, Time: 1.25992s, Loss: 0.21182\n",
      "Epoch: 48, Time: 1.31817s, Loss: 0.20342\n",
      "Epoch: 49, Time: 1.46455s, Loss: 0.19555\n",
      "update best: 0.27500\n",
      "Epoch: 50, Time: 1.30664s, Loss: 0.18163\n",
      "update best: 0.28000\n",
      "Epoch: 51, Time: 1.44705s, Loss: 0.17652\n",
      "Epoch: 52, Time: 1.16958s, Loss: 0.16214\n",
      "Epoch: 53, Time: 1.26941s, Loss: 0.15734\n",
      "Epoch: 54, Time: 1.34628s, Loss: 0.14466\n",
      "Epoch: 55, Time: 1.22824s, Loss: 0.14203\n",
      "update best: 0.28500\n",
      "Epoch: 56, Time: 1.34575s, Loss: 0.13349\n",
      "update best: 0.33000\n",
      "Epoch: 57, Time: 1.28025s, Loss: 0.12913\n",
      "update best: 0.34500\n",
      "Epoch: 58, Time: 1.30383s, Loss: 0.12390\n",
      "Epoch: 59, Time: 1.24166s, Loss: 0.11869\n",
      "Epoch: 60, Time: 1.29664s, Loss: 0.11507\n",
      "Epoch: 61, Time: 1.29878s, Loss: 0.11152\n",
      "Epoch: 62, Time: 1.50177s, Loss: 0.10387\n",
      "Epoch: 63, Time: 1.30076s, Loss: 0.10022\n",
      "Epoch: 64, Time: 1.24141s, Loss: 0.09575\n",
      "update best: 0.36500\n",
      "Epoch: 65, Time: 1.22373s, Loss: 0.09613\n",
      "Epoch: 66, Time: 1.36058s, Loss: 0.09143\n",
      "Epoch: 67, Time: 1.39538s, Loss: 0.09173\n",
      "update best: 0.38500\n",
      "Epoch: 68, Time: 1.21041s, Loss: 0.08172\n",
      "Epoch: 69, Time: 1.31102s, Loss: 0.08590\n",
      "Epoch: 70, Time: 1.16226s, Loss: 0.08126\n",
      "update best: 0.39500\n",
      "Epoch: 71, Time: 1.38064s, Loss: 0.07688\n",
      "Epoch: 72, Time: 1.21638s, Loss: 0.07967\n",
      "Epoch: 73, Time: 1.16902s, Loss: 0.07509\n",
      "update best: 0.42500\n",
      "Epoch: 74, Time: 1.25318s, Loss: 0.07688\n",
      "Epoch: 75, Time: 1.19744s, Loss: 0.07053\n",
      "Epoch: 76, Time: 1.23429s, Loss: 0.06861\n",
      "Epoch: 77, Time: 1.18709s, Loss: 0.06849\n",
      "update best: 0.43500\n",
      "Epoch: 78, Time: 1.27665s, Loss: 0.06825\n",
      "Epoch: 79, Time: 1.34091s, Loss: 0.06182\n",
      "Epoch: 80, Time: 1.33787s, Loss: 0.06590\n",
      "Epoch: 81, Time: 1.26965s, Loss: 0.06319\n",
      "Epoch: 82, Time: 1.19577s, Loss: 0.06313\n",
      "Epoch: 83, Time: 1.32820s, Loss: 0.06400\n",
      "Epoch: 84, Time: 1.30356s, Loss: 0.06643\n",
      "Epoch: 85, Time: 1.32382s, Loss: 0.05816\n",
      "Epoch: 86, Time: 1.27220s, Loss: 0.05907\n",
      "Epoch: 87, Time: 1.24621s, Loss: 0.05757\n",
      "Epoch: 88, Time: 1.26966s, Loss: 0.05826\n",
      "Epoch: 89, Time: 1.16190s, Loss: 0.05518\n",
      "update best: 0.45000\n",
      "Epoch: 90, Time: 1.21202s, Loss: 0.05523\n",
      "Epoch: 91, Time: 1.17484s, Loss: 0.05426\n",
      "Epoch: 92, Time: 1.21003s, Loss: 0.05314\n",
      "Epoch: 93, Time: 1.17332s, Loss: 0.05175\n",
      "Epoch: 94, Time: 1.15069s, Loss: 0.05127\n",
      "update best: 0.45500\n",
      "Epoch: 95, Time: 1.19286s, Loss: 0.05020\n",
      "Epoch: 96, Time: 1.24532s, Loss: 0.05462\n",
      "Epoch: 97, Time: 1.27569s, Loss: 0.04893\n",
      "Epoch: 98, Time: 1.21614s, Loss: 0.05175\n",
      "Epoch: 99, Time: 1.39048s, Loss: 0.04687\n",
      "Epoch: 100, Time: 1.24326s, Loss: 0.04922\n",
      "Epoch: 101, Time: 1.24431s, Loss: 0.04435\n",
      "Epoch: 102, Time: 1.19226s, Loss: 0.04717\n",
      "Epoch: 103, Time: 1.35605s, Loss: 0.04634\n",
      "Epoch: 104, Time: 1.18235s, Loss: 0.04488\n",
      "Epoch: 105, Time: 1.17856s, Loss: 0.04129\n",
      "Epoch: 106, Time: 1.23227s, Loss: 0.04684\n",
      "Epoch: 107, Time: 1.26220s, Loss: 0.04706\n",
      "Epoch: 108, Time: 1.23621s, Loss: 0.04364\n",
      "Epoch: 109, Time: 1.18581s, Loss: 0.04682\n",
      "Epoch: 110, Time: 1.22108s, Loss: 0.04590\n",
      "Epoch: 111, Time: 1.33051s, Loss: 0.04269\n",
      "Epoch: 112, Time: 1.14877s, Loss: 0.04778\n",
      "Epoch: 113, Time: 1.16577s, Loss: 0.04367\n",
      "update best: 0.46000\n",
      "Epoch: 114, Time: 1.27170s, Loss: 0.04239\n",
      "Epoch: 115, Time: 1.19230s, Loss: 0.04223\n",
      "Epoch: 116, Time: 1.30985s, Loss: 0.04184\n",
      "Epoch: 117, Time: 1.34204s, Loss: 0.04116\n",
      "Epoch: 118, Time: 1.19596s, Loss: 0.04068\n",
      "Epoch: 119, Time: 1.23354s, Loss: 0.03786\n",
      "Epoch: 120, Time: 1.42493s, Loss: 0.04072\n",
      "update best: 0.47500\n",
      "Epoch: 121, Time: 1.24169s, Loss: 0.03664\n",
      "Epoch: 122, Time: 1.24111s, Loss: 0.03947\n",
      "Epoch: 123, Time: 1.23722s, Loss: 0.03868\n",
      "Epoch: 124, Time: 1.31976s, Loss: 0.03620\n",
      "Epoch: 125, Time: 1.19306s, Loss: 0.03710\n",
      "Epoch: 126, Time: 1.18464s, Loss: 0.03816\n",
      "Epoch: 127, Time: 1.23385s, Loss: 0.03773\n",
      "Epoch: 128, Time: 1.15674s, Loss: 0.03621\n",
      "Epoch: 129, Time: 1.22596s, Loss: 0.03730\n",
      "Epoch: 130, Time: 1.14205s, Loss: 0.03664\n",
      "Epoch: 131, Time: 1.15340s, Loss: 0.03373\n",
      "Epoch: 132, Time: 1.32747s, Loss: 0.03674\n",
      "Epoch: 133, Time: 1.22853s, Loss: 0.04360\n",
      "Epoch: 134, Time: 1.34267s, Loss: 0.03768\n",
      "Epoch: 135, Time: 1.41897s, Loss: 0.03557\n",
      "Epoch: 136, Time: 1.31278s, Loss: 0.03471\n",
      "Epoch: 137, Time: 1.20829s, Loss: 0.03593\n",
      "Epoch: 138, Time: 1.20432s, Loss: 0.03424\n",
      "Epoch: 139, Time: 1.29845s, Loss: 0.03539\n",
      "Epoch: 140, Time: 1.38204s, Loss: 0.03442\n",
      "Epoch: 141, Time: 1.27594s, Loss: 0.03629\n",
      "Epoch: 142, Time: 1.30157s, Loss: 0.03312\n",
      "Epoch: 143, Time: 1.18935s, Loss: 0.03841\n",
      "Epoch: 144, Time: 1.25200s, Loss: 0.04076\n",
      "Epoch: 145, Time: 1.23712s, Loss: 0.06876\n",
      "Epoch: 146, Time: 1.13729s, Loss: 0.03650\n",
      "Epoch: 147, Time: 1.23742s, Loss: 0.06332\n",
      "Epoch: 148, Time: 1.35226s, Loss: 0.04392\n",
      "Epoch: 149, Time: 1.48591s, Loss: 0.06565\n",
      "Epoch: 150, Time: 1.18588s, Loss: 0.04007\n",
      "Epoch: 151, Time: 1.20833s, Loss: 0.07563\n",
      "Epoch: 152, Time: 1.35463s, Loss: 0.03581\n",
      "Epoch: 153, Time: 1.26155s, Loss: 0.09672\n",
      "Epoch: 154, Time: 1.18569s, Loss: 0.05547\n",
      "Epoch: 155, Time: 1.18879s, Loss: 0.04690\n",
      "Epoch: 156, Time: 1.28119s, Loss: 0.05561\n",
      "Epoch: 157, Time: 1.25364s, Loss: 0.05781\n",
      "Epoch: 158, Time: 1.22963s, Loss: 0.04668\n",
      "Epoch: 159, Time: 1.18674s, Loss: 0.04285\n",
      "Epoch: 160, Time: 1.33488s, Loss: 0.04534\n",
      "Epoch: 161, Time: 1.34731s, Loss: 0.04604\n",
      "Epoch: 162, Time: 1.34935s, Loss: 0.03965\n",
      "Epoch: 163, Time: 1.26145s, Loss: 0.03661\n",
      "Epoch: 164, Time: 1.27685s, Loss: 0.03958\n",
      "Epoch: 165, Time: 1.24223s, Loss: 0.03301\n",
      "Epoch: 166, Time: 1.29492s, Loss: 0.03590\n",
      "Epoch: 167, Time: 1.16915s, Loss: 0.02961\n",
      "Epoch: 168, Time: 1.18475s, Loss: 0.03390\n",
      "Epoch: 169, Time: 1.22329s, Loss: 0.03066\n",
      "Epoch: 170, Time: 1.19592s, Loss: 0.03081\n",
      "Epoch: 171, Time: 1.24858s, Loss: 0.02582\n",
      "Epoch: 172, Time: 1.29168s, Loss: 0.02749\n",
      "Epoch: 173, Time: 1.19019s, Loss: 0.02687\n",
      "Epoch: 174, Time: 1.21539s, Loss: 0.02652\n",
      "Epoch: 175, Time: 1.30223s, Loss: 0.02704\n",
      "Epoch: 176, Time: 1.52252s, Loss: 0.02565\n",
      "Epoch: 177, Time: 1.42731s, Loss: 0.03043\n",
      "Epoch: 178, Time: 1.26753s, Loss: 0.02406\n",
      "Epoch: 179, Time: 2.60946s, Loss: 0.03481\n",
      "Epoch: 180, Time: 2.52241s, Loss: 0.02621\n",
      "Epoch: 181, Time: 2.60322s, Loss: 0.04958\n",
      "Epoch: 182, Time: 2.56016s, Loss: 0.03141\n",
      "Epoch: 183, Time: 2.58308s, Loss: 0.04558\n",
      "Epoch: 184, Time: 2.49239s, Loss: 0.05380\n",
      "Epoch: 185, Time: 2.60713s, Loss: 0.04330\n",
      "Epoch: 186, Time: 2.59470s, Loss: 0.03930\n",
      "Epoch: 187, Time: 2.72259s, Loss: 0.04647\n",
      "Epoch: 188, Time: 2.51598s, Loss: 0.04012\n",
      "Epoch: 189, Time: 2.54825s, Loss: 0.03249\n",
      "Epoch: 190, Time: 2.59999s, Loss: 0.03584\n",
      "Epoch: 191, Time: 2.76925s, Loss: 0.03525\n",
      "Epoch: 192, Time: 2.05392s, Loss: 0.03017\n",
      "Epoch: 193, Time: 1.00734s, Loss: 0.03143\n",
      "Epoch: 194, Time: 1.02639s, Loss: 0.03074\n",
      "Epoch: 195, Time: 1.19079s, Loss: 0.02683\n",
      "Epoch: 196, Time: 0.99288s, Loss: 0.02414\n",
      "Epoch: 197, Time: 1.00549s, Loss: 0.02461\n",
      "Epoch: 198, Time: 1.13103s, Loss: 0.02479\n",
      "Epoch: 199, Time: 1.02974s, Loss: 0.02296\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 120\n",
      "{'accuracy': 0.45623305439949036, 'f1_score': 0.3653164109596893, 'f1_score -> average@micro': 0.4562330429815793}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 1.27188s, Loss: 3.00386\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.54415s, Loss: 2.70985\n",
      "Epoch: 2, Time: 1.50128s, Loss: 2.37330\n",
      "Epoch: 3, Time: 1.81600s, Loss: 2.18877\n",
      "Epoch: 4, Time: 1.50025s, Loss: 2.05079\n",
      "Epoch: 5, Time: 1.49115s, Loss: 1.92147\n",
      "Epoch: 6, Time: 1.39025s, Loss: 1.80867\n",
      "update best: 0.09000\n",
      "Epoch: 7, Time: 1.39048s, Loss: 1.68405\n",
      "update best: 0.09500\n",
      "Epoch: 8, Time: 1.40198s, Loss: 1.55780\n",
      "Epoch: 9, Time: 1.32872s, Loss: 1.45725\n",
      "Epoch: 10, Time: 1.33453s, Loss: 1.36144\n",
      "Epoch: 11, Time: 1.98168s, Loss: 1.23599\n",
      "Epoch: 12, Time: 1.50997s, Loss: 1.14826\n",
      "Epoch: 13, Time: 1.64627s, Loss: 1.03714\n",
      "Epoch: 14, Time: 1.72242s, Loss: 0.94961\n",
      "update best: 0.10000\n",
      "Epoch: 15, Time: 1.53512s, Loss: 0.87812\n",
      "update best: 0.10500\n",
      "Epoch: 16, Time: 1.35259s, Loss: 0.79208\n",
      "Epoch: 17, Time: 1.35914s, Loss: 0.72543\n",
      "Epoch: 18, Time: 1.38262s, Loss: 0.65884\n",
      "Epoch: 19, Time: 1.37241s, Loss: 0.59730\n",
      "Epoch: 20, Time: 1.41043s, Loss: 0.55186\n",
      "Epoch: 21, Time: 1.45364s, Loss: 0.48377\n",
      "Epoch: 22, Time: 1.33518s, Loss: 0.44027\n",
      "Epoch: 23, Time: 1.42071s, Loss: 0.39254\n",
      "Epoch: 24, Time: 1.34800s, Loss: 0.35922\n",
      "Epoch: 25, Time: 1.33080s, Loss: 0.31876\n",
      "Epoch: 26, Time: 1.29542s, Loss: 0.29382\n",
      "Epoch: 27, Time: 1.43415s, Loss: 0.25786\n",
      "Epoch: 28, Time: 1.37351s, Loss: 0.23379\n",
      "Epoch: 29, Time: 1.43001s, Loss: 0.21166\n",
      "update best: 0.11000\n",
      "Epoch: 30, Time: 1.27605s, Loss: 0.18889\n",
      "Epoch: 31, Time: 1.30442s, Loss: 0.17234\n",
      "Epoch: 32, Time: 1.25549s, Loss: 0.15468\n",
      "update best: 0.12000\n",
      "Epoch: 33, Time: 1.28189s, Loss: 0.14374\n",
      "update best: 0.12500\n",
      "Epoch: 34, Time: 1.25804s, Loss: 0.13073\n",
      "Epoch: 35, Time: 1.24336s, Loss: 0.11533\n",
      "update best: 0.13500\n",
      "Epoch: 36, Time: 1.28270s, Loss: 0.10756\n",
      "update best: 0.14000\n",
      "Epoch: 37, Time: 1.22811s, Loss: 0.09825\n",
      "update best: 0.15000\n",
      "Epoch: 38, Time: 1.37117s, Loss: 0.09213\n",
      "update best: 0.15500\n",
      "Epoch: 39, Time: 1.32696s, Loss: 0.08591\n",
      "Epoch: 40, Time: 1.31704s, Loss: 0.08078\n",
      "Epoch: 41, Time: 1.43949s, Loss: 0.07544\n",
      "Epoch: 42, Time: 1.37561s, Loss: 0.07192\n",
      "Epoch: 43, Time: 1.38291s, Loss: 0.06496\n",
      "Epoch: 44, Time: 1.30007s, Loss: 0.06504\n",
      "Epoch: 45, Time: 1.34068s, Loss: 0.06036\n",
      "Epoch: 46, Time: 1.35244s, Loss: 0.05478\n",
      "Epoch: 47, Time: 1.37770s, Loss: 0.05395\n",
      "Epoch: 48, Time: 1.43613s, Loss: 0.05261\n",
      "Epoch: 49, Time: 1.39924s, Loss: 0.05150\n",
      "Epoch: 50, Time: 1.44943s, Loss: 0.04703\n",
      "Epoch: 51, Time: 1.46634s, Loss: 0.04725\n",
      "Epoch: 52, Time: 1.47481s, Loss: 0.04718\n",
      "Epoch: 53, Time: 1.44230s, Loss: 0.04429\n",
      "update best: 0.16500\n",
      "Epoch: 54, Time: 1.46572s, Loss: 0.04436\n",
      "update best: 0.19500\n",
      "Epoch: 55, Time: 1.43431s, Loss: 0.04274\n",
      "update best: 0.21000\n",
      "Epoch: 56, Time: 1.49898s, Loss: 0.04083\n",
      "Epoch: 57, Time: 1.40041s, Loss: 0.04188\n",
      "update best: 0.23500\n",
      "Epoch: 58, Time: 1.43422s, Loss: 0.04199\n",
      "update best: 0.25000\n",
      "Epoch: 59, Time: 1.37231s, Loss: 0.04150\n",
      "update best: 0.28000\n",
      "Epoch: 60, Time: 1.53890s, Loss: 0.03965\n",
      "update best: 0.28500\n",
      "Epoch: 61, Time: 1.41634s, Loss: 0.03752\n",
      "Epoch: 62, Time: 1.51327s, Loss: 0.03809\n",
      "Epoch: 63, Time: 1.36452s, Loss: 0.03837\n",
      "Epoch: 64, Time: 1.44806s, Loss: 0.03919\n",
      "Epoch: 65, Time: 1.47120s, Loss: 0.03748\n",
      "Epoch: 66, Time: 1.32771s, Loss: 0.03714\n",
      "Epoch: 67, Time: 1.32778s, Loss: 0.03501\n",
      "Epoch: 68, Time: 1.31287s, Loss: 0.03392\n",
      "update best: 0.31000\n",
      "Epoch: 69, Time: 1.32246s, Loss: 0.03251\n",
      "update best: 0.34500\n",
      "Epoch: 70, Time: 1.40177s, Loss: 0.03559\n",
      "update best: 0.37000\n",
      "Epoch: 71, Time: 1.32568s, Loss: 0.03304\n",
      "Epoch: 72, Time: 1.42692s, Loss: 0.03425\n",
      "Epoch: 73, Time: 1.72085s, Loss: 0.03320\n",
      "update best: 0.37500\n",
      "Epoch: 74, Time: 1.43077s, Loss: 0.03200\n",
      "Epoch: 75, Time: 1.35294s, Loss: 0.03212\n",
      "update best: 0.39000\n",
      "Epoch: 76, Time: 1.30638s, Loss: 0.03018\n",
      "Epoch: 77, Time: 1.30985s, Loss: 0.03008\n",
      "update best: 0.39500\n",
      "Epoch: 78, Time: 1.31392s, Loss: 0.03139\n",
      "Epoch: 79, Time: 1.43192s, Loss: 0.02849\n",
      "update best: 0.40000\n",
      "Epoch: 80, Time: 1.40814s, Loss: 0.03067\n",
      "update best: 0.41000\n",
      "Epoch: 81, Time: 1.31110s, Loss: 0.02883\n",
      "update best: 0.45500\n",
      "Epoch: 82, Time: 1.43553s, Loss: 0.02801\n",
      "update best: 0.46000\n",
      "Epoch: 83, Time: 1.42795s, Loss: 0.02717\n",
      "update best: 0.47500\n",
      "Epoch: 84, Time: 1.36155s, Loss: 0.02615\n",
      "Epoch: 85, Time: 1.50270s, Loss: 0.02605\n",
      "Epoch: 86, Time: 1.58583s, Loss: 0.02592\n",
      "Epoch: 87, Time: 1.50066s, Loss: 0.02628\n",
      "Epoch: 88, Time: 1.85431s, Loss: 0.02556\n",
      "Epoch: 89, Time: 1.68659s, Loss: 0.02421\n",
      "Epoch: 90, Time: 1.32606s, Loss: 0.02532\n",
      "Epoch: 91, Time: 1.39409s, Loss: 0.02611\n",
      "Epoch: 92, Time: 1.45525s, Loss: 0.02378\n",
      "Epoch: 93, Time: 1.37677s, Loss: 0.02510\n",
      "Epoch: 94, Time: 1.31949s, Loss: 0.02376\n",
      "Epoch: 95, Time: 1.29086s, Loss: 0.02301\n",
      "Epoch: 96, Time: 1.28152s, Loss: 0.02325\n",
      "Epoch: 97, Time: 1.22093s, Loss: 0.02275\n",
      "Epoch: 98, Time: 1.31907s, Loss: 0.02393\n",
      "update best: 0.48500\n",
      "Epoch: 99, Time: 1.37080s, Loss: 0.02081\n",
      "Epoch: 100, Time: 1.40924s, Loss: 0.02302\n",
      "Epoch: 101, Time: 1.69402s, Loss: 0.02299\n",
      "update best: 0.49500\n",
      "Epoch: 102, Time: 1.32635s, Loss: 0.02197\n",
      "Epoch: 103, Time: 1.33762s, Loss: 0.02165\n",
      "Epoch: 104, Time: 1.32808s, Loss: 0.02216\n",
      "Epoch: 105, Time: 1.32325s, Loss: 0.02217\n",
      "Epoch: 106, Time: 1.39723s, Loss: 0.02029\n",
      "Epoch: 107, Time: 1.37769s, Loss: 0.02081\n",
      "Epoch: 108, Time: 1.46459s, Loss: 0.02086\n",
      "Epoch: 109, Time: 1.31666s, Loss: 0.01997\n",
      "Epoch: 110, Time: 1.33238s, Loss: 0.02177\n",
      "Epoch: 111, Time: 1.56508s, Loss: 0.02051\n",
      "Epoch: 112, Time: 1.42414s, Loss: 0.02007\n",
      "Epoch: 113, Time: 1.27276s, Loss: 0.02031\n",
      "Epoch: 114, Time: 1.27076s, Loss: 0.01989\n",
      "Epoch: 115, Time: 1.33571s, Loss: 0.01988\n",
      "Epoch: 116, Time: 1.46237s, Loss: 0.02025\n",
      "Epoch: 117, Time: 1.29031s, Loss: 0.01742\n",
      "Epoch: 118, Time: 1.35665s, Loss: 0.01813\n",
      "update best: 0.50000\n",
      "Epoch: 119, Time: 1.33894s, Loss: 0.01844\n",
      "Epoch: 120, Time: 1.48070s, Loss: 0.01895\n",
      "Epoch: 121, Time: 1.43016s, Loss: 0.01908\n",
      "Epoch: 122, Time: 1.44818s, Loss: 0.01838\n",
      "Epoch: 123, Time: 1.33738s, Loss: 0.01817\n",
      "Epoch: 124, Time: 1.56268s, Loss: 0.01969\n",
      "Epoch: 125, Time: 1.36915s, Loss: 0.01744\n",
      "Epoch: 126, Time: 1.29028s, Loss: 0.01882\n",
      "Epoch: 127, Time: 1.34672s, Loss: 0.01783\n",
      "Epoch: 128, Time: 1.47206s, Loss: 0.01902\n",
      "Epoch: 129, Time: 1.49847s, Loss: 0.01799\n",
      "Epoch: 130, Time: 1.34965s, Loss: 0.01803\n",
      "Epoch: 131, Time: 1.35305s, Loss: 0.01799\n",
      "Epoch: 132, Time: 1.27873s, Loss: 0.01825\n",
      "Epoch: 133, Time: 1.29053s, Loss: 0.01852\n",
      "Epoch: 134, Time: 1.37004s, Loss: 0.01676\n",
      "Epoch: 135, Time: 1.32739s, Loss: 0.01684\n",
      "Epoch: 136, Time: 1.29580s, Loss: 0.01663\n",
      "Epoch: 137, Time: 1.31958s, Loss: 0.01641\n",
      "Epoch: 138, Time: 1.31170s, Loss: 0.01648\n",
      "Epoch: 139, Time: 1.30674s, Loss: 0.01749\n",
      "Epoch: 140, Time: 1.28187s, Loss: 0.01731\n",
      "Epoch: 141, Time: 1.27024s, Loss: 0.01744\n",
      "Epoch: 142, Time: 1.33814s, Loss: 0.01645\n",
      "Epoch: 143, Time: 1.42886s, Loss: 0.01751\n",
      "Epoch: 144, Time: 1.47848s, Loss: 0.01728\n",
      "Epoch: 145, Time: 1.38144s, Loss: 0.01801\n",
      "Epoch: 146, Time: 1.46823s, Loss: 0.01696\n",
      "Epoch: 147, Time: 1.45598s, Loss: 0.02160\n",
      "Epoch: 148, Time: 1.72187s, Loss: 0.01769\n",
      "Epoch: 149, Time: 1.70991s, Loss: 0.02529\n",
      "Epoch: 150, Time: 1.31159s, Loss: 0.02321\n",
      "Epoch: 151, Time: 1.46281s, Loss: 0.03171\n",
      "Epoch: 152, Time: 1.41087s, Loss: 0.01670\n",
      "Epoch: 153, Time: 1.51190s, Loss: 0.04494\n",
      "Epoch: 154, Time: 1.42654s, Loss: 0.01860\n",
      "Epoch: 155, Time: 1.53499s, Loss: 0.02177\n",
      "Epoch: 156, Time: 1.52821s, Loss: 0.03615\n",
      "Epoch: 157, Time: 1.38554s, Loss: 0.01814\n",
      "Epoch: 158, Time: 1.37736s, Loss: 0.01865\n",
      "Epoch: 159, Time: 1.54945s, Loss: 0.02191\n",
      "Epoch: 160, Time: 1.44442s, Loss: 0.02447\n",
      "Epoch: 161, Time: 1.44328s, Loss: 0.02430\n",
      "Epoch: 162, Time: 1.43570s, Loss: 0.01935\n",
      "Epoch: 163, Time: 1.39545s, Loss: 0.01849\n",
      "Epoch: 164, Time: 1.39675s, Loss: 0.01774\n",
      "Epoch: 165, Time: 1.39179s, Loss: 0.01589\n",
      "Epoch: 166, Time: 1.36605s, Loss: 0.01702\n",
      "Epoch: 167, Time: 1.40766s, Loss: 0.01585\n",
      "Epoch: 168, Time: 1.46108s, Loss: 0.01639\n",
      "Epoch: 169, Time: 1.38070s, Loss: 0.01560\n",
      "Epoch: 170, Time: 1.45014s, Loss: 0.01578\n",
      "Epoch: 171, Time: 1.57744s, Loss: 0.01494\n",
      "Epoch: 172, Time: 1.47759s, Loss: 0.01431\n",
      "Epoch: 173, Time: 1.40481s, Loss: 0.01409\n",
      "Epoch: 174, Time: 1.49832s, Loss: 0.01369\n",
      "Epoch: 175, Time: 1.50229s, Loss: 0.01306\n",
      "Epoch: 176, Time: 1.45149s, Loss: 0.01334\n",
      "Epoch: 177, Time: 1.46486s, Loss: 0.01242\n",
      "Epoch: 178, Time: 1.63365s, Loss: 0.01266\n",
      "Epoch: 179, Time: 2.42034s, Loss: 0.01165\n",
      "Epoch: 180, Time: 1.51814s, Loss: 0.01231\n",
      "Epoch: 181, Time: 1.64823s, Loss: 0.01221\n",
      "Epoch: 182, Time: 1.37344s, Loss: 0.01222\n",
      "Epoch: 183, Time: 1.35596s, Loss: 0.01264\n",
      "Epoch: 184, Time: 1.41736s, Loss: 0.01289\n",
      "Epoch: 185, Time: 1.38307s, Loss: 0.01289\n",
      "Epoch: 186, Time: 1.40860s, Loss: 0.01329\n",
      "Epoch: 187, Time: 1.40430s, Loss: 0.01247\n",
      "Epoch: 188, Time: 1.48603s, Loss: 0.01202\n",
      "Epoch: 189, Time: 1.38514s, Loss: 0.01232\n",
      "Epoch: 190, Time: 1.34340s, Loss: 0.01294\n",
      "Epoch: 191, Time: 1.43782s, Loss: 0.01216\n",
      "Epoch: 192, Time: 1.48778s, Loss: 0.01211\n",
      "Epoch: 193, Time: 1.38422s, Loss: 0.01375\n",
      "Epoch: 194, Time: 1.39332s, Loss: 0.01281\n",
      "Epoch: 195, Time: 1.32660s, Loss: 0.01381\n",
      "Epoch: 196, Time: 1.30333s, Loss: 0.01335\n",
      "Epoch: 197, Time: 1.49123s, Loss: 0.01330\n",
      "Epoch: 198, Time: 1.79635s, Loss: 0.01353\n",
      "Epoch: 199, Time: 1.50184s, Loss: 0.01235\n",
      "\n",
      "train finished!\n",
      "best val: 0.50000\n",
      "test...\n",
      "final result: epoch: 118\n",
      "{'accuracy': 0.5289161801338196, 'f1_score': 0.40503309054521114, 'f1_score -> average@micro': 0.5289161787805227}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.10855s, Loss: 3.00464\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.08103s, Loss: 2.85373\n",
      "Epoch: 2, Time: 0.09561s, Loss: 2.39547\n",
      "Epoch: 3, Time: 0.08431s, Loss: 2.16992\n",
      "Epoch: 4, Time: 0.08556s, Loss: 1.98238\n",
      "Epoch: 5, Time: 0.08011s, Loss: 1.82536\n",
      "update best: 0.07500\n",
      "Epoch: 6, Time: 0.09422s, Loss: 1.69112\n",
      "update best: 0.08000\n",
      "Epoch: 7, Time: 0.08904s, Loss: 1.55686\n",
      "Epoch: 8, Time: 0.09059s, Loss: 1.43632\n",
      "update best: 0.11000\n",
      "Epoch: 9, Time: 0.07604s, Loss: 1.31332\n",
      "update best: 0.22000\n",
      "Epoch: 10, Time: 0.08363s, Loss: 1.20795\n",
      "Epoch: 11, Time: 0.07904s, Loss: 1.09272\n",
      "Epoch: 12, Time: 0.07904s, Loss: 1.00381\n",
      "Epoch: 13, Time: 0.14408s, Loss: 0.90282\n",
      "Epoch: 14, Time: 0.08704s, Loss: 0.81816\n",
      "Epoch: 15, Time: 0.07804s, Loss: 0.74913\n",
      "Epoch: 16, Time: 0.08156s, Loss: 0.66262\n",
      "Epoch: 17, Time: 0.08205s, Loss: 0.60239\n",
      "Epoch: 18, Time: 0.11778s, Loss: 0.53853\n",
      "Epoch: 19, Time: 0.09161s, Loss: 0.48864\n",
      "Epoch: 20, Time: 0.09068s, Loss: 0.42724\n",
      "Epoch: 21, Time: 0.08355s, Loss: 0.38623\n",
      "Epoch: 22, Time: 0.10086s, Loss: 0.34136\n",
      "Epoch: 23, Time: 0.09659s, Loss: 0.30420\n",
      "Epoch: 24, Time: 0.09107s, Loss: 0.27242\n",
      "Epoch: 25, Time: 0.08208s, Loss: 0.23802\n",
      "Epoch: 26, Time: 0.09509s, Loss: 0.21636\n",
      "Epoch: 27, Time: 0.08206s, Loss: 0.19498\n",
      "Epoch: 28, Time: 0.08175s, Loss: 0.17500\n",
      "Epoch: 29, Time: 0.08208s, Loss: 0.15270\n",
      "Epoch: 30, Time: 0.07806s, Loss: 0.13763\n",
      "Epoch: 31, Time: 0.09466s, Loss: 0.12347\n",
      "Epoch: 32, Time: 0.09861s, Loss: 0.11316\n",
      "Epoch: 33, Time: 0.09258s, Loss: 0.10082\n",
      "Epoch: 34, Time: 0.09119s, Loss: 0.09246\n",
      "Epoch: 35, Time: 0.08960s, Loss: 0.08394\n",
      "Epoch: 36, Time: 0.08559s, Loss: 0.07804\n",
      "Epoch: 37, Time: 0.08110s, Loss: 0.07072\n",
      "Epoch: 38, Time: 0.09057s, Loss: 0.06743\n",
      "Epoch: 39, Time: 0.10018s, Loss: 0.06343\n",
      "Epoch: 40, Time: 0.09457s, Loss: 0.05752\n",
      "Epoch: 41, Time: 0.09508s, Loss: 0.05332\n",
      "Epoch: 42, Time: 0.08177s, Loss: 0.05070\n",
      "Epoch: 43, Time: 0.08888s, Loss: 0.04732\n",
      "Epoch: 44, Time: 0.09961s, Loss: 0.04493\n",
      "Epoch: 45, Time: 0.08239s, Loss: 0.04645\n",
      "Epoch: 46, Time: 0.09278s, Loss: 0.04098\n",
      "Epoch: 47, Time: 0.10937s, Loss: 0.04156\n",
      "Epoch: 48, Time: 0.09224s, Loss: 0.03786\n",
      "update best: 0.23500\n",
      "Epoch: 49, Time: 0.08105s, Loss: 0.03783\n",
      "update best: 0.25500\n",
      "Epoch: 50, Time: 0.09759s, Loss: 0.03815\n",
      "update best: 0.26500\n",
      "Epoch: 51, Time: 0.11816s, Loss: 0.03539\n",
      "Epoch: 52, Time: 0.12713s, Loss: 0.03394\n",
      "Epoch: 53, Time: 0.11458s, Loss: 0.03498\n",
      "Epoch: 54, Time: 0.11960s, Loss: 0.03471\n",
      "Epoch: 55, Time: 0.12266s, Loss: 0.03187\n",
      "Epoch: 56, Time: 0.11107s, Loss: 0.03311\n",
      "Epoch: 57, Time: 0.10058s, Loss: 0.03236\n",
      "Epoch: 58, Time: 0.12010s, Loss: 0.03206\n",
      "Epoch: 59, Time: 0.11112s, Loss: 0.03009\n",
      "Epoch: 60, Time: 0.11309s, Loss: 0.03027\n",
      "Epoch: 61, Time: 0.10956s, Loss: 0.03062\n",
      "Epoch: 62, Time: 0.11258s, Loss: 0.03146\n",
      "Epoch: 63, Time: 0.10657s, Loss: 0.02908\n",
      "Epoch: 64, Time: 0.12109s, Loss: 0.03267\n",
      "update best: 0.27000\n",
      "Epoch: 65, Time: 0.10493s, Loss: 0.02796\n",
      "update best: 0.28500\n",
      "Epoch: 66, Time: 0.11657s, Loss: 0.02921\n",
      "update best: 0.31000\n",
      "Epoch: 67, Time: 0.10761s, Loss: 0.02787\n",
      "Epoch: 68, Time: 0.11518s, Loss: 0.02633\n",
      "update best: 0.31500\n",
      "Epoch: 69, Time: 0.11161s, Loss: 0.02667\n",
      "update best: 0.32500\n",
      "Epoch: 70, Time: 0.10609s, Loss: 0.02667\n",
      "update best: 0.33000\n",
      "Epoch: 71, Time: 0.11265s, Loss: 0.02442\n",
      "update best: 0.33500\n",
      "Epoch: 72, Time: 0.10656s, Loss: 0.02611\n",
      "update best: 0.35000\n",
      "Epoch: 73, Time: 0.11760s, Loss: 0.02556\n",
      "Epoch: 74, Time: 0.11409s, Loss: 0.02474\n",
      "Epoch: 75, Time: 0.11714s, Loss: 0.02414\n",
      "Epoch: 76, Time: 0.10258s, Loss: 0.02310\n",
      "Epoch: 77, Time: 0.12112s, Loss: 0.02235\n",
      "Epoch: 78, Time: 0.11957s, Loss: 0.02200\n",
      "Epoch: 79, Time: 0.11412s, Loss: 0.02231\n",
      "Epoch: 80, Time: 0.10962s, Loss: 0.02164\n",
      "update best: 0.35500\n",
      "Epoch: 81, Time: 0.12378s, Loss: 0.02111\n",
      "update best: 0.41000\n",
      "Epoch: 82, Time: 0.11511s, Loss: 0.02094\n",
      "update best: 0.44500\n",
      "Epoch: 83, Time: 0.10360s, Loss: 0.02243\n",
      "Epoch: 84, Time: 0.11059s, Loss: 0.02188\n",
      "Epoch: 85, Time: 0.11810s, Loss: 0.02204\n",
      "Epoch: 86, Time: 0.12082s, Loss: 0.01969\n",
      "Epoch: 87, Time: 0.11518s, Loss: 0.02077\n",
      "Epoch: 88, Time: 0.11263s, Loss: 0.01974\n",
      "Epoch: 89, Time: 0.10610s, Loss: 0.01937\n",
      "Epoch: 90, Time: 0.11911s, Loss: 0.02079\n",
      "Epoch: 91, Time: 0.12612s, Loss: 0.01972\n",
      "Epoch: 92, Time: 0.12459s, Loss: 0.01940\n",
      "Epoch: 93, Time: 0.13528s, Loss: 0.01956\n",
      "Epoch: 94, Time: 0.21866s, Loss: 0.01888\n",
      "Epoch: 95, Time: 0.11109s, Loss: 0.01911\n",
      "Epoch: 96, Time: 0.12051s, Loss: 0.01821\n",
      "Epoch: 97, Time: 0.11910s, Loss: 0.01749\n",
      "Epoch: 98, Time: 0.11008s, Loss: 0.01748\n",
      "Epoch: 99, Time: 0.11158s, Loss: 0.01779\n",
      "Epoch: 100, Time: 0.11512s, Loss: 0.01833\n",
      "Epoch: 101, Time: 0.11421s, Loss: 0.01708\n",
      "Epoch: 102, Time: 0.12238s, Loss: 0.01692\n",
      "Epoch: 103, Time: 0.11256s, Loss: 0.01745\n",
      "update best: 0.45000\n",
      "Epoch: 104, Time: 0.10949s, Loss: 0.01689\n",
      "Epoch: 105, Time: 0.11056s, Loss: 0.01643\n",
      "Epoch: 106, Time: 0.10858s, Loss: 0.01698\n",
      "Epoch: 107, Time: 0.12488s, Loss: 0.01411\n",
      "update best: 0.45500\n",
      "Epoch: 108, Time: 0.11812s, Loss: 0.01557\n",
      "Epoch: 109, Time: 0.11909s, Loss: 0.01596\n",
      "update best: 0.46000\n",
      "Epoch: 110, Time: 0.10959s, Loss: 0.01623\n",
      "Epoch: 111, Time: 0.11502s, Loss: 0.01557\n",
      "update best: 0.47500\n",
      "Epoch: 112, Time: 0.10019s, Loss: 0.01667\n",
      "Epoch: 113, Time: 0.11613s, Loss: 0.01662\n",
      "Epoch: 114, Time: 0.10405s, Loss: 0.01519\n",
      "Epoch: 115, Time: 0.12112s, Loss: 0.01566\n",
      "Epoch: 116, Time: 0.11459s, Loss: 0.01502\n",
      "Epoch: 117, Time: 0.11607s, Loss: 0.01577\n",
      "Epoch: 118, Time: 0.11427s, Loss: 0.01539\n",
      "Epoch: 119, Time: 0.11919s, Loss: 0.01461\n",
      "Epoch: 120, Time: 0.10885s, Loss: 0.01566\n",
      "Epoch: 121, Time: 0.12457s, Loss: 0.01513\n",
      "Epoch: 122, Time: 0.12417s, Loss: 0.01506\n",
      "Epoch: 123, Time: 0.12285s, Loss: 0.01606\n",
      "Epoch: 124, Time: 0.12110s, Loss: 0.01423\n",
      "Epoch: 125, Time: 0.11711s, Loss: 0.01454\n",
      "Epoch: 126, Time: 0.12613s, Loss: 0.01424\n",
      "Epoch: 127, Time: 0.10858s, Loss: 0.01419\n",
      "Epoch: 128, Time: 0.10856s, Loss: 0.01394\n",
      "Epoch: 129, Time: 0.10873s, Loss: 0.01380\n",
      "Epoch: 130, Time: 0.10656s, Loss: 0.01443\n",
      "Epoch: 131, Time: 0.11498s, Loss: 0.01307\n",
      "Epoch: 132, Time: 0.11861s, Loss: 0.01310\n",
      "Epoch: 133, Time: 0.12256s, Loss: 0.01282\n",
      "Epoch: 134, Time: 0.11357s, Loss: 0.01314\n",
      "Epoch: 135, Time: 0.11219s, Loss: 0.01436\n",
      "Epoch: 136, Time: 0.10663s, Loss: 0.01356\n",
      "Epoch: 137, Time: 0.11052s, Loss: 0.01348\n",
      "Epoch: 138, Time: 0.11165s, Loss: 0.01287\n",
      "Epoch: 139, Time: 0.11514s, Loss: 0.01285\n",
      "Epoch: 140, Time: 0.16839s, Loss: 0.01282\n",
      "Epoch: 141, Time: 0.12262s, Loss: 0.01282\n",
      "Epoch: 142, Time: 0.11057s, Loss: 0.01368\n",
      "Epoch: 143, Time: 0.11360s, Loss: 0.01278\n",
      "Epoch: 144, Time: 0.10558s, Loss: 0.01363\n",
      "Epoch: 145, Time: 0.11208s, Loss: 0.01316\n",
      "Epoch: 146, Time: 0.11662s, Loss: 0.01272\n",
      "Epoch: 147, Time: 0.11758s, Loss: 0.01261\n",
      "Epoch: 148, Time: 0.12404s, Loss: 0.01321\n",
      "Epoch: 149, Time: 0.10555s, Loss: 0.01308\n",
      "Epoch: 150, Time: 0.12204s, Loss: 0.01287\n",
      "Epoch: 151, Time: 0.11496s, Loss: 0.01174\n",
      "Epoch: 152, Time: 0.10958s, Loss: 0.01301\n",
      "Epoch: 153, Time: 0.11310s, Loss: 0.01285\n",
      "Epoch: 154, Time: 0.11858s, Loss: 0.01269\n",
      "Epoch: 155, Time: 0.12313s, Loss: 0.01245\n",
      "Epoch: 156, Time: 0.10827s, Loss: 0.01330\n",
      "Epoch: 157, Time: 0.12212s, Loss: 0.01280\n",
      "Epoch: 158, Time: 0.10757s, Loss: 0.01255\n",
      "Epoch: 159, Time: 0.10765s, Loss: 0.01220\n",
      "Epoch: 160, Time: 0.12216s, Loss: 0.01134\n",
      "Epoch: 161, Time: 0.12055s, Loss: 0.01127\n",
      "Epoch: 162, Time: 0.10458s, Loss: 0.01153\n",
      "Epoch: 163, Time: 0.10329s, Loss: 0.01129\n",
      "Epoch: 164, Time: 0.10956s, Loss: 0.01120\n",
      "Epoch: 165, Time: 0.10456s, Loss: 0.01126\n",
      "Epoch: 166, Time: 0.10960s, Loss: 0.01091\n",
      "Epoch: 167, Time: 0.10909s, Loss: 0.01097\n",
      "Epoch: 168, Time: 0.12124s, Loss: 0.01180\n",
      "Epoch: 169, Time: 0.12729s, Loss: 0.01197\n",
      "Epoch: 170, Time: 0.11857s, Loss: 0.01069\n",
      "Epoch: 171, Time: 0.10259s, Loss: 0.01087\n",
      "Epoch: 172, Time: 0.11161s, Loss: 0.01150\n",
      "Epoch: 173, Time: 0.10921s, Loss: 0.01196\n",
      "Epoch: 174, Time: 0.10658s, Loss: 0.01160\n",
      "Epoch: 175, Time: 0.11360s, Loss: 0.01139\n",
      "Epoch: 176, Time: 0.11757s, Loss: 0.01102\n",
      "Epoch: 177, Time: 0.10807s, Loss: 0.01557\n",
      "Epoch: 178, Time: 0.11157s, Loss: 0.03755\n",
      "Epoch: 179, Time: 0.10435s, Loss: 0.01259\n",
      "Epoch: 180, Time: 0.11604s, Loss: 0.04858\n",
      "Epoch: 181, Time: 0.10258s, Loss: 0.04517\n",
      "Epoch: 182, Time: 0.12508s, Loss: 0.01797\n",
      "Epoch: 183, Time: 0.11606s, Loss: 0.02016\n",
      "Epoch: 184, Time: 0.12010s, Loss: 0.02098\n",
      "Epoch: 185, Time: 0.10121s, Loss: 0.02448\n",
      "Epoch: 186, Time: 0.11561s, Loss: 0.02332\n",
      "Epoch: 187, Time: 0.11158s, Loss: 0.02146\n",
      "Epoch: 188, Time: 0.11559s, Loss: 0.01876\n",
      "Epoch: 189, Time: 0.10807s, Loss: 0.01704\n",
      "Epoch: 190, Time: 0.11109s, Loss: 0.01545\n",
      "Epoch: 191, Time: 0.11456s, Loss: 0.01565\n",
      "Epoch: 192, Time: 0.10509s, Loss: 0.01590\n",
      "Epoch: 193, Time: 0.11213s, Loss: 0.01600\n",
      "Epoch: 194, Time: 0.10910s, Loss: 0.01321\n",
      "Epoch: 195, Time: 0.11056s, Loss: 0.01114\n",
      "Epoch: 196, Time: 0.10511s, Loss: 0.01090\n",
      "Epoch: 197, Time: 0.10709s, Loss: 0.01140\n",
      "Epoch: 198, Time: 0.10055s, Loss: 0.01134\n",
      "Epoch: 199, Time: 0.10959s, Loss: 0.01008\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 111\n",
      "{'accuracy': 0.4370983839035034, 'f1_score': 0.3630498297451491, 'f1_score -> average@micro': 0.4370983864058261}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-citation Citaseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is cocitation_citeseer dataset:\n",
       "  ->  num_classes\n",
       "  ->  num_vertices\n",
       "  ->  num_edges\n",
       "  ->  dim_features\n",
       "  ->  features\n",
       "  ->  edge_list\n",
       "  ->  labels\n",
       "  ->  train_mask\n",
       "  ->  val_mask\n",
       "  ->  test_mask\n",
       "Please try `data['name']` to get the specified data."
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = dhg.data.CocitationCiteseer(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['num_vertices'])\n",
    "data['num_edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.07557s, Loss: 1.84384\n",
      "update best: 0.18053\n",
      "Epoch: 1, Time: 0.01852s, Loss: 1.32729\n",
      "Epoch: 2, Time: 0.02073s, Loss: 0.89455\n",
      "Epoch: 3, Time: 0.01821s, Loss: 0.58217\n",
      "Epoch: 4, Time: 0.02025s, Loss: 0.33297\n",
      "Epoch: 5, Time: 0.01954s, Loss: 0.21457\n",
      "Epoch: 6, Time: 0.01855s, Loss: 0.16569\n",
      "update best: 0.18399\n",
      "Epoch: 7, Time: 0.01419s, Loss: 0.11529\n",
      "update best: 0.20258\n",
      "Epoch: 8, Time: 0.02618s, Loss: 0.11493\n",
      "update best: 0.21928\n",
      "Epoch: 9, Time: 0.02598s, Loss: 0.08893\n",
      "update best: 0.22716\n",
      "Epoch: 10, Time: 0.01781s, Loss: 0.04256\n",
      "update best: 0.23503\n",
      "Epoch: 11, Time: 0.01740s, Loss: 0.05914\n",
      "update best: 0.24071\n",
      "Epoch: 12, Time: 0.01607s, Loss: 0.03737\n",
      "update best: 0.24354\n",
      "Epoch: 13, Time: 0.01743s, Loss: 0.03301\n",
      "update best: 0.25457\n",
      "Epoch: 14, Time: 0.01613s, Loss: 0.02690\n",
      "update best: 0.25772\n",
      "Epoch: 15, Time: 0.01786s, Loss: 0.01817\n",
      "update best: 0.25835\n",
      "Epoch: 16, Time: 0.01224s, Loss: 0.04195\n",
      "Epoch: 17, Time: 0.01985s, Loss: 0.04121\n",
      "Epoch: 18, Time: 0.02273s, Loss: 0.01221\n",
      "Epoch: 19, Time: 0.02539s, Loss: 0.01795\n",
      "Epoch: 20, Time: 0.02135s, Loss: 0.03318\n",
      "Epoch: 21, Time: 0.02106s, Loss: 0.01417\n",
      "Epoch: 22, Time: 0.02165s, Loss: 0.01202\n",
      "Epoch: 23, Time: 0.02191s, Loss: 0.00621\n",
      "Epoch: 24, Time: 0.02259s, Loss: 0.01527\n",
      "Epoch: 25, Time: 0.01803s, Loss: 0.00521\n",
      "Epoch: 26, Time: 0.02032s, Loss: 0.01303\n",
      "Epoch: 27, Time: 0.02088s, Loss: 0.01279\n",
      "Epoch: 28, Time: 0.01999s, Loss: 0.00548\n",
      "Epoch: 29, Time: 0.01940s, Loss: 0.00536\n",
      "Epoch: 30, Time: 0.01582s, Loss: 0.01232\n",
      "Epoch: 31, Time: 0.02379s, Loss: 0.00742\n",
      "Epoch: 32, Time: 0.01838s, Loss: 0.00472\n",
      "Epoch: 33, Time: 0.01057s, Loss: 0.01984\n",
      "Epoch: 34, Time: 0.02498s, Loss: 0.00515\n",
      "Epoch: 35, Time: 0.01750s, Loss: 0.00302\n",
      "Epoch: 36, Time: 0.01646s, Loss: 0.01405\n",
      "Epoch: 37, Time: 0.01622s, Loss: 0.00204\n",
      "Epoch: 38, Time: 0.01946s, Loss: 0.00648\n",
      "Epoch: 39, Time: 0.01720s, Loss: 0.00954\n",
      "Epoch: 40, Time: 0.01785s, Loss: 0.00736\n",
      "Epoch: 41, Time: 0.01665s, Loss: 0.00238\n",
      "Epoch: 42, Time: 0.01664s, Loss: 0.00368\n",
      "Epoch: 43, Time: 0.01792s, Loss: 0.00203\n",
      "Epoch: 44, Time: 0.01528s, Loss: 0.00152\n",
      "Epoch: 45, Time: 0.01737s, Loss: 0.00106\n",
      "Epoch: 46, Time: 0.01749s, Loss: 0.00479\n",
      "Epoch: 47, Time: 0.01904s, Loss: 0.00352\n",
      "Epoch: 48, Time: 0.01904s, Loss: 0.01470\n",
      "Epoch: 49, Time: 0.02189s, Loss: 0.00244\n",
      "Epoch: 50, Time: 0.02238s, Loss: 0.00861\n",
      "Epoch: 51, Time: 0.01767s, Loss: 0.00358\n",
      "Epoch: 52, Time: 0.02101s, Loss: 0.00322\n",
      "Epoch: 53, Time: 0.01907s, Loss: 0.00118\n",
      "Epoch: 54, Time: 0.01659s, Loss: 0.00263\n",
      "Epoch: 55, Time: 0.02012s, Loss: 0.01326\n",
      "Epoch: 56, Time: 0.01604s, Loss: 0.00693\n",
      "Epoch: 57, Time: 0.01645s, Loss: 0.00257\n",
      "Epoch: 58, Time: 0.01814s, Loss: 0.00101\n",
      "Epoch: 59, Time: 0.01666s, Loss: 0.00911\n",
      "Epoch: 60, Time: 0.01661s, Loss: 0.00936\n",
      "Epoch: 61, Time: 0.01737s, Loss: 0.00123\n",
      "Epoch: 62, Time: 0.01507s, Loss: 0.01041\n",
      "Epoch: 63, Time: 0.01793s, Loss: 0.01212\n",
      "Epoch: 64, Time: 0.02018s, Loss: 0.00214\n",
      "Epoch: 65, Time: 0.02066s, Loss: 0.00262\n",
      "Epoch: 66, Time: 0.01813s, Loss: 0.00584\n",
      "Epoch: 67, Time: 0.01768s, Loss: 0.00087\n",
      "Epoch: 68, Time: 0.01970s, Loss: 0.01147\n",
      "Epoch: 69, Time: 0.02049s, Loss: 0.01437\n",
      "Epoch: 70, Time: 0.02059s, Loss: 0.00154\n",
      "Epoch: 71, Time: 0.01939s, Loss: 0.00321\n",
      "Epoch: 72, Time: 0.02094s, Loss: 0.00186\n",
      "Epoch: 73, Time: 0.01794s, Loss: 0.00224\n",
      "Epoch: 74, Time: 0.01856s, Loss: 0.00588\n",
      "Epoch: 75, Time: 0.01907s, Loss: 0.02405\n",
      "Epoch: 76, Time: 0.01663s, Loss: 0.00334\n",
      "Epoch: 77, Time: 0.02108s, Loss: 0.00402\n",
      "update best: 0.26402\n",
      "Epoch: 78, Time: 0.01662s, Loss: 0.00632\n",
      "update best: 0.27190\n",
      "Epoch: 79, Time: 0.01623s, Loss: 0.00302\n",
      "update best: 0.27347\n",
      "Epoch: 80, Time: 0.01536s, Loss: 0.00129\n",
      "update best: 0.27599\n",
      "Epoch: 81, Time: 0.02290s, Loss: 0.00393\n",
      "Epoch: 82, Time: 0.01831s, Loss: 0.00084\n",
      "Epoch: 83, Time: 0.01609s, Loss: 0.00363\n",
      "Epoch: 84, Time: 0.01558s, Loss: 0.00176\n",
      "Epoch: 85, Time: 0.01565s, Loss: 0.01176\n",
      "Epoch: 86, Time: 0.01718s, Loss: 0.00149\n",
      "Epoch: 87, Time: 0.00983s, Loss: 0.00122\n",
      "Epoch: 88, Time: 0.01315s, Loss: 0.00072\n",
      "Epoch: 89, Time: 0.01795s, Loss: 0.00815\n",
      "Epoch: 90, Time: 0.01974s, Loss: 0.00086\n",
      "Epoch: 91, Time: 0.01664s, Loss: 0.00212\n",
      "Epoch: 92, Time: 0.01660s, Loss: 0.00074\n",
      "Epoch: 93, Time: 0.01622s, Loss: 0.00070\n",
      "Epoch: 94, Time: 0.01654s, Loss: 0.00234\n",
      "Epoch: 95, Time: 0.01416s, Loss: 0.00231\n",
      "Epoch: 96, Time: 0.01375s, Loss: 0.01695\n",
      "Epoch: 97, Time: 0.01603s, Loss: 0.00428\n",
      "Epoch: 98, Time: 0.01818s, Loss: 0.00508\n",
      "Epoch: 99, Time: 0.01530s, Loss: 0.00125\n",
      "Epoch: 100, Time: 0.01592s, Loss: 0.00322\n",
      "Epoch: 101, Time: 0.01391s, Loss: 0.00112\n",
      "Epoch: 102, Time: 0.01667s, Loss: 0.00270\n",
      "Epoch: 103, Time: 0.01702s, Loss: 0.00323\n",
      "Epoch: 104, Time: 0.01760s, Loss: 0.00200\n",
      "Epoch: 105, Time: 0.02148s, Loss: 0.00109\n",
      "Epoch: 106, Time: 0.01639s, Loss: 0.01693\n",
      "Epoch: 107, Time: 0.01756s, Loss: 0.00453\n",
      "Epoch: 108, Time: 0.01256s, Loss: 0.00070\n",
      "Epoch: 109, Time: 0.01725s, Loss: 0.00992\n",
      "Epoch: 110, Time: 0.01615s, Loss: 0.01284\n",
      "Epoch: 111, Time: 0.02071s, Loss: 0.00394\n",
      "Epoch: 112, Time: 0.01622s, Loss: 0.00095\n",
      "Epoch: 113, Time: 0.01667s, Loss: 0.00139\n",
      "Epoch: 114, Time: 0.01630s, Loss: 0.00704\n",
      "Epoch: 115, Time: 0.02093s, Loss: 0.01456\n",
      "Epoch: 116, Time: 0.01951s, Loss: 0.00099\n",
      "Epoch: 117, Time: 0.01817s, Loss: 0.00307\n",
      "Epoch: 118, Time: 0.01674s, Loss: 0.01291\n",
      "Epoch: 119, Time: 0.01944s, Loss: 0.00089\n",
      "Epoch: 120, Time: 0.01007s, Loss: 0.00867\n",
      "Epoch: 121, Time: 0.01729s, Loss: 0.00143\n",
      "Epoch: 122, Time: 0.01749s, Loss: 0.00413\n",
      "Epoch: 123, Time: 0.01876s, Loss: 0.00115\n",
      "Epoch: 124, Time: 0.02038s, Loss: 0.00473\n",
      "Epoch: 125, Time: 0.01647s, Loss: 0.00423\n",
      "Epoch: 126, Time: 0.01669s, Loss: 0.00284\n",
      "Epoch: 127, Time: 0.01748s, Loss: 0.00151\n",
      "Epoch: 128, Time: 0.01738s, Loss: 0.00751\n",
      "Epoch: 129, Time: 0.01639s, Loss: 0.00503\n",
      "Epoch: 130, Time: 0.02013s, Loss: 0.00140\n",
      "Epoch: 131, Time: 0.01960s, Loss: 0.00309\n",
      "Epoch: 132, Time: 0.01598s, Loss: 0.00381\n",
      "update best: 0.27977\n",
      "Epoch: 133, Time: 0.01861s, Loss: 0.00176\n",
      "update best: 0.28607\n",
      "Epoch: 134, Time: 0.01128s, Loss: 0.00439\n",
      "update best: 0.29332\n",
      "Epoch: 135, Time: 0.01695s, Loss: 0.00276\n",
      "Epoch: 136, Time: 0.01538s, Loss: 0.00186\n",
      "Epoch: 137, Time: 0.01881s, Loss: 0.00234\n",
      "Epoch: 138, Time: 0.01441s, Loss: 0.00368\n",
      "Epoch: 139, Time: 0.01693s, Loss: 0.00487\n",
      "Epoch: 140, Time: 0.02137s, Loss: 0.00286\n",
      "Epoch: 141, Time: 0.01160s, Loss: 0.00101\n",
      "Epoch: 142, Time: 0.01736s, Loss: 0.00221\n",
      "Epoch: 143, Time: 0.01846s, Loss: 0.00689\n",
      "Epoch: 144, Time: 0.01861s, Loss: 0.00191\n",
      "Epoch: 145, Time: 0.01987s, Loss: 0.00288\n",
      "Epoch: 146, Time: 0.01812s, Loss: 0.00674\n",
      "Epoch: 147, Time: 0.01984s, Loss: 0.00450\n",
      "Epoch: 148, Time: 0.01314s, Loss: 0.00107\n",
      "Epoch: 149, Time: 0.01819s, Loss: 0.00232\n",
      "Epoch: 150, Time: 0.01849s, Loss: 0.00186\n",
      "Epoch: 151, Time: 0.01651s, Loss: 0.00094\n",
      "Epoch: 152, Time: 0.01596s, Loss: 0.01419\n",
      "Epoch: 153, Time: 0.01539s, Loss: 0.00092\n",
      "Epoch: 154, Time: 0.01435s, Loss: 0.00103\n",
      "Epoch: 155, Time: 0.01859s, Loss: 0.00089\n",
      "Epoch: 156, Time: 0.01928s, Loss: 0.00860\n",
      "Epoch: 157, Time: 0.02187s, Loss: 0.00186\n",
      "Epoch: 158, Time: 0.01796s, Loss: 0.04007\n",
      "Epoch: 159, Time: 0.02077s, Loss: 0.01148\n",
      "Epoch: 160, Time: 0.01883s, Loss: 0.01324\n",
      "Epoch: 161, Time: 0.01697s, Loss: 0.00500\n",
      "Epoch: 162, Time: 0.01291s, Loss: 0.02147\n",
      "Epoch: 163, Time: 0.01828s, Loss: 0.00268\n",
      "Epoch: 164, Time: 0.01877s, Loss: 0.00114\n",
      "Epoch: 165, Time: 0.01660s, Loss: 0.00103\n",
      "Epoch: 166, Time: 0.01687s, Loss: 0.00143\n",
      "Epoch: 167, Time: 0.01458s, Loss: 0.00802\n",
      "Epoch: 168, Time: 0.01996s, Loss: 0.00196\n",
      "Epoch: 169, Time: 0.02152s, Loss: 0.00107\n",
      "Epoch: 170, Time: 0.02028s, Loss: 0.00500\n",
      "Epoch: 171, Time: 0.01687s, Loss: 0.00304\n",
      "Epoch: 172, Time: 0.02011s, Loss: 0.00600\n",
      "Epoch: 173, Time: 0.02363s, Loss: 0.00318\n",
      "Epoch: 174, Time: 0.02392s, Loss: 0.00424\n",
      "Epoch: 175, Time: 0.02470s, Loss: 0.00479\n",
      "Epoch: 176, Time: 0.01839s, Loss: 0.00095\n",
      "Epoch: 177, Time: 0.01289s, Loss: 0.00309\n",
      "Epoch: 178, Time: 0.01559s, Loss: 0.01341\n",
      "Epoch: 179, Time: 0.01824s, Loss: 0.00103\n",
      "Epoch: 180, Time: 0.02663s, Loss: 0.00252\n",
      "Epoch: 181, Time: 0.01736s, Loss: 0.00145\n",
      "Epoch: 182, Time: 0.01917s, Loss: 0.00156\n",
      "Epoch: 183, Time: 0.02012s, Loss: 0.00154\n",
      "Epoch: 184, Time: 0.02048s, Loss: 0.00170\n",
      "Epoch: 185, Time: 0.01641s, Loss: 0.00569\n",
      "Epoch: 186, Time: 0.01850s, Loss: 0.00530\n",
      "Epoch: 187, Time: 0.01862s, Loss: 0.00718\n",
      "Epoch: 188, Time: 0.01619s, Loss: 0.00135\n",
      "Epoch: 189, Time: 0.02075s, Loss: 0.00858\n",
      "Epoch: 190, Time: 0.01812s, Loss: 0.00070\n",
      "Epoch: 191, Time: 0.01697s, Loss: 0.00161\n",
      "Epoch: 192, Time: 0.01435s, Loss: 0.00187\n",
      "Epoch: 193, Time: 0.02099s, Loss: 0.01581\n",
      "Epoch: 194, Time: 0.01537s, Loss: 0.00251\n",
      "Epoch: 195, Time: 0.02052s, Loss: 0.00978\n",
      "Epoch: 196, Time: 0.01819s, Loss: 0.00208\n",
      "Epoch: 197, Time: 0.01932s, Loss: 0.00614\n",
      "Epoch: 198, Time: 0.01474s, Loss: 0.01333\n",
      "Epoch: 199, Time: 0.02028s, Loss: 0.00244\n",
      "\n",
      "train finished!\n",
      "best val: 0.29332\n",
      "test...\n",
      "final result: epoch: 134\n",
      "{'accuracy': 0.29332074522972107, 'f1_score': 0.24036976724855488, 'f1_score -> average@micro': 0.2933207309388784}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.02666s, Loss: 1.81649\n",
      "update best: 0.21424\n",
      "Epoch: 1, Time: 0.02883s, Loss: 1.43765\n",
      "update best: 0.21865\n",
      "Epoch: 2, Time: 0.02005s, Loss: 1.22796\n",
      "Epoch: 3, Time: 0.01828s, Loss: 1.15794\n",
      "Epoch: 4, Time: 0.01936s, Loss: 1.12441\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.02366s, Loss: 1.10783\n",
      "update best: 0.22275\n",
      "Epoch: 6, Time: 0.01793s, Loss: 1.09250\n",
      "update best: 0.22369\n",
      "Epoch: 7, Time: 0.02164s, Loss: 1.08502\n",
      "update best: 0.22495\n",
      "Epoch: 8, Time: 0.02327s, Loss: 1.07616\n",
      "update best: 0.22527\n",
      "Epoch: 9, Time: 0.02244s, Loss: 1.07437\n",
      "Epoch: 10, Time: 0.01950s, Loss: 1.07368\n",
      "update best: 0.22590\n",
      "Epoch: 11, Time: 0.01640s, Loss: 1.06851\n",
      "Epoch: 12, Time: 0.01686s, Loss: 1.06937\n",
      "Epoch: 13, Time: 0.01804s, Loss: 1.07050\n",
      "Epoch: 14, Time: 0.01849s, Loss: 1.06823\n",
      "update best: 0.22684\n",
      "Epoch: 15, Time: 0.02724s, Loss: 1.06642\n",
      "update best: 0.22810\n",
      "Epoch: 16, Time: 0.04121s, Loss: 1.06693\n",
      "update best: 0.22842\n",
      "Epoch: 17, Time: 0.01856s, Loss: 1.06795\n",
      "update best: 0.22873\n",
      "Epoch: 18, Time: 0.01636s, Loss: 1.06691\n",
      "update best: 0.23031\n",
      "Epoch: 19, Time: 0.01900s, Loss: 1.06681\n",
      "update best: 0.23125\n",
      "Epoch: 20, Time: 0.01937s, Loss: 1.06605\n",
      "update best: 0.23346\n",
      "Epoch: 21, Time: 0.02448s, Loss: 1.06690\n",
      "Epoch: 22, Time: 0.01854s, Loss: 1.06663\n",
      "update best: 0.23377\n",
      "Epoch: 23, Time: 0.02143s, Loss: 1.06689\n",
      "Epoch: 24, Time: 0.02080s, Loss: 1.06694\n",
      "Epoch: 25, Time: 0.01814s, Loss: 1.06539\n",
      "update best: 0.23440\n",
      "Epoch: 26, Time: 0.02194s, Loss: 1.07002\n",
      "Epoch: 27, Time: 0.02151s, Loss: 1.06522\n",
      "Epoch: 28, Time: 0.02571s, Loss: 1.06616\n",
      "update best: 0.23693\n",
      "Epoch: 29, Time: 0.02481s, Loss: 1.06519\n",
      "update best: 0.23913\n",
      "Epoch: 30, Time: 0.01903s, Loss: 1.06896\n",
      "update best: 0.23976\n",
      "Epoch: 31, Time: 0.01825s, Loss: 1.06508\n",
      "update best: 0.24228\n",
      "Epoch: 32, Time: 0.01837s, Loss: 1.06613\n",
      "update best: 0.24354\n",
      "Epoch: 33, Time: 0.02131s, Loss: 1.06547\n",
      "Epoch: 34, Time: 0.01762s, Loss: 1.06507\n",
      "Epoch: 35, Time: 0.02063s, Loss: 1.06493\n",
      "Epoch: 36, Time: 0.02128s, Loss: 1.06490\n",
      "Epoch: 37, Time: 0.01768s, Loss: 1.06516\n",
      "Epoch: 38, Time: 0.01795s, Loss: 1.06532\n",
      "Epoch: 39, Time: 0.01812s, Loss: 1.06980\n",
      "Epoch: 40, Time: 0.01674s, Loss: 1.06490\n",
      "Epoch: 41, Time: 0.01593s, Loss: 1.06702\n",
      "Epoch: 42, Time: 0.01489s, Loss: 1.06530\n",
      "Epoch: 43, Time: 0.01657s, Loss: 1.06540\n",
      "Epoch: 44, Time: 0.01704s, Loss: 1.06647\n",
      "update best: 0.24417\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06484\n",
      "update best: 0.24543\n",
      "Epoch: 46, Time: 0.01660s, Loss: 1.06728\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.01782s, Loss: 1.06481\n",
      "Epoch: 48, Time: 0.01757s, Loss: 1.06632\n",
      "Epoch: 49, Time: 0.01485s, Loss: 1.06521\n",
      "Epoch: 50, Time: 0.01675s, Loss: 1.06575\n",
      "Epoch: 51, Time: 0.01786s, Loss: 1.06525\n",
      "Epoch: 52, Time: 0.02277s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.02166s, Loss: 1.06507\n",
      "Epoch: 54, Time: 0.01743s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01731s, Loss: 1.06621\n",
      "Epoch: 56, Time: 0.01820s, Loss: 1.06738\n",
      "Epoch: 57, Time: 0.01791s, Loss: 1.06510\n",
      "Epoch: 58, Time: 0.02247s, Loss: 1.06537\n",
      "Epoch: 59, Time: 0.02010s, Loss: 1.06517\n",
      "Epoch: 60, Time: 0.01746s, Loss: 1.06534\n",
      "Epoch: 61, Time: 0.01992s, Loss: 1.06506\n",
      "Epoch: 62, Time: 0.01787s, Loss: 1.06508\n",
      "update best: 0.24764\n",
      "Epoch: 63, Time: 0.02259s, Loss: 1.06570\n",
      "update best: 0.24953\n",
      "Epoch: 64, Time: 0.01247s, Loss: 1.06505\n",
      "update best: 0.25142\n",
      "Epoch: 65, Time: 0.01587s, Loss: 1.06507\n",
      "Epoch: 66, Time: 0.01652s, Loss: 1.06519\n",
      "update best: 0.25268\n",
      "Epoch: 67, Time: 0.01768s, Loss: 1.06502\n",
      "update best: 0.25299\n",
      "Epoch: 68, Time: 0.01546s, Loss: 1.06523\n",
      "update best: 0.25331\n",
      "Epoch: 69, Time: 0.01801s, Loss: 1.07305\n",
      "update best: 0.25551\n",
      "Epoch: 70, Time: 0.01831s, Loss: 1.06549\n",
      "update best: 0.25583\n",
      "Epoch: 71, Time: 0.01908s, Loss: 1.06533\n",
      "Epoch: 72, Time: 0.02021s, Loss: 1.06593\n",
      "Epoch: 73, Time: 0.01627s, Loss: 1.06515\n",
      "Epoch: 74, Time: 0.01677s, Loss: 1.06505\n",
      "Epoch: 75, Time: 0.01870s, Loss: 1.06534\n",
      "update best: 0.25677\n",
      "Epoch: 76, Time: 0.01737s, Loss: 1.06526\n",
      "update best: 0.25992\n",
      "Epoch: 77, Time: 0.01953s, Loss: 1.06741\n",
      "update best: 0.26654\n",
      "Epoch: 78, Time: 0.02144s, Loss: 1.06646\n",
      "update best: 0.26906\n",
      "Epoch: 79, Time: 0.02197s, Loss: 1.06549\n",
      "update best: 0.27095\n",
      "Epoch: 80, Time: 0.02261s, Loss: 1.06540\n",
      "update best: 0.27190\n",
      "Epoch: 81, Time: 0.02229s, Loss: 1.06554\n",
      "Epoch: 82, Time: 0.02355s, Loss: 1.06607\n",
      "update best: 0.27473\n",
      "Epoch: 83, Time: 0.01814s, Loss: 1.06520\n",
      "Epoch: 84, Time: 0.01883s, Loss: 1.06574\n",
      "update best: 0.27505\n",
      "Epoch: 85, Time: 0.01675s, Loss: 1.06542\n",
      "update best: 0.27662\n",
      "Epoch: 86, Time: 0.01655s, Loss: 1.06497\n",
      "update best: 0.27788\n",
      "Epoch: 87, Time: 0.01687s, Loss: 1.06536\n",
      "update best: 0.27820\n",
      "Epoch: 88, Time: 0.01637s, Loss: 1.06712\n",
      "update best: 0.27851\n",
      "Epoch: 89, Time: 0.01623s, Loss: 1.06507\n",
      "Epoch: 90, Time: 0.01593s, Loss: 1.06662\n",
      "Epoch: 91, Time: 0.01839s, Loss: 1.06555\n",
      "Epoch: 92, Time: 0.01371s, Loss: 1.06724\n",
      "Epoch: 93, Time: 0.01482s, Loss: 1.06553\n",
      "update best: 0.28040\n",
      "Epoch: 94, Time: 0.01767s, Loss: 1.06508\n",
      "Epoch: 95, Time: 0.01703s, Loss: 1.06521\n",
      "update best: 0.28166\n",
      "Epoch: 96, Time: 0.01775s, Loss: 1.06502\n",
      "update best: 0.28261\n",
      "Epoch: 97, Time: 0.01963s, Loss: 1.06545\n",
      "update best: 0.28387\n",
      "Epoch: 98, Time: 0.02233s, Loss: 1.06513\n",
      "update best: 0.28481\n",
      "Epoch: 99, Time: 0.01748s, Loss: 1.06547\n",
      "Epoch: 100, Time: 0.01556s, Loss: 1.06497\n",
      "Epoch: 101, Time: 0.01438s, Loss: 1.06506\n",
      "update best: 0.28544\n",
      "Epoch: 102, Time: 0.01760s, Loss: 1.06503\n",
      "update best: 0.28639\n",
      "Epoch: 103, Time: 0.01655s, Loss: 1.06542\n",
      "update best: 0.28702\n",
      "Epoch: 104, Time: 0.01242s, Loss: 1.06585\n",
      "Epoch: 105, Time: 0.01357s, Loss: 1.06535\n",
      "Epoch: 106, Time: 0.01611s, Loss: 1.06541\n",
      "Epoch: 107, Time: 0.01720s, Loss: 1.06517\n",
      "Epoch: 108, Time: 0.01844s, Loss: 1.06508\n",
      "Epoch: 109, Time: 0.01629s, Loss: 1.06523\n",
      "Epoch: 110, Time: 0.01776s, Loss: 1.06669\n",
      "Epoch: 111, Time: 0.02685s, Loss: 1.06504\n",
      "Epoch: 112, Time: 0.01608s, Loss: 1.06830\n",
      "Epoch: 113, Time: 0.02013s, Loss: 1.06510\n",
      "Epoch: 114, Time: 0.01823s, Loss: 1.06535\n",
      "Epoch: 115, Time: 0.01609s, Loss: 1.06590\n",
      "Epoch: 116, Time: 0.01215s, Loss: 1.06531\n",
      "Epoch: 117, Time: 0.01296s, Loss: 1.06569\n",
      "Epoch: 118, Time: 0.01771s, Loss: 1.06568\n",
      "Epoch: 119, Time: 0.02355s, Loss: 1.06620\n",
      "Epoch: 120, Time: 0.01563s, Loss: 1.06539\n",
      "Epoch: 121, Time: 0.01465s, Loss: 1.06579\n",
      "Epoch: 122, Time: 0.01447s, Loss: 1.06534\n",
      "Epoch: 123, Time: 0.01752s, Loss: 1.06539\n",
      "Epoch: 124, Time: 0.01862s, Loss: 1.06516\n",
      "Epoch: 125, Time: 0.01149s, Loss: 1.06508\n",
      "update best: 0.28733\n",
      "Epoch: 126, Time: 0.01664s, Loss: 1.06511\n",
      "update best: 0.28922\n",
      "Epoch: 127, Time: 0.01757s, Loss: 1.06536\n",
      "update best: 0.29017\n",
      "Epoch: 128, Time: 0.01645s, Loss: 1.06550\n",
      "update best: 0.29080\n",
      "Epoch: 129, Time: 0.01800s, Loss: 1.06510\n",
      "update best: 0.29238\n",
      "Epoch: 130, Time: 0.01650s, Loss: 1.06519\n",
      "Epoch: 131, Time: 0.02048s, Loss: 1.06574\n",
      "Epoch: 132, Time: 0.01997s, Loss: 1.06540\n",
      "Epoch: 133, Time: 0.02081s, Loss: 1.06570\n",
      "Epoch: 134, Time: 0.01810s, Loss: 1.06556\n",
      "Epoch: 135, Time: 0.01953s, Loss: 1.06589\n",
      "update best: 0.29364\n",
      "Epoch: 136, Time: 0.01920s, Loss: 1.06544\n",
      "Epoch: 137, Time: 0.02303s, Loss: 1.06539\n",
      "Epoch: 138, Time: 0.01508s, Loss: 1.06505\n",
      "Epoch: 139, Time: 0.01842s, Loss: 1.06553\n",
      "Epoch: 140, Time: 0.02009s, Loss: 1.06553\n",
      "Epoch: 141, Time: 0.02001s, Loss: 1.06507\n",
      "Epoch: 142, Time: 0.02050s, Loss: 1.06608\n",
      "Epoch: 143, Time: 0.02168s, Loss: 1.06573\n",
      "Epoch: 144, Time: 0.01856s, Loss: 1.06533\n",
      "Epoch: 145, Time: 0.01972s, Loss: 1.06542\n",
      "Epoch: 146, Time: 0.01655s, Loss: 1.06536\n",
      "Epoch: 147, Time: 0.01834s, Loss: 1.06544\n",
      "Epoch: 148, Time: 0.01313s, Loss: 1.06630\n",
      "Epoch: 149, Time: 0.01821s, Loss: 1.06515\n",
      "Epoch: 150, Time: 0.01853s, Loss: 1.06549\n",
      "Epoch: 151, Time: 0.01937s, Loss: 1.06519\n",
      "Epoch: 152, Time: 0.01679s, Loss: 1.06495\n",
      "Epoch: 153, Time: 0.01791s, Loss: 1.06549\n",
      "Epoch: 154, Time: 0.01480s, Loss: 1.06536\n",
      "Epoch: 155, Time: 0.01520s, Loss: 1.06516\n",
      "Epoch: 156, Time: 0.01627s, Loss: 1.06595\n",
      "Epoch: 157, Time: 0.01555s, Loss: 1.06553\n",
      "Epoch: 158, Time: 0.02305s, Loss: 1.06554\n",
      "Epoch: 159, Time: 0.01351s, Loss: 1.06529\n",
      "Epoch: 160, Time: 0.01414s, Loss: 1.06528\n",
      "Epoch: 161, Time: 0.01765s, Loss: 1.06544\n",
      "Epoch: 162, Time: 0.01710s, Loss: 1.06587\n",
      "Epoch: 163, Time: 0.02061s, Loss: 1.06593\n",
      "Epoch: 164, Time: 0.02033s, Loss: 1.06537\n",
      "Epoch: 165, Time: 0.01972s, Loss: 1.06553\n",
      "Epoch: 166, Time: 0.01385s, Loss: 1.06530\n",
      "Epoch: 167, Time: 0.01608s, Loss: 1.06550\n",
      "Epoch: 168, Time: 0.01672s, Loss: 1.06533\n",
      "Epoch: 169, Time: 0.01599s, Loss: 1.06530\n",
      "Epoch: 170, Time: 0.01611s, Loss: 1.06539\n",
      "Epoch: 171, Time: 0.01605s, Loss: 1.06524\n",
      "Epoch: 172, Time: 0.01934s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01833s, Loss: 1.06556\n",
      "Epoch: 174, Time: 0.01524s, Loss: 1.06522\n",
      "update best: 0.29490\n",
      "Epoch: 175, Time: 0.01827s, Loss: 1.06555\n",
      "update best: 0.29521\n",
      "Epoch: 176, Time: 0.01649s, Loss: 1.06508\n",
      "Epoch: 177, Time: 0.01646s, Loss: 1.06528\n",
      "update best: 0.29647\n",
      "Epoch: 178, Time: 0.02095s, Loss: 1.06525\n",
      "update best: 0.29805\n",
      "Epoch: 179, Time: 0.01807s, Loss: 1.06549\n",
      "Epoch: 180, Time: 0.01872s, Loss: 1.06556\n",
      "Epoch: 181, Time: 0.01824s, Loss: 1.06508\n",
      "Epoch: 182, Time: 0.01404s, Loss: 1.06543\n",
      "Epoch: 183, Time: 0.01711s, Loss: 1.06532\n",
      "Epoch: 184, Time: 0.01564s, Loss: 1.06961\n",
      "Epoch: 185, Time: 0.01837s, Loss: 1.06526\n",
      "Epoch: 186, Time: 0.01663s, Loss: 1.06531\n",
      "Epoch: 187, Time: 0.00980s, Loss: 1.06604\n",
      "Epoch: 188, Time: 0.01678s, Loss: 1.06544\n",
      "Epoch: 189, Time: 0.01867s, Loss: 1.06573\n",
      "Epoch: 190, Time: 0.01688s, Loss: 1.06559\n",
      "Epoch: 191, Time: 0.01976s, Loss: 1.06660\n",
      "Epoch: 192, Time: 0.01817s, Loss: 1.06627\n",
      "Epoch: 193, Time: 0.02570s, Loss: 1.06583\n",
      "Epoch: 194, Time: 0.01776s, Loss: 1.06552\n",
      "Epoch: 195, Time: 0.01549s, Loss: 1.06540\n",
      "Epoch: 196, Time: 0.01602s, Loss: 1.06679\n",
      "Epoch: 197, Time: 0.01553s, Loss: 1.06629\n",
      "Epoch: 198, Time: 0.01786s, Loss: 1.06716\n",
      "Epoch: 199, Time: 0.01856s, Loss: 1.06667\n",
      "\n",
      "train finished!\n",
      "best val: 0.29805\n",
      "test...\n",
      "final result: epoch: 178\n",
      "{'accuracy': 0.29804661870002747, 'f1_score': 0.21974182550479207, 'f1_score -> average@micro': 0.2980466288594833}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.04270s, Loss: 1.82284\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02071s, Loss: 1.51886\n",
      "Epoch: 2, Time: 0.02244s, Loss: 1.30142\n",
      "Epoch: 3, Time: 0.01422s, Loss: 1.19327\n",
      "Epoch: 4, Time: 0.01827s, Loss: 1.14901\n",
      "update best: 0.21456\n",
      "Epoch: 5, Time: 0.01704s, Loss: 1.11603\n",
      "update best: 0.21582\n",
      "Epoch: 6, Time: 0.01643s, Loss: 1.09219\n",
      "update best: 0.21676\n",
      "Epoch: 7, Time: 0.01922s, Loss: 1.08688\n",
      "Epoch: 8, Time: 0.02138s, Loss: 1.08081\n",
      "update best: 0.21802\n",
      "Epoch: 9, Time: 0.02001s, Loss: 1.07927\n",
      "update best: 0.21991\n",
      "Epoch: 10, Time: 0.02502s, Loss: 1.07121\n",
      "update best: 0.22401\n",
      "Epoch: 11, Time: 0.01725s, Loss: 1.07070\n",
      "update best: 0.22842\n",
      "Epoch: 12, Time: 0.01808s, Loss: 1.06910\n",
      "update best: 0.23220\n",
      "Epoch: 13, Time: 0.01767s, Loss: 1.06739\n",
      "update best: 0.23756\n",
      "Epoch: 14, Time: 0.01468s, Loss: 1.06867\n",
      "update best: 0.24228\n",
      "Epoch: 15, Time: 0.02318s, Loss: 1.06965\n",
      "update best: 0.24890\n",
      "Epoch: 16, Time: 0.01846s, Loss: 1.06565\n",
      "update best: 0.25488\n",
      "Epoch: 17, Time: 0.02093s, Loss: 1.06624\n",
      "update best: 0.26024\n",
      "Epoch: 18, Time: 0.01909s, Loss: 1.06569\n",
      "update best: 0.26591\n",
      "Epoch: 19, Time: 0.01976s, Loss: 1.06635\n",
      "update best: 0.26938\n",
      "Epoch: 20, Time: 0.02251s, Loss: 1.06535\n",
      "update best: 0.27347\n",
      "Epoch: 21, Time: 0.01153s, Loss: 1.06856\n",
      "update best: 0.27788\n",
      "Epoch: 22, Time: 0.01665s, Loss: 1.07035\n",
      "update best: 0.28135\n",
      "Epoch: 23, Time: 0.02305s, Loss: 1.06538\n",
      "update best: 0.28670\n",
      "Epoch: 24, Time: 0.02006s, Loss: 1.06527\n",
      "update best: 0.28859\n",
      "Epoch: 25, Time: 0.02235s, Loss: 1.06552\n",
      "update best: 0.29049\n",
      "Epoch: 26, Time: 0.01630s, Loss: 1.06530\n",
      "update best: 0.29080\n",
      "Epoch: 27, Time: 0.02152s, Loss: 1.06492\n",
      "update best: 0.29112\n",
      "Epoch: 28, Time: 0.01894s, Loss: 1.06538\n",
      "update best: 0.29301\n",
      "Epoch: 29, Time: 0.02661s, Loss: 1.06589\n",
      "update best: 0.29553\n",
      "Epoch: 30, Time: 0.02075s, Loss: 1.06627\n",
      "Epoch: 31, Time: 0.02027s, Loss: 1.06568\n",
      "Epoch: 32, Time: 0.01624s, Loss: 1.06576\n",
      "update best: 0.29584\n",
      "Epoch: 33, Time: 0.01920s, Loss: 1.06508\n",
      "update best: 0.29647\n",
      "Epoch: 34, Time: 0.01991s, Loss: 1.06536\n",
      "update best: 0.29679\n",
      "Epoch: 35, Time: 0.01840s, Loss: 1.06501\n",
      "update best: 0.29710\n",
      "Epoch: 36, Time: 0.01724s, Loss: 1.06524\n",
      "Epoch: 37, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 38, Time: 0.02036s, Loss: 1.06499\n",
      "Epoch: 39, Time: 0.01830s, Loss: 1.06507\n",
      "Epoch: 40, Time: 0.01822s, Loss: 1.06540\n",
      "update best: 0.29805\n",
      "Epoch: 41, Time: 0.01940s, Loss: 1.06509\n",
      "Epoch: 42, Time: 0.01928s, Loss: 1.06522\n",
      "update best: 0.29836\n",
      "Epoch: 43, Time: 0.01889s, Loss: 1.06523\n",
      "Epoch: 44, Time: 0.02267s, Loss: 1.06530\n",
      "update best: 0.29931\n",
      "Epoch: 45, Time: 0.02758s, Loss: 1.06553\n",
      "Epoch: 46, Time: 0.02273s, Loss: 1.06594\n",
      "update best: 0.30183\n",
      "Epoch: 47, Time: 0.02789s, Loss: 1.06519\n",
      "update best: 0.30372\n",
      "Epoch: 48, Time: 0.01263s, Loss: 1.06524\n",
      "update best: 0.30466\n",
      "Epoch: 49, Time: 0.01633s, Loss: 1.06493\n",
      "update best: 0.30592\n",
      "Epoch: 50, Time: 0.02007s, Loss: 1.06548\n",
      "Epoch: 51, Time: 0.01813s, Loss: 1.06498\n",
      "Epoch: 52, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.01938s, Loss: 1.06489\n",
      "update best: 0.30718\n",
      "Epoch: 54, Time: 0.01763s, Loss: 1.06500\n",
      "update best: 0.30876\n",
      "Epoch: 55, Time: 0.01670s, Loss: 1.06570\n",
      "update best: 0.31128\n",
      "Epoch: 56, Time: 0.01785s, Loss: 1.06502\n",
      "Epoch: 57, Time: 0.01928s, Loss: 1.06531\n",
      "Epoch: 58, Time: 0.02257s, Loss: 1.06599\n",
      "update best: 0.31159\n",
      "Epoch: 59, Time: 0.02586s, Loss: 1.06529\n",
      "Epoch: 60, Time: 0.02166s, Loss: 1.06604\n",
      "Epoch: 61, Time: 0.01866s, Loss: 1.06585\n",
      "Epoch: 62, Time: 0.02232s, Loss: 1.06490\n",
      "Epoch: 63, Time: 0.01852s, Loss: 1.06505\n",
      "Epoch: 64, Time: 0.02313s, Loss: 1.06505\n",
      "Epoch: 65, Time: 0.01801s, Loss: 1.06512\n",
      "Epoch: 66, Time: 0.02064s, Loss: 1.06598\n",
      "Epoch: 67, Time: 0.02058s, Loss: 1.06529\n",
      "Epoch: 68, Time: 0.01511s, Loss: 1.06910\n",
      "update best: 0.31191\n",
      "Epoch: 69, Time: 0.02213s, Loss: 1.06611\n",
      "update best: 0.31317\n",
      "Epoch: 70, Time: 0.02055s, Loss: 1.06589\n",
      "update best: 0.31411\n",
      "Epoch: 71, Time: 0.01592s, Loss: 1.06512\n",
      "Epoch: 72, Time: 0.02295s, Loss: 1.06520\n",
      "Epoch: 73, Time: 0.02415s, Loss: 1.06497\n",
      "Epoch: 74, Time: 0.01714s, Loss: 1.06511\n",
      "Epoch: 75, Time: 0.01442s, Loss: 1.06539\n",
      "Epoch: 76, Time: 0.02115s, Loss: 1.06501\n",
      "Epoch: 77, Time: 0.01995s, Loss: 1.06772\n",
      "Epoch: 78, Time: 0.01706s, Loss: 1.06505\n",
      "Epoch: 79, Time: 0.01946s, Loss: 1.06511\n",
      "Epoch: 80, Time: 0.01968s, Loss: 1.06532\n",
      "Epoch: 81, Time: 0.02007s, Loss: 1.06583\n",
      "Epoch: 82, Time: 0.01847s, Loss: 1.06540\n",
      "Epoch: 83, Time: 0.01677s, Loss: 1.06522\n",
      "Epoch: 84, Time: 0.01916s, Loss: 1.06552\n",
      "Epoch: 85, Time: 0.01701s, Loss: 1.06588\n",
      "Epoch: 86, Time: 0.01758s, Loss: 1.06530\n",
      "Epoch: 87, Time: 0.01681s, Loss: 1.06513\n",
      "Epoch: 88, Time: 0.01884s, Loss: 1.06695\n",
      "Epoch: 89, Time: 0.01809s, Loss: 1.06502\n",
      "Epoch: 90, Time: 0.01902s, Loss: 1.06542\n",
      "Epoch: 91, Time: 0.01726s, Loss: 1.06501\n",
      "Epoch: 92, Time: 0.01917s, Loss: 1.07111\n",
      "Epoch: 93, Time: 0.02157s, Loss: 1.06501\n",
      "Epoch: 94, Time: 0.01932s, Loss: 1.06579\n",
      "Epoch: 95, Time: 0.01742s, Loss: 1.06544\n",
      "Epoch: 96, Time: 0.01689s, Loss: 1.06592\n",
      "Epoch: 97, Time: 0.02184s, Loss: 1.06546\n",
      "Epoch: 98, Time: 0.01851s, Loss: 1.06559\n",
      "Epoch: 99, Time: 0.01974s, Loss: 1.06655\n",
      "Epoch: 100, Time: 0.02153s, Loss: 1.06613\n",
      "Epoch: 101, Time: 0.02100s, Loss: 1.06529\n",
      "Epoch: 102, Time: 0.02183s, Loss: 1.06513\n",
      "update best: 0.31537\n",
      "Epoch: 103, Time: 0.01861s, Loss: 1.06602\n",
      "update best: 0.31664\n",
      "Epoch: 104, Time: 0.02366s, Loss: 1.06506\n",
      "Epoch: 105, Time: 0.02303s, Loss: 1.06543\n",
      "Epoch: 106, Time: 0.01787s, Loss: 1.06549\n",
      "Epoch: 107, Time: 0.02056s, Loss: 1.06499\n",
      "Epoch: 108, Time: 0.02198s, Loss: 1.06506\n",
      "Epoch: 109, Time: 0.02249s, Loss: 1.06522\n",
      "Epoch: 110, Time: 0.02321s, Loss: 1.06508\n",
      "Epoch: 111, Time: 0.02333s, Loss: 1.06503\n",
      "Epoch: 112, Time: 0.02006s, Loss: 1.06550\n",
      "Epoch: 113, Time: 0.01862s, Loss: 1.06498\n",
      "Epoch: 114, Time: 0.01844s, Loss: 1.06520\n",
      "Epoch: 115, Time: 0.01950s, Loss: 1.06557\n",
      "Epoch: 116, Time: 0.01550s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.01858s, Loss: 1.06584\n",
      "Epoch: 118, Time: 0.01633s, Loss: 1.06530\n",
      "Epoch: 119, Time: 0.02066s, Loss: 1.06533\n",
      "Epoch: 120, Time: 0.01966s, Loss: 1.06518\n",
      "Epoch: 121, Time: 0.01675s, Loss: 1.06587\n",
      "Epoch: 122, Time: 0.02211s, Loss: 1.06538\n",
      "Epoch: 123, Time: 0.01883s, Loss: 1.06525\n",
      "Epoch: 124, Time: 0.01462s, Loss: 1.06792\n",
      "Epoch: 125, Time: 0.01905s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.02171s, Loss: 1.06519\n",
      "Epoch: 127, Time: 0.01829s, Loss: 1.06529\n",
      "Epoch: 128, Time: 0.01958s, Loss: 1.06525\n",
      "Epoch: 129, Time: 0.01915s, Loss: 1.06577\n",
      "Epoch: 130, Time: 0.02259s, Loss: 1.06504\n",
      "Epoch: 131, Time: 0.01623s, Loss: 1.06562\n",
      "Epoch: 132, Time: 0.01713s, Loss: 1.06536\n",
      "Epoch: 133, Time: 0.01912s, Loss: 1.06614\n",
      "Epoch: 134, Time: 0.01647s, Loss: 1.06519\n",
      "Epoch: 135, Time: 0.01784s, Loss: 1.06531\n",
      "Epoch: 136, Time: 0.01811s, Loss: 1.06508\n",
      "Epoch: 137, Time: 0.01808s, Loss: 1.06545\n",
      "Epoch: 138, Time: 0.01983s, Loss: 1.06556\n",
      "Epoch: 139, Time: 0.01699s, Loss: 1.06502\n",
      "Epoch: 140, Time: 0.01636s, Loss: 1.06519\n",
      "Epoch: 141, Time: 0.01850s, Loss: 1.06617\n",
      "Epoch: 142, Time: 0.01634s, Loss: 1.06517\n",
      "Epoch: 143, Time: 0.01739s, Loss: 1.06538\n",
      "Epoch: 144, Time: 0.01833s, Loss: 1.06525\n",
      "Epoch: 145, Time: 0.01783s, Loss: 1.06514\n",
      "Epoch: 146, Time: 0.01796s, Loss: 1.06501\n",
      "Epoch: 147, Time: 0.01856s, Loss: 1.06551\n",
      "Epoch: 148, Time: 0.02472s, Loss: 1.06531\n",
      "Epoch: 149, Time: 0.01940s, Loss: 1.06562\n",
      "Epoch: 150, Time: 0.01549s, Loss: 1.06684\n",
      "Epoch: 151, Time: 0.01839s, Loss: 1.06553\n",
      "Epoch: 152, Time: 0.02012s, Loss: 1.06618\n",
      "Epoch: 153, Time: 0.02237s, Loss: 1.06527\n",
      "Epoch: 154, Time: 0.01830s, Loss: 1.07220\n",
      "Epoch: 155, Time: 0.02025s, Loss: 1.06542\n",
      "Epoch: 156, Time: 0.02134s, Loss: 1.06580\n",
      "Epoch: 157, Time: 0.01560s, Loss: 1.06539\n",
      "Epoch: 158, Time: 0.01388s, Loss: 1.06653\n",
      "Epoch: 159, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 160, Time: 0.01732s, Loss: 1.06567\n",
      "Epoch: 161, Time: 0.01975s, Loss: 1.06511\n",
      "Epoch: 162, Time: 0.01835s, Loss: 1.06567\n",
      "Epoch: 163, Time: 0.01690s, Loss: 1.06527\n",
      "Epoch: 164, Time: 0.01715s, Loss: 1.06890\n",
      "Epoch: 165, Time: 0.02312s, Loss: 1.06546\n",
      "Epoch: 166, Time: 0.01980s, Loss: 1.06614\n",
      "Epoch: 167, Time: 0.02019s, Loss: 1.06497\n",
      "Epoch: 168, Time: 0.01731s, Loss: 1.06529\n",
      "Epoch: 169, Time: 0.01955s, Loss: 1.06562\n",
      "Epoch: 170, Time: 0.01743s, Loss: 1.06597\n",
      "Epoch: 171, Time: 0.01865s, Loss: 1.06532\n",
      "Epoch: 172, Time: 0.01997s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01628s, Loss: 1.06550\n",
      "Epoch: 174, Time: 0.01769s, Loss: 1.06514\n",
      "Epoch: 175, Time: 0.01837s, Loss: 1.06553\n",
      "Epoch: 176, Time: 0.02044s, Loss: 1.06574\n",
      "Epoch: 177, Time: 0.02251s, Loss: 1.06569\n",
      "Epoch: 178, Time: 0.01868s, Loss: 1.06546\n",
      "Epoch: 179, Time: 0.02551s, Loss: 1.06544\n",
      "Epoch: 180, Time: 0.01607s, Loss: 1.06540\n",
      "Epoch: 181, Time: 0.01928s, Loss: 1.06541\n",
      "Epoch: 182, Time: 0.02239s, Loss: 1.06517\n",
      "Epoch: 183, Time: 0.02016s, Loss: 1.06519\n",
      "Epoch: 184, Time: 0.02079s, Loss: 1.06513\n",
      "Epoch: 185, Time: 0.02155s, Loss: 1.06497\n",
      "Epoch: 186, Time: 0.01768s, Loss: 1.06551\n",
      "Epoch: 187, Time: 0.02082s, Loss: 1.06646\n",
      "Epoch: 188, Time: 0.01630s, Loss: 1.06507\n",
      "Epoch: 189, Time: 0.02140s, Loss: 1.06555\n",
      "Epoch: 190, Time: 0.01823s, Loss: 1.06513\n",
      "Epoch: 191, Time: 0.02309s, Loss: 1.06557\n",
      "Epoch: 192, Time: 0.01628s, Loss: 1.06538\n",
      "Epoch: 193, Time: 0.01901s, Loss: 1.06559\n",
      "Epoch: 194, Time: 0.02195s, Loss: 1.06503\n",
      "Epoch: 195, Time: 0.01769s, Loss: 1.06567\n",
      "Epoch: 196, Time: 0.01739s, Loss: 1.06650\n",
      "Epoch: 197, Time: 0.01997s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01885s, Loss: 1.06514\n",
      "Epoch: 199, Time: 0.01803s, Loss: 1.06532\n",
      "\n",
      "train finished!\n",
      "best val: 0.31664\n",
      "test...\n",
      "final result: epoch: 103\n",
      "{'accuracy': 0.3166351616382599, 'f1_score': 0.24460102376042017, 'f1_score -> average@micro': 0.3166351606805293}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Experiment (No Top-k) ===\n",
      "Using original hypergraph (no Top-k preprocessing)\n",
      "Epoch: 0, Time: 0.06937s, Loss: 1.84667\n",
      "update best: 0.21014\n",
      "Epoch: 1, Time: 0.03478s, Loss: 1.56151\n",
      "Epoch: 2, Time: 0.02969s, Loss: 1.32516\n",
      "update best: 0.21361\n",
      "Epoch: 3, Time: 0.04779s, Loss: 1.21602\n",
      "update best: 0.21960\n",
      "Epoch: 4, Time: 0.02403s, Loss: 1.15841\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.01888s, Loss: 1.12637\n",
      "Epoch: 6, Time: 0.01813s, Loss: 1.11725\n",
      "Epoch: 7, Time: 0.01915s, Loss: 1.10174\n",
      "Epoch: 8, Time: 0.01913s, Loss: 1.10152\n",
      "Epoch: 9, Time: 0.01982s, Loss: 1.08360\n",
      "update best: 0.22054\n",
      "Epoch: 10, Time: 0.01888s, Loss: 1.07704\n",
      "update best: 0.22149\n",
      "Epoch: 11, Time: 0.01946s, Loss: 1.07767\n",
      "Epoch: 12, Time: 0.01809s, Loss: 1.07627\n",
      "update best: 0.22180\n",
      "Epoch: 13, Time: 0.01632s, Loss: 1.07444\n",
      "update best: 0.22243\n",
      "Epoch: 14, Time: 0.02069s, Loss: 1.07337\n",
      "update best: 0.22369\n",
      "Epoch: 15, Time: 0.01838s, Loss: 1.07144\n",
      "update best: 0.22464\n",
      "Epoch: 16, Time: 0.02056s, Loss: 1.06839\n",
      "update best: 0.22527\n",
      "Epoch: 17, Time: 0.01934s, Loss: 1.07026\n",
      "update best: 0.22590\n",
      "Epoch: 18, Time: 0.01997s, Loss: 1.06749\n",
      "update best: 0.22810\n",
      "Epoch: 19, Time: 0.02518s, Loss: 1.06799\n",
      "update best: 0.22873\n",
      "Epoch: 20, Time: 0.02006s, Loss: 1.06585\n",
      "update best: 0.23031\n",
      "Epoch: 21, Time: 0.03864s, Loss: 1.06540\n",
      "update best: 0.23094\n",
      "Epoch: 22, Time: 0.02472s, Loss: 1.06563\n",
      "update best: 0.23283\n",
      "Epoch: 23, Time: 0.01746s, Loss: 1.06588\n",
      "update best: 0.23440\n",
      "Epoch: 24, Time: 0.01816s, Loss: 1.06663\n",
      "update best: 0.23503\n",
      "Epoch: 25, Time: 0.01646s, Loss: 1.06579\n",
      "update best: 0.23661\n",
      "Epoch: 26, Time: 0.01418s, Loss: 1.06555\n",
      "update best: 0.23850\n",
      "Epoch: 27, Time: 0.01940s, Loss: 1.06543\n",
      "update best: 0.23913\n",
      "Epoch: 28, Time: 0.01814s, Loss: 1.06576\n",
      "update best: 0.24228\n",
      "Epoch: 29, Time: 0.01654s, Loss: 1.06630\n",
      "update best: 0.24354\n",
      "Epoch: 30, Time: 0.02108s, Loss: 1.06971\n",
      "update best: 0.24480\n",
      "Epoch: 31, Time: 0.02196s, Loss: 1.06689\n",
      "update best: 0.24606\n",
      "Epoch: 32, Time: 0.02126s, Loss: 1.06523\n",
      "update best: 0.24732\n",
      "Epoch: 33, Time: 0.02229s, Loss: 1.06573\n",
      "update best: 0.24921\n",
      "Epoch: 34, Time: 0.02110s, Loss: 1.06611\n",
      "update best: 0.25205\n",
      "Epoch: 35, Time: 0.01491s, Loss: 1.06583\n",
      "update best: 0.25268\n",
      "Epoch: 36, Time: 0.02071s, Loss: 1.06536\n",
      "update best: 0.25299\n",
      "Epoch: 37, Time: 0.02373s, Loss: 1.06547\n",
      "update best: 0.25520\n",
      "Epoch: 38, Time: 0.01887s, Loss: 1.06559\n",
      "update best: 0.25772\n",
      "Epoch: 39, Time: 0.01726s, Loss: 1.06651\n",
      "update best: 0.26024\n",
      "Epoch: 40, Time: 0.01665s, Loss: 1.06545\n",
      "update best: 0.26213\n",
      "Epoch: 41, Time: 0.01568s, Loss: 1.06932\n",
      "update best: 0.26686\n",
      "Epoch: 42, Time: 0.01882s, Loss: 1.06518\n",
      "update best: 0.27284\n",
      "Epoch: 43, Time: 0.01847s, Loss: 1.06632\n",
      "update best: 0.27694\n",
      "Epoch: 44, Time: 0.02051s, Loss: 1.06535\n",
      "update best: 0.28261\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06487\n",
      "update best: 0.28922\n",
      "Epoch: 46, Time: 0.01278s, Loss: 1.06737\n",
      "update best: 0.29395\n",
      "Epoch: 47, Time: 0.02161s, Loss: 1.06573\n",
      "update best: 0.30025\n",
      "Epoch: 48, Time: 0.01658s, Loss: 1.06771\n",
      "update best: 0.30529\n",
      "Epoch: 49, Time: 0.02203s, Loss: 1.06531\n",
      "update best: 0.30844\n",
      "Epoch: 50, Time: 0.02295s, Loss: 1.06524\n",
      "Epoch: 51, Time: 0.01611s, Loss: 1.06735\n",
      "Epoch: 52, Time: 0.01487s, Loss: 1.06611\n",
      "Epoch: 53, Time: 0.01798s, Loss: 1.06505\n",
      "Epoch: 54, Time: 0.01751s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01700s, Loss: 1.06523\n",
      "Epoch: 56, Time: 0.01782s, Loss: 1.06524\n",
      "update best: 0.30907\n",
      "Epoch: 57, Time: 0.01779s, Loss: 1.06639\n",
      "Epoch: 58, Time: 0.01718s, Loss: 1.06497\n",
      "Epoch: 59, Time: 0.01743s, Loss: 1.06528\n",
      "Epoch: 60, Time: 0.01958s, Loss: 1.06625\n",
      "Epoch: 61, Time: 0.02069s, Loss: 1.06578\n",
      "Epoch: 62, Time: 0.02205s, Loss: 1.06531\n",
      "Epoch: 63, Time: 0.01974s, Loss: 1.06577\n",
      "Epoch: 64, Time: 0.01765s, Loss: 1.06954\n",
      "Epoch: 65, Time: 0.01933s, Loss: 1.06588\n",
      "Epoch: 66, Time: 0.02240s, Loss: 1.06586\n",
      "Epoch: 67, Time: 0.01643s, Loss: 1.06498\n",
      "Epoch: 68, Time: 0.02143s, Loss: 1.06496\n",
      "Epoch: 69, Time: 0.01870s, Loss: 1.06564\n",
      "Epoch: 70, Time: 0.01784s, Loss: 1.06518\n",
      "Epoch: 71, Time: 0.01618s, Loss: 1.06531\n",
      "Epoch: 72, Time: 0.01860s, Loss: 1.06525\n",
      "Epoch: 73, Time: 0.01958s, Loss: 1.06627\n",
      "Epoch: 74, Time: 0.01948s, Loss: 1.06626\n",
      "Epoch: 75, Time: 0.01534s, Loss: 1.06529\n",
      "Epoch: 76, Time: 0.02089s, Loss: 1.06523\n",
      "Epoch: 77, Time: 0.01645s, Loss: 1.06529\n",
      "Epoch: 78, Time: 0.01748s, Loss: 1.06489\n",
      "Epoch: 79, Time: 0.01631s, Loss: 1.06540\n",
      "Epoch: 80, Time: 0.01887s, Loss: 1.06661\n",
      "Epoch: 81, Time: 0.01971s, Loss: 1.06601\n",
      "Epoch: 82, Time: 0.01765s, Loss: 1.06509\n",
      "Epoch: 83, Time: 0.01417s, Loss: 1.06528\n",
      "Epoch: 84, Time: 0.02164s, Loss: 1.06500\n",
      "Epoch: 85, Time: 0.01546s, Loss: 1.06513\n",
      "Epoch: 86, Time: 0.01848s, Loss: 1.06620\n",
      "Epoch: 87, Time: 0.01827s, Loss: 1.06537\n",
      "Epoch: 88, Time: 0.02056s, Loss: 1.06561\n",
      "Epoch: 89, Time: 0.01637s, Loss: 1.06596\n",
      "Epoch: 90, Time: 0.01818s, Loss: 1.06500\n",
      "Epoch: 91, Time: 0.01835s, Loss: 1.06544\n",
      "Epoch: 92, Time: 0.01780s, Loss: 1.06622\n",
      "Epoch: 93, Time: 0.01545s, Loss: 1.06540\n",
      "Epoch: 94, Time: 0.01683s, Loss: 1.06533\n",
      "Epoch: 95, Time: 0.00961s, Loss: 1.06524\n",
      "Epoch: 96, Time: 0.01486s, Loss: 1.06558\n",
      "Epoch: 97, Time: 0.01445s, Loss: 1.06523\n",
      "Epoch: 98, Time: 0.01343s, Loss: 1.06561\n",
      "Epoch: 99, Time: 0.01764s, Loss: 1.06520\n",
      "Epoch: 100, Time: 0.01711s, Loss: 1.06568\n",
      "Epoch: 101, Time: 0.01642s, Loss: 1.06556\n",
      "Epoch: 102, Time: 0.01880s, Loss: 1.06547\n",
      "Epoch: 103, Time: 0.02141s, Loss: 1.06525\n",
      "Epoch: 104, Time: 0.02041s, Loss: 1.06551\n",
      "Epoch: 105, Time: 0.01838s, Loss: 1.06518\n",
      "Epoch: 106, Time: 0.02069s, Loss: 1.06562\n",
      "Epoch: 107, Time: 0.01834s, Loss: 1.06548\n",
      "Epoch: 108, Time: 0.01653s, Loss: 1.06662\n",
      "Epoch: 109, Time: 0.01662s, Loss: 1.06501\n",
      "Epoch: 110, Time: 0.01663s, Loss: 1.06511\n",
      "Epoch: 111, Time: 0.01668s, Loss: 1.06533\n",
      "Epoch: 112, Time: 0.01661s, Loss: 1.06556\n",
      "Epoch: 113, Time: 0.01515s, Loss: 1.06608\n",
      "Epoch: 114, Time: 0.01924s, Loss: 1.06579\n",
      "Epoch: 115, Time: 0.01623s, Loss: 1.06643\n",
      "Epoch: 116, Time: 0.01687s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.02302s, Loss: 1.06563\n",
      "Epoch: 118, Time: 0.02006s, Loss: 1.06565\n",
      "Epoch: 119, Time: 0.01810s, Loss: 1.06541\n",
      "Epoch: 120, Time: 0.01875s, Loss: 1.06568\n",
      "Epoch: 121, Time: 0.01556s, Loss: 1.06552\n",
      "Epoch: 122, Time: 0.01580s, Loss: 1.06582\n",
      "Epoch: 123, Time: 0.01397s, Loss: 1.06587\n",
      "Epoch: 124, Time: 0.01464s, Loss: 1.06540\n",
      "Epoch: 125, Time: 0.01721s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.01737s, Loss: 1.06720\n",
      "Epoch: 127, Time: 0.01820s, Loss: 1.06625\n",
      "Epoch: 128, Time: 0.01723s, Loss: 1.06561\n",
      "Epoch: 129, Time: 0.01836s, Loss: 1.06507\n",
      "Epoch: 130, Time: 0.02318s, Loss: 1.06653\n",
      "Epoch: 131, Time: 0.01722s, Loss: 1.06598\n",
      "Epoch: 132, Time: 0.01641s, Loss: 1.06506\n",
      "Epoch: 133, Time: 0.01652s, Loss: 1.06573\n",
      "Epoch: 134, Time: 0.02001s, Loss: 1.07398\n",
      "Epoch: 135, Time: 0.02194s, Loss: 1.06520\n",
      "Epoch: 136, Time: 0.01737s, Loss: 1.06595\n",
      "Epoch: 137, Time: 0.01596s, Loss: 1.06582\n",
      "Epoch: 138, Time: 0.01830s, Loss: 1.06600\n",
      "Epoch: 139, Time: 0.01419s, Loss: 1.06568\n",
      "Epoch: 140, Time: 0.01439s, Loss: 1.06647\n",
      "Epoch: 141, Time: 0.01596s, Loss: 1.06581\n",
      "Epoch: 142, Time: 0.01908s, Loss: 1.06571\n",
      "Epoch: 143, Time: 0.01774s, Loss: 1.06544\n",
      "Epoch: 144, Time: 0.01544s, Loss: 1.06689\n",
      "Epoch: 145, Time: 0.01647s, Loss: 1.06594\n",
      "Epoch: 146, Time: 0.01264s, Loss: 1.06531\n",
      "Epoch: 147, Time: 0.01781s, Loss: 1.06523\n",
      "Epoch: 148, Time: 0.01835s, Loss: 1.06504\n",
      "Epoch: 149, Time: 0.01573s, Loss: 1.06545\n",
      "Epoch: 150, Time: 0.02005s, Loss: 1.06600\n",
      "Epoch: 151, Time: 0.01521s, Loss: 1.06673\n",
      "Epoch: 152, Time: 0.01710s, Loss: 1.06540\n",
      "Epoch: 153, Time: 0.01482s, Loss: 1.06659\n",
      "Epoch: 154, Time: 0.01641s, Loss: 1.06537\n",
      "Epoch: 155, Time: 0.01722s, Loss: 1.06526\n",
      "Epoch: 156, Time: 0.01513s, Loss: 1.06598\n",
      "Epoch: 157, Time: 0.01639s, Loss: 1.06505\n",
      "Epoch: 158, Time: 0.01848s, Loss: 1.06601\n",
      "Epoch: 159, Time: 0.01890s, Loss: 1.07954\n",
      "Epoch: 160, Time: 0.01546s, Loss: 1.06505\n",
      "Epoch: 161, Time: 0.01483s, Loss: 1.06558\n",
      "Epoch: 162, Time: 0.01780s, Loss: 1.06533\n",
      "Epoch: 163, Time: 0.01427s, Loss: 1.06565\n",
      "Epoch: 164, Time: 0.01694s, Loss: 1.06739\n",
      "Epoch: 165, Time: 0.01820s, Loss: 1.06558\n",
      "Epoch: 166, Time: 0.01369s, Loss: 1.06535\n",
      "update best: 0.31065\n",
      "Epoch: 167, Time: 0.01588s, Loss: 1.06600\n",
      "update best: 0.31222\n",
      "Epoch: 168, Time: 0.01453s, Loss: 1.06576\n",
      "update best: 0.31411\n",
      "Epoch: 169, Time: 0.01234s, Loss: 1.06575\n",
      "Epoch: 170, Time: 0.01634s, Loss: 1.06672\n",
      "Epoch: 171, Time: 0.01523s, Loss: 1.07067\n",
      "Epoch: 172, Time: 0.01926s, Loss: 1.06599\n",
      "Epoch: 173, Time: 0.01206s, Loss: 1.06577\n",
      "Epoch: 174, Time: 0.01843s, Loss: 1.06545\n",
      "Epoch: 175, Time: 0.01554s, Loss: 1.06565\n",
      "Epoch: 176, Time: 0.01660s, Loss: 1.06719\n",
      "Epoch: 177, Time: 0.01723s, Loss: 1.06799\n",
      "Epoch: 178, Time: 0.01610s, Loss: 1.06552\n",
      "Epoch: 179, Time: 0.01629s, Loss: 1.06543\n",
      "Epoch: 180, Time: 0.01191s, Loss: 1.06553\n",
      "Epoch: 181, Time: 0.01449s, Loss: 1.06599\n",
      "Epoch: 182, Time: 0.01931s, Loss: 1.06560\n",
      "Epoch: 183, Time: 0.01864s, Loss: 1.06541\n",
      "Epoch: 184, Time: 0.01652s, Loss: 1.06559\n",
      "Epoch: 185, Time: 0.02020s, Loss: 1.06522\n",
      "Epoch: 186, Time: 0.01820s, Loss: 1.06582\n",
      "Epoch: 187, Time: 0.01541s, Loss: 1.06633\n",
      "Epoch: 188, Time: 0.01390s, Loss: 1.06574\n",
      "Epoch: 189, Time: 0.01462s, Loss: 1.06550\n",
      "Epoch: 190, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 191, Time: 0.01547s, Loss: 1.06542\n",
      "Epoch: 192, Time: 0.01745s, Loss: 1.06509\n",
      "Epoch: 193, Time: 0.01468s, Loss: 1.06807\n",
      "Epoch: 194, Time: 0.01670s, Loss: 1.06519\n",
      "Epoch: 195, Time: 0.02036s, Loss: 1.06596\n",
      "Epoch: 196, Time: 0.01871s, Loss: 1.06728\n",
      "Epoch: 197, Time: 0.01775s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01911s, Loss: 1.06590\n",
      "Epoch: 199, Time: 0.01631s, Loss: 1.06588\n",
      "\n",
      "train finished!\n",
      "best val: 0.31411\n",
      "test...\n",
      "final result: epoch: 168\n",
      "{'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "\n",
      "=== Experiment with Top-k Densest Subgraphs ===\n",
      "Using Top-3 Densest Subgraphs preprocessing\n",
      "Epoch: 0, Time: 0.06559s, Loss: 1.82510\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02902s, Loss: 1.62440\n",
      "Epoch: 2, Time: 0.02847s, Loss: 1.39076\n",
      "Epoch: 3, Time: 0.02935s, Loss: 1.26225\n",
      "Epoch: 4, Time: 0.03275s, Loss: 1.21692\n",
      "Epoch: 5, Time: 0.03256s, Loss: 1.19965\n",
      "Epoch: 6, Time: 0.03542s, Loss: 1.19788\n",
      "Epoch: 7, Time: 0.02822s, Loss: 1.16639\n",
      "Epoch: 8, Time: 0.02576s, Loss: 1.16581\n",
      "Epoch: 9, Time: 0.02931s, Loss: 1.16196\n",
      "Epoch: 10, Time: 0.02472s, Loss: 1.16319\n",
      "Epoch: 11, Time: 0.03068s, Loss: 1.15742\n",
      "Epoch: 12, Time: 0.02637s, Loss: 1.15874\n",
      "Epoch: 13, Time: 0.02626s, Loss: 1.15653\n",
      "Epoch: 14, Time: 0.02875s, Loss: 1.15744\n",
      "Epoch: 15, Time: 0.03449s, Loss: 1.15601\n",
      "Epoch: 16, Time: 0.03268s, Loss: 1.15622\n",
      "Epoch: 17, Time: 0.03114s, Loss: 1.15699\n",
      "Epoch: 18, Time: 0.02857s, Loss: 1.15729\n",
      "Epoch: 19, Time: 0.02153s, Loss: 1.15600\n",
      "Epoch: 20, Time: 0.03093s, Loss: 1.15623\n",
      "Epoch: 21, Time: 0.03050s, Loss: 1.15606\n",
      "Epoch: 22, Time: 0.02683s, Loss: 1.15718\n",
      "update best: 0.21456\n",
      "Epoch: 23, Time: 0.03116s, Loss: 1.15620\n",
      "update best: 0.21550\n",
      "Epoch: 24, Time: 0.02556s, Loss: 1.15575\n",
      "update best: 0.21834\n",
      "Epoch: 25, Time: 0.03636s, Loss: 1.15600\n",
      "update best: 0.21960\n",
      "Epoch: 26, Time: 0.03473s, Loss: 1.15822\n",
      "update best: 0.21991\n",
      "Epoch: 27, Time: 0.02267s, Loss: 1.15585\n",
      "update best: 0.22117\n",
      "Epoch: 28, Time: 0.02434s, Loss: 1.15610\n",
      "update best: 0.22369\n",
      "Epoch: 29, Time: 0.03203s, Loss: 1.15606\n",
      "update best: 0.22464\n",
      "Epoch: 30, Time: 0.03443s, Loss: 1.15708\n",
      "update best: 0.22716\n",
      "Epoch: 31, Time: 0.02783s, Loss: 1.15568\n",
      "Epoch: 32, Time: 0.02653s, Loss: 1.15782\n",
      "update best: 0.22779\n",
      "Epoch: 33, Time: 0.02463s, Loss: 1.15597\n",
      "update best: 0.22999\n",
      "Epoch: 34, Time: 0.03037s, Loss: 1.15607\n",
      "Epoch: 35, Time: 0.02218s, Loss: 1.16056\n",
      "update best: 0.23125\n",
      "Epoch: 36, Time: 0.02994s, Loss: 1.15738\n",
      "update best: 0.23251\n",
      "Epoch: 37, Time: 0.03148s, Loss: 1.15582\n",
      "update best: 0.23566\n",
      "Epoch: 38, Time: 0.02190s, Loss: 1.15774\n",
      "update best: 0.23693\n",
      "Epoch: 39, Time: 0.03193s, Loss: 1.15581\n",
      "update best: 0.23787\n",
      "Epoch: 40, Time: 0.03539s, Loss: 1.15770\n",
      "update best: 0.23913\n",
      "Epoch: 41, Time: 0.03075s, Loss: 1.15707\n",
      "update best: 0.24008\n",
      "Epoch: 42, Time: 0.02908s, Loss: 1.15580\n",
      "update best: 0.24134\n",
      "Epoch: 43, Time: 0.02855s, Loss: 1.16010\n",
      "update best: 0.24417\n",
      "Epoch: 44, Time: 0.02485s, Loss: 1.15595\n",
      "update best: 0.24480\n",
      "Epoch: 45, Time: 0.02687s, Loss: 1.15583\n",
      "update best: 0.24606\n",
      "Epoch: 46, Time: 0.02626s, Loss: 1.15602\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.02447s, Loss: 1.15593\n",
      "update best: 0.24764\n",
      "Epoch: 48, Time: 0.02797s, Loss: 1.15584\n",
      "Epoch: 49, Time: 0.02457s, Loss: 1.15605\n",
      "update best: 0.24858\n",
      "Epoch: 50, Time: 0.03008s, Loss: 1.15597\n",
      "update best: 0.24890\n",
      "Epoch: 51, Time: 0.02840s, Loss: 1.15596\n",
      "update best: 0.24921\n",
      "Epoch: 52, Time: 0.02966s, Loss: 1.15576\n",
      "update best: 0.25047\n",
      "Epoch: 53, Time: 0.02614s, Loss: 1.15571\n",
      "Epoch: 54, Time: 0.02849s, Loss: 1.15593\n",
      "Epoch: 55, Time: 0.03094s, Loss: 1.15567\n",
      "Epoch: 56, Time: 0.02563s, Loss: 1.15609\n",
      "update best: 0.25110\n",
      "Epoch: 57, Time: 0.02449s, Loss: 1.15612\n",
      "update best: 0.25236\n",
      "Epoch: 58, Time: 0.02537s, Loss: 1.15575\n",
      "update best: 0.25299\n",
      "Epoch: 59, Time: 0.02872s, Loss: 1.15582\n",
      "update best: 0.25425\n",
      "Epoch: 60, Time: 0.03122s, Loss: 1.15583\n",
      "update best: 0.25457\n",
      "Epoch: 61, Time: 0.03164s, Loss: 1.15581\n",
      "Epoch: 62, Time: 0.03158s, Loss: 1.15587\n",
      "Epoch: 63, Time: 0.02415s, Loss: 1.15588\n",
      "update best: 0.25520\n",
      "Epoch: 64, Time: 0.03063s, Loss: 1.15595\n",
      "update best: 0.25551\n",
      "Epoch: 65, Time: 0.02690s, Loss: 1.15598\n",
      "Epoch: 66, Time: 0.02762s, Loss: 1.15590\n",
      "Epoch: 67, Time: 0.02945s, Loss: 1.15575\n",
      "update best: 0.25677\n",
      "Epoch: 68, Time: 0.03464s, Loss: 1.15585\n",
      "update best: 0.25803\n",
      "Epoch: 69, Time: 0.03152s, Loss: 1.15595\n",
      "update best: 0.25835\n",
      "Epoch: 70, Time: 0.03722s, Loss: 1.15586\n",
      "update best: 0.25866\n",
      "Epoch: 71, Time: 0.02826s, Loss: 1.15621\n",
      "Epoch: 72, Time: 0.03272s, Loss: 1.15589\n",
      "Epoch: 73, Time: 0.03118s, Loss: 1.15693\n",
      "Epoch: 74, Time: 0.02265s, Loss: 1.15596\n",
      "update best: 0.25961\n",
      "Epoch: 75, Time: 0.03024s, Loss: 1.15583\n",
      "Epoch: 76, Time: 0.02835s, Loss: 1.15777\n",
      "Epoch: 77, Time: 0.03109s, Loss: 1.15602\n",
      "Epoch: 78, Time: 0.02700s, Loss: 1.15594\n",
      "Epoch: 79, Time: 0.03067s, Loss: 1.15586\n",
      "update best: 0.25992\n",
      "Epoch: 80, Time: 0.02818s, Loss: 1.15567\n",
      "Epoch: 81, Time: 0.03168s, Loss: 1.15586\n",
      "Epoch: 82, Time: 0.03077s, Loss: 1.15606\n",
      "update best: 0.26024\n",
      "Epoch: 83, Time: 0.02648s, Loss: 1.15578\n",
      "update best: 0.26055\n",
      "Epoch: 84, Time: 0.03257s, Loss: 1.15599\n",
      "update best: 0.26087\n",
      "Epoch: 85, Time: 0.03028s, Loss: 1.15595\n",
      "Epoch: 86, Time: 0.03238s, Loss: 1.15612\n",
      "Epoch: 87, Time: 0.02833s, Loss: 1.15617\n",
      "Epoch: 88, Time: 0.02255s, Loss: 1.15612\n",
      "Epoch: 89, Time: 0.03143s, Loss: 1.15581\n",
      "Epoch: 90, Time: 0.03042s, Loss: 1.15598\n",
      "update best: 0.26150\n",
      "Epoch: 91, Time: 0.03111s, Loss: 1.15649\n",
      "Epoch: 92, Time: 0.02915s, Loss: 1.15632\n",
      "Epoch: 93, Time: 0.03287s, Loss: 1.15573\n",
      "update best: 0.26181\n",
      "Epoch: 94, Time: 0.03230s, Loss: 1.15726\n",
      "update best: 0.26213\n",
      "Epoch: 95, Time: 0.02853s, Loss: 1.15721\n",
      "Epoch: 96, Time: 0.03244s, Loss: 1.15582\n",
      "Epoch: 97, Time: 0.03126s, Loss: 1.15621\n",
      "update best: 0.26244\n",
      "Epoch: 98, Time: 0.02367s, Loss: 1.15600\n",
      "update best: 0.26307\n",
      "Epoch: 99, Time: 0.03417s, Loss: 1.15601\n",
      "Epoch: 100, Time: 0.02501s, Loss: 1.15614\n",
      "Epoch: 101, Time: 0.03208s, Loss: 1.15612\n",
      "Epoch: 102, Time: 0.02693s, Loss: 1.15597\n",
      "Epoch: 103, Time: 0.02612s, Loss: 1.15600\n",
      "Epoch: 104, Time: 0.02501s, Loss: 1.15746\n",
      "update best: 0.26402\n",
      "Epoch: 105, Time: 0.02798s, Loss: 1.15580\n",
      "update best: 0.26560\n",
      "Epoch: 106, Time: 0.02560s, Loss: 1.15584\n",
      "update best: 0.26654\n",
      "Epoch: 107, Time: 0.02655s, Loss: 1.15595\n",
      "update best: 0.26717\n",
      "Epoch: 108, Time: 0.03282s, Loss: 1.15584\n",
      "Epoch: 109, Time: 0.02493s, Loss: 1.15637\n",
      "update best: 0.26875\n",
      "Epoch: 110, Time: 0.02819s, Loss: 1.15628\n",
      "Epoch: 111, Time: 0.03212s, Loss: 1.15633\n",
      "Epoch: 112, Time: 0.02784s, Loss: 1.15634\n",
      "update best: 0.26969\n",
      "Epoch: 113, Time: 0.03127s, Loss: 1.15574\n",
      "Epoch: 114, Time: 0.02956s, Loss: 1.15815\n",
      "Epoch: 115, Time: 0.03216s, Loss: 1.15664\n",
      "Epoch: 116, Time: 0.02278s, Loss: 1.15611\n",
      "Epoch: 117, Time: 0.03280s, Loss: 1.15593\n",
      "Epoch: 118, Time: 0.02437s, Loss: 1.15600\n",
      "Epoch: 119, Time: 0.02233s, Loss: 1.15648\n",
      "Epoch: 120, Time: 0.03344s, Loss: 1.15758\n",
      "Epoch: 121, Time: 0.03560s, Loss: 1.15583\n",
      "Epoch: 122, Time: 0.03149s, Loss: 1.15573\n",
      "Epoch: 123, Time: 0.03710s, Loss: 1.15598\n",
      "Epoch: 124, Time: 0.02458s, Loss: 1.15600\n",
      "Epoch: 125, Time: 0.02412s, Loss: 1.15620\n",
      "Epoch: 126, Time: 0.03206s, Loss: 1.15640\n",
      "Epoch: 127, Time: 0.02860s, Loss: 1.15602\n",
      "Epoch: 128, Time: 0.02843s, Loss: 1.15700\n",
      "Epoch: 129, Time: 0.02828s, Loss: 1.15622\n",
      "Epoch: 130, Time: 0.02049s, Loss: 1.15669\n",
      "Epoch: 131, Time: 0.02248s, Loss: 1.15658\n",
      "Epoch: 132, Time: 0.02196s, Loss: 1.15610\n",
      "Epoch: 133, Time: 0.02219s, Loss: 1.15620\n",
      "Epoch: 134, Time: 0.02394s, Loss: 1.15636\n",
      "Epoch: 135, Time: 0.03258s, Loss: 1.15674\n",
      "Epoch: 136, Time: 0.02634s, Loss: 1.15605\n",
      "Epoch: 137, Time: 0.03094s, Loss: 1.15939\n",
      "update best: 0.27064\n",
      "Epoch: 138, Time: 0.02247s, Loss: 1.15665\n",
      "update best: 0.27127\n",
      "Epoch: 139, Time: 0.02647s, Loss: 1.15624\n",
      "update best: 0.27190\n",
      "Epoch: 140, Time: 0.02678s, Loss: 1.15860\n",
      "update best: 0.27505\n",
      "Epoch: 141, Time: 0.03109s, Loss: 1.15604\n",
      "Epoch: 142, Time: 0.02169s, Loss: 1.15715\n",
      "Epoch: 143, Time: 0.03042s, Loss: 1.15603\n",
      "Epoch: 144, Time: 0.02342s, Loss: 1.15832\n",
      "Epoch: 145, Time: 0.03094s, Loss: 1.15732\n",
      "Epoch: 146, Time: 0.03019s, Loss: 1.15621\n",
      "Epoch: 147, Time: 0.02492s, Loss: 1.15594\n",
      "Epoch: 148, Time: 0.02848s, Loss: 1.15635\n",
      "Epoch: 149, Time: 0.03001s, Loss: 1.15624\n",
      "Epoch: 150, Time: 0.03320s, Loss: 1.15611\n",
      "Epoch: 151, Time: 0.03006s, Loss: 1.15612\n",
      "Epoch: 152, Time: 0.03538s, Loss: 1.15713\n",
      "Epoch: 153, Time: 0.03224s, Loss: 1.15649\n",
      "Epoch: 154, Time: 0.03126s, Loss: 1.15601\n",
      "Epoch: 155, Time: 0.03383s, Loss: 1.15616\n",
      "Epoch: 156, Time: 0.02578s, Loss: 1.15601\n",
      "Epoch: 157, Time: 0.02708s, Loss: 1.15609\n",
      "Epoch: 158, Time: 0.03211s, Loss: 1.15645\n",
      "Epoch: 159, Time: 0.02848s, Loss: 1.15666\n",
      "Epoch: 160, Time: 0.02643s, Loss: 1.15719\n",
      "Epoch: 161, Time: 0.03038s, Loss: 1.15599\n",
      "Epoch: 162, Time: 0.03179s, Loss: 1.15762\n",
      "Epoch: 163, Time: 0.03095s, Loss: 1.15674\n",
      "Epoch: 164, Time: 0.02292s, Loss: 1.15618\n",
      "Epoch: 165, Time: 0.02855s, Loss: 1.15583\n",
      "Epoch: 166, Time: 0.02436s, Loss: 1.15596\n",
      "Epoch: 167, Time: 0.03066s, Loss: 1.15606\n",
      "Epoch: 168, Time: 0.03028s, Loss: 1.15703\n",
      "Epoch: 169, Time: 0.03328s, Loss: 1.15598\n",
      "Epoch: 170, Time: 0.03524s, Loss: 1.15600\n",
      "Epoch: 171, Time: 0.02893s, Loss: 1.15637\n",
      "Epoch: 172, Time: 0.02216s, Loss: 1.15700\n",
      "Epoch: 173, Time: 0.03289s, Loss: 1.15589\n",
      "Epoch: 174, Time: 0.02688s, Loss: 1.15718\n",
      "Epoch: 175, Time: 0.02044s, Loss: 1.15571\n",
      "Epoch: 176, Time: 0.02914s, Loss: 1.15585\n",
      "Epoch: 177, Time: 0.02653s, Loss: 1.15718\n",
      "Epoch: 178, Time: 0.02829s, Loss: 1.15691\n",
      "Epoch: 179, Time: 0.03039s, Loss: 1.15713\n",
      "Epoch: 180, Time: 0.03574s, Loss: 1.15608\n",
      "Epoch: 181, Time: 0.02998s, Loss: 1.15629\n",
      "Epoch: 182, Time: 0.03262s, Loss: 1.15587\n",
      "Epoch: 183, Time: 0.02428s, Loss: 1.15661\n",
      "Epoch: 184, Time: 0.03045s, Loss: 1.15654\n",
      "Epoch: 185, Time: 0.02416s, Loss: 1.15611\n",
      "Epoch: 186, Time: 0.03054s, Loss: 1.15641\n",
      "Epoch: 187, Time: 0.02843s, Loss: 1.15752\n",
      "Epoch: 188, Time: 0.02668s, Loss: 1.15607\n",
      "Epoch: 189, Time: 0.03046s, Loss: 1.15627\n",
      "Epoch: 190, Time: 0.02530s, Loss: 1.15581\n",
      "Epoch: 191, Time: 0.03288s, Loss: 1.15662\n",
      "Epoch: 192, Time: 0.03075s, Loss: 1.15663\n",
      "Epoch: 193, Time: 0.03123s, Loss: 1.15609\n",
      "Epoch: 194, Time: 0.02127s, Loss: 1.15605\n",
      "Epoch: 195, Time: 0.02872s, Loss: 1.15676\n",
      "Epoch: 196, Time: 0.02444s, Loss: 1.15607\n",
      "Epoch: 197, Time: 0.02503s, Loss: 1.15975\n",
      "Epoch: 198, Time: 0.03029s, Loss: 1.15604\n",
      "Epoch: 199, Time: 0.03272s, Loss: 1.15623\n",
      "\n",
      "train finished!\n",
      "best val: 0.27505\n",
      "test...\n",
      "final result: epoch: 140\n",
      "{'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n",
      "\n",
      "=== Comparison of Results ===\n",
      "Baseline performance: {'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "Top-k performance: {'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import os\n",
    "# from copy import deepcopy\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import dhg\n",
    "# from dhg import Graph, Hypergraph\n",
    "# from dhg.data import Cooking200, News20\n",
    "# from dhg.models import GCN, HGNN, HGNNP, HNHN\n",
    "# from dhg.random import set_seed\n",
    "# from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "# from dhg.utils import split_by_ratio\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def get_top_k_densest_subgraphs(hg: Hypergraph, k: int = 3) -> list:\n",
    "    \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "    nodes = set(range(hg.num_v))\n",
    "    subgraphs = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if len(nodes) < 3:  # min_size\n",
    "            break\n",
    "            \n",
    "        current_nodes = set(nodes)\n",
    "        best_subset = None\n",
    "        best_density = -1\n",
    "        \n",
    "        while len(current_nodes) >= 3:\n",
    "            edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "            density = edge_count / len(current_nodes)\n",
    "            \n",
    "            if density > best_density:\n",
    "                best_density = density\n",
    "                best_subset = set(current_nodes)\n",
    "            \n",
    "            # Remove node with lowest degree\n",
    "            degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "            node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "            current_nodes.remove(node_to_remove)\n",
    "        \n",
    "        if best_subset:\n",
    "            subgraphs.append((best_subset, best_density))\n",
    "            nodes -= best_subset\n",
    "    \n",
    "    return subgraphs\n",
    "\n",
    "def preprocess_hypergraph_with_topk(hg: Hypergraph, k: int = 3) -> Hypergraph:\n",
    "    \"\"\"Preprocess hypergraph by focusing on top-k densest subgraphs\"\"\"\n",
    "    subgraphs = get_top_k_densest_subgraphs(hg, k)\n",
    "    if not subgraphs:\n",
    "        return hg\n",
    "    \n",
    "    # Combine all nodes from top-k subgraphs\n",
    "    important_nodes = set()\n",
    "    for subset, _ in subgraphs:\n",
    "        important_nodes.update(subset)\n",
    "    \n",
    "    # Create new hypergraph with only important edges\n",
    "    new_edges = []\n",
    "    for e in hg.e[0]:\n",
    "        if set(e).issubset(important_nodes):\n",
    "            new_edges.append(e)\n",
    "    \n",
    "    return Hypergraph(hg.num_v, new_edges)\n",
    "\n",
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res\n",
    "\n",
    "def run_experiment(use_topk=False, k=3):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    \n",
    "    if use_topk:\n",
    "        print(f\"Using Top-{k} Densest Subgraphs preprocessing\")\n",
    "        G = preprocess_hypergraph_with_topk(G, k)\n",
    "    else:\n",
    "        print(\"Using original hypergraph (no Top-k preprocessing)\")\n",
    "    \n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    test_mask = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch, best_val = 0, 0\n",
    "    for epoch in range(200):\n",
    "        # train\n",
    "        train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(net, X, G, lbl, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_epoch = epoch\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "# Run experiments\n",
    "print(\"=== Baseline Experiment (No Top-k) ===\")\n",
    "baseline_results = run_experiment(use_topk=False)\n",
    "\n",
    "print(\"\\n=== Experiment with Top-k Densest Subgraphs ===\")\n",
    "topk_results = run_experiment(use_topk=True, k=3)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Comparison of Results ===\")\n",
    "print(f\"Baseline performance: {baseline_results}\")\n",
    "print(f\"Top-k performance: {topk_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.11)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/utkarshx27/movies-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5.13M/5.13M [00:00<00:00, 23.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n",
      "Path to dataset files: C:\\Users\\rustem_izmailov\\.cache\\kagglehub\\datasets\\utkarshx27\\movies-dataset\\versions\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"utkarshx27/movies-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(movie_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4803 entries, 0 to 4802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 4803 non-null   int64  \n",
      " 1   budget                4803 non-null   int64  \n",
      " 2   genres                4775 non-null   object \n",
      " 3   homepage              1712 non-null   object \n",
      " 4   id                    4803 non-null   int64  \n",
      " 5   keywords              4391 non-null   object \n",
      " 6   original_language     4803 non-null   object \n",
      " 7   original_title        4803 non-null   object \n",
      " 8   overview              4800 non-null   object \n",
      " 9   popularity            4803 non-null   float64\n",
      " 10  production_companies  4803 non-null   object \n",
      " 11  production_countries  4803 non-null   object \n",
      " 12  release_date          4802 non-null   object \n",
      " 13  revenue               4803 non-null   int64  \n",
      " 14  runtime               4801 non-null   float64\n",
      " 15  spoken_languages      4803 non-null   object \n",
      " 16  status                4803 non-null   object \n",
      " 17  tagline               3959 non-null   object \n",
      " 18  title                 4803 non-null   object \n",
      " 19  vote_average          4803 non-null   float64\n",
      " 20  vote_count            4803 non-null   int64  \n",
      " 21  cast                  4760 non-null   object \n",
      " 22  crew                  4803 non-null   object \n",
      " 23  director              4773 non-null   object \n",
      "dtypes: float64(3), int64(5), object(16)\n",
      "memory usage: 900.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Action Adventure Fantasy Science Fiction\n",
       "1                    Adventure Fantasy Action\n",
       "2                      Action Adventure Crime\n",
       "3                 Action Crime Drama Thriller\n",
       "4            Action Adventure Science Fiction\n",
       "Name: genres, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genres.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphExperiment:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.results = []\n",
    "        \n",
    "    def run_experiments(self, n_samples=500):\n",
    "        \"\"\"   \"\"\"\n",
    "        # 1.   (  )\n",
    "        print(\"\\n=== Experiment 1: Base Hypergraph ===\")\n",
    "        base_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=False\n",
    "        )\n",
    "        base_results = self._train_and_evaluate(base_data)\n",
    "        self.results.append((\"Base\", base_results))\n",
    "        \n",
    "        # 2.  -k  \n",
    "        print(\"\\n=== Experiment 2: With Densest Subgraphs ===\")\n",
    "        dense_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=True,\n",
    "            k=5\n",
    "        )\n",
    "        dense_results = self._train_and_evaluate(dense_data)\n",
    "        self.results.append((\"With Densest Subgraphs\", dense_results))\n",
    "        \n",
    "        # # 3.   \n",
    "        # print(\"\\n=== Experiment 3: Genre-Only Hypergraph ===\")\n",
    "        # genre_data = MovieHypergraphDataset(\n",
    "        #     data_root=self.data_path,\n",
    "        #     n_samples=n_samples,\n",
    "        #     hyperedge_types=[\"genre\"]\n",
    "        # )\n",
    "        # genre_results = self._train_and_evaluate(genre_data)\n",
    "        # self.results.append((\"Genre-Only\", genre_results))\n",
    "        \n",
    "        #  \n",
    "        self._visualize_results()\n",
    "\n",
    "    def _train_and_evaluate(self, data) -> Dict[str, float]:\n",
    "        \"\"\"    \"\"\"\n",
    "        # Get all the data we need\n",
    "        X = data[\"features\"]\n",
    "        lbl = data[\"labels\"]\n",
    "        edge_list = data[\"edge_list\"]\n",
    "        num_vertices = data[\"num_vertices\"]\n",
    "        train_mask = data[\"train_mask\"]\n",
    "        val_mask = data[\"val_mask\"]\n",
    "        test_mask = data[\"test_mask\"]\n",
    "        num_classes = data[\"num_classes\"]\n",
    "        \n",
    "        # Check if edge_list is empty\n",
    "        if not edge_list:\n",
    "            print(\"Warning: Empty edge list! Creating a fallback edge list based on nearest neighbors.\")\n",
    "            # Create a fallback edge list using k-nearest neighbors\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X.numpy())\n",
    "            _, indices = knn.kneighbors(X.numpy())\n",
    "            edge_list = [list(idx) for idx in indices]\n",
    "        \n",
    "        # Now use the potentially updated edge_list\n",
    "        hg = Hypergraph(num_vertices, edge_list)\n",
    "        masks = {\n",
    "            \"train\": train_mask,\n",
    "            \"val\": val_mask,\n",
    "            \"test\": test_mask\n",
    "        }\n",
    "        \n",
    "        models = {\n",
    "            \"HGNN\": HGNN(X.shape[1], 64, num_classes),\n",
    "            \"HGNNP\": HGNNP(X.shape[1], 64, num_classes, use_bn=True),\n",
    "            \"UniGCN\": UniGCN(X.shape[1], 64, num_classes, use_bn=True),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model = model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            \n",
    "            best_val = 0\n",
    "            for epoch in range(100):\n",
    "                train(model, X, hg, lbl, masks[\"train\"], optimizer, epoch)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    val_res = infer(model, X, hg, lbl, masks[\"val\"])\n",
    "                    # Extract the accuracy value from the dictionary\n",
    "                    val_accuracy = val_res.get('accuracy', 0)  # Assuming accuracy is the key\n",
    "                    if val_accuracy > best_val:\n",
    "                        best_val = val_accuracy\n",
    "            \n",
    "            test_res = infer(model, X, hg, lbl, masks[\"test\"], test=True)\n",
    "            results[name] = {\n",
    "                \"val_accuracy\": best_val,\n",
    "                \"test_accuracy\": test_res.get('accuracy', 0)  # Also extract accuracy\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _visualize_results(self):\n",
    "        \"\"\"  \"\"\"\n",
    "        df_val = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        df_test = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            # \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_val.plot(\n",
    "            x=\"Experiment\",\n",
    "            # y=[\"HGNN Val\", \"HGNN Test\", \"HGNNP Val\", \"HGNNP Test\", \"UniGCN Val\", \"UniGCN Test\"],\n",
    "            y=[\"HGNN Val\", \"HGNNP Val\", \"UniGCN Val\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Val)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"hypergraph_comparison_val.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_test.plot(\n",
    "            x=\"Experiment\",\n",
    "            y=[\"HGNN Test\", \"HGNNP Test\", \"UniGCN Test\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Test)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"hypergraph_comparison_test.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Results Summary Val ===\")\n",
    "        print(df_val.to_string(index=False))\n",
    "        print()\n",
    "        print(\"\\n=== Results Summary Test ===\")\n",
    "        print(df_test.to_string(index=False))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieHypergraphDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        n_samples: Optional[int] = None,\n",
    "        use_densest_subgraphs: bool = False,\n",
    "        k: int = 3,\n",
    "        hyperedge_types: List[str] = [\"genre\", \"director\", \"numerical\"]\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.n_samples = n_samples\n",
    "        self.use_densest_subgraphs = use_densest_subgraphs\n",
    "        self.k = k\n",
    "        self.hyperedge_types = hyperedge_types\n",
    "        self._content = self._build_dataset()\n",
    "\n",
    "    def _build_dataset(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build the complete dataset dictionary\"\"\"\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(self.data_root)\n",
    "        if self.n_samples:\n",
    "            df = df.sample(min(self.n_samples, len(df)))\n",
    "\n",
    "        df = self._preprocess_data(df)\n",
    "        \n",
    "        # Create features and labels\n",
    "        features = self._create_features(df)\n",
    "        labels = (df['revenue'] > df['budget']).astype(int).values\n",
    "        \n",
    "        # Create splits\n",
    "        train_mask, val_mask, test_mask = self._create_splits(labels)\n",
    "        \n",
    "        # Create hyperedges\n",
    "        edge_list = self._create_hyperedges(df)\n",
    "        \n",
    "        # Add densest subgraphs if enabled\n",
    "        if self.use_densest_subgraphs and len(edge_list) > 0:\n",
    "            temp_hg = Hypergraph(len(df), edge_list)\n",
    "            top_k_subgraphs = self._get_top_k_densest_subgraphs(temp_hg, k=self.k)\n",
    "            edge_list.extend([list(subset) for subset, _ in top_k_subgraphs])\n",
    "        \n",
    "        return {\n",
    "            \"num_classes\": 2,\n",
    "            \"num_vertices\": len(df),\n",
    "            \"num_edges\": len(edge_list),\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"labels\": torch.LongTensor(labels),\n",
    "            \"edge_list\": edge_list,\n",
    "            \"train_mask\": train_mask,\n",
    "            \"val_mask\": val_mask,\n",
    "            \"test_mask\": test_mask,\n",
    "        }\n",
    "\n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
    "        # Convert stringified lists to actual lists\n",
    "        for col in ['genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: literal_eval(x) \n",
    "                    if pd.notna(x) and isinstance(x, str) and x.startswith('[') \n",
    "                    else []\n",
    "                )\n",
    "        \n",
    "        # Handle genres specially if they're space-separated strings\n",
    "        if 'genres' in df.columns:\n",
    "            df['genres'] = df['genres'].apply(\n",
    "                lambda x: [{'name': g.strip()} for g in x.split()] \n",
    "                if pd.notna(x) and isinstance(x, str) and not x.startswith('[')\n",
    "                else x\n",
    "            )\n",
    "        \n",
    "        # Fill missing values\n",
    "        text_cols = ['overview', 'tagline', 'director']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "        \n",
    "        num_cols = ['runtime', 'budget', 'revenue', 'popularity', 'vote_average', 'vote_count']\n",
    "        for col in num_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _create_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create feature matrix combining numerical and text features\"\"\"\n",
    "        # Numerical features\n",
    "        num_features = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
    "        X_num = StandardScaler().fit_transform(df[num_features].values) if num_features else np.zeros((len(df), 0))\n",
    "        \n",
    "        # Text features from overview\n",
    "        tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "        X_text = tfidf.fit_transform(df['overview']).toarray() if 'overview' in df.columns else np.zeros((len(df), 0))\n",
    "        \n",
    "        return np.concatenate([X_num, X_text], axis=1)\n",
    "\n",
    "    def _create_splits(self, labels: np.ndarray) -> tuple:\n",
    "        \"\"\"Create train/val/test splits with stratification\"\"\"\n",
    "        indices = np.arange(len(labels))\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=0.3, stratify=labels)\n",
    "        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=labels[test_idx])\n",
    "        \n",
    "        train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        \n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "        \n",
    "        return train_mask, val_mask, test_mask\n",
    "\n",
    "    def _create_hyperedges(self, df: pd.DataFrame) -> list:\n",
    "        \"\"\"Create hyperedges based on specified types\"\"\"\n",
    "        edge_list = []\n",
    "        \n",
    "        if \"genre\" in self.hyperedge_types and 'genres' in df.columns:\n",
    "            genre_to_movies = {}\n",
    "            for idx, genres in enumerate(df['genres']):\n",
    "                if isinstance(genres, list):\n",
    "                    for genre in genres:\n",
    "                        name = genre['name'] if isinstance(genre, dict) else genre\n",
    "                        if name not in genre_to_movies:\n",
    "                            genre_to_movies[name] = []\n",
    "                        genre_to_movies[name].append(idx)\n",
    "            edge_list.extend(list(genre_to_movies.values()))\n",
    "        \n",
    "        if \"director\" in self.hyperedge_types and 'director' in df.columns:\n",
    "            director_to_movies = {}\n",
    "            for idx, director in enumerate(df['director']):\n",
    "                if pd.notna(director):\n",
    "                    if director not in director_to_movies:\n",
    "                        director_to_movies[director] = []\n",
    "                    director_to_movies[director].append(idx)\n",
    "            edge_list.extend(list(director_to_movies.values()))\n",
    "        \n",
    "        if \"numerical\" in self.hyperedge_types:\n",
    "            numerical_features = ['budget', 'popularity', 'runtime', 'vote_average']\n",
    "            X_num = df[numerical_features].values\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X_num)\n",
    "            _, indices = knn.kneighbors(X_num)\n",
    "            edge_list.extend([list(idx) for idx in indices])\n",
    "        \n",
    "        return edge_list\n",
    "\n",
    "    def _get_top_k_densest_subgraphs(self, hg: Hypergraph, k: int = 3) -> list:\n",
    "        \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "        nodes = set(range(hg.num_v))\n",
    "        subgraphs = []\n",
    "        \n",
    "        for _ in range(k):\n",
    "            if len(nodes) < 3:  # min_size\n",
    "                break\n",
    "                \n",
    "            current_nodes = set(nodes)\n",
    "            best_subset = None\n",
    "            best_density = -1\n",
    "            \n",
    "            while len(current_nodes) >= 3:\n",
    "                edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "                density = edge_count / len(current_nodes)\n",
    "                \n",
    "                if density > best_density:\n",
    "                    best_density = density\n",
    "                    best_subset = set(current_nodes)\n",
    "                \n",
    "                # Remove node with lowest degree\n",
    "                degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "                node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "                current_nodes.remove(node_to_remove)\n",
    "            \n",
    "            if best_subset:\n",
    "                subgraphs.append((best_subset, best_density))\n",
    "                nodes -= best_subset\n",
    "        \n",
    "        return subgraphs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._content[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    \n",
    "    # Handle both cases: when evaluator returns dict or float\n",
    "    if isinstance(res, dict):\n",
    "        return res\n",
    "    else:\n",
    "        # If single float returned, assume it's accuracy\n",
    "        return {'accuracy': res, 'f1_score': res}  # Using same value for both for compatibility\n",
    "\n",
    "def evaluate_model(model, hypergraph_type, X, hg, labels, masks):\n",
    "    results = {}\n",
    "    for name, mask in masks.items():\n",
    "        res = infer(model, X, hg, labels, mask, test=(name == \"test\"))\n",
    "        results[f\"{hypergraph_type}_{name}\"] = res\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1: Base Hypergraph ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69870\n",
      "Epoch: 1, Loss: 0.67420\n",
      "Epoch: 2, Loss: 0.65288\n",
      "Epoch: 3, Loss: 0.63459\n",
      "Epoch: 4, Loss: 0.61122\n",
      "Epoch: 5, Loss: 0.58895\n",
      "Epoch: 6, Loss: 0.56624\n",
      "Epoch: 7, Loss: 0.55094\n",
      "Epoch: 8, Loss: 0.53419\n",
      "Epoch: 9, Loss: 0.52674\n",
      "Epoch: 10, Loss: 0.51615\n",
      "Epoch: 11, Loss: 0.50940\n",
      "Epoch: 12, Loss: 0.50712\n",
      "Epoch: 13, Loss: 0.50202\n",
      "Epoch: 14, Loss: 0.48364\n",
      "Epoch: 15, Loss: 0.46859\n",
      "Epoch: 16, Loss: 0.46076\n",
      "Epoch: 17, Loss: 0.45353\n",
      "Epoch: 18, Loss: 0.44148\n",
      "Epoch: 19, Loss: 0.43598\n",
      "Epoch: 20, Loss: 0.43759\n",
      "Epoch: 21, Loss: 0.43149\n",
      "Epoch: 22, Loss: 0.42110\n",
      "Epoch: 23, Loss: 0.41639\n",
      "Epoch: 24, Loss: 0.40661\n",
      "Epoch: 25, Loss: 0.40693\n",
      "Epoch: 26, Loss: 0.39868\n",
      "Epoch: 27, Loss: 0.39743\n",
      "Epoch: 28, Loss: 0.39411\n",
      "Epoch: 29, Loss: 0.38634\n",
      "Epoch: 30, Loss: 0.38473\n",
      "Epoch: 31, Loss: 0.37813\n",
      "Epoch: 32, Loss: 0.37645\n",
      "Epoch: 33, Loss: 0.37379\n",
      "Epoch: 34, Loss: 0.36807\n",
      "Epoch: 35, Loss: 0.37258\n",
      "Epoch: 36, Loss: 0.35548\n",
      "Epoch: 37, Loss: 0.35746\n",
      "Epoch: 38, Loss: 0.35103\n",
      "Epoch: 39, Loss: 0.35516\n",
      "Epoch: 40, Loss: 0.34747\n",
      "Epoch: 41, Loss: 0.34336\n",
      "Epoch: 42, Loss: 0.34442\n",
      "Epoch: 43, Loss: 0.34523\n",
      "Epoch: 44, Loss: 0.33777\n",
      "Epoch: 45, Loss: 0.33699\n",
      "Epoch: 46, Loss: 0.32692\n",
      "Epoch: 47, Loss: 0.33945\n",
      "Epoch: 48, Loss: 0.33211\n",
      "Epoch: 49, Loss: 0.32901\n",
      "Epoch: 50, Loss: 0.32436\n",
      "Epoch: 51, Loss: 0.31447\n",
      "Epoch: 52, Loss: 0.32800\n",
      "Epoch: 53, Loss: 0.31938\n",
      "Epoch: 54, Loss: 0.31598\n",
      "Epoch: 55, Loss: 0.30744\n",
      "Epoch: 56, Loss: 0.31339\n",
      "Epoch: 57, Loss: 0.32076\n",
      "Epoch: 58, Loss: 0.30800\n",
      "Epoch: 59, Loss: 0.31518\n",
      "Epoch: 60, Loss: 0.30656\n",
      "Epoch: 61, Loss: 0.31153\n",
      "Epoch: 62, Loss: 0.30016\n",
      "Epoch: 63, Loss: 0.30102\n",
      "Epoch: 64, Loss: 0.30463\n",
      "Epoch: 65, Loss: 0.29495\n",
      "Epoch: 66, Loss: 0.29415\n",
      "Epoch: 67, Loss: 0.29009\n",
      "Epoch: 68, Loss: 0.30450\n",
      "Epoch: 69, Loss: 0.29231\n",
      "Epoch: 70, Loss: 0.29586\n",
      "Epoch: 71, Loss: 0.29505\n",
      "Epoch: 72, Loss: 0.28791\n",
      "Epoch: 73, Loss: 0.28308\n",
      "Epoch: 74, Loss: 0.28052\n",
      "Epoch: 75, Loss: 0.29305\n",
      "Epoch: 76, Loss: 0.28659\n",
      "Epoch: 77, Loss: 0.28178\n",
      "Epoch: 78, Loss: 0.27907\n",
      "Epoch: 79, Loss: 0.27869\n",
      "Epoch: 80, Loss: 0.28982\n",
      "Epoch: 81, Loss: 0.27951\n",
      "Epoch: 82, Loss: 0.27919\n",
      "Epoch: 83, Loss: 0.27977\n",
      "Epoch: 84, Loss: 0.28081\n",
      "Epoch: 85, Loss: 0.28236\n",
      "Epoch: 86, Loss: 0.26981\n",
      "Epoch: 87, Loss: 0.27873\n",
      "Epoch: 88, Loss: 0.27820\n",
      "Epoch: 89, Loss: 0.29633\n",
      "Epoch: 90, Loss: 0.27319\n",
      "Epoch: 91, Loss: 0.26760\n",
      "Epoch: 92, Loss: 0.29216\n",
      "Epoch: 93, Loss: 0.27408\n",
      "Epoch: 94, Loss: 0.27113\n",
      "Epoch: 95, Loss: 0.27922\n",
      "Epoch: 96, Loss: 0.28443\n",
      "Epoch: 97, Loss: 0.26920\n",
      "Epoch: 98, Loss: 0.27244\n",
      "Epoch: 99, Loss: 0.27772\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.71888\n",
      "Epoch: 1, Loss: 0.53775\n",
      "Epoch: 2, Loss: 0.50648\n",
      "Epoch: 3, Loss: 0.47168\n",
      "Epoch: 4, Loss: 0.43739\n",
      "Epoch: 5, Loss: 0.41973\n",
      "Epoch: 6, Loss: 0.41999\n",
      "Epoch: 7, Loss: 0.40139\n",
      "Epoch: 8, Loss: 0.38914\n",
      "Epoch: 9, Loss: 0.37106\n",
      "Epoch: 10, Loss: 0.35858\n",
      "Epoch: 11, Loss: 0.35916\n",
      "Epoch: 12, Loss: 0.35514\n",
      "Epoch: 13, Loss: 0.34800\n",
      "Epoch: 14, Loss: 0.31949\n",
      "Epoch: 15, Loss: 0.33163\n",
      "Epoch: 16, Loss: 0.31054\n",
      "Epoch: 17, Loss: 0.30227\n",
      "Epoch: 18, Loss: 0.30981\n",
      "Epoch: 19, Loss: 0.28573\n",
      "Epoch: 20, Loss: 0.28924\n",
      "Epoch: 21, Loss: 0.28479\n",
      "Epoch: 22, Loss: 0.27341\n",
      "Epoch: 23, Loss: 0.26530\n",
      "Epoch: 24, Loss: 0.26027\n",
      "Epoch: 25, Loss: 0.25114\n",
      "Epoch: 26, Loss: 0.25160\n",
      "Epoch: 27, Loss: 0.24842\n",
      "Epoch: 28, Loss: 0.23903\n",
      "Epoch: 29, Loss: 0.24622\n",
      "Epoch: 30, Loss: 0.22872\n",
      "Epoch: 31, Loss: 0.22760\n",
      "Epoch: 32, Loss: 0.22509\n",
      "Epoch: 33, Loss: 0.21334\n",
      "Epoch: 34, Loss: 0.22115\n",
      "Epoch: 35, Loss: 0.20706\n",
      "Epoch: 36, Loss: 0.20400\n",
      "Epoch: 37, Loss: 0.21754\n",
      "Epoch: 38, Loss: 0.19976\n",
      "Epoch: 39, Loss: 0.19581\n",
      "Epoch: 40, Loss: 0.19101\n",
      "Epoch: 41, Loss: 0.20118\n",
      "Epoch: 42, Loss: 0.17940\n",
      "Epoch: 43, Loss: 0.17515\n",
      "Epoch: 44, Loss: 0.17993\n",
      "Epoch: 45, Loss: 0.18582\n",
      "Epoch: 46, Loss: 0.17899\n",
      "Epoch: 47, Loss: 0.16499\n",
      "Epoch: 48, Loss: 0.19228\n",
      "Epoch: 49, Loss: 0.19277\n",
      "Epoch: 50, Loss: 0.16188\n",
      "Epoch: 51, Loss: 0.17448\n",
      "Epoch: 52, Loss: 0.16559\n",
      "Epoch: 53, Loss: 0.16136\n",
      "Epoch: 54, Loss: 0.15535\n",
      "Epoch: 55, Loss: 0.16659\n",
      "Epoch: 56, Loss: 0.15883\n",
      "Epoch: 57, Loss: 0.15976\n",
      "Epoch: 58, Loss: 0.16790\n",
      "Epoch: 59, Loss: 0.17923\n",
      "Epoch: 60, Loss: 0.16626\n",
      "Epoch: 61, Loss: 0.15417\n",
      "Epoch: 62, Loss: 0.15507\n",
      "Epoch: 63, Loss: 0.16965\n",
      "Epoch: 64, Loss: 0.16738\n",
      "Epoch: 65, Loss: 0.14936\n",
      "Epoch: 66, Loss: 0.16436\n",
      "Epoch: 67, Loss: 0.19078\n",
      "Epoch: 68, Loss: 0.13725\n",
      "Epoch: 69, Loss: 0.14954\n",
      "Epoch: 70, Loss: 0.17424\n",
      "Epoch: 71, Loss: 0.18393\n",
      "Epoch: 72, Loss: 0.16646\n",
      "Epoch: 73, Loss: 0.15265\n",
      "Epoch: 74, Loss: 0.15589\n",
      "Epoch: 75, Loss: 0.14598\n",
      "Epoch: 76, Loss: 0.16993\n",
      "Epoch: 77, Loss: 0.16646\n",
      "Epoch: 78, Loss: 0.15213\n",
      "Epoch: 79, Loss: 0.13838\n",
      "Epoch: 80, Loss: 0.15259\n",
      "Epoch: 81, Loss: 0.14914\n",
      "Epoch: 82, Loss: 0.15539\n",
      "Epoch: 83, Loss: 0.14237\n",
      "Epoch: 84, Loss: 0.15430\n",
      "Epoch: 85, Loss: 0.16028\n",
      "Epoch: 86, Loss: 0.16235\n",
      "Epoch: 87, Loss: 0.15507\n",
      "Epoch: 88, Loss: 0.12995\n",
      "Epoch: 89, Loss: 0.12147\n",
      "Epoch: 90, Loss: 0.13776\n",
      "Epoch: 91, Loss: 0.14246\n",
      "Epoch: 92, Loss: 0.13127\n",
      "Epoch: 93, Loss: 0.12459\n",
      "Epoch: 94, Loss: 0.14467\n",
      "Epoch: 95, Loss: 0.16137\n",
      "Epoch: 96, Loss: 0.12901\n",
      "Epoch: 97, Loss: 0.12038\n",
      "Epoch: 98, Loss: 0.17264\n",
      "Epoch: 99, Loss: 0.13765\n",
      "\n",
      "Training UniGCN...\n",
      "Epoch: 0, Loss: 0.80603\n",
      "Epoch: 1, Loss: 0.52543\n",
      "Epoch: 2, Loss: 0.47790\n",
      "Epoch: 3, Loss: 0.45118\n",
      "Epoch: 4, Loss: 0.40917\n",
      "Epoch: 5, Loss: 0.40204\n",
      "Epoch: 6, Loss: 0.39275\n",
      "Epoch: 7, Loss: 0.38196\n",
      "Epoch: 8, Loss: 0.35940\n",
      "Epoch: 9, Loss: 0.34428\n",
      "Epoch: 10, Loss: 0.34842\n",
      "Epoch: 11, Loss: 0.33298\n",
      "Epoch: 12, Loss: 0.32562\n",
      "Epoch: 13, Loss: 0.31178\n",
      "Epoch: 14, Loss: 0.30697\n",
      "Epoch: 15, Loss: 0.30473\n",
      "Epoch: 16, Loss: 0.28111\n",
      "Epoch: 17, Loss: 0.27510\n",
      "Epoch: 18, Loss: 0.27138\n",
      "Epoch: 19, Loss: 0.25527\n",
      "Epoch: 20, Loss: 0.25124\n",
      "Epoch: 21, Loss: 0.24656\n",
      "Epoch: 22, Loss: 0.25129\n",
      "Epoch: 23, Loss: 0.23389\n",
      "Epoch: 24, Loss: 0.22215\n",
      "Epoch: 25, Loss: 0.22614\n",
      "Epoch: 26, Loss: 0.20906\n",
      "Epoch: 27, Loss: 0.21699\n",
      "Epoch: 28, Loss: 0.20676\n",
      "Epoch: 29, Loss: 0.21657\n",
      "Epoch: 30, Loss: 0.20711\n",
      "Epoch: 31, Loss: 0.20466\n",
      "Epoch: 32, Loss: 0.18575\n",
      "Epoch: 33, Loss: 0.19382\n",
      "Epoch: 34, Loss: 0.18891\n",
      "Epoch: 35, Loss: 0.19307\n",
      "Epoch: 36, Loss: 0.18961\n",
      "Epoch: 37, Loss: 0.19030\n",
      "Epoch: 38, Loss: 0.19541\n",
      "Epoch: 39, Loss: 0.17467\n",
      "Epoch: 40, Loss: 0.16523\n",
      "Epoch: 41, Loss: 0.15949\n",
      "Epoch: 42, Loss: 0.17359\n",
      "Epoch: 43, Loss: 0.17865\n",
      "Epoch: 44, Loss: 0.17483\n",
      "Epoch: 45, Loss: 0.17164\n",
      "Epoch: 46, Loss: 0.15713\n",
      "Epoch: 47, Loss: 0.15890\n",
      "Epoch: 48, Loss: 0.15909\n",
      "Epoch: 49, Loss: 0.15305\n",
      "Epoch: 50, Loss: 0.16385\n",
      "Epoch: 51, Loss: 0.14250\n",
      "Epoch: 52, Loss: 0.14305\n",
      "Epoch: 53, Loss: 0.14564\n",
      "Epoch: 54, Loss: 0.13348\n",
      "Epoch: 55, Loss: 0.14376\n",
      "Epoch: 56, Loss: 0.13909\n",
      "Epoch: 57, Loss: 0.13527\n",
      "Epoch: 58, Loss: 0.13473\n",
      "Epoch: 59, Loss: 0.14200\n",
      "Epoch: 60, Loss: 0.14040\n",
      "Epoch: 61, Loss: 0.11945\n",
      "Epoch: 62, Loss: 0.11621\n",
      "Epoch: 63, Loss: 0.13650\n",
      "Epoch: 64, Loss: 0.10680\n",
      "Epoch: 65, Loss: 0.12015\n",
      "Epoch: 66, Loss: 0.12021\n",
      "Epoch: 67, Loss: 0.13349\n",
      "Epoch: 68, Loss: 0.12623\n",
      "Epoch: 69, Loss: 0.11952\n",
      "Epoch: 70, Loss: 0.12053\n",
      "Epoch: 71, Loss: 0.12244\n",
      "Epoch: 72, Loss: 0.13697\n",
      "Epoch: 73, Loss: 0.11926\n",
      "Epoch: 74, Loss: 0.12730\n",
      "Epoch: 75, Loss: 0.11621\n",
      "Epoch: 76, Loss: 0.11191\n",
      "Epoch: 77, Loss: 0.10613\n",
      "Epoch: 78, Loss: 0.12144\n",
      "Epoch: 79, Loss: 0.11566\n",
      "Epoch: 80, Loss: 0.12737\n",
      "Epoch: 81, Loss: 0.12956\n",
      "Epoch: 82, Loss: 0.11978\n",
      "Epoch: 83, Loss: 0.10874\n",
      "Epoch: 84, Loss: 0.10317\n",
      "Epoch: 85, Loss: 0.12145\n",
      "Epoch: 86, Loss: 0.11194\n",
      "Epoch: 87, Loss: 0.11042\n",
      "Epoch: 88, Loss: 0.11506\n",
      "Epoch: 89, Loss: 0.10742\n",
      "Epoch: 90, Loss: 0.13737\n",
      "Epoch: 91, Loss: 0.12975\n",
      "Epoch: 92, Loss: 0.12128\n",
      "Epoch: 93, Loss: 0.10915\n",
      "Epoch: 94, Loss: 0.12121\n",
      "Epoch: 95, Loss: 0.12200\n",
      "Epoch: 96, Loss: 0.11287\n",
      "Epoch: 97, Loss: 0.12171\n",
      "Epoch: 98, Loss: 0.10941\n",
      "Epoch: 99, Loss: 0.11057\n",
      "\n",
      "=== Experiment 2: With Densest Subgraphs ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69255\n",
      "Epoch: 1, Loss: 0.67043\n",
      "Epoch: 2, Loss: 0.65093\n",
      "Epoch: 3, Loss: 0.62857\n",
      "Epoch: 4, Loss: 0.60613\n",
      "Epoch: 5, Loss: 0.58838\n",
      "Epoch: 6, Loss: 0.56718\n",
      "Epoch: 7, Loss: 0.54882\n",
      "Epoch: 8, Loss: 0.53874\n",
      "Epoch: 9, Loss: 0.52807\n",
      "Epoch: 10, Loss: 0.52455\n",
      "Epoch: 11, Loss: 0.52252\n",
      "Epoch: 12, Loss: 0.51426\n",
      "Epoch: 13, Loss: 0.50630\n",
      "Epoch: 14, Loss: 0.50469\n",
      "Epoch: 15, Loss: 0.50753\n",
      "Epoch: 16, Loss: 0.49365\n",
      "Epoch: 17, Loss: 0.48851\n",
      "Epoch: 18, Loss: 0.48018\n",
      "Epoch: 19, Loss: 0.47358\n",
      "Epoch: 20, Loss: 0.46480\n",
      "Epoch: 21, Loss: 0.45980\n",
      "Epoch: 22, Loss: 0.45346\n",
      "Epoch: 23, Loss: 0.45674\n",
      "Epoch: 24, Loss: 0.45023\n",
      "Epoch: 25, Loss: 0.44938\n",
      "Epoch: 26, Loss: 0.45203\n",
      "Epoch: 27, Loss: 0.44394\n",
      "Epoch: 28, Loss: 0.43880\n",
      "Epoch: 29, Loss: 0.43337\n",
      "Epoch: 30, Loss: 0.43632\n",
      "Epoch: 31, Loss: 0.42664\n",
      "Epoch: 32, Loss: 0.42336\n",
      "Epoch: 33, Loss: 0.41821\n",
      "Epoch: 34, Loss: 0.41799\n",
      "Epoch: 35, Loss: 0.41511\n",
      "Epoch: 36, Loss: 0.41235\n",
      "Epoch: 37, Loss: 0.41340\n",
      "Epoch: 38, Loss: 0.41100\n",
      "Epoch: 39, Loss: 0.40863\n",
      "Epoch: 40, Loss: 0.39814\n",
      "Epoch: 41, Loss: 0.39906\n",
      "Epoch: 42, Loss: 0.39698\n",
      "Epoch: 43, Loss: 0.39585\n",
      "Epoch: 44, Loss: 0.39372\n",
      "Epoch: 45, Loss: 0.38863\n",
      "Epoch: 46, Loss: 0.38556\n",
      "Epoch: 47, Loss: 0.38443\n",
      "Epoch: 48, Loss: 0.38100\n",
      "Epoch: 49, Loss: 0.38892\n",
      "Epoch: 50, Loss: 0.38149\n",
      "Epoch: 51, Loss: 0.37717\n",
      "Epoch: 52, Loss: 0.37209\n",
      "Epoch: 53, Loss: 0.37204\n",
      "Epoch: 54, Loss: 0.36482\n",
      "Epoch: 55, Loss: 0.37121\n",
      "Epoch: 56, Loss: 0.37502\n",
      "Epoch: 57, Loss: 0.36751\n",
      "Epoch: 58, Loss: 0.36073\n",
      "Epoch: 59, Loss: 0.36734\n",
      "Epoch: 60, Loss: 0.35533\n",
      "Epoch: 61, Loss: 0.36291\n",
      "Epoch: 62, Loss: 0.35596\n",
      "Epoch: 63, Loss: 0.36156\n",
      "Epoch: 64, Loss: 0.35310\n",
      "Epoch: 65, Loss: 0.34878\n",
      "Epoch: 66, Loss: 0.35331\n",
      "Epoch: 67, Loss: 0.36165\n",
      "Epoch: 68, Loss: 0.35664\n",
      "Epoch: 69, Loss: 0.34901\n",
      "Epoch: 70, Loss: 0.34483\n",
      "Epoch: 71, Loss: 0.34052\n",
      "Epoch: 72, Loss: 0.34480\n",
      "Epoch: 73, Loss: 0.34697\n",
      "Epoch: 74, Loss: 0.33931\n",
      "Epoch: 75, Loss: 0.35169\n",
      "Epoch: 76, Loss: 0.33502\n",
      "Epoch: 77, Loss: 0.34312\n",
      "Epoch: 78, Loss: 0.33133\n",
      "Epoch: 79, Loss: 0.33792\n",
      "Epoch: 80, Loss: 0.33620\n",
      "Epoch: 81, Loss: 0.32846\n",
      "Epoch: 82, Loss: 0.32506\n",
      "Epoch: 83, Loss: 0.32634\n",
      "Epoch: 84, Loss: 0.33162\n",
      "Epoch: 85, Loss: 0.32859\n",
      "Epoch: 86, Loss: 0.33164\n",
      "Epoch: 87, Loss: 0.32442\n",
      "Epoch: 88, Loss: 0.31963\n",
      "Epoch: 89, Loss: 0.32257\n",
      "Epoch: 90, Loss: 0.32779\n",
      "Epoch: 91, Loss: 0.32076\n",
      "Epoch: 92, Loss: 0.31665\n",
      "Epoch: 93, Loss: 0.31643\n",
      "Epoch: 94, Loss: 0.32260\n",
      "Epoch: 95, Loss: 0.31587\n",
      "Epoch: 96, Loss: 0.31281\n",
      "Epoch: 97, Loss: 0.30908\n",
      "Epoch: 98, Loss: 0.30569\n",
      "Epoch: 99, Loss: 0.31592\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.83731\n",
      "Epoch: 1, Loss: 0.56390\n",
      "Epoch: 2, Loss: 0.53530\n",
      "Epoch: 3, Loss: 0.50419\n",
      "Epoch: 4, Loss: 0.47381\n",
      "Epoch: 5, Loss: 0.45849\n",
      "Epoch: 6, Loss: 0.45347\n",
      "Epoch: 7, Loss: 0.43981\n",
      "Epoch: 8, Loss: 0.43259\n",
      "Epoch: 9, Loss: 0.42358\n",
      "Epoch: 10, Loss: 0.41747\n",
      "Epoch: 11, Loss: 0.40459\n",
      "Epoch: 12, Loss: 0.39938\n",
      "Epoch: 13, Loss: 0.39515\n",
      "Epoch: 14, Loss: 0.38986\n",
      "Epoch: 15, Loss: 0.37992\n",
      "Epoch: 16, Loss: 0.36672\n",
      "Epoch: 17, Loss: 0.35895\n",
      "Epoch: 18, Loss: 0.35573\n",
      "Epoch: 19, Loss: 0.34753\n",
      "Epoch: 20, Loss: 0.33787\n",
      "Epoch: 21, Loss: 0.32989\n",
      "Epoch: 22, Loss: 0.32874\n",
      "Epoch: 23, Loss: 0.31901\n",
      "Epoch: 24, Loss: 0.33271\n",
      "Epoch: 25, Loss: 0.30806\n",
      "Epoch: 26, Loss: 0.29844\n",
      "Epoch: 27, Loss: 0.29710\n",
      "Epoch: 28, Loss: 0.27921\n",
      "Epoch: 29, Loss: 0.28414\n",
      "Epoch: 30, Loss: 0.26623\n",
      "Epoch: 31, Loss: 0.25174\n",
      "Epoch: 32, Loss: 0.24651\n",
      "Epoch: 33, Loss: 0.24397\n",
      "Epoch: 34, Loss: 0.23668\n",
      "Epoch: 35, Loss: 0.22143\n",
      "Epoch: 36, Loss: 0.22531\n",
      "Epoch: 37, Loss: 0.23037\n",
      "Epoch: 38, Loss: 0.21574\n",
      "Epoch: 39, Loss: 0.21788\n",
      "Epoch: 40, Loss: 0.20363\n",
      "Epoch: 41, Loss: 0.17667\n",
      "Epoch: 42, Loss: 0.17500\n",
      "Epoch: 43, Loss: 0.18289\n",
      "Epoch: 44, Loss: 0.19158\n",
      "Epoch: 45, Loss: 0.17105\n",
      "Epoch: 46, Loss: 0.17284\n",
      "Epoch: 47, Loss: 0.17129\n",
      "Epoch: 48, Loss: 0.16304\n",
      "Epoch: 49, Loss: 0.17088\n",
      "Epoch: 50, Loss: 0.16488\n",
      "Epoch: 51, Loss: 0.14495\n",
      "Epoch: 52, Loss: 0.16048\n",
      "Epoch: 53, Loss: 0.16421\n",
      "Epoch: 54, Loss: 0.15152\n",
      "Epoch: 55, Loss: 0.13154\n",
      "Epoch: 56, Loss: 0.13748\n",
      "Epoch: 57, Loss: 0.13511\n",
      "Epoch: 58, Loss: 0.14627\n",
      "Epoch: 59, Loss: 0.11190\n",
      "Epoch: 60, Loss: 0.13609\n",
      "Epoch: 61, Loss: 0.13048\n",
      "Epoch: 62, Loss: 0.13502\n",
      "Epoch: 63, Loss: 0.18223\n",
      "Epoch: 64, Loss: 0.12874\n",
      "Epoch: 65, Loss: 0.12234\n",
      "Epoch: 66, Loss: 0.11295\n",
      "Epoch: 67, Loss: 0.14012\n",
      "Epoch: 68, Loss: 0.13280\n",
      "Epoch: 69, Loss: 0.13092\n",
      "Epoch: 70, Loss: 0.11035\n",
      "Epoch: 71, Loss: 0.13578\n",
      "Epoch: 72, Loss: 0.12510\n",
      "Epoch: 73, Loss: 0.11434\n",
      "Epoch: 74, Loss: 0.13972\n",
      "Epoch: 75, Loss: 0.10043\n",
      "Epoch: 76, Loss: 0.13325\n",
      "Epoch: 77, Loss: 0.10610\n",
      "Epoch: 78, Loss: 0.11598\n",
      "Epoch: 79, Loss: 0.09931\n",
      "Epoch: 80, Loss: 0.12820\n",
      "Epoch: 81, Loss: 0.11215\n",
      "Epoch: 82, Loss: 0.11995\n",
      "Epoch: 83, Loss: 0.11854\n",
      "Epoch: 84, Loss: 0.11962\n",
      "Epoch: 85, Loss: 0.11485\n",
      "Epoch: 86, Loss: 0.13617\n",
      "Epoch: 87, Loss: 0.16378\n",
      "Epoch: 88, Loss: 0.11524\n",
      "Epoch: 89, Loss: 0.15311\n",
      "Epoch: 90, Loss: 0.15315\n",
      "Epoch: 91, Loss: 0.13413\n",
      "Epoch: 92, Loss: 0.14700\n",
      "Epoch: 93, Loss: 0.11679\n",
      "Epoch: 94, Loss: 0.13218\n",
      "Epoch: 95, Loss: 0.12188\n",
      "Epoch: 96, Loss: 0.11739\n",
      "Epoch: 97, Loss: 0.13773\n",
      "Epoch: 98, Loss: 0.13357\n",
      "Epoch: 99, Loss: 0.10084\n",
      "\n",
      "Training UniGCN...\n",
      "Epoch: 0, Loss: 0.71484\n",
      "Epoch: 1, Loss: 0.55088\n",
      "Epoch: 2, Loss: 0.48029\n",
      "Epoch: 3, Loss: 0.49167\n",
      "Epoch: 4, Loss: 0.43385\n",
      "Epoch: 5, Loss: 0.42873\n",
      "Epoch: 6, Loss: 0.41020\n",
      "Epoch: 7, Loss: 0.39412\n",
      "Epoch: 8, Loss: 0.38265\n",
      "Epoch: 9, Loss: 0.37807\n",
      "Epoch: 10, Loss: 0.36571\n",
      "Epoch: 11, Loss: 0.34796\n",
      "Epoch: 12, Loss: 0.34158\n",
      "Epoch: 13, Loss: 0.33098\n",
      "Epoch: 14, Loss: 0.31095\n",
      "Epoch: 15, Loss: 0.31955\n",
      "Epoch: 16, Loss: 0.30317\n",
      "Epoch: 17, Loss: 0.29736\n",
      "Epoch: 18, Loss: 0.27186\n",
      "Epoch: 19, Loss: 0.26795\n",
      "Epoch: 20, Loss: 0.24886\n",
      "Epoch: 21, Loss: 0.23897\n",
      "Epoch: 22, Loss: 0.24065\n",
      "Epoch: 23, Loss: 0.22361\n",
      "Epoch: 24, Loss: 0.22186\n",
      "Epoch: 25, Loss: 0.20602\n",
      "Epoch: 26, Loss: 0.20682\n",
      "Epoch: 27, Loss: 0.19927\n",
      "Epoch: 28, Loss: 0.18193\n",
      "Epoch: 29, Loss: 0.18041\n",
      "Epoch: 30, Loss: 0.17905\n",
      "Epoch: 31, Loss: 0.17451\n",
      "Epoch: 32, Loss: 0.17544\n",
      "Epoch: 33, Loss: 0.15437\n",
      "Epoch: 34, Loss: 0.15208\n",
      "Epoch: 35, Loss: 0.13639\n",
      "Epoch: 36, Loss: 0.13099\n",
      "Epoch: 37, Loss: 0.13323\n",
      "Epoch: 38, Loss: 0.12732\n",
      "Epoch: 39, Loss: 0.14593\n",
      "Epoch: 40, Loss: 0.11948\n",
      "Epoch: 41, Loss: 0.14431\n",
      "Epoch: 42, Loss: 0.13589\n",
      "Epoch: 43, Loss: 0.12969\n",
      "Epoch: 44, Loss: 0.16649\n",
      "Epoch: 45, Loss: 0.12909\n",
      "Epoch: 46, Loss: 0.11198\n",
      "Epoch: 47, Loss: 0.12503\n",
      "Epoch: 48, Loss: 0.10845\n",
      "Epoch: 49, Loss: 0.11180\n",
      "Epoch: 50, Loss: 0.11120\n",
      "Epoch: 51, Loss: 0.10897\n",
      "Epoch: 52, Loss: 0.11627\n",
      "Epoch: 53, Loss: 0.07337\n",
      "Epoch: 54, Loss: 0.10155\n",
      "Epoch: 55, Loss: 0.12983\n",
      "Epoch: 56, Loss: 0.09777\n",
      "Epoch: 57, Loss: 0.10896\n",
      "Epoch: 58, Loss: 0.08993\n",
      "Epoch: 59, Loss: 0.08952\n",
      "Epoch: 60, Loss: 0.10382\n",
      "Epoch: 61, Loss: 0.09698\n",
      "Epoch: 62, Loss: 0.11280\n",
      "Epoch: 63, Loss: 0.08535\n",
      "Epoch: 64, Loss: 0.10037\n",
      "Epoch: 65, Loss: 0.07109\n",
      "Epoch: 66, Loss: 0.07668\n",
      "Epoch: 67, Loss: 0.10317\n",
      "Epoch: 68, Loss: 0.07182\n",
      "Epoch: 69, Loss: 0.08534\n",
      "Epoch: 70, Loss: 0.05636\n",
      "Epoch: 71, Loss: 0.06623\n",
      "Epoch: 72, Loss: 0.05981\n",
      "Epoch: 73, Loss: 0.07741\n",
      "Epoch: 74, Loss: 0.08906\n",
      "Epoch: 75, Loss: 0.06403\n",
      "Epoch: 76, Loss: 0.07735\n",
      "Epoch: 77, Loss: 0.08663\n",
      "Epoch: 78, Loss: 0.08355\n",
      "Epoch: 79, Loss: 0.06491\n",
      "Epoch: 80, Loss: 0.07115\n",
      "Epoch: 81, Loss: 0.08682\n",
      "Epoch: 82, Loss: 0.09131\n",
      "Epoch: 83, Loss: 0.05784\n",
      "Epoch: 84, Loss: 0.06868\n",
      "Epoch: 85, Loss: 0.08466\n",
      "Epoch: 86, Loss: 0.09045\n",
      "Epoch: 87, Loss: 0.07253\n",
      "Epoch: 88, Loss: 0.08512\n",
      "Epoch: 89, Loss: 0.06584\n",
      "Epoch: 90, Loss: 0.07955\n",
      "Epoch: 91, Loss: 0.11974\n",
      "Epoch: 92, Loss: 0.09953\n",
      "Epoch: 93, Loss: 0.11852\n",
      "Epoch: 94, Loss: 0.07321\n",
      "Epoch: 95, Loss: 0.08538\n",
      "Epoch: 96, Loss: 0.16226\n",
      "Epoch: 97, Loss: 0.11452\n",
      "Epoch: 98, Loss: 0.11616\n",
      "Epoch: 99, Loss: 0.07687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Results Summary Val ===\n",
      "            Experiment  HGNN Val  HGNNP Val  UniGCN Val\n",
      "                  Base  0.693333   0.693333        0.64\n",
      "With Densest Subgraphs  0.706667   0.706667        0.72\n",
      "\n",
      "\n",
      "=== Results Summary Test ===\n",
      "            Experiment  HGNN Test  HGNNP Test  UniGCN Test\n",
      "                  Base   0.693333        0.64     0.626667\n",
      "With Densest Subgraphs   0.800000        0.72     0.733333\n"
     ]
    }
   ],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "experiment = HypergraphExperiment(\n",
    "    data_path=movie_path\n",
    ")\n",
    "experiment.run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
