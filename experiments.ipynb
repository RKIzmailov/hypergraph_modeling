{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Hypergraph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import dhg\n",
    "from dhg import Graph, Hypergraph\n",
    "from dhg.data import Cooking200, News20\n",
    "from dhg.models import GCN, HGNN, HGNNP, UniGCN\n",
    "from dhg.random import set_seed\n",
    "from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "from dhg.utils import split_by_ratio\n",
    "\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ast import literal_eval\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\.dhg\n",
      "d:\\Rustem\\2_Education\\9_UWindsor_CSS\\COMP_8720-Topics_in_AI\\project\\hyper_modeling\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "print(dhg.CACHE_ROOT)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=dhg.data.Cooking200()\n",
    "# for key in d.content:\n",
    "#     d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cooking200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = Cooking200(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 27.05090s, Loss: 3.00726\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.11877s, Loss: 2.87365\n",
      "Epoch: 2, Time: 0.97288s, Loss: 2.63956\n",
      "update best: 0.09500\n",
      "Epoch: 3, Time: 0.97195s, Loss: 2.46788\n",
      "Epoch: 4, Time: 0.95451s, Loss: 2.36260\n",
      "Epoch: 5, Time: 0.97938s, Loss: 2.28780\n",
      "update best: 0.13000\n",
      "Epoch: 6, Time: 1.09381s, Loss: 2.21204\n",
      "update best: 0.13500\n",
      "Epoch: 7, Time: 0.93865s, Loss: 2.13388\n",
      "Epoch: 8, Time: 1.13131s, Loss: 2.06262\n",
      "update best: 0.14000\n",
      "Epoch: 9, Time: 1.40688s, Loss: 1.99684\n",
      "Epoch: 10, Time: 1.38883s, Loss: 1.91773\n",
      "Epoch: 11, Time: 1.27973s, Loss: 1.84279\n",
      "Epoch: 12, Time: 1.31780s, Loss: 1.76692\n",
      "Epoch: 13, Time: 1.21678s, Loss: 1.69607\n",
      "Epoch: 14, Time: 1.50236s, Loss: 1.62226\n",
      "Epoch: 15, Time: 1.37457s, Loss: 1.55436\n",
      "Epoch: 16, Time: 1.23136s, Loss: 1.48475\n",
      "Epoch: 17, Time: 1.41042s, Loss: 1.41734\n",
      "Epoch: 18, Time: 1.25632s, Loss: 1.34714\n",
      "Epoch: 19, Time: 1.30601s, Loss: 1.27656\n",
      "update best: 0.16500\n",
      "Epoch: 20, Time: 1.20952s, Loss: 1.20234\n",
      "Epoch: 21, Time: 1.19049s, Loss: 1.14656\n",
      "Epoch: 22, Time: 1.15300s, Loss: 1.08389\n",
      "Epoch: 23, Time: 1.26429s, Loss: 1.02032\n",
      "Epoch: 24, Time: 1.30182s, Loss: 0.95723\n",
      "Epoch: 25, Time: 1.42021s, Loss: 0.90450\n",
      "Epoch: 26, Time: 1.22268s, Loss: 0.84604\n",
      "Epoch: 27, Time: 1.20182s, Loss: 0.79651\n",
      "Epoch: 28, Time: 1.27569s, Loss: 0.74074\n",
      "Epoch: 29, Time: 1.19896s, Loss: 0.69620\n",
      "Epoch: 30, Time: 1.54059s, Loss: 0.66053\n",
      "Epoch: 31, Time: 1.35103s, Loss: 0.61037\n",
      "Epoch: 32, Time: 1.32461s, Loss: 0.56589\n",
      "update best: 0.17000\n",
      "Epoch: 33, Time: 1.33072s, Loss: 0.53298\n",
      "update best: 0.19000\n",
      "Epoch: 34, Time: 1.29055s, Loss: 0.49662\n",
      "update best: 0.21000\n",
      "Epoch: 35, Time: 1.22395s, Loss: 0.46070\n",
      "update best: 0.22500\n",
      "Epoch: 36, Time: 1.18261s, Loss: 0.43163\n",
      "Epoch: 37, Time: 1.18048s, Loss: 0.40783\n",
      "Epoch: 38, Time: 1.32629s, Loss: 0.37449\n",
      "Epoch: 39, Time: 1.18640s, Loss: 0.34452\n",
      "Epoch: 40, Time: 1.17261s, Loss: 0.32959\n",
      "Epoch: 41, Time: 1.35494s, Loss: 0.30877\n",
      "Epoch: 42, Time: 1.29722s, Loss: 0.29094\n",
      "update best: 0.23000\n",
      "Epoch: 43, Time: 1.23423s, Loss: 0.27251\n",
      "update best: 0.23500\n",
      "Epoch: 44, Time: 1.29604s, Loss: 0.25644\n",
      "Epoch: 45, Time: 1.26459s, Loss: 0.24776\n",
      "update best: 0.24500\n",
      "Epoch: 46, Time: 1.37385s, Loss: 0.23207\n",
      "Epoch: 47, Time: 1.25992s, Loss: 0.21182\n",
      "Epoch: 48, Time: 1.31817s, Loss: 0.20342\n",
      "Epoch: 49, Time: 1.46455s, Loss: 0.19555\n",
      "update best: 0.27500\n",
      "Epoch: 50, Time: 1.30664s, Loss: 0.18163\n",
      "update best: 0.28000\n",
      "Epoch: 51, Time: 1.44705s, Loss: 0.17652\n",
      "Epoch: 52, Time: 1.16958s, Loss: 0.16214\n",
      "Epoch: 53, Time: 1.26941s, Loss: 0.15734\n",
      "Epoch: 54, Time: 1.34628s, Loss: 0.14466\n",
      "Epoch: 55, Time: 1.22824s, Loss: 0.14203\n",
      "update best: 0.28500\n",
      "Epoch: 56, Time: 1.34575s, Loss: 0.13349\n",
      "update best: 0.33000\n",
      "Epoch: 57, Time: 1.28025s, Loss: 0.12913\n",
      "update best: 0.34500\n",
      "Epoch: 58, Time: 1.30383s, Loss: 0.12390\n",
      "Epoch: 59, Time: 1.24166s, Loss: 0.11869\n",
      "Epoch: 60, Time: 1.29664s, Loss: 0.11507\n",
      "Epoch: 61, Time: 1.29878s, Loss: 0.11152\n",
      "Epoch: 62, Time: 1.50177s, Loss: 0.10387\n",
      "Epoch: 63, Time: 1.30076s, Loss: 0.10022\n",
      "Epoch: 64, Time: 1.24141s, Loss: 0.09575\n",
      "update best: 0.36500\n",
      "Epoch: 65, Time: 1.22373s, Loss: 0.09613\n",
      "Epoch: 66, Time: 1.36058s, Loss: 0.09143\n",
      "Epoch: 67, Time: 1.39538s, Loss: 0.09173\n",
      "update best: 0.38500\n",
      "Epoch: 68, Time: 1.21041s, Loss: 0.08172\n",
      "Epoch: 69, Time: 1.31102s, Loss: 0.08590\n",
      "Epoch: 70, Time: 1.16226s, Loss: 0.08126\n",
      "update best: 0.39500\n",
      "Epoch: 71, Time: 1.38064s, Loss: 0.07688\n",
      "Epoch: 72, Time: 1.21638s, Loss: 0.07967\n",
      "Epoch: 73, Time: 1.16902s, Loss: 0.07509\n",
      "update best: 0.42500\n",
      "Epoch: 74, Time: 1.25318s, Loss: 0.07688\n",
      "Epoch: 75, Time: 1.19744s, Loss: 0.07053\n",
      "Epoch: 76, Time: 1.23429s, Loss: 0.06861\n",
      "Epoch: 77, Time: 1.18709s, Loss: 0.06849\n",
      "update best: 0.43500\n",
      "Epoch: 78, Time: 1.27665s, Loss: 0.06825\n",
      "Epoch: 79, Time: 1.34091s, Loss: 0.06182\n",
      "Epoch: 80, Time: 1.33787s, Loss: 0.06590\n",
      "Epoch: 81, Time: 1.26965s, Loss: 0.06319\n",
      "Epoch: 82, Time: 1.19577s, Loss: 0.06313\n",
      "Epoch: 83, Time: 1.32820s, Loss: 0.06400\n",
      "Epoch: 84, Time: 1.30356s, Loss: 0.06643\n",
      "Epoch: 85, Time: 1.32382s, Loss: 0.05816\n",
      "Epoch: 86, Time: 1.27220s, Loss: 0.05907\n",
      "Epoch: 87, Time: 1.24621s, Loss: 0.05757\n",
      "Epoch: 88, Time: 1.26966s, Loss: 0.05826\n",
      "Epoch: 89, Time: 1.16190s, Loss: 0.05518\n",
      "update best: 0.45000\n",
      "Epoch: 90, Time: 1.21202s, Loss: 0.05523\n",
      "Epoch: 91, Time: 1.17484s, Loss: 0.05426\n",
      "Epoch: 92, Time: 1.21003s, Loss: 0.05314\n",
      "Epoch: 93, Time: 1.17332s, Loss: 0.05175\n",
      "Epoch: 94, Time: 1.15069s, Loss: 0.05127\n",
      "update best: 0.45500\n",
      "Epoch: 95, Time: 1.19286s, Loss: 0.05020\n",
      "Epoch: 96, Time: 1.24532s, Loss: 0.05462\n",
      "Epoch: 97, Time: 1.27569s, Loss: 0.04893\n",
      "Epoch: 98, Time: 1.21614s, Loss: 0.05175\n",
      "Epoch: 99, Time: 1.39048s, Loss: 0.04687\n",
      "Epoch: 100, Time: 1.24326s, Loss: 0.04922\n",
      "Epoch: 101, Time: 1.24431s, Loss: 0.04435\n",
      "Epoch: 102, Time: 1.19226s, Loss: 0.04717\n",
      "Epoch: 103, Time: 1.35605s, Loss: 0.04634\n",
      "Epoch: 104, Time: 1.18235s, Loss: 0.04488\n",
      "Epoch: 105, Time: 1.17856s, Loss: 0.04129\n",
      "Epoch: 106, Time: 1.23227s, Loss: 0.04684\n",
      "Epoch: 107, Time: 1.26220s, Loss: 0.04706\n",
      "Epoch: 108, Time: 1.23621s, Loss: 0.04364\n",
      "Epoch: 109, Time: 1.18581s, Loss: 0.04682\n",
      "Epoch: 110, Time: 1.22108s, Loss: 0.04590\n",
      "Epoch: 111, Time: 1.33051s, Loss: 0.04269\n",
      "Epoch: 112, Time: 1.14877s, Loss: 0.04778\n",
      "Epoch: 113, Time: 1.16577s, Loss: 0.04367\n",
      "update best: 0.46000\n",
      "Epoch: 114, Time: 1.27170s, Loss: 0.04239\n",
      "Epoch: 115, Time: 1.19230s, Loss: 0.04223\n",
      "Epoch: 116, Time: 1.30985s, Loss: 0.04184\n",
      "Epoch: 117, Time: 1.34204s, Loss: 0.04116\n",
      "Epoch: 118, Time: 1.19596s, Loss: 0.04068\n",
      "Epoch: 119, Time: 1.23354s, Loss: 0.03786\n",
      "Epoch: 120, Time: 1.42493s, Loss: 0.04072\n",
      "update best: 0.47500\n",
      "Epoch: 121, Time: 1.24169s, Loss: 0.03664\n",
      "Epoch: 122, Time: 1.24111s, Loss: 0.03947\n",
      "Epoch: 123, Time: 1.23722s, Loss: 0.03868\n",
      "Epoch: 124, Time: 1.31976s, Loss: 0.03620\n",
      "Epoch: 125, Time: 1.19306s, Loss: 0.03710\n",
      "Epoch: 126, Time: 1.18464s, Loss: 0.03816\n",
      "Epoch: 127, Time: 1.23385s, Loss: 0.03773\n",
      "Epoch: 128, Time: 1.15674s, Loss: 0.03621\n",
      "Epoch: 129, Time: 1.22596s, Loss: 0.03730\n",
      "Epoch: 130, Time: 1.14205s, Loss: 0.03664\n",
      "Epoch: 131, Time: 1.15340s, Loss: 0.03373\n",
      "Epoch: 132, Time: 1.32747s, Loss: 0.03674\n",
      "Epoch: 133, Time: 1.22853s, Loss: 0.04360\n",
      "Epoch: 134, Time: 1.34267s, Loss: 0.03768\n",
      "Epoch: 135, Time: 1.41897s, Loss: 0.03557\n",
      "Epoch: 136, Time: 1.31278s, Loss: 0.03471\n",
      "Epoch: 137, Time: 1.20829s, Loss: 0.03593\n",
      "Epoch: 138, Time: 1.20432s, Loss: 0.03424\n",
      "Epoch: 139, Time: 1.29845s, Loss: 0.03539\n",
      "Epoch: 140, Time: 1.38204s, Loss: 0.03442\n",
      "Epoch: 141, Time: 1.27594s, Loss: 0.03629\n",
      "Epoch: 142, Time: 1.30157s, Loss: 0.03312\n",
      "Epoch: 143, Time: 1.18935s, Loss: 0.03841\n",
      "Epoch: 144, Time: 1.25200s, Loss: 0.04076\n",
      "Epoch: 145, Time: 1.23712s, Loss: 0.06876\n",
      "Epoch: 146, Time: 1.13729s, Loss: 0.03650\n",
      "Epoch: 147, Time: 1.23742s, Loss: 0.06332\n",
      "Epoch: 148, Time: 1.35226s, Loss: 0.04392\n",
      "Epoch: 149, Time: 1.48591s, Loss: 0.06565\n",
      "Epoch: 150, Time: 1.18588s, Loss: 0.04007\n",
      "Epoch: 151, Time: 1.20833s, Loss: 0.07563\n",
      "Epoch: 152, Time: 1.35463s, Loss: 0.03581\n",
      "Epoch: 153, Time: 1.26155s, Loss: 0.09672\n",
      "Epoch: 154, Time: 1.18569s, Loss: 0.05547\n",
      "Epoch: 155, Time: 1.18879s, Loss: 0.04690\n",
      "Epoch: 156, Time: 1.28119s, Loss: 0.05561\n",
      "Epoch: 157, Time: 1.25364s, Loss: 0.05781\n",
      "Epoch: 158, Time: 1.22963s, Loss: 0.04668\n",
      "Epoch: 159, Time: 1.18674s, Loss: 0.04285\n",
      "Epoch: 160, Time: 1.33488s, Loss: 0.04534\n",
      "Epoch: 161, Time: 1.34731s, Loss: 0.04604\n",
      "Epoch: 162, Time: 1.34935s, Loss: 0.03965\n",
      "Epoch: 163, Time: 1.26145s, Loss: 0.03661\n",
      "Epoch: 164, Time: 1.27685s, Loss: 0.03958\n",
      "Epoch: 165, Time: 1.24223s, Loss: 0.03301\n",
      "Epoch: 166, Time: 1.29492s, Loss: 0.03590\n",
      "Epoch: 167, Time: 1.16915s, Loss: 0.02961\n",
      "Epoch: 168, Time: 1.18475s, Loss: 0.03390\n",
      "Epoch: 169, Time: 1.22329s, Loss: 0.03066\n",
      "Epoch: 170, Time: 1.19592s, Loss: 0.03081\n",
      "Epoch: 171, Time: 1.24858s, Loss: 0.02582\n",
      "Epoch: 172, Time: 1.29168s, Loss: 0.02749\n",
      "Epoch: 173, Time: 1.19019s, Loss: 0.02687\n",
      "Epoch: 174, Time: 1.21539s, Loss: 0.02652\n",
      "Epoch: 175, Time: 1.30223s, Loss: 0.02704\n",
      "Epoch: 176, Time: 1.52252s, Loss: 0.02565\n",
      "Epoch: 177, Time: 1.42731s, Loss: 0.03043\n",
      "Epoch: 178, Time: 1.26753s, Loss: 0.02406\n",
      "Epoch: 179, Time: 2.60946s, Loss: 0.03481\n",
      "Epoch: 180, Time: 2.52241s, Loss: 0.02621\n",
      "Epoch: 181, Time: 2.60322s, Loss: 0.04958\n",
      "Epoch: 182, Time: 2.56016s, Loss: 0.03141\n",
      "Epoch: 183, Time: 2.58308s, Loss: 0.04558\n",
      "Epoch: 184, Time: 2.49239s, Loss: 0.05380\n",
      "Epoch: 185, Time: 2.60713s, Loss: 0.04330\n",
      "Epoch: 186, Time: 2.59470s, Loss: 0.03930\n",
      "Epoch: 187, Time: 2.72259s, Loss: 0.04647\n",
      "Epoch: 188, Time: 2.51598s, Loss: 0.04012\n",
      "Epoch: 189, Time: 2.54825s, Loss: 0.03249\n",
      "Epoch: 190, Time: 2.59999s, Loss: 0.03584\n",
      "Epoch: 191, Time: 2.76925s, Loss: 0.03525\n",
      "Epoch: 192, Time: 2.05392s, Loss: 0.03017\n",
      "Epoch: 193, Time: 1.00734s, Loss: 0.03143\n",
      "Epoch: 194, Time: 1.02639s, Loss: 0.03074\n",
      "Epoch: 195, Time: 1.19079s, Loss: 0.02683\n",
      "Epoch: 196, Time: 0.99288s, Loss: 0.02414\n",
      "Epoch: 197, Time: 1.00549s, Loss: 0.02461\n",
      "Epoch: 198, Time: 1.13103s, Loss: 0.02479\n",
      "Epoch: 199, Time: 1.02974s, Loss: 0.02296\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 120\n",
      "{'accuracy': 0.45623305439949036, 'f1_score': 0.3653164109596893, 'f1_score -> average@micro': 0.4562330429815793}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 1.27188s, Loss: 3.00386\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 1.54415s, Loss: 2.70985\n",
      "Epoch: 2, Time: 1.50128s, Loss: 2.37330\n",
      "Epoch: 3, Time: 1.81600s, Loss: 2.18877\n",
      "Epoch: 4, Time: 1.50025s, Loss: 2.05079\n",
      "Epoch: 5, Time: 1.49115s, Loss: 1.92147\n",
      "Epoch: 6, Time: 1.39025s, Loss: 1.80867\n",
      "update best: 0.09000\n",
      "Epoch: 7, Time: 1.39048s, Loss: 1.68405\n",
      "update best: 0.09500\n",
      "Epoch: 8, Time: 1.40198s, Loss: 1.55780\n",
      "Epoch: 9, Time: 1.32872s, Loss: 1.45725\n",
      "Epoch: 10, Time: 1.33453s, Loss: 1.36144\n",
      "Epoch: 11, Time: 1.98168s, Loss: 1.23599\n",
      "Epoch: 12, Time: 1.50997s, Loss: 1.14826\n",
      "Epoch: 13, Time: 1.64627s, Loss: 1.03714\n",
      "Epoch: 14, Time: 1.72242s, Loss: 0.94961\n",
      "update best: 0.10000\n",
      "Epoch: 15, Time: 1.53512s, Loss: 0.87812\n",
      "update best: 0.10500\n",
      "Epoch: 16, Time: 1.35259s, Loss: 0.79208\n",
      "Epoch: 17, Time: 1.35914s, Loss: 0.72543\n",
      "Epoch: 18, Time: 1.38262s, Loss: 0.65884\n",
      "Epoch: 19, Time: 1.37241s, Loss: 0.59730\n",
      "Epoch: 20, Time: 1.41043s, Loss: 0.55186\n",
      "Epoch: 21, Time: 1.45364s, Loss: 0.48377\n",
      "Epoch: 22, Time: 1.33518s, Loss: 0.44027\n",
      "Epoch: 23, Time: 1.42071s, Loss: 0.39254\n",
      "Epoch: 24, Time: 1.34800s, Loss: 0.35922\n",
      "Epoch: 25, Time: 1.33080s, Loss: 0.31876\n",
      "Epoch: 26, Time: 1.29542s, Loss: 0.29382\n",
      "Epoch: 27, Time: 1.43415s, Loss: 0.25786\n",
      "Epoch: 28, Time: 1.37351s, Loss: 0.23379\n",
      "Epoch: 29, Time: 1.43001s, Loss: 0.21166\n",
      "update best: 0.11000\n",
      "Epoch: 30, Time: 1.27605s, Loss: 0.18889\n",
      "Epoch: 31, Time: 1.30442s, Loss: 0.17234\n",
      "Epoch: 32, Time: 1.25549s, Loss: 0.15468\n",
      "update best: 0.12000\n",
      "Epoch: 33, Time: 1.28189s, Loss: 0.14374\n",
      "update best: 0.12500\n",
      "Epoch: 34, Time: 1.25804s, Loss: 0.13073\n",
      "Epoch: 35, Time: 1.24336s, Loss: 0.11533\n",
      "update best: 0.13500\n",
      "Epoch: 36, Time: 1.28270s, Loss: 0.10756\n",
      "update best: 0.14000\n",
      "Epoch: 37, Time: 1.22811s, Loss: 0.09825\n",
      "update best: 0.15000\n",
      "Epoch: 38, Time: 1.37117s, Loss: 0.09213\n",
      "update best: 0.15500\n",
      "Epoch: 39, Time: 1.32696s, Loss: 0.08591\n",
      "Epoch: 40, Time: 1.31704s, Loss: 0.08078\n",
      "Epoch: 41, Time: 1.43949s, Loss: 0.07544\n",
      "Epoch: 42, Time: 1.37561s, Loss: 0.07192\n",
      "Epoch: 43, Time: 1.38291s, Loss: 0.06496\n",
      "Epoch: 44, Time: 1.30007s, Loss: 0.06504\n",
      "Epoch: 45, Time: 1.34068s, Loss: 0.06036\n",
      "Epoch: 46, Time: 1.35244s, Loss: 0.05478\n",
      "Epoch: 47, Time: 1.37770s, Loss: 0.05395\n",
      "Epoch: 48, Time: 1.43613s, Loss: 0.05261\n",
      "Epoch: 49, Time: 1.39924s, Loss: 0.05150\n",
      "Epoch: 50, Time: 1.44943s, Loss: 0.04703\n",
      "Epoch: 51, Time: 1.46634s, Loss: 0.04725\n",
      "Epoch: 52, Time: 1.47481s, Loss: 0.04718\n",
      "Epoch: 53, Time: 1.44230s, Loss: 0.04429\n",
      "update best: 0.16500\n",
      "Epoch: 54, Time: 1.46572s, Loss: 0.04436\n",
      "update best: 0.19500\n",
      "Epoch: 55, Time: 1.43431s, Loss: 0.04274\n",
      "update best: 0.21000\n",
      "Epoch: 56, Time: 1.49898s, Loss: 0.04083\n",
      "Epoch: 57, Time: 1.40041s, Loss: 0.04188\n",
      "update best: 0.23500\n",
      "Epoch: 58, Time: 1.43422s, Loss: 0.04199\n",
      "update best: 0.25000\n",
      "Epoch: 59, Time: 1.37231s, Loss: 0.04150\n",
      "update best: 0.28000\n",
      "Epoch: 60, Time: 1.53890s, Loss: 0.03965\n",
      "update best: 0.28500\n",
      "Epoch: 61, Time: 1.41634s, Loss: 0.03752\n",
      "Epoch: 62, Time: 1.51327s, Loss: 0.03809\n",
      "Epoch: 63, Time: 1.36452s, Loss: 0.03837\n",
      "Epoch: 64, Time: 1.44806s, Loss: 0.03919\n",
      "Epoch: 65, Time: 1.47120s, Loss: 0.03748\n",
      "Epoch: 66, Time: 1.32771s, Loss: 0.03714\n",
      "Epoch: 67, Time: 1.32778s, Loss: 0.03501\n",
      "Epoch: 68, Time: 1.31287s, Loss: 0.03392\n",
      "update best: 0.31000\n",
      "Epoch: 69, Time: 1.32246s, Loss: 0.03251\n",
      "update best: 0.34500\n",
      "Epoch: 70, Time: 1.40177s, Loss: 0.03559\n",
      "update best: 0.37000\n",
      "Epoch: 71, Time: 1.32568s, Loss: 0.03304\n",
      "Epoch: 72, Time: 1.42692s, Loss: 0.03425\n",
      "Epoch: 73, Time: 1.72085s, Loss: 0.03320\n",
      "update best: 0.37500\n",
      "Epoch: 74, Time: 1.43077s, Loss: 0.03200\n",
      "Epoch: 75, Time: 1.35294s, Loss: 0.03212\n",
      "update best: 0.39000\n",
      "Epoch: 76, Time: 1.30638s, Loss: 0.03018\n",
      "Epoch: 77, Time: 1.30985s, Loss: 0.03008\n",
      "update best: 0.39500\n",
      "Epoch: 78, Time: 1.31392s, Loss: 0.03139\n",
      "Epoch: 79, Time: 1.43192s, Loss: 0.02849\n",
      "update best: 0.40000\n",
      "Epoch: 80, Time: 1.40814s, Loss: 0.03067\n",
      "update best: 0.41000\n",
      "Epoch: 81, Time: 1.31110s, Loss: 0.02883\n",
      "update best: 0.45500\n",
      "Epoch: 82, Time: 1.43553s, Loss: 0.02801\n",
      "update best: 0.46000\n",
      "Epoch: 83, Time: 1.42795s, Loss: 0.02717\n",
      "update best: 0.47500\n",
      "Epoch: 84, Time: 1.36155s, Loss: 0.02615\n",
      "Epoch: 85, Time: 1.50270s, Loss: 0.02605\n",
      "Epoch: 86, Time: 1.58583s, Loss: 0.02592\n",
      "Epoch: 87, Time: 1.50066s, Loss: 0.02628\n",
      "Epoch: 88, Time: 1.85431s, Loss: 0.02556\n",
      "Epoch: 89, Time: 1.68659s, Loss: 0.02421\n",
      "Epoch: 90, Time: 1.32606s, Loss: 0.02532\n",
      "Epoch: 91, Time: 1.39409s, Loss: 0.02611\n",
      "Epoch: 92, Time: 1.45525s, Loss: 0.02378\n",
      "Epoch: 93, Time: 1.37677s, Loss: 0.02510\n",
      "Epoch: 94, Time: 1.31949s, Loss: 0.02376\n",
      "Epoch: 95, Time: 1.29086s, Loss: 0.02301\n",
      "Epoch: 96, Time: 1.28152s, Loss: 0.02325\n",
      "Epoch: 97, Time: 1.22093s, Loss: 0.02275\n",
      "Epoch: 98, Time: 1.31907s, Loss: 0.02393\n",
      "update best: 0.48500\n",
      "Epoch: 99, Time: 1.37080s, Loss: 0.02081\n",
      "Epoch: 100, Time: 1.40924s, Loss: 0.02302\n",
      "Epoch: 101, Time: 1.69402s, Loss: 0.02299\n",
      "update best: 0.49500\n",
      "Epoch: 102, Time: 1.32635s, Loss: 0.02197\n",
      "Epoch: 103, Time: 1.33762s, Loss: 0.02165\n",
      "Epoch: 104, Time: 1.32808s, Loss: 0.02216\n",
      "Epoch: 105, Time: 1.32325s, Loss: 0.02217\n",
      "Epoch: 106, Time: 1.39723s, Loss: 0.02029\n",
      "Epoch: 107, Time: 1.37769s, Loss: 0.02081\n",
      "Epoch: 108, Time: 1.46459s, Loss: 0.02086\n",
      "Epoch: 109, Time: 1.31666s, Loss: 0.01997\n",
      "Epoch: 110, Time: 1.33238s, Loss: 0.02177\n",
      "Epoch: 111, Time: 1.56508s, Loss: 0.02051\n",
      "Epoch: 112, Time: 1.42414s, Loss: 0.02007\n",
      "Epoch: 113, Time: 1.27276s, Loss: 0.02031\n",
      "Epoch: 114, Time: 1.27076s, Loss: 0.01989\n",
      "Epoch: 115, Time: 1.33571s, Loss: 0.01988\n",
      "Epoch: 116, Time: 1.46237s, Loss: 0.02025\n",
      "Epoch: 117, Time: 1.29031s, Loss: 0.01742\n",
      "Epoch: 118, Time: 1.35665s, Loss: 0.01813\n",
      "update best: 0.50000\n",
      "Epoch: 119, Time: 1.33894s, Loss: 0.01844\n",
      "Epoch: 120, Time: 1.48070s, Loss: 0.01895\n",
      "Epoch: 121, Time: 1.43016s, Loss: 0.01908\n",
      "Epoch: 122, Time: 1.44818s, Loss: 0.01838\n",
      "Epoch: 123, Time: 1.33738s, Loss: 0.01817\n",
      "Epoch: 124, Time: 1.56268s, Loss: 0.01969\n",
      "Epoch: 125, Time: 1.36915s, Loss: 0.01744\n",
      "Epoch: 126, Time: 1.29028s, Loss: 0.01882\n",
      "Epoch: 127, Time: 1.34672s, Loss: 0.01783\n",
      "Epoch: 128, Time: 1.47206s, Loss: 0.01902\n",
      "Epoch: 129, Time: 1.49847s, Loss: 0.01799\n",
      "Epoch: 130, Time: 1.34965s, Loss: 0.01803\n",
      "Epoch: 131, Time: 1.35305s, Loss: 0.01799\n",
      "Epoch: 132, Time: 1.27873s, Loss: 0.01825\n",
      "Epoch: 133, Time: 1.29053s, Loss: 0.01852\n",
      "Epoch: 134, Time: 1.37004s, Loss: 0.01676\n",
      "Epoch: 135, Time: 1.32739s, Loss: 0.01684\n",
      "Epoch: 136, Time: 1.29580s, Loss: 0.01663\n",
      "Epoch: 137, Time: 1.31958s, Loss: 0.01641\n",
      "Epoch: 138, Time: 1.31170s, Loss: 0.01648\n",
      "Epoch: 139, Time: 1.30674s, Loss: 0.01749\n",
      "Epoch: 140, Time: 1.28187s, Loss: 0.01731\n",
      "Epoch: 141, Time: 1.27024s, Loss: 0.01744\n",
      "Epoch: 142, Time: 1.33814s, Loss: 0.01645\n",
      "Epoch: 143, Time: 1.42886s, Loss: 0.01751\n",
      "Epoch: 144, Time: 1.47848s, Loss: 0.01728\n",
      "Epoch: 145, Time: 1.38144s, Loss: 0.01801\n",
      "Epoch: 146, Time: 1.46823s, Loss: 0.01696\n",
      "Epoch: 147, Time: 1.45598s, Loss: 0.02160\n",
      "Epoch: 148, Time: 1.72187s, Loss: 0.01769\n",
      "Epoch: 149, Time: 1.70991s, Loss: 0.02529\n",
      "Epoch: 150, Time: 1.31159s, Loss: 0.02321\n",
      "Epoch: 151, Time: 1.46281s, Loss: 0.03171\n",
      "Epoch: 152, Time: 1.41087s, Loss: 0.01670\n",
      "Epoch: 153, Time: 1.51190s, Loss: 0.04494\n",
      "Epoch: 154, Time: 1.42654s, Loss: 0.01860\n",
      "Epoch: 155, Time: 1.53499s, Loss: 0.02177\n",
      "Epoch: 156, Time: 1.52821s, Loss: 0.03615\n",
      "Epoch: 157, Time: 1.38554s, Loss: 0.01814\n",
      "Epoch: 158, Time: 1.37736s, Loss: 0.01865\n",
      "Epoch: 159, Time: 1.54945s, Loss: 0.02191\n",
      "Epoch: 160, Time: 1.44442s, Loss: 0.02447\n",
      "Epoch: 161, Time: 1.44328s, Loss: 0.02430\n",
      "Epoch: 162, Time: 1.43570s, Loss: 0.01935\n",
      "Epoch: 163, Time: 1.39545s, Loss: 0.01849\n",
      "Epoch: 164, Time: 1.39675s, Loss: 0.01774\n",
      "Epoch: 165, Time: 1.39179s, Loss: 0.01589\n",
      "Epoch: 166, Time: 1.36605s, Loss: 0.01702\n",
      "Epoch: 167, Time: 1.40766s, Loss: 0.01585\n",
      "Epoch: 168, Time: 1.46108s, Loss: 0.01639\n",
      "Epoch: 169, Time: 1.38070s, Loss: 0.01560\n",
      "Epoch: 170, Time: 1.45014s, Loss: 0.01578\n",
      "Epoch: 171, Time: 1.57744s, Loss: 0.01494\n",
      "Epoch: 172, Time: 1.47759s, Loss: 0.01431\n",
      "Epoch: 173, Time: 1.40481s, Loss: 0.01409\n",
      "Epoch: 174, Time: 1.49832s, Loss: 0.01369\n",
      "Epoch: 175, Time: 1.50229s, Loss: 0.01306\n",
      "Epoch: 176, Time: 1.45149s, Loss: 0.01334\n",
      "Epoch: 177, Time: 1.46486s, Loss: 0.01242\n",
      "Epoch: 178, Time: 1.63365s, Loss: 0.01266\n",
      "Epoch: 179, Time: 2.42034s, Loss: 0.01165\n",
      "Epoch: 180, Time: 1.51814s, Loss: 0.01231\n",
      "Epoch: 181, Time: 1.64823s, Loss: 0.01221\n",
      "Epoch: 182, Time: 1.37344s, Loss: 0.01222\n",
      "Epoch: 183, Time: 1.35596s, Loss: 0.01264\n",
      "Epoch: 184, Time: 1.41736s, Loss: 0.01289\n",
      "Epoch: 185, Time: 1.38307s, Loss: 0.01289\n",
      "Epoch: 186, Time: 1.40860s, Loss: 0.01329\n",
      "Epoch: 187, Time: 1.40430s, Loss: 0.01247\n",
      "Epoch: 188, Time: 1.48603s, Loss: 0.01202\n",
      "Epoch: 189, Time: 1.38514s, Loss: 0.01232\n",
      "Epoch: 190, Time: 1.34340s, Loss: 0.01294\n",
      "Epoch: 191, Time: 1.43782s, Loss: 0.01216\n",
      "Epoch: 192, Time: 1.48778s, Loss: 0.01211\n",
      "Epoch: 193, Time: 1.38422s, Loss: 0.01375\n",
      "Epoch: 194, Time: 1.39332s, Loss: 0.01281\n",
      "Epoch: 195, Time: 1.32660s, Loss: 0.01381\n",
      "Epoch: 196, Time: 1.30333s, Loss: 0.01335\n",
      "Epoch: 197, Time: 1.49123s, Loss: 0.01330\n",
      "Epoch: 198, Time: 1.79635s, Loss: 0.01353\n",
      "Epoch: 199, Time: 1.50184s, Loss: 0.01235\n",
      "\n",
      "train finished!\n",
      "best val: 0.50000\n",
      "test...\n",
      "final result: epoch: 118\n",
      "{'accuracy': 0.5289161801338196, 'f1_score': 0.40503309054521114, 'f1_score -> average@micro': 0.5289161787805227}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.10855s, Loss: 3.00464\n",
      "update best: 0.05000\n",
      "Epoch: 1, Time: 0.08103s, Loss: 2.85373\n",
      "Epoch: 2, Time: 0.09561s, Loss: 2.39547\n",
      "Epoch: 3, Time: 0.08431s, Loss: 2.16992\n",
      "Epoch: 4, Time: 0.08556s, Loss: 1.98238\n",
      "Epoch: 5, Time: 0.08011s, Loss: 1.82536\n",
      "update best: 0.07500\n",
      "Epoch: 6, Time: 0.09422s, Loss: 1.69112\n",
      "update best: 0.08000\n",
      "Epoch: 7, Time: 0.08904s, Loss: 1.55686\n",
      "Epoch: 8, Time: 0.09059s, Loss: 1.43632\n",
      "update best: 0.11000\n",
      "Epoch: 9, Time: 0.07604s, Loss: 1.31332\n",
      "update best: 0.22000\n",
      "Epoch: 10, Time: 0.08363s, Loss: 1.20795\n",
      "Epoch: 11, Time: 0.07904s, Loss: 1.09272\n",
      "Epoch: 12, Time: 0.07904s, Loss: 1.00381\n",
      "Epoch: 13, Time: 0.14408s, Loss: 0.90282\n",
      "Epoch: 14, Time: 0.08704s, Loss: 0.81816\n",
      "Epoch: 15, Time: 0.07804s, Loss: 0.74913\n",
      "Epoch: 16, Time: 0.08156s, Loss: 0.66262\n",
      "Epoch: 17, Time: 0.08205s, Loss: 0.60239\n",
      "Epoch: 18, Time: 0.11778s, Loss: 0.53853\n",
      "Epoch: 19, Time: 0.09161s, Loss: 0.48864\n",
      "Epoch: 20, Time: 0.09068s, Loss: 0.42724\n",
      "Epoch: 21, Time: 0.08355s, Loss: 0.38623\n",
      "Epoch: 22, Time: 0.10086s, Loss: 0.34136\n",
      "Epoch: 23, Time: 0.09659s, Loss: 0.30420\n",
      "Epoch: 24, Time: 0.09107s, Loss: 0.27242\n",
      "Epoch: 25, Time: 0.08208s, Loss: 0.23802\n",
      "Epoch: 26, Time: 0.09509s, Loss: 0.21636\n",
      "Epoch: 27, Time: 0.08206s, Loss: 0.19498\n",
      "Epoch: 28, Time: 0.08175s, Loss: 0.17500\n",
      "Epoch: 29, Time: 0.08208s, Loss: 0.15270\n",
      "Epoch: 30, Time: 0.07806s, Loss: 0.13763\n",
      "Epoch: 31, Time: 0.09466s, Loss: 0.12347\n",
      "Epoch: 32, Time: 0.09861s, Loss: 0.11316\n",
      "Epoch: 33, Time: 0.09258s, Loss: 0.10082\n",
      "Epoch: 34, Time: 0.09119s, Loss: 0.09246\n",
      "Epoch: 35, Time: 0.08960s, Loss: 0.08394\n",
      "Epoch: 36, Time: 0.08559s, Loss: 0.07804\n",
      "Epoch: 37, Time: 0.08110s, Loss: 0.07072\n",
      "Epoch: 38, Time: 0.09057s, Loss: 0.06743\n",
      "Epoch: 39, Time: 0.10018s, Loss: 0.06343\n",
      "Epoch: 40, Time: 0.09457s, Loss: 0.05752\n",
      "Epoch: 41, Time: 0.09508s, Loss: 0.05332\n",
      "Epoch: 42, Time: 0.08177s, Loss: 0.05070\n",
      "Epoch: 43, Time: 0.08888s, Loss: 0.04732\n",
      "Epoch: 44, Time: 0.09961s, Loss: 0.04493\n",
      "Epoch: 45, Time: 0.08239s, Loss: 0.04645\n",
      "Epoch: 46, Time: 0.09278s, Loss: 0.04098\n",
      "Epoch: 47, Time: 0.10937s, Loss: 0.04156\n",
      "Epoch: 48, Time: 0.09224s, Loss: 0.03786\n",
      "update best: 0.23500\n",
      "Epoch: 49, Time: 0.08105s, Loss: 0.03783\n",
      "update best: 0.25500\n",
      "Epoch: 50, Time: 0.09759s, Loss: 0.03815\n",
      "update best: 0.26500\n",
      "Epoch: 51, Time: 0.11816s, Loss: 0.03539\n",
      "Epoch: 52, Time: 0.12713s, Loss: 0.03394\n",
      "Epoch: 53, Time: 0.11458s, Loss: 0.03498\n",
      "Epoch: 54, Time: 0.11960s, Loss: 0.03471\n",
      "Epoch: 55, Time: 0.12266s, Loss: 0.03187\n",
      "Epoch: 56, Time: 0.11107s, Loss: 0.03311\n",
      "Epoch: 57, Time: 0.10058s, Loss: 0.03236\n",
      "Epoch: 58, Time: 0.12010s, Loss: 0.03206\n",
      "Epoch: 59, Time: 0.11112s, Loss: 0.03009\n",
      "Epoch: 60, Time: 0.11309s, Loss: 0.03027\n",
      "Epoch: 61, Time: 0.10956s, Loss: 0.03062\n",
      "Epoch: 62, Time: 0.11258s, Loss: 0.03146\n",
      "Epoch: 63, Time: 0.10657s, Loss: 0.02908\n",
      "Epoch: 64, Time: 0.12109s, Loss: 0.03267\n",
      "update best: 0.27000\n",
      "Epoch: 65, Time: 0.10493s, Loss: 0.02796\n",
      "update best: 0.28500\n",
      "Epoch: 66, Time: 0.11657s, Loss: 0.02921\n",
      "update best: 0.31000\n",
      "Epoch: 67, Time: 0.10761s, Loss: 0.02787\n",
      "Epoch: 68, Time: 0.11518s, Loss: 0.02633\n",
      "update best: 0.31500\n",
      "Epoch: 69, Time: 0.11161s, Loss: 0.02667\n",
      "update best: 0.32500\n",
      "Epoch: 70, Time: 0.10609s, Loss: 0.02667\n",
      "update best: 0.33000\n",
      "Epoch: 71, Time: 0.11265s, Loss: 0.02442\n",
      "update best: 0.33500\n",
      "Epoch: 72, Time: 0.10656s, Loss: 0.02611\n",
      "update best: 0.35000\n",
      "Epoch: 73, Time: 0.11760s, Loss: 0.02556\n",
      "Epoch: 74, Time: 0.11409s, Loss: 0.02474\n",
      "Epoch: 75, Time: 0.11714s, Loss: 0.02414\n",
      "Epoch: 76, Time: 0.10258s, Loss: 0.02310\n",
      "Epoch: 77, Time: 0.12112s, Loss: 0.02235\n",
      "Epoch: 78, Time: 0.11957s, Loss: 0.02200\n",
      "Epoch: 79, Time: 0.11412s, Loss: 0.02231\n",
      "Epoch: 80, Time: 0.10962s, Loss: 0.02164\n",
      "update best: 0.35500\n",
      "Epoch: 81, Time: 0.12378s, Loss: 0.02111\n",
      "update best: 0.41000\n",
      "Epoch: 82, Time: 0.11511s, Loss: 0.02094\n",
      "update best: 0.44500\n",
      "Epoch: 83, Time: 0.10360s, Loss: 0.02243\n",
      "Epoch: 84, Time: 0.11059s, Loss: 0.02188\n",
      "Epoch: 85, Time: 0.11810s, Loss: 0.02204\n",
      "Epoch: 86, Time: 0.12082s, Loss: 0.01969\n",
      "Epoch: 87, Time: 0.11518s, Loss: 0.02077\n",
      "Epoch: 88, Time: 0.11263s, Loss: 0.01974\n",
      "Epoch: 89, Time: 0.10610s, Loss: 0.01937\n",
      "Epoch: 90, Time: 0.11911s, Loss: 0.02079\n",
      "Epoch: 91, Time: 0.12612s, Loss: 0.01972\n",
      "Epoch: 92, Time: 0.12459s, Loss: 0.01940\n",
      "Epoch: 93, Time: 0.13528s, Loss: 0.01956\n",
      "Epoch: 94, Time: 0.21866s, Loss: 0.01888\n",
      "Epoch: 95, Time: 0.11109s, Loss: 0.01911\n",
      "Epoch: 96, Time: 0.12051s, Loss: 0.01821\n",
      "Epoch: 97, Time: 0.11910s, Loss: 0.01749\n",
      "Epoch: 98, Time: 0.11008s, Loss: 0.01748\n",
      "Epoch: 99, Time: 0.11158s, Loss: 0.01779\n",
      "Epoch: 100, Time: 0.11512s, Loss: 0.01833\n",
      "Epoch: 101, Time: 0.11421s, Loss: 0.01708\n",
      "Epoch: 102, Time: 0.12238s, Loss: 0.01692\n",
      "Epoch: 103, Time: 0.11256s, Loss: 0.01745\n",
      "update best: 0.45000\n",
      "Epoch: 104, Time: 0.10949s, Loss: 0.01689\n",
      "Epoch: 105, Time: 0.11056s, Loss: 0.01643\n",
      "Epoch: 106, Time: 0.10858s, Loss: 0.01698\n",
      "Epoch: 107, Time: 0.12488s, Loss: 0.01411\n",
      "update best: 0.45500\n",
      "Epoch: 108, Time: 0.11812s, Loss: 0.01557\n",
      "Epoch: 109, Time: 0.11909s, Loss: 0.01596\n",
      "update best: 0.46000\n",
      "Epoch: 110, Time: 0.10959s, Loss: 0.01623\n",
      "Epoch: 111, Time: 0.11502s, Loss: 0.01557\n",
      "update best: 0.47500\n",
      "Epoch: 112, Time: 0.10019s, Loss: 0.01667\n",
      "Epoch: 113, Time: 0.11613s, Loss: 0.01662\n",
      "Epoch: 114, Time: 0.10405s, Loss: 0.01519\n",
      "Epoch: 115, Time: 0.12112s, Loss: 0.01566\n",
      "Epoch: 116, Time: 0.11459s, Loss: 0.01502\n",
      "Epoch: 117, Time: 0.11607s, Loss: 0.01577\n",
      "Epoch: 118, Time: 0.11427s, Loss: 0.01539\n",
      "Epoch: 119, Time: 0.11919s, Loss: 0.01461\n",
      "Epoch: 120, Time: 0.10885s, Loss: 0.01566\n",
      "Epoch: 121, Time: 0.12457s, Loss: 0.01513\n",
      "Epoch: 122, Time: 0.12417s, Loss: 0.01506\n",
      "Epoch: 123, Time: 0.12285s, Loss: 0.01606\n",
      "Epoch: 124, Time: 0.12110s, Loss: 0.01423\n",
      "Epoch: 125, Time: 0.11711s, Loss: 0.01454\n",
      "Epoch: 126, Time: 0.12613s, Loss: 0.01424\n",
      "Epoch: 127, Time: 0.10858s, Loss: 0.01419\n",
      "Epoch: 128, Time: 0.10856s, Loss: 0.01394\n",
      "Epoch: 129, Time: 0.10873s, Loss: 0.01380\n",
      "Epoch: 130, Time: 0.10656s, Loss: 0.01443\n",
      "Epoch: 131, Time: 0.11498s, Loss: 0.01307\n",
      "Epoch: 132, Time: 0.11861s, Loss: 0.01310\n",
      "Epoch: 133, Time: 0.12256s, Loss: 0.01282\n",
      "Epoch: 134, Time: 0.11357s, Loss: 0.01314\n",
      "Epoch: 135, Time: 0.11219s, Loss: 0.01436\n",
      "Epoch: 136, Time: 0.10663s, Loss: 0.01356\n",
      "Epoch: 137, Time: 0.11052s, Loss: 0.01348\n",
      "Epoch: 138, Time: 0.11165s, Loss: 0.01287\n",
      "Epoch: 139, Time: 0.11514s, Loss: 0.01285\n",
      "Epoch: 140, Time: 0.16839s, Loss: 0.01282\n",
      "Epoch: 141, Time: 0.12262s, Loss: 0.01282\n",
      "Epoch: 142, Time: 0.11057s, Loss: 0.01368\n",
      "Epoch: 143, Time: 0.11360s, Loss: 0.01278\n",
      "Epoch: 144, Time: 0.10558s, Loss: 0.01363\n",
      "Epoch: 145, Time: 0.11208s, Loss: 0.01316\n",
      "Epoch: 146, Time: 0.11662s, Loss: 0.01272\n",
      "Epoch: 147, Time: 0.11758s, Loss: 0.01261\n",
      "Epoch: 148, Time: 0.12404s, Loss: 0.01321\n",
      "Epoch: 149, Time: 0.10555s, Loss: 0.01308\n",
      "Epoch: 150, Time: 0.12204s, Loss: 0.01287\n",
      "Epoch: 151, Time: 0.11496s, Loss: 0.01174\n",
      "Epoch: 152, Time: 0.10958s, Loss: 0.01301\n",
      "Epoch: 153, Time: 0.11310s, Loss: 0.01285\n",
      "Epoch: 154, Time: 0.11858s, Loss: 0.01269\n",
      "Epoch: 155, Time: 0.12313s, Loss: 0.01245\n",
      "Epoch: 156, Time: 0.10827s, Loss: 0.01330\n",
      "Epoch: 157, Time: 0.12212s, Loss: 0.01280\n",
      "Epoch: 158, Time: 0.10757s, Loss: 0.01255\n",
      "Epoch: 159, Time: 0.10765s, Loss: 0.01220\n",
      "Epoch: 160, Time: 0.12216s, Loss: 0.01134\n",
      "Epoch: 161, Time: 0.12055s, Loss: 0.01127\n",
      "Epoch: 162, Time: 0.10458s, Loss: 0.01153\n",
      "Epoch: 163, Time: 0.10329s, Loss: 0.01129\n",
      "Epoch: 164, Time: 0.10956s, Loss: 0.01120\n",
      "Epoch: 165, Time: 0.10456s, Loss: 0.01126\n",
      "Epoch: 166, Time: 0.10960s, Loss: 0.01091\n",
      "Epoch: 167, Time: 0.10909s, Loss: 0.01097\n",
      "Epoch: 168, Time: 0.12124s, Loss: 0.01180\n",
      "Epoch: 169, Time: 0.12729s, Loss: 0.01197\n",
      "Epoch: 170, Time: 0.11857s, Loss: 0.01069\n",
      "Epoch: 171, Time: 0.10259s, Loss: 0.01087\n",
      "Epoch: 172, Time: 0.11161s, Loss: 0.01150\n",
      "Epoch: 173, Time: 0.10921s, Loss: 0.01196\n",
      "Epoch: 174, Time: 0.10658s, Loss: 0.01160\n",
      "Epoch: 175, Time: 0.11360s, Loss: 0.01139\n",
      "Epoch: 176, Time: 0.11757s, Loss: 0.01102\n",
      "Epoch: 177, Time: 0.10807s, Loss: 0.01557\n",
      "Epoch: 178, Time: 0.11157s, Loss: 0.03755\n",
      "Epoch: 179, Time: 0.10435s, Loss: 0.01259\n",
      "Epoch: 180, Time: 0.11604s, Loss: 0.04858\n",
      "Epoch: 181, Time: 0.10258s, Loss: 0.04517\n",
      "Epoch: 182, Time: 0.12508s, Loss: 0.01797\n",
      "Epoch: 183, Time: 0.11606s, Loss: 0.02016\n",
      "Epoch: 184, Time: 0.12010s, Loss: 0.02098\n",
      "Epoch: 185, Time: 0.10121s, Loss: 0.02448\n",
      "Epoch: 186, Time: 0.11561s, Loss: 0.02332\n",
      "Epoch: 187, Time: 0.11158s, Loss: 0.02146\n",
      "Epoch: 188, Time: 0.11559s, Loss: 0.01876\n",
      "Epoch: 189, Time: 0.10807s, Loss: 0.01704\n",
      "Epoch: 190, Time: 0.11109s, Loss: 0.01545\n",
      "Epoch: 191, Time: 0.11456s, Loss: 0.01565\n",
      "Epoch: 192, Time: 0.10509s, Loss: 0.01590\n",
      "Epoch: 193, Time: 0.11213s, Loss: 0.01600\n",
      "Epoch: 194, Time: 0.10910s, Loss: 0.01321\n",
      "Epoch: 195, Time: 0.11056s, Loss: 0.01114\n",
      "Epoch: 196, Time: 0.10511s, Loss: 0.01090\n",
      "Epoch: 197, Time: 0.10709s, Loss: 0.01140\n",
      "Epoch: 198, Time: 0.10055s, Loss: 0.01134\n",
      "Epoch: 199, Time: 0.10959s, Loss: 0.01008\n",
      "\n",
      "train finished!\n",
      "best val: 0.47500\n",
      "test...\n",
      "final result: epoch: 111\n",
      "{'accuracy': 0.4370983839035034, 'f1_score': 0.3630498297451491, 'f1_score -> average@micro': 0.4370983864058261}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-citation Citaseer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "This is cocitation_citeseer dataset:\n",
       "  ->  num_classes\n",
       "  ->  num_vertices\n",
       "  ->  num_edges\n",
       "  ->  dim_features\n",
       "  ->  features\n",
       "  ->  edge_list\n",
       "  ->  labels\n",
       "  ->  train_mask\n",
       "  ->  val_mask\n",
       "  ->  test_mask\n",
       "Please try `data['name']` to get the specified data."
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = r'.\\datasets\\dhg_datasets'\n",
    "data = dhg.data.CocitationCiteseer(data_path)\n",
    "\n",
    "if not 'train_mask' in data.content:\n",
    "    train_mask, test_mask, val_mask = split_by_ratio(\n",
    "        num_v = data[\"num_vertices\"],\n",
    "        v_label = data[\"labels\"],\n",
    "        train_ratio = 0.6,\n",
    "        test_ratio = 0.2,\n",
    "        val_ratio = 0.2\n",
    "        )\n",
    "\n",
    "    data._content.update({\"train_mask\": train_mask, \"test_mask\": test_mask, \"val_mask\": val_mask})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(data['num_vertices'])\n",
    "data['num_edges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.07557s, Loss: 1.84384\n",
      "update best: 0.18053\n",
      "Epoch: 1, Time: 0.01852s, Loss: 1.32729\n",
      "Epoch: 2, Time: 0.02073s, Loss: 0.89455\n",
      "Epoch: 3, Time: 0.01821s, Loss: 0.58217\n",
      "Epoch: 4, Time: 0.02025s, Loss: 0.33297\n",
      "Epoch: 5, Time: 0.01954s, Loss: 0.21457\n",
      "Epoch: 6, Time: 0.01855s, Loss: 0.16569\n",
      "update best: 0.18399\n",
      "Epoch: 7, Time: 0.01419s, Loss: 0.11529\n",
      "update best: 0.20258\n",
      "Epoch: 8, Time: 0.02618s, Loss: 0.11493\n",
      "update best: 0.21928\n",
      "Epoch: 9, Time: 0.02598s, Loss: 0.08893\n",
      "update best: 0.22716\n",
      "Epoch: 10, Time: 0.01781s, Loss: 0.04256\n",
      "update best: 0.23503\n",
      "Epoch: 11, Time: 0.01740s, Loss: 0.05914\n",
      "update best: 0.24071\n",
      "Epoch: 12, Time: 0.01607s, Loss: 0.03737\n",
      "update best: 0.24354\n",
      "Epoch: 13, Time: 0.01743s, Loss: 0.03301\n",
      "update best: 0.25457\n",
      "Epoch: 14, Time: 0.01613s, Loss: 0.02690\n",
      "update best: 0.25772\n",
      "Epoch: 15, Time: 0.01786s, Loss: 0.01817\n",
      "update best: 0.25835\n",
      "Epoch: 16, Time: 0.01224s, Loss: 0.04195\n",
      "Epoch: 17, Time: 0.01985s, Loss: 0.04121\n",
      "Epoch: 18, Time: 0.02273s, Loss: 0.01221\n",
      "Epoch: 19, Time: 0.02539s, Loss: 0.01795\n",
      "Epoch: 20, Time: 0.02135s, Loss: 0.03318\n",
      "Epoch: 21, Time: 0.02106s, Loss: 0.01417\n",
      "Epoch: 22, Time: 0.02165s, Loss: 0.01202\n",
      "Epoch: 23, Time: 0.02191s, Loss: 0.00621\n",
      "Epoch: 24, Time: 0.02259s, Loss: 0.01527\n",
      "Epoch: 25, Time: 0.01803s, Loss: 0.00521\n",
      "Epoch: 26, Time: 0.02032s, Loss: 0.01303\n",
      "Epoch: 27, Time: 0.02088s, Loss: 0.01279\n",
      "Epoch: 28, Time: 0.01999s, Loss: 0.00548\n",
      "Epoch: 29, Time: 0.01940s, Loss: 0.00536\n",
      "Epoch: 30, Time: 0.01582s, Loss: 0.01232\n",
      "Epoch: 31, Time: 0.02379s, Loss: 0.00742\n",
      "Epoch: 32, Time: 0.01838s, Loss: 0.00472\n",
      "Epoch: 33, Time: 0.01057s, Loss: 0.01984\n",
      "Epoch: 34, Time: 0.02498s, Loss: 0.00515\n",
      "Epoch: 35, Time: 0.01750s, Loss: 0.00302\n",
      "Epoch: 36, Time: 0.01646s, Loss: 0.01405\n",
      "Epoch: 37, Time: 0.01622s, Loss: 0.00204\n",
      "Epoch: 38, Time: 0.01946s, Loss: 0.00648\n",
      "Epoch: 39, Time: 0.01720s, Loss: 0.00954\n",
      "Epoch: 40, Time: 0.01785s, Loss: 0.00736\n",
      "Epoch: 41, Time: 0.01665s, Loss: 0.00238\n",
      "Epoch: 42, Time: 0.01664s, Loss: 0.00368\n",
      "Epoch: 43, Time: 0.01792s, Loss: 0.00203\n",
      "Epoch: 44, Time: 0.01528s, Loss: 0.00152\n",
      "Epoch: 45, Time: 0.01737s, Loss: 0.00106\n",
      "Epoch: 46, Time: 0.01749s, Loss: 0.00479\n",
      "Epoch: 47, Time: 0.01904s, Loss: 0.00352\n",
      "Epoch: 48, Time: 0.01904s, Loss: 0.01470\n",
      "Epoch: 49, Time: 0.02189s, Loss: 0.00244\n",
      "Epoch: 50, Time: 0.02238s, Loss: 0.00861\n",
      "Epoch: 51, Time: 0.01767s, Loss: 0.00358\n",
      "Epoch: 52, Time: 0.02101s, Loss: 0.00322\n",
      "Epoch: 53, Time: 0.01907s, Loss: 0.00118\n",
      "Epoch: 54, Time: 0.01659s, Loss: 0.00263\n",
      "Epoch: 55, Time: 0.02012s, Loss: 0.01326\n",
      "Epoch: 56, Time: 0.01604s, Loss: 0.00693\n",
      "Epoch: 57, Time: 0.01645s, Loss: 0.00257\n",
      "Epoch: 58, Time: 0.01814s, Loss: 0.00101\n",
      "Epoch: 59, Time: 0.01666s, Loss: 0.00911\n",
      "Epoch: 60, Time: 0.01661s, Loss: 0.00936\n",
      "Epoch: 61, Time: 0.01737s, Loss: 0.00123\n",
      "Epoch: 62, Time: 0.01507s, Loss: 0.01041\n",
      "Epoch: 63, Time: 0.01793s, Loss: 0.01212\n",
      "Epoch: 64, Time: 0.02018s, Loss: 0.00214\n",
      "Epoch: 65, Time: 0.02066s, Loss: 0.00262\n",
      "Epoch: 66, Time: 0.01813s, Loss: 0.00584\n",
      "Epoch: 67, Time: 0.01768s, Loss: 0.00087\n",
      "Epoch: 68, Time: 0.01970s, Loss: 0.01147\n",
      "Epoch: 69, Time: 0.02049s, Loss: 0.01437\n",
      "Epoch: 70, Time: 0.02059s, Loss: 0.00154\n",
      "Epoch: 71, Time: 0.01939s, Loss: 0.00321\n",
      "Epoch: 72, Time: 0.02094s, Loss: 0.00186\n",
      "Epoch: 73, Time: 0.01794s, Loss: 0.00224\n",
      "Epoch: 74, Time: 0.01856s, Loss: 0.00588\n",
      "Epoch: 75, Time: 0.01907s, Loss: 0.02405\n",
      "Epoch: 76, Time: 0.01663s, Loss: 0.00334\n",
      "Epoch: 77, Time: 0.02108s, Loss: 0.00402\n",
      "update best: 0.26402\n",
      "Epoch: 78, Time: 0.01662s, Loss: 0.00632\n",
      "update best: 0.27190\n",
      "Epoch: 79, Time: 0.01623s, Loss: 0.00302\n",
      "update best: 0.27347\n",
      "Epoch: 80, Time: 0.01536s, Loss: 0.00129\n",
      "update best: 0.27599\n",
      "Epoch: 81, Time: 0.02290s, Loss: 0.00393\n",
      "Epoch: 82, Time: 0.01831s, Loss: 0.00084\n",
      "Epoch: 83, Time: 0.01609s, Loss: 0.00363\n",
      "Epoch: 84, Time: 0.01558s, Loss: 0.00176\n",
      "Epoch: 85, Time: 0.01565s, Loss: 0.01176\n",
      "Epoch: 86, Time: 0.01718s, Loss: 0.00149\n",
      "Epoch: 87, Time: 0.00983s, Loss: 0.00122\n",
      "Epoch: 88, Time: 0.01315s, Loss: 0.00072\n",
      "Epoch: 89, Time: 0.01795s, Loss: 0.00815\n",
      "Epoch: 90, Time: 0.01974s, Loss: 0.00086\n",
      "Epoch: 91, Time: 0.01664s, Loss: 0.00212\n",
      "Epoch: 92, Time: 0.01660s, Loss: 0.00074\n",
      "Epoch: 93, Time: 0.01622s, Loss: 0.00070\n",
      "Epoch: 94, Time: 0.01654s, Loss: 0.00234\n",
      "Epoch: 95, Time: 0.01416s, Loss: 0.00231\n",
      "Epoch: 96, Time: 0.01375s, Loss: 0.01695\n",
      "Epoch: 97, Time: 0.01603s, Loss: 0.00428\n",
      "Epoch: 98, Time: 0.01818s, Loss: 0.00508\n",
      "Epoch: 99, Time: 0.01530s, Loss: 0.00125\n",
      "Epoch: 100, Time: 0.01592s, Loss: 0.00322\n",
      "Epoch: 101, Time: 0.01391s, Loss: 0.00112\n",
      "Epoch: 102, Time: 0.01667s, Loss: 0.00270\n",
      "Epoch: 103, Time: 0.01702s, Loss: 0.00323\n",
      "Epoch: 104, Time: 0.01760s, Loss: 0.00200\n",
      "Epoch: 105, Time: 0.02148s, Loss: 0.00109\n",
      "Epoch: 106, Time: 0.01639s, Loss: 0.01693\n",
      "Epoch: 107, Time: 0.01756s, Loss: 0.00453\n",
      "Epoch: 108, Time: 0.01256s, Loss: 0.00070\n",
      "Epoch: 109, Time: 0.01725s, Loss: 0.00992\n",
      "Epoch: 110, Time: 0.01615s, Loss: 0.01284\n",
      "Epoch: 111, Time: 0.02071s, Loss: 0.00394\n",
      "Epoch: 112, Time: 0.01622s, Loss: 0.00095\n",
      "Epoch: 113, Time: 0.01667s, Loss: 0.00139\n",
      "Epoch: 114, Time: 0.01630s, Loss: 0.00704\n",
      "Epoch: 115, Time: 0.02093s, Loss: 0.01456\n",
      "Epoch: 116, Time: 0.01951s, Loss: 0.00099\n",
      "Epoch: 117, Time: 0.01817s, Loss: 0.00307\n",
      "Epoch: 118, Time: 0.01674s, Loss: 0.01291\n",
      "Epoch: 119, Time: 0.01944s, Loss: 0.00089\n",
      "Epoch: 120, Time: 0.01007s, Loss: 0.00867\n",
      "Epoch: 121, Time: 0.01729s, Loss: 0.00143\n",
      "Epoch: 122, Time: 0.01749s, Loss: 0.00413\n",
      "Epoch: 123, Time: 0.01876s, Loss: 0.00115\n",
      "Epoch: 124, Time: 0.02038s, Loss: 0.00473\n",
      "Epoch: 125, Time: 0.01647s, Loss: 0.00423\n",
      "Epoch: 126, Time: 0.01669s, Loss: 0.00284\n",
      "Epoch: 127, Time: 0.01748s, Loss: 0.00151\n",
      "Epoch: 128, Time: 0.01738s, Loss: 0.00751\n",
      "Epoch: 129, Time: 0.01639s, Loss: 0.00503\n",
      "Epoch: 130, Time: 0.02013s, Loss: 0.00140\n",
      "Epoch: 131, Time: 0.01960s, Loss: 0.00309\n",
      "Epoch: 132, Time: 0.01598s, Loss: 0.00381\n",
      "update best: 0.27977\n",
      "Epoch: 133, Time: 0.01861s, Loss: 0.00176\n",
      "update best: 0.28607\n",
      "Epoch: 134, Time: 0.01128s, Loss: 0.00439\n",
      "update best: 0.29332\n",
      "Epoch: 135, Time: 0.01695s, Loss: 0.00276\n",
      "Epoch: 136, Time: 0.01538s, Loss: 0.00186\n",
      "Epoch: 137, Time: 0.01881s, Loss: 0.00234\n",
      "Epoch: 138, Time: 0.01441s, Loss: 0.00368\n",
      "Epoch: 139, Time: 0.01693s, Loss: 0.00487\n",
      "Epoch: 140, Time: 0.02137s, Loss: 0.00286\n",
      "Epoch: 141, Time: 0.01160s, Loss: 0.00101\n",
      "Epoch: 142, Time: 0.01736s, Loss: 0.00221\n",
      "Epoch: 143, Time: 0.01846s, Loss: 0.00689\n",
      "Epoch: 144, Time: 0.01861s, Loss: 0.00191\n",
      "Epoch: 145, Time: 0.01987s, Loss: 0.00288\n",
      "Epoch: 146, Time: 0.01812s, Loss: 0.00674\n",
      "Epoch: 147, Time: 0.01984s, Loss: 0.00450\n",
      "Epoch: 148, Time: 0.01314s, Loss: 0.00107\n",
      "Epoch: 149, Time: 0.01819s, Loss: 0.00232\n",
      "Epoch: 150, Time: 0.01849s, Loss: 0.00186\n",
      "Epoch: 151, Time: 0.01651s, Loss: 0.00094\n",
      "Epoch: 152, Time: 0.01596s, Loss: 0.01419\n",
      "Epoch: 153, Time: 0.01539s, Loss: 0.00092\n",
      "Epoch: 154, Time: 0.01435s, Loss: 0.00103\n",
      "Epoch: 155, Time: 0.01859s, Loss: 0.00089\n",
      "Epoch: 156, Time: 0.01928s, Loss: 0.00860\n",
      "Epoch: 157, Time: 0.02187s, Loss: 0.00186\n",
      "Epoch: 158, Time: 0.01796s, Loss: 0.04007\n",
      "Epoch: 159, Time: 0.02077s, Loss: 0.01148\n",
      "Epoch: 160, Time: 0.01883s, Loss: 0.01324\n",
      "Epoch: 161, Time: 0.01697s, Loss: 0.00500\n",
      "Epoch: 162, Time: 0.01291s, Loss: 0.02147\n",
      "Epoch: 163, Time: 0.01828s, Loss: 0.00268\n",
      "Epoch: 164, Time: 0.01877s, Loss: 0.00114\n",
      "Epoch: 165, Time: 0.01660s, Loss: 0.00103\n",
      "Epoch: 166, Time: 0.01687s, Loss: 0.00143\n",
      "Epoch: 167, Time: 0.01458s, Loss: 0.00802\n",
      "Epoch: 168, Time: 0.01996s, Loss: 0.00196\n",
      "Epoch: 169, Time: 0.02152s, Loss: 0.00107\n",
      "Epoch: 170, Time: 0.02028s, Loss: 0.00500\n",
      "Epoch: 171, Time: 0.01687s, Loss: 0.00304\n",
      "Epoch: 172, Time: 0.02011s, Loss: 0.00600\n",
      "Epoch: 173, Time: 0.02363s, Loss: 0.00318\n",
      "Epoch: 174, Time: 0.02392s, Loss: 0.00424\n",
      "Epoch: 175, Time: 0.02470s, Loss: 0.00479\n",
      "Epoch: 176, Time: 0.01839s, Loss: 0.00095\n",
      "Epoch: 177, Time: 0.01289s, Loss: 0.00309\n",
      "Epoch: 178, Time: 0.01559s, Loss: 0.01341\n",
      "Epoch: 179, Time: 0.01824s, Loss: 0.00103\n",
      "Epoch: 180, Time: 0.02663s, Loss: 0.00252\n",
      "Epoch: 181, Time: 0.01736s, Loss: 0.00145\n",
      "Epoch: 182, Time: 0.01917s, Loss: 0.00156\n",
      "Epoch: 183, Time: 0.02012s, Loss: 0.00154\n",
      "Epoch: 184, Time: 0.02048s, Loss: 0.00170\n",
      "Epoch: 185, Time: 0.01641s, Loss: 0.00569\n",
      "Epoch: 186, Time: 0.01850s, Loss: 0.00530\n",
      "Epoch: 187, Time: 0.01862s, Loss: 0.00718\n",
      "Epoch: 188, Time: 0.01619s, Loss: 0.00135\n",
      "Epoch: 189, Time: 0.02075s, Loss: 0.00858\n",
      "Epoch: 190, Time: 0.01812s, Loss: 0.00070\n",
      "Epoch: 191, Time: 0.01697s, Loss: 0.00161\n",
      "Epoch: 192, Time: 0.01435s, Loss: 0.00187\n",
      "Epoch: 193, Time: 0.02099s, Loss: 0.01581\n",
      "Epoch: 194, Time: 0.01537s, Loss: 0.00251\n",
      "Epoch: 195, Time: 0.02052s, Loss: 0.00978\n",
      "Epoch: 196, Time: 0.01819s, Loss: 0.00208\n",
      "Epoch: 197, Time: 0.01932s, Loss: 0.00614\n",
      "Epoch: 198, Time: 0.01474s, Loss: 0.01333\n",
      "Epoch: 199, Time: 0.02028s, Loss: 0.00244\n",
      "\n",
      "train finished!\n",
      "best val: 0.29332\n",
      "test...\n",
      "final result: epoch: 134\n",
      "{'accuracy': 0.29332074522972107, 'f1_score': 0.24036976724855488, 'f1_score -> average@micro': 0.2933207309388784}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "ft_dim = X.shape[1]\n",
    "HG = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "G = Graph.from_hypergraph_clique(HG, weighted=True)\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = GCN(ft_dim, 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.02666s, Loss: 1.81649\n",
      "update best: 0.21424\n",
      "Epoch: 1, Time: 0.02883s, Loss: 1.43765\n",
      "update best: 0.21865\n",
      "Epoch: 2, Time: 0.02005s, Loss: 1.22796\n",
      "Epoch: 3, Time: 0.01828s, Loss: 1.15794\n",
      "Epoch: 4, Time: 0.01936s, Loss: 1.12441\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.02366s, Loss: 1.10783\n",
      "update best: 0.22275\n",
      "Epoch: 6, Time: 0.01793s, Loss: 1.09250\n",
      "update best: 0.22369\n",
      "Epoch: 7, Time: 0.02164s, Loss: 1.08502\n",
      "update best: 0.22495\n",
      "Epoch: 8, Time: 0.02327s, Loss: 1.07616\n",
      "update best: 0.22527\n",
      "Epoch: 9, Time: 0.02244s, Loss: 1.07437\n",
      "Epoch: 10, Time: 0.01950s, Loss: 1.07368\n",
      "update best: 0.22590\n",
      "Epoch: 11, Time: 0.01640s, Loss: 1.06851\n",
      "Epoch: 12, Time: 0.01686s, Loss: 1.06937\n",
      "Epoch: 13, Time: 0.01804s, Loss: 1.07050\n",
      "Epoch: 14, Time: 0.01849s, Loss: 1.06823\n",
      "update best: 0.22684\n",
      "Epoch: 15, Time: 0.02724s, Loss: 1.06642\n",
      "update best: 0.22810\n",
      "Epoch: 16, Time: 0.04121s, Loss: 1.06693\n",
      "update best: 0.22842\n",
      "Epoch: 17, Time: 0.01856s, Loss: 1.06795\n",
      "update best: 0.22873\n",
      "Epoch: 18, Time: 0.01636s, Loss: 1.06691\n",
      "update best: 0.23031\n",
      "Epoch: 19, Time: 0.01900s, Loss: 1.06681\n",
      "update best: 0.23125\n",
      "Epoch: 20, Time: 0.01937s, Loss: 1.06605\n",
      "update best: 0.23346\n",
      "Epoch: 21, Time: 0.02448s, Loss: 1.06690\n",
      "Epoch: 22, Time: 0.01854s, Loss: 1.06663\n",
      "update best: 0.23377\n",
      "Epoch: 23, Time: 0.02143s, Loss: 1.06689\n",
      "Epoch: 24, Time: 0.02080s, Loss: 1.06694\n",
      "Epoch: 25, Time: 0.01814s, Loss: 1.06539\n",
      "update best: 0.23440\n",
      "Epoch: 26, Time: 0.02194s, Loss: 1.07002\n",
      "Epoch: 27, Time: 0.02151s, Loss: 1.06522\n",
      "Epoch: 28, Time: 0.02571s, Loss: 1.06616\n",
      "update best: 0.23693\n",
      "Epoch: 29, Time: 0.02481s, Loss: 1.06519\n",
      "update best: 0.23913\n",
      "Epoch: 30, Time: 0.01903s, Loss: 1.06896\n",
      "update best: 0.23976\n",
      "Epoch: 31, Time: 0.01825s, Loss: 1.06508\n",
      "update best: 0.24228\n",
      "Epoch: 32, Time: 0.01837s, Loss: 1.06613\n",
      "update best: 0.24354\n",
      "Epoch: 33, Time: 0.02131s, Loss: 1.06547\n",
      "Epoch: 34, Time: 0.01762s, Loss: 1.06507\n",
      "Epoch: 35, Time: 0.02063s, Loss: 1.06493\n",
      "Epoch: 36, Time: 0.02128s, Loss: 1.06490\n",
      "Epoch: 37, Time: 0.01768s, Loss: 1.06516\n",
      "Epoch: 38, Time: 0.01795s, Loss: 1.06532\n",
      "Epoch: 39, Time: 0.01812s, Loss: 1.06980\n",
      "Epoch: 40, Time: 0.01674s, Loss: 1.06490\n",
      "Epoch: 41, Time: 0.01593s, Loss: 1.06702\n",
      "Epoch: 42, Time: 0.01489s, Loss: 1.06530\n",
      "Epoch: 43, Time: 0.01657s, Loss: 1.06540\n",
      "Epoch: 44, Time: 0.01704s, Loss: 1.06647\n",
      "update best: 0.24417\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06484\n",
      "update best: 0.24543\n",
      "Epoch: 46, Time: 0.01660s, Loss: 1.06728\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.01782s, Loss: 1.06481\n",
      "Epoch: 48, Time: 0.01757s, Loss: 1.06632\n",
      "Epoch: 49, Time: 0.01485s, Loss: 1.06521\n",
      "Epoch: 50, Time: 0.01675s, Loss: 1.06575\n",
      "Epoch: 51, Time: 0.01786s, Loss: 1.06525\n",
      "Epoch: 52, Time: 0.02277s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.02166s, Loss: 1.06507\n",
      "Epoch: 54, Time: 0.01743s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01731s, Loss: 1.06621\n",
      "Epoch: 56, Time: 0.01820s, Loss: 1.06738\n",
      "Epoch: 57, Time: 0.01791s, Loss: 1.06510\n",
      "Epoch: 58, Time: 0.02247s, Loss: 1.06537\n",
      "Epoch: 59, Time: 0.02010s, Loss: 1.06517\n",
      "Epoch: 60, Time: 0.01746s, Loss: 1.06534\n",
      "Epoch: 61, Time: 0.01992s, Loss: 1.06506\n",
      "Epoch: 62, Time: 0.01787s, Loss: 1.06508\n",
      "update best: 0.24764\n",
      "Epoch: 63, Time: 0.02259s, Loss: 1.06570\n",
      "update best: 0.24953\n",
      "Epoch: 64, Time: 0.01247s, Loss: 1.06505\n",
      "update best: 0.25142\n",
      "Epoch: 65, Time: 0.01587s, Loss: 1.06507\n",
      "Epoch: 66, Time: 0.01652s, Loss: 1.06519\n",
      "update best: 0.25268\n",
      "Epoch: 67, Time: 0.01768s, Loss: 1.06502\n",
      "update best: 0.25299\n",
      "Epoch: 68, Time: 0.01546s, Loss: 1.06523\n",
      "update best: 0.25331\n",
      "Epoch: 69, Time: 0.01801s, Loss: 1.07305\n",
      "update best: 0.25551\n",
      "Epoch: 70, Time: 0.01831s, Loss: 1.06549\n",
      "update best: 0.25583\n",
      "Epoch: 71, Time: 0.01908s, Loss: 1.06533\n",
      "Epoch: 72, Time: 0.02021s, Loss: 1.06593\n",
      "Epoch: 73, Time: 0.01627s, Loss: 1.06515\n",
      "Epoch: 74, Time: 0.01677s, Loss: 1.06505\n",
      "Epoch: 75, Time: 0.01870s, Loss: 1.06534\n",
      "update best: 0.25677\n",
      "Epoch: 76, Time: 0.01737s, Loss: 1.06526\n",
      "update best: 0.25992\n",
      "Epoch: 77, Time: 0.01953s, Loss: 1.06741\n",
      "update best: 0.26654\n",
      "Epoch: 78, Time: 0.02144s, Loss: 1.06646\n",
      "update best: 0.26906\n",
      "Epoch: 79, Time: 0.02197s, Loss: 1.06549\n",
      "update best: 0.27095\n",
      "Epoch: 80, Time: 0.02261s, Loss: 1.06540\n",
      "update best: 0.27190\n",
      "Epoch: 81, Time: 0.02229s, Loss: 1.06554\n",
      "Epoch: 82, Time: 0.02355s, Loss: 1.06607\n",
      "update best: 0.27473\n",
      "Epoch: 83, Time: 0.01814s, Loss: 1.06520\n",
      "Epoch: 84, Time: 0.01883s, Loss: 1.06574\n",
      "update best: 0.27505\n",
      "Epoch: 85, Time: 0.01675s, Loss: 1.06542\n",
      "update best: 0.27662\n",
      "Epoch: 86, Time: 0.01655s, Loss: 1.06497\n",
      "update best: 0.27788\n",
      "Epoch: 87, Time: 0.01687s, Loss: 1.06536\n",
      "update best: 0.27820\n",
      "Epoch: 88, Time: 0.01637s, Loss: 1.06712\n",
      "update best: 0.27851\n",
      "Epoch: 89, Time: 0.01623s, Loss: 1.06507\n",
      "Epoch: 90, Time: 0.01593s, Loss: 1.06662\n",
      "Epoch: 91, Time: 0.01839s, Loss: 1.06555\n",
      "Epoch: 92, Time: 0.01371s, Loss: 1.06724\n",
      "Epoch: 93, Time: 0.01482s, Loss: 1.06553\n",
      "update best: 0.28040\n",
      "Epoch: 94, Time: 0.01767s, Loss: 1.06508\n",
      "Epoch: 95, Time: 0.01703s, Loss: 1.06521\n",
      "update best: 0.28166\n",
      "Epoch: 96, Time: 0.01775s, Loss: 1.06502\n",
      "update best: 0.28261\n",
      "Epoch: 97, Time: 0.01963s, Loss: 1.06545\n",
      "update best: 0.28387\n",
      "Epoch: 98, Time: 0.02233s, Loss: 1.06513\n",
      "update best: 0.28481\n",
      "Epoch: 99, Time: 0.01748s, Loss: 1.06547\n",
      "Epoch: 100, Time: 0.01556s, Loss: 1.06497\n",
      "Epoch: 101, Time: 0.01438s, Loss: 1.06506\n",
      "update best: 0.28544\n",
      "Epoch: 102, Time: 0.01760s, Loss: 1.06503\n",
      "update best: 0.28639\n",
      "Epoch: 103, Time: 0.01655s, Loss: 1.06542\n",
      "update best: 0.28702\n",
      "Epoch: 104, Time: 0.01242s, Loss: 1.06585\n",
      "Epoch: 105, Time: 0.01357s, Loss: 1.06535\n",
      "Epoch: 106, Time: 0.01611s, Loss: 1.06541\n",
      "Epoch: 107, Time: 0.01720s, Loss: 1.06517\n",
      "Epoch: 108, Time: 0.01844s, Loss: 1.06508\n",
      "Epoch: 109, Time: 0.01629s, Loss: 1.06523\n",
      "Epoch: 110, Time: 0.01776s, Loss: 1.06669\n",
      "Epoch: 111, Time: 0.02685s, Loss: 1.06504\n",
      "Epoch: 112, Time: 0.01608s, Loss: 1.06830\n",
      "Epoch: 113, Time: 0.02013s, Loss: 1.06510\n",
      "Epoch: 114, Time: 0.01823s, Loss: 1.06535\n",
      "Epoch: 115, Time: 0.01609s, Loss: 1.06590\n",
      "Epoch: 116, Time: 0.01215s, Loss: 1.06531\n",
      "Epoch: 117, Time: 0.01296s, Loss: 1.06569\n",
      "Epoch: 118, Time: 0.01771s, Loss: 1.06568\n",
      "Epoch: 119, Time: 0.02355s, Loss: 1.06620\n",
      "Epoch: 120, Time: 0.01563s, Loss: 1.06539\n",
      "Epoch: 121, Time: 0.01465s, Loss: 1.06579\n",
      "Epoch: 122, Time: 0.01447s, Loss: 1.06534\n",
      "Epoch: 123, Time: 0.01752s, Loss: 1.06539\n",
      "Epoch: 124, Time: 0.01862s, Loss: 1.06516\n",
      "Epoch: 125, Time: 0.01149s, Loss: 1.06508\n",
      "update best: 0.28733\n",
      "Epoch: 126, Time: 0.01664s, Loss: 1.06511\n",
      "update best: 0.28922\n",
      "Epoch: 127, Time: 0.01757s, Loss: 1.06536\n",
      "update best: 0.29017\n",
      "Epoch: 128, Time: 0.01645s, Loss: 1.06550\n",
      "update best: 0.29080\n",
      "Epoch: 129, Time: 0.01800s, Loss: 1.06510\n",
      "update best: 0.29238\n",
      "Epoch: 130, Time: 0.01650s, Loss: 1.06519\n",
      "Epoch: 131, Time: 0.02048s, Loss: 1.06574\n",
      "Epoch: 132, Time: 0.01997s, Loss: 1.06540\n",
      "Epoch: 133, Time: 0.02081s, Loss: 1.06570\n",
      "Epoch: 134, Time: 0.01810s, Loss: 1.06556\n",
      "Epoch: 135, Time: 0.01953s, Loss: 1.06589\n",
      "update best: 0.29364\n",
      "Epoch: 136, Time: 0.01920s, Loss: 1.06544\n",
      "Epoch: 137, Time: 0.02303s, Loss: 1.06539\n",
      "Epoch: 138, Time: 0.01508s, Loss: 1.06505\n",
      "Epoch: 139, Time: 0.01842s, Loss: 1.06553\n",
      "Epoch: 140, Time: 0.02009s, Loss: 1.06553\n",
      "Epoch: 141, Time: 0.02001s, Loss: 1.06507\n",
      "Epoch: 142, Time: 0.02050s, Loss: 1.06608\n",
      "Epoch: 143, Time: 0.02168s, Loss: 1.06573\n",
      "Epoch: 144, Time: 0.01856s, Loss: 1.06533\n",
      "Epoch: 145, Time: 0.01972s, Loss: 1.06542\n",
      "Epoch: 146, Time: 0.01655s, Loss: 1.06536\n",
      "Epoch: 147, Time: 0.01834s, Loss: 1.06544\n",
      "Epoch: 148, Time: 0.01313s, Loss: 1.06630\n",
      "Epoch: 149, Time: 0.01821s, Loss: 1.06515\n",
      "Epoch: 150, Time: 0.01853s, Loss: 1.06549\n",
      "Epoch: 151, Time: 0.01937s, Loss: 1.06519\n",
      "Epoch: 152, Time: 0.01679s, Loss: 1.06495\n",
      "Epoch: 153, Time: 0.01791s, Loss: 1.06549\n",
      "Epoch: 154, Time: 0.01480s, Loss: 1.06536\n",
      "Epoch: 155, Time: 0.01520s, Loss: 1.06516\n",
      "Epoch: 156, Time: 0.01627s, Loss: 1.06595\n",
      "Epoch: 157, Time: 0.01555s, Loss: 1.06553\n",
      "Epoch: 158, Time: 0.02305s, Loss: 1.06554\n",
      "Epoch: 159, Time: 0.01351s, Loss: 1.06529\n",
      "Epoch: 160, Time: 0.01414s, Loss: 1.06528\n",
      "Epoch: 161, Time: 0.01765s, Loss: 1.06544\n",
      "Epoch: 162, Time: 0.01710s, Loss: 1.06587\n",
      "Epoch: 163, Time: 0.02061s, Loss: 1.06593\n",
      "Epoch: 164, Time: 0.02033s, Loss: 1.06537\n",
      "Epoch: 165, Time: 0.01972s, Loss: 1.06553\n",
      "Epoch: 166, Time: 0.01385s, Loss: 1.06530\n",
      "Epoch: 167, Time: 0.01608s, Loss: 1.06550\n",
      "Epoch: 168, Time: 0.01672s, Loss: 1.06533\n",
      "Epoch: 169, Time: 0.01599s, Loss: 1.06530\n",
      "Epoch: 170, Time: 0.01611s, Loss: 1.06539\n",
      "Epoch: 171, Time: 0.01605s, Loss: 1.06524\n",
      "Epoch: 172, Time: 0.01934s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01833s, Loss: 1.06556\n",
      "Epoch: 174, Time: 0.01524s, Loss: 1.06522\n",
      "update best: 0.29490\n",
      "Epoch: 175, Time: 0.01827s, Loss: 1.06555\n",
      "update best: 0.29521\n",
      "Epoch: 176, Time: 0.01649s, Loss: 1.06508\n",
      "Epoch: 177, Time: 0.01646s, Loss: 1.06528\n",
      "update best: 0.29647\n",
      "Epoch: 178, Time: 0.02095s, Loss: 1.06525\n",
      "update best: 0.29805\n",
      "Epoch: 179, Time: 0.01807s, Loss: 1.06549\n",
      "Epoch: 180, Time: 0.01872s, Loss: 1.06556\n",
      "Epoch: 181, Time: 0.01824s, Loss: 1.06508\n",
      "Epoch: 182, Time: 0.01404s, Loss: 1.06543\n",
      "Epoch: 183, Time: 0.01711s, Loss: 1.06532\n",
      "Epoch: 184, Time: 0.01564s, Loss: 1.06961\n",
      "Epoch: 185, Time: 0.01837s, Loss: 1.06526\n",
      "Epoch: 186, Time: 0.01663s, Loss: 1.06531\n",
      "Epoch: 187, Time: 0.00980s, Loss: 1.06604\n",
      "Epoch: 188, Time: 0.01678s, Loss: 1.06544\n",
      "Epoch: 189, Time: 0.01867s, Loss: 1.06573\n",
      "Epoch: 190, Time: 0.01688s, Loss: 1.06559\n",
      "Epoch: 191, Time: 0.01976s, Loss: 1.06660\n",
      "Epoch: 192, Time: 0.01817s, Loss: 1.06627\n",
      "Epoch: 193, Time: 0.02570s, Loss: 1.06583\n",
      "Epoch: 194, Time: 0.01776s, Loss: 1.06552\n",
      "Epoch: 195, Time: 0.01549s, Loss: 1.06540\n",
      "Epoch: 196, Time: 0.01602s, Loss: 1.06679\n",
      "Epoch: 197, Time: 0.01553s, Loss: 1.06629\n",
      "Epoch: 198, Time: 0.01786s, Loss: 1.06716\n",
      "Epoch: 199, Time: 0.01856s, Loss: 1.06667\n",
      "\n",
      "train finished!\n",
      "best val: 0.29805\n",
      "test...\n",
      "final result: epoch: 178\n",
      "{'accuracy': 0.29804661870002747, 'f1_score': 0.21974182550479207, 'f1_score -> average@micro': 0.2980466288594833}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HGNN+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Time: 0.04270s, Loss: 1.82284\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02071s, Loss: 1.51886\n",
      "Epoch: 2, Time: 0.02244s, Loss: 1.30142\n",
      "Epoch: 3, Time: 0.01422s, Loss: 1.19327\n",
      "Epoch: 4, Time: 0.01827s, Loss: 1.14901\n",
      "update best: 0.21456\n",
      "Epoch: 5, Time: 0.01704s, Loss: 1.11603\n",
      "update best: 0.21582\n",
      "Epoch: 6, Time: 0.01643s, Loss: 1.09219\n",
      "update best: 0.21676\n",
      "Epoch: 7, Time: 0.01922s, Loss: 1.08688\n",
      "Epoch: 8, Time: 0.02138s, Loss: 1.08081\n",
      "update best: 0.21802\n",
      "Epoch: 9, Time: 0.02001s, Loss: 1.07927\n",
      "update best: 0.21991\n",
      "Epoch: 10, Time: 0.02502s, Loss: 1.07121\n",
      "update best: 0.22401\n",
      "Epoch: 11, Time: 0.01725s, Loss: 1.07070\n",
      "update best: 0.22842\n",
      "Epoch: 12, Time: 0.01808s, Loss: 1.06910\n",
      "update best: 0.23220\n",
      "Epoch: 13, Time: 0.01767s, Loss: 1.06739\n",
      "update best: 0.23756\n",
      "Epoch: 14, Time: 0.01468s, Loss: 1.06867\n",
      "update best: 0.24228\n",
      "Epoch: 15, Time: 0.02318s, Loss: 1.06965\n",
      "update best: 0.24890\n",
      "Epoch: 16, Time: 0.01846s, Loss: 1.06565\n",
      "update best: 0.25488\n",
      "Epoch: 17, Time: 0.02093s, Loss: 1.06624\n",
      "update best: 0.26024\n",
      "Epoch: 18, Time: 0.01909s, Loss: 1.06569\n",
      "update best: 0.26591\n",
      "Epoch: 19, Time: 0.01976s, Loss: 1.06635\n",
      "update best: 0.26938\n",
      "Epoch: 20, Time: 0.02251s, Loss: 1.06535\n",
      "update best: 0.27347\n",
      "Epoch: 21, Time: 0.01153s, Loss: 1.06856\n",
      "update best: 0.27788\n",
      "Epoch: 22, Time: 0.01665s, Loss: 1.07035\n",
      "update best: 0.28135\n",
      "Epoch: 23, Time: 0.02305s, Loss: 1.06538\n",
      "update best: 0.28670\n",
      "Epoch: 24, Time: 0.02006s, Loss: 1.06527\n",
      "update best: 0.28859\n",
      "Epoch: 25, Time: 0.02235s, Loss: 1.06552\n",
      "update best: 0.29049\n",
      "Epoch: 26, Time: 0.01630s, Loss: 1.06530\n",
      "update best: 0.29080\n",
      "Epoch: 27, Time: 0.02152s, Loss: 1.06492\n",
      "update best: 0.29112\n",
      "Epoch: 28, Time: 0.01894s, Loss: 1.06538\n",
      "update best: 0.29301\n",
      "Epoch: 29, Time: 0.02661s, Loss: 1.06589\n",
      "update best: 0.29553\n",
      "Epoch: 30, Time: 0.02075s, Loss: 1.06627\n",
      "Epoch: 31, Time: 0.02027s, Loss: 1.06568\n",
      "Epoch: 32, Time: 0.01624s, Loss: 1.06576\n",
      "update best: 0.29584\n",
      "Epoch: 33, Time: 0.01920s, Loss: 1.06508\n",
      "update best: 0.29647\n",
      "Epoch: 34, Time: 0.01991s, Loss: 1.06536\n",
      "update best: 0.29679\n",
      "Epoch: 35, Time: 0.01840s, Loss: 1.06501\n",
      "update best: 0.29710\n",
      "Epoch: 36, Time: 0.01724s, Loss: 1.06524\n",
      "Epoch: 37, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 38, Time: 0.02036s, Loss: 1.06499\n",
      "Epoch: 39, Time: 0.01830s, Loss: 1.06507\n",
      "Epoch: 40, Time: 0.01822s, Loss: 1.06540\n",
      "update best: 0.29805\n",
      "Epoch: 41, Time: 0.01940s, Loss: 1.06509\n",
      "Epoch: 42, Time: 0.01928s, Loss: 1.06522\n",
      "update best: 0.29836\n",
      "Epoch: 43, Time: 0.01889s, Loss: 1.06523\n",
      "Epoch: 44, Time: 0.02267s, Loss: 1.06530\n",
      "update best: 0.29931\n",
      "Epoch: 45, Time: 0.02758s, Loss: 1.06553\n",
      "Epoch: 46, Time: 0.02273s, Loss: 1.06594\n",
      "update best: 0.30183\n",
      "Epoch: 47, Time: 0.02789s, Loss: 1.06519\n",
      "update best: 0.30372\n",
      "Epoch: 48, Time: 0.01263s, Loss: 1.06524\n",
      "update best: 0.30466\n",
      "Epoch: 49, Time: 0.01633s, Loss: 1.06493\n",
      "update best: 0.30592\n",
      "Epoch: 50, Time: 0.02007s, Loss: 1.06548\n",
      "Epoch: 51, Time: 0.01813s, Loss: 1.06498\n",
      "Epoch: 52, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 53, Time: 0.01938s, Loss: 1.06489\n",
      "update best: 0.30718\n",
      "Epoch: 54, Time: 0.01763s, Loss: 1.06500\n",
      "update best: 0.30876\n",
      "Epoch: 55, Time: 0.01670s, Loss: 1.06570\n",
      "update best: 0.31128\n",
      "Epoch: 56, Time: 0.01785s, Loss: 1.06502\n",
      "Epoch: 57, Time: 0.01928s, Loss: 1.06531\n",
      "Epoch: 58, Time: 0.02257s, Loss: 1.06599\n",
      "update best: 0.31159\n",
      "Epoch: 59, Time: 0.02586s, Loss: 1.06529\n",
      "Epoch: 60, Time: 0.02166s, Loss: 1.06604\n",
      "Epoch: 61, Time: 0.01866s, Loss: 1.06585\n",
      "Epoch: 62, Time: 0.02232s, Loss: 1.06490\n",
      "Epoch: 63, Time: 0.01852s, Loss: 1.06505\n",
      "Epoch: 64, Time: 0.02313s, Loss: 1.06505\n",
      "Epoch: 65, Time: 0.01801s, Loss: 1.06512\n",
      "Epoch: 66, Time: 0.02064s, Loss: 1.06598\n",
      "Epoch: 67, Time: 0.02058s, Loss: 1.06529\n",
      "Epoch: 68, Time: 0.01511s, Loss: 1.06910\n",
      "update best: 0.31191\n",
      "Epoch: 69, Time: 0.02213s, Loss: 1.06611\n",
      "update best: 0.31317\n",
      "Epoch: 70, Time: 0.02055s, Loss: 1.06589\n",
      "update best: 0.31411\n",
      "Epoch: 71, Time: 0.01592s, Loss: 1.06512\n",
      "Epoch: 72, Time: 0.02295s, Loss: 1.06520\n",
      "Epoch: 73, Time: 0.02415s, Loss: 1.06497\n",
      "Epoch: 74, Time: 0.01714s, Loss: 1.06511\n",
      "Epoch: 75, Time: 0.01442s, Loss: 1.06539\n",
      "Epoch: 76, Time: 0.02115s, Loss: 1.06501\n",
      "Epoch: 77, Time: 0.01995s, Loss: 1.06772\n",
      "Epoch: 78, Time: 0.01706s, Loss: 1.06505\n",
      "Epoch: 79, Time: 0.01946s, Loss: 1.06511\n",
      "Epoch: 80, Time: 0.01968s, Loss: 1.06532\n",
      "Epoch: 81, Time: 0.02007s, Loss: 1.06583\n",
      "Epoch: 82, Time: 0.01847s, Loss: 1.06540\n",
      "Epoch: 83, Time: 0.01677s, Loss: 1.06522\n",
      "Epoch: 84, Time: 0.01916s, Loss: 1.06552\n",
      "Epoch: 85, Time: 0.01701s, Loss: 1.06588\n",
      "Epoch: 86, Time: 0.01758s, Loss: 1.06530\n",
      "Epoch: 87, Time: 0.01681s, Loss: 1.06513\n",
      "Epoch: 88, Time: 0.01884s, Loss: 1.06695\n",
      "Epoch: 89, Time: 0.01809s, Loss: 1.06502\n",
      "Epoch: 90, Time: 0.01902s, Loss: 1.06542\n",
      "Epoch: 91, Time: 0.01726s, Loss: 1.06501\n",
      "Epoch: 92, Time: 0.01917s, Loss: 1.07111\n",
      "Epoch: 93, Time: 0.02157s, Loss: 1.06501\n",
      "Epoch: 94, Time: 0.01932s, Loss: 1.06579\n",
      "Epoch: 95, Time: 0.01742s, Loss: 1.06544\n",
      "Epoch: 96, Time: 0.01689s, Loss: 1.06592\n",
      "Epoch: 97, Time: 0.02184s, Loss: 1.06546\n",
      "Epoch: 98, Time: 0.01851s, Loss: 1.06559\n",
      "Epoch: 99, Time: 0.01974s, Loss: 1.06655\n",
      "Epoch: 100, Time: 0.02153s, Loss: 1.06613\n",
      "Epoch: 101, Time: 0.02100s, Loss: 1.06529\n",
      "Epoch: 102, Time: 0.02183s, Loss: 1.06513\n",
      "update best: 0.31537\n",
      "Epoch: 103, Time: 0.01861s, Loss: 1.06602\n",
      "update best: 0.31664\n",
      "Epoch: 104, Time: 0.02366s, Loss: 1.06506\n",
      "Epoch: 105, Time: 0.02303s, Loss: 1.06543\n",
      "Epoch: 106, Time: 0.01787s, Loss: 1.06549\n",
      "Epoch: 107, Time: 0.02056s, Loss: 1.06499\n",
      "Epoch: 108, Time: 0.02198s, Loss: 1.06506\n",
      "Epoch: 109, Time: 0.02249s, Loss: 1.06522\n",
      "Epoch: 110, Time: 0.02321s, Loss: 1.06508\n",
      "Epoch: 111, Time: 0.02333s, Loss: 1.06503\n",
      "Epoch: 112, Time: 0.02006s, Loss: 1.06550\n",
      "Epoch: 113, Time: 0.01862s, Loss: 1.06498\n",
      "Epoch: 114, Time: 0.01844s, Loss: 1.06520\n",
      "Epoch: 115, Time: 0.01950s, Loss: 1.06557\n",
      "Epoch: 116, Time: 0.01550s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.01858s, Loss: 1.06584\n",
      "Epoch: 118, Time: 0.01633s, Loss: 1.06530\n",
      "Epoch: 119, Time: 0.02066s, Loss: 1.06533\n",
      "Epoch: 120, Time: 0.01966s, Loss: 1.06518\n",
      "Epoch: 121, Time: 0.01675s, Loss: 1.06587\n",
      "Epoch: 122, Time: 0.02211s, Loss: 1.06538\n",
      "Epoch: 123, Time: 0.01883s, Loss: 1.06525\n",
      "Epoch: 124, Time: 0.01462s, Loss: 1.06792\n",
      "Epoch: 125, Time: 0.01905s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.02171s, Loss: 1.06519\n",
      "Epoch: 127, Time: 0.01829s, Loss: 1.06529\n",
      "Epoch: 128, Time: 0.01958s, Loss: 1.06525\n",
      "Epoch: 129, Time: 0.01915s, Loss: 1.06577\n",
      "Epoch: 130, Time: 0.02259s, Loss: 1.06504\n",
      "Epoch: 131, Time: 0.01623s, Loss: 1.06562\n",
      "Epoch: 132, Time: 0.01713s, Loss: 1.06536\n",
      "Epoch: 133, Time: 0.01912s, Loss: 1.06614\n",
      "Epoch: 134, Time: 0.01647s, Loss: 1.06519\n",
      "Epoch: 135, Time: 0.01784s, Loss: 1.06531\n",
      "Epoch: 136, Time: 0.01811s, Loss: 1.06508\n",
      "Epoch: 137, Time: 0.01808s, Loss: 1.06545\n",
      "Epoch: 138, Time: 0.01983s, Loss: 1.06556\n",
      "Epoch: 139, Time: 0.01699s, Loss: 1.06502\n",
      "Epoch: 140, Time: 0.01636s, Loss: 1.06519\n",
      "Epoch: 141, Time: 0.01850s, Loss: 1.06617\n",
      "Epoch: 142, Time: 0.01634s, Loss: 1.06517\n",
      "Epoch: 143, Time: 0.01739s, Loss: 1.06538\n",
      "Epoch: 144, Time: 0.01833s, Loss: 1.06525\n",
      "Epoch: 145, Time: 0.01783s, Loss: 1.06514\n",
      "Epoch: 146, Time: 0.01796s, Loss: 1.06501\n",
      "Epoch: 147, Time: 0.01856s, Loss: 1.06551\n",
      "Epoch: 148, Time: 0.02472s, Loss: 1.06531\n",
      "Epoch: 149, Time: 0.01940s, Loss: 1.06562\n",
      "Epoch: 150, Time: 0.01549s, Loss: 1.06684\n",
      "Epoch: 151, Time: 0.01839s, Loss: 1.06553\n",
      "Epoch: 152, Time: 0.02012s, Loss: 1.06618\n",
      "Epoch: 153, Time: 0.02237s, Loss: 1.06527\n",
      "Epoch: 154, Time: 0.01830s, Loss: 1.07220\n",
      "Epoch: 155, Time: 0.02025s, Loss: 1.06542\n",
      "Epoch: 156, Time: 0.02134s, Loss: 1.06580\n",
      "Epoch: 157, Time: 0.01560s, Loss: 1.06539\n",
      "Epoch: 158, Time: 0.01388s, Loss: 1.06653\n",
      "Epoch: 159, Time: 0.01442s, Loss: 1.06542\n",
      "Epoch: 160, Time: 0.01732s, Loss: 1.06567\n",
      "Epoch: 161, Time: 0.01975s, Loss: 1.06511\n",
      "Epoch: 162, Time: 0.01835s, Loss: 1.06567\n",
      "Epoch: 163, Time: 0.01690s, Loss: 1.06527\n",
      "Epoch: 164, Time: 0.01715s, Loss: 1.06890\n",
      "Epoch: 165, Time: 0.02312s, Loss: 1.06546\n",
      "Epoch: 166, Time: 0.01980s, Loss: 1.06614\n",
      "Epoch: 167, Time: 0.02019s, Loss: 1.06497\n",
      "Epoch: 168, Time: 0.01731s, Loss: 1.06529\n",
      "Epoch: 169, Time: 0.01955s, Loss: 1.06562\n",
      "Epoch: 170, Time: 0.01743s, Loss: 1.06597\n",
      "Epoch: 171, Time: 0.01865s, Loss: 1.06532\n",
      "Epoch: 172, Time: 0.01997s, Loss: 1.06531\n",
      "Epoch: 173, Time: 0.01628s, Loss: 1.06550\n",
      "Epoch: 174, Time: 0.01769s, Loss: 1.06514\n",
      "Epoch: 175, Time: 0.01837s, Loss: 1.06553\n",
      "Epoch: 176, Time: 0.02044s, Loss: 1.06574\n",
      "Epoch: 177, Time: 0.02251s, Loss: 1.06569\n",
      "Epoch: 178, Time: 0.01868s, Loss: 1.06546\n",
      "Epoch: 179, Time: 0.02551s, Loss: 1.06544\n",
      "Epoch: 180, Time: 0.01607s, Loss: 1.06540\n",
      "Epoch: 181, Time: 0.01928s, Loss: 1.06541\n",
      "Epoch: 182, Time: 0.02239s, Loss: 1.06517\n",
      "Epoch: 183, Time: 0.02016s, Loss: 1.06519\n",
      "Epoch: 184, Time: 0.02079s, Loss: 1.06513\n",
      "Epoch: 185, Time: 0.02155s, Loss: 1.06497\n",
      "Epoch: 186, Time: 0.01768s, Loss: 1.06551\n",
      "Epoch: 187, Time: 0.02082s, Loss: 1.06646\n",
      "Epoch: 188, Time: 0.01630s, Loss: 1.06507\n",
      "Epoch: 189, Time: 0.02140s, Loss: 1.06555\n",
      "Epoch: 190, Time: 0.01823s, Loss: 1.06513\n",
      "Epoch: 191, Time: 0.02309s, Loss: 1.06557\n",
      "Epoch: 192, Time: 0.01628s, Loss: 1.06538\n",
      "Epoch: 193, Time: 0.01901s, Loss: 1.06559\n",
      "Epoch: 194, Time: 0.02195s, Loss: 1.06503\n",
      "Epoch: 195, Time: 0.01769s, Loss: 1.06567\n",
      "Epoch: 196, Time: 0.01739s, Loss: 1.06650\n",
      "Epoch: 197, Time: 0.01997s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01885s, Loss: 1.06514\n",
      "Epoch: 199, Time: 0.01803s, Loss: 1.06532\n",
      "\n",
      "train finished!\n",
      "best val: 0.31664\n",
      "test...\n",
      "final result: epoch: 103\n",
      "{'accuracy': 0.3166351616382599, 'f1_score': 0.24460102376042017, 'f1_score -> average@micro': 0.3166351606805293}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "train_mask = data[\"train_mask\"]\n",
    "val_mask = data[\"val_mask\"]\n",
    "test_mask = data[\"test_mask\"]\n",
    "\n",
    "net = HGNNP(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "X, lbl = X.to(device), lbl.to(device)\n",
    "G = G.to(device)\n",
    "net = net.to(device)\n",
    "\n",
    "best_state = None\n",
    "best_epoch, best_val = 0, 0\n",
    "for epoch in range(200):\n",
    "    # train\n",
    "    train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "    # validation\n",
    "    if epoch % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            val_res = infer(net, X, G, lbl, val_mask)\n",
    "        if val_res > best_val:\n",
    "            print(f\"update best: {val_res:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            best_val = val_res\n",
    "            best_state = deepcopy(net.state_dict())\n",
    "print(\"\\ntrain finished!\")\n",
    "print(f\"best val: {best_val:.5f}\")\n",
    "# test\n",
    "print(\"test...\")\n",
    "net.load_state_dict(best_state)\n",
    "res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "print(f\"final result: epoch: {best_epoch}\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Experiment (No Top-k) ===\n",
      "Using original hypergraph (no Top-k preprocessing)\n",
      "Epoch: 0, Time: 0.06937s, Loss: 1.84667\n",
      "update best: 0.21014\n",
      "Epoch: 1, Time: 0.03478s, Loss: 1.56151\n",
      "Epoch: 2, Time: 0.02969s, Loss: 1.32516\n",
      "update best: 0.21361\n",
      "Epoch: 3, Time: 0.04779s, Loss: 1.21602\n",
      "update best: 0.21960\n",
      "Epoch: 4, Time: 0.02403s, Loss: 1.15841\n",
      "update best: 0.22023\n",
      "Epoch: 5, Time: 0.01888s, Loss: 1.12637\n",
      "Epoch: 6, Time: 0.01813s, Loss: 1.11725\n",
      "Epoch: 7, Time: 0.01915s, Loss: 1.10174\n",
      "Epoch: 8, Time: 0.01913s, Loss: 1.10152\n",
      "Epoch: 9, Time: 0.01982s, Loss: 1.08360\n",
      "update best: 0.22054\n",
      "Epoch: 10, Time: 0.01888s, Loss: 1.07704\n",
      "update best: 0.22149\n",
      "Epoch: 11, Time: 0.01946s, Loss: 1.07767\n",
      "Epoch: 12, Time: 0.01809s, Loss: 1.07627\n",
      "update best: 0.22180\n",
      "Epoch: 13, Time: 0.01632s, Loss: 1.07444\n",
      "update best: 0.22243\n",
      "Epoch: 14, Time: 0.02069s, Loss: 1.07337\n",
      "update best: 0.22369\n",
      "Epoch: 15, Time: 0.01838s, Loss: 1.07144\n",
      "update best: 0.22464\n",
      "Epoch: 16, Time: 0.02056s, Loss: 1.06839\n",
      "update best: 0.22527\n",
      "Epoch: 17, Time: 0.01934s, Loss: 1.07026\n",
      "update best: 0.22590\n",
      "Epoch: 18, Time: 0.01997s, Loss: 1.06749\n",
      "update best: 0.22810\n",
      "Epoch: 19, Time: 0.02518s, Loss: 1.06799\n",
      "update best: 0.22873\n",
      "Epoch: 20, Time: 0.02006s, Loss: 1.06585\n",
      "update best: 0.23031\n",
      "Epoch: 21, Time: 0.03864s, Loss: 1.06540\n",
      "update best: 0.23094\n",
      "Epoch: 22, Time: 0.02472s, Loss: 1.06563\n",
      "update best: 0.23283\n",
      "Epoch: 23, Time: 0.01746s, Loss: 1.06588\n",
      "update best: 0.23440\n",
      "Epoch: 24, Time: 0.01816s, Loss: 1.06663\n",
      "update best: 0.23503\n",
      "Epoch: 25, Time: 0.01646s, Loss: 1.06579\n",
      "update best: 0.23661\n",
      "Epoch: 26, Time: 0.01418s, Loss: 1.06555\n",
      "update best: 0.23850\n",
      "Epoch: 27, Time: 0.01940s, Loss: 1.06543\n",
      "update best: 0.23913\n",
      "Epoch: 28, Time: 0.01814s, Loss: 1.06576\n",
      "update best: 0.24228\n",
      "Epoch: 29, Time: 0.01654s, Loss: 1.06630\n",
      "update best: 0.24354\n",
      "Epoch: 30, Time: 0.02108s, Loss: 1.06971\n",
      "update best: 0.24480\n",
      "Epoch: 31, Time: 0.02196s, Loss: 1.06689\n",
      "update best: 0.24606\n",
      "Epoch: 32, Time: 0.02126s, Loss: 1.06523\n",
      "update best: 0.24732\n",
      "Epoch: 33, Time: 0.02229s, Loss: 1.06573\n",
      "update best: 0.24921\n",
      "Epoch: 34, Time: 0.02110s, Loss: 1.06611\n",
      "update best: 0.25205\n",
      "Epoch: 35, Time: 0.01491s, Loss: 1.06583\n",
      "update best: 0.25268\n",
      "Epoch: 36, Time: 0.02071s, Loss: 1.06536\n",
      "update best: 0.25299\n",
      "Epoch: 37, Time: 0.02373s, Loss: 1.06547\n",
      "update best: 0.25520\n",
      "Epoch: 38, Time: 0.01887s, Loss: 1.06559\n",
      "update best: 0.25772\n",
      "Epoch: 39, Time: 0.01726s, Loss: 1.06651\n",
      "update best: 0.26024\n",
      "Epoch: 40, Time: 0.01665s, Loss: 1.06545\n",
      "update best: 0.26213\n",
      "Epoch: 41, Time: 0.01568s, Loss: 1.06932\n",
      "update best: 0.26686\n",
      "Epoch: 42, Time: 0.01882s, Loss: 1.06518\n",
      "update best: 0.27284\n",
      "Epoch: 43, Time: 0.01847s, Loss: 1.06632\n",
      "update best: 0.27694\n",
      "Epoch: 44, Time: 0.02051s, Loss: 1.06535\n",
      "update best: 0.28261\n",
      "Epoch: 45, Time: 0.01440s, Loss: 1.06487\n",
      "update best: 0.28922\n",
      "Epoch: 46, Time: 0.01278s, Loss: 1.06737\n",
      "update best: 0.29395\n",
      "Epoch: 47, Time: 0.02161s, Loss: 1.06573\n",
      "update best: 0.30025\n",
      "Epoch: 48, Time: 0.01658s, Loss: 1.06771\n",
      "update best: 0.30529\n",
      "Epoch: 49, Time: 0.02203s, Loss: 1.06531\n",
      "update best: 0.30844\n",
      "Epoch: 50, Time: 0.02295s, Loss: 1.06524\n",
      "Epoch: 51, Time: 0.01611s, Loss: 1.06735\n",
      "Epoch: 52, Time: 0.01487s, Loss: 1.06611\n",
      "Epoch: 53, Time: 0.01798s, Loss: 1.06505\n",
      "Epoch: 54, Time: 0.01751s, Loss: 1.06494\n",
      "Epoch: 55, Time: 0.01700s, Loss: 1.06523\n",
      "Epoch: 56, Time: 0.01782s, Loss: 1.06524\n",
      "update best: 0.30907\n",
      "Epoch: 57, Time: 0.01779s, Loss: 1.06639\n",
      "Epoch: 58, Time: 0.01718s, Loss: 1.06497\n",
      "Epoch: 59, Time: 0.01743s, Loss: 1.06528\n",
      "Epoch: 60, Time: 0.01958s, Loss: 1.06625\n",
      "Epoch: 61, Time: 0.02069s, Loss: 1.06578\n",
      "Epoch: 62, Time: 0.02205s, Loss: 1.06531\n",
      "Epoch: 63, Time: 0.01974s, Loss: 1.06577\n",
      "Epoch: 64, Time: 0.01765s, Loss: 1.06954\n",
      "Epoch: 65, Time: 0.01933s, Loss: 1.06588\n",
      "Epoch: 66, Time: 0.02240s, Loss: 1.06586\n",
      "Epoch: 67, Time: 0.01643s, Loss: 1.06498\n",
      "Epoch: 68, Time: 0.02143s, Loss: 1.06496\n",
      "Epoch: 69, Time: 0.01870s, Loss: 1.06564\n",
      "Epoch: 70, Time: 0.01784s, Loss: 1.06518\n",
      "Epoch: 71, Time: 0.01618s, Loss: 1.06531\n",
      "Epoch: 72, Time: 0.01860s, Loss: 1.06525\n",
      "Epoch: 73, Time: 0.01958s, Loss: 1.06627\n",
      "Epoch: 74, Time: 0.01948s, Loss: 1.06626\n",
      "Epoch: 75, Time: 0.01534s, Loss: 1.06529\n",
      "Epoch: 76, Time: 0.02089s, Loss: 1.06523\n",
      "Epoch: 77, Time: 0.01645s, Loss: 1.06529\n",
      "Epoch: 78, Time: 0.01748s, Loss: 1.06489\n",
      "Epoch: 79, Time: 0.01631s, Loss: 1.06540\n",
      "Epoch: 80, Time: 0.01887s, Loss: 1.06661\n",
      "Epoch: 81, Time: 0.01971s, Loss: 1.06601\n",
      "Epoch: 82, Time: 0.01765s, Loss: 1.06509\n",
      "Epoch: 83, Time: 0.01417s, Loss: 1.06528\n",
      "Epoch: 84, Time: 0.02164s, Loss: 1.06500\n",
      "Epoch: 85, Time: 0.01546s, Loss: 1.06513\n",
      "Epoch: 86, Time: 0.01848s, Loss: 1.06620\n",
      "Epoch: 87, Time: 0.01827s, Loss: 1.06537\n",
      "Epoch: 88, Time: 0.02056s, Loss: 1.06561\n",
      "Epoch: 89, Time: 0.01637s, Loss: 1.06596\n",
      "Epoch: 90, Time: 0.01818s, Loss: 1.06500\n",
      "Epoch: 91, Time: 0.01835s, Loss: 1.06544\n",
      "Epoch: 92, Time: 0.01780s, Loss: 1.06622\n",
      "Epoch: 93, Time: 0.01545s, Loss: 1.06540\n",
      "Epoch: 94, Time: 0.01683s, Loss: 1.06533\n",
      "Epoch: 95, Time: 0.00961s, Loss: 1.06524\n",
      "Epoch: 96, Time: 0.01486s, Loss: 1.06558\n",
      "Epoch: 97, Time: 0.01445s, Loss: 1.06523\n",
      "Epoch: 98, Time: 0.01343s, Loss: 1.06561\n",
      "Epoch: 99, Time: 0.01764s, Loss: 1.06520\n",
      "Epoch: 100, Time: 0.01711s, Loss: 1.06568\n",
      "Epoch: 101, Time: 0.01642s, Loss: 1.06556\n",
      "Epoch: 102, Time: 0.01880s, Loss: 1.06547\n",
      "Epoch: 103, Time: 0.02141s, Loss: 1.06525\n",
      "Epoch: 104, Time: 0.02041s, Loss: 1.06551\n",
      "Epoch: 105, Time: 0.01838s, Loss: 1.06518\n",
      "Epoch: 106, Time: 0.02069s, Loss: 1.06562\n",
      "Epoch: 107, Time: 0.01834s, Loss: 1.06548\n",
      "Epoch: 108, Time: 0.01653s, Loss: 1.06662\n",
      "Epoch: 109, Time: 0.01662s, Loss: 1.06501\n",
      "Epoch: 110, Time: 0.01663s, Loss: 1.06511\n",
      "Epoch: 111, Time: 0.01668s, Loss: 1.06533\n",
      "Epoch: 112, Time: 0.01661s, Loss: 1.06556\n",
      "Epoch: 113, Time: 0.01515s, Loss: 1.06608\n",
      "Epoch: 114, Time: 0.01924s, Loss: 1.06579\n",
      "Epoch: 115, Time: 0.01623s, Loss: 1.06643\n",
      "Epoch: 116, Time: 0.01687s, Loss: 1.06668\n",
      "Epoch: 117, Time: 0.02302s, Loss: 1.06563\n",
      "Epoch: 118, Time: 0.02006s, Loss: 1.06565\n",
      "Epoch: 119, Time: 0.01810s, Loss: 1.06541\n",
      "Epoch: 120, Time: 0.01875s, Loss: 1.06568\n",
      "Epoch: 121, Time: 0.01556s, Loss: 1.06552\n",
      "Epoch: 122, Time: 0.01580s, Loss: 1.06582\n",
      "Epoch: 123, Time: 0.01397s, Loss: 1.06587\n",
      "Epoch: 124, Time: 0.01464s, Loss: 1.06540\n",
      "Epoch: 125, Time: 0.01721s, Loss: 1.06555\n",
      "Epoch: 126, Time: 0.01737s, Loss: 1.06720\n",
      "Epoch: 127, Time: 0.01820s, Loss: 1.06625\n",
      "Epoch: 128, Time: 0.01723s, Loss: 1.06561\n",
      "Epoch: 129, Time: 0.01836s, Loss: 1.06507\n",
      "Epoch: 130, Time: 0.02318s, Loss: 1.06653\n",
      "Epoch: 131, Time: 0.01722s, Loss: 1.06598\n",
      "Epoch: 132, Time: 0.01641s, Loss: 1.06506\n",
      "Epoch: 133, Time: 0.01652s, Loss: 1.06573\n",
      "Epoch: 134, Time: 0.02001s, Loss: 1.07398\n",
      "Epoch: 135, Time: 0.02194s, Loss: 1.06520\n",
      "Epoch: 136, Time: 0.01737s, Loss: 1.06595\n",
      "Epoch: 137, Time: 0.01596s, Loss: 1.06582\n",
      "Epoch: 138, Time: 0.01830s, Loss: 1.06600\n",
      "Epoch: 139, Time: 0.01419s, Loss: 1.06568\n",
      "Epoch: 140, Time: 0.01439s, Loss: 1.06647\n",
      "Epoch: 141, Time: 0.01596s, Loss: 1.06581\n",
      "Epoch: 142, Time: 0.01908s, Loss: 1.06571\n",
      "Epoch: 143, Time: 0.01774s, Loss: 1.06544\n",
      "Epoch: 144, Time: 0.01544s, Loss: 1.06689\n",
      "Epoch: 145, Time: 0.01647s, Loss: 1.06594\n",
      "Epoch: 146, Time: 0.01264s, Loss: 1.06531\n",
      "Epoch: 147, Time: 0.01781s, Loss: 1.06523\n",
      "Epoch: 148, Time: 0.01835s, Loss: 1.06504\n",
      "Epoch: 149, Time: 0.01573s, Loss: 1.06545\n",
      "Epoch: 150, Time: 0.02005s, Loss: 1.06600\n",
      "Epoch: 151, Time: 0.01521s, Loss: 1.06673\n",
      "Epoch: 152, Time: 0.01710s, Loss: 1.06540\n",
      "Epoch: 153, Time: 0.01482s, Loss: 1.06659\n",
      "Epoch: 154, Time: 0.01641s, Loss: 1.06537\n",
      "Epoch: 155, Time: 0.01722s, Loss: 1.06526\n",
      "Epoch: 156, Time: 0.01513s, Loss: 1.06598\n",
      "Epoch: 157, Time: 0.01639s, Loss: 1.06505\n",
      "Epoch: 158, Time: 0.01848s, Loss: 1.06601\n",
      "Epoch: 159, Time: 0.01890s, Loss: 1.07954\n",
      "Epoch: 160, Time: 0.01546s, Loss: 1.06505\n",
      "Epoch: 161, Time: 0.01483s, Loss: 1.06558\n",
      "Epoch: 162, Time: 0.01780s, Loss: 1.06533\n",
      "Epoch: 163, Time: 0.01427s, Loss: 1.06565\n",
      "Epoch: 164, Time: 0.01694s, Loss: 1.06739\n",
      "Epoch: 165, Time: 0.01820s, Loss: 1.06558\n",
      "Epoch: 166, Time: 0.01369s, Loss: 1.06535\n",
      "update best: 0.31065\n",
      "Epoch: 167, Time: 0.01588s, Loss: 1.06600\n",
      "update best: 0.31222\n",
      "Epoch: 168, Time: 0.01453s, Loss: 1.06576\n",
      "update best: 0.31411\n",
      "Epoch: 169, Time: 0.01234s, Loss: 1.06575\n",
      "Epoch: 170, Time: 0.01634s, Loss: 1.06672\n",
      "Epoch: 171, Time: 0.01523s, Loss: 1.07067\n",
      "Epoch: 172, Time: 0.01926s, Loss: 1.06599\n",
      "Epoch: 173, Time: 0.01206s, Loss: 1.06577\n",
      "Epoch: 174, Time: 0.01843s, Loss: 1.06545\n",
      "Epoch: 175, Time: 0.01554s, Loss: 1.06565\n",
      "Epoch: 176, Time: 0.01660s, Loss: 1.06719\n",
      "Epoch: 177, Time: 0.01723s, Loss: 1.06799\n",
      "Epoch: 178, Time: 0.01610s, Loss: 1.06552\n",
      "Epoch: 179, Time: 0.01629s, Loss: 1.06543\n",
      "Epoch: 180, Time: 0.01191s, Loss: 1.06553\n",
      "Epoch: 181, Time: 0.01449s, Loss: 1.06599\n",
      "Epoch: 182, Time: 0.01931s, Loss: 1.06560\n",
      "Epoch: 183, Time: 0.01864s, Loss: 1.06541\n",
      "Epoch: 184, Time: 0.01652s, Loss: 1.06559\n",
      "Epoch: 185, Time: 0.02020s, Loss: 1.06522\n",
      "Epoch: 186, Time: 0.01820s, Loss: 1.06582\n",
      "Epoch: 187, Time: 0.01541s, Loss: 1.06633\n",
      "Epoch: 188, Time: 0.01390s, Loss: 1.06574\n",
      "Epoch: 189, Time: 0.01462s, Loss: 1.06550\n",
      "Epoch: 190, Time: 0.01733s, Loss: 1.06523\n",
      "Epoch: 191, Time: 0.01547s, Loss: 1.06542\n",
      "Epoch: 192, Time: 0.01745s, Loss: 1.06509\n",
      "Epoch: 193, Time: 0.01468s, Loss: 1.06807\n",
      "Epoch: 194, Time: 0.01670s, Loss: 1.06519\n",
      "Epoch: 195, Time: 0.02036s, Loss: 1.06596\n",
      "Epoch: 196, Time: 0.01871s, Loss: 1.06728\n",
      "Epoch: 197, Time: 0.01775s, Loss: 1.06608\n",
      "Epoch: 198, Time: 0.01911s, Loss: 1.06590\n",
      "Epoch: 199, Time: 0.01631s, Loss: 1.06588\n",
      "\n",
      "train finished!\n",
      "best val: 0.31411\n",
      "test...\n",
      "final result: epoch: 168\n",
      "{'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "\n",
      "=== Experiment with Top-k Densest Subgraphs ===\n",
      "Using Top-3 Densest Subgraphs preprocessing\n",
      "Epoch: 0, Time: 0.06559s, Loss: 1.82510\n",
      "update best: 0.21361\n",
      "Epoch: 1, Time: 0.02902s, Loss: 1.62440\n",
      "Epoch: 2, Time: 0.02847s, Loss: 1.39076\n",
      "Epoch: 3, Time: 0.02935s, Loss: 1.26225\n",
      "Epoch: 4, Time: 0.03275s, Loss: 1.21692\n",
      "Epoch: 5, Time: 0.03256s, Loss: 1.19965\n",
      "Epoch: 6, Time: 0.03542s, Loss: 1.19788\n",
      "Epoch: 7, Time: 0.02822s, Loss: 1.16639\n",
      "Epoch: 8, Time: 0.02576s, Loss: 1.16581\n",
      "Epoch: 9, Time: 0.02931s, Loss: 1.16196\n",
      "Epoch: 10, Time: 0.02472s, Loss: 1.16319\n",
      "Epoch: 11, Time: 0.03068s, Loss: 1.15742\n",
      "Epoch: 12, Time: 0.02637s, Loss: 1.15874\n",
      "Epoch: 13, Time: 0.02626s, Loss: 1.15653\n",
      "Epoch: 14, Time: 0.02875s, Loss: 1.15744\n",
      "Epoch: 15, Time: 0.03449s, Loss: 1.15601\n",
      "Epoch: 16, Time: 0.03268s, Loss: 1.15622\n",
      "Epoch: 17, Time: 0.03114s, Loss: 1.15699\n",
      "Epoch: 18, Time: 0.02857s, Loss: 1.15729\n",
      "Epoch: 19, Time: 0.02153s, Loss: 1.15600\n",
      "Epoch: 20, Time: 0.03093s, Loss: 1.15623\n",
      "Epoch: 21, Time: 0.03050s, Loss: 1.15606\n",
      "Epoch: 22, Time: 0.02683s, Loss: 1.15718\n",
      "update best: 0.21456\n",
      "Epoch: 23, Time: 0.03116s, Loss: 1.15620\n",
      "update best: 0.21550\n",
      "Epoch: 24, Time: 0.02556s, Loss: 1.15575\n",
      "update best: 0.21834\n",
      "Epoch: 25, Time: 0.03636s, Loss: 1.15600\n",
      "update best: 0.21960\n",
      "Epoch: 26, Time: 0.03473s, Loss: 1.15822\n",
      "update best: 0.21991\n",
      "Epoch: 27, Time: 0.02267s, Loss: 1.15585\n",
      "update best: 0.22117\n",
      "Epoch: 28, Time: 0.02434s, Loss: 1.15610\n",
      "update best: 0.22369\n",
      "Epoch: 29, Time: 0.03203s, Loss: 1.15606\n",
      "update best: 0.22464\n",
      "Epoch: 30, Time: 0.03443s, Loss: 1.15708\n",
      "update best: 0.22716\n",
      "Epoch: 31, Time: 0.02783s, Loss: 1.15568\n",
      "Epoch: 32, Time: 0.02653s, Loss: 1.15782\n",
      "update best: 0.22779\n",
      "Epoch: 33, Time: 0.02463s, Loss: 1.15597\n",
      "update best: 0.22999\n",
      "Epoch: 34, Time: 0.03037s, Loss: 1.15607\n",
      "Epoch: 35, Time: 0.02218s, Loss: 1.16056\n",
      "update best: 0.23125\n",
      "Epoch: 36, Time: 0.02994s, Loss: 1.15738\n",
      "update best: 0.23251\n",
      "Epoch: 37, Time: 0.03148s, Loss: 1.15582\n",
      "update best: 0.23566\n",
      "Epoch: 38, Time: 0.02190s, Loss: 1.15774\n",
      "update best: 0.23693\n",
      "Epoch: 39, Time: 0.03193s, Loss: 1.15581\n",
      "update best: 0.23787\n",
      "Epoch: 40, Time: 0.03539s, Loss: 1.15770\n",
      "update best: 0.23913\n",
      "Epoch: 41, Time: 0.03075s, Loss: 1.15707\n",
      "update best: 0.24008\n",
      "Epoch: 42, Time: 0.02908s, Loss: 1.15580\n",
      "update best: 0.24134\n",
      "Epoch: 43, Time: 0.02855s, Loss: 1.16010\n",
      "update best: 0.24417\n",
      "Epoch: 44, Time: 0.02485s, Loss: 1.15595\n",
      "update best: 0.24480\n",
      "Epoch: 45, Time: 0.02687s, Loss: 1.15583\n",
      "update best: 0.24606\n",
      "Epoch: 46, Time: 0.02626s, Loss: 1.15602\n",
      "update best: 0.24732\n",
      "Epoch: 47, Time: 0.02447s, Loss: 1.15593\n",
      "update best: 0.24764\n",
      "Epoch: 48, Time: 0.02797s, Loss: 1.15584\n",
      "Epoch: 49, Time: 0.02457s, Loss: 1.15605\n",
      "update best: 0.24858\n",
      "Epoch: 50, Time: 0.03008s, Loss: 1.15597\n",
      "update best: 0.24890\n",
      "Epoch: 51, Time: 0.02840s, Loss: 1.15596\n",
      "update best: 0.24921\n",
      "Epoch: 52, Time: 0.02966s, Loss: 1.15576\n",
      "update best: 0.25047\n",
      "Epoch: 53, Time: 0.02614s, Loss: 1.15571\n",
      "Epoch: 54, Time: 0.02849s, Loss: 1.15593\n",
      "Epoch: 55, Time: 0.03094s, Loss: 1.15567\n",
      "Epoch: 56, Time: 0.02563s, Loss: 1.15609\n",
      "update best: 0.25110\n",
      "Epoch: 57, Time: 0.02449s, Loss: 1.15612\n",
      "update best: 0.25236\n",
      "Epoch: 58, Time: 0.02537s, Loss: 1.15575\n",
      "update best: 0.25299\n",
      "Epoch: 59, Time: 0.02872s, Loss: 1.15582\n",
      "update best: 0.25425\n",
      "Epoch: 60, Time: 0.03122s, Loss: 1.15583\n",
      "update best: 0.25457\n",
      "Epoch: 61, Time: 0.03164s, Loss: 1.15581\n",
      "Epoch: 62, Time: 0.03158s, Loss: 1.15587\n",
      "Epoch: 63, Time: 0.02415s, Loss: 1.15588\n",
      "update best: 0.25520\n",
      "Epoch: 64, Time: 0.03063s, Loss: 1.15595\n",
      "update best: 0.25551\n",
      "Epoch: 65, Time: 0.02690s, Loss: 1.15598\n",
      "Epoch: 66, Time: 0.02762s, Loss: 1.15590\n",
      "Epoch: 67, Time: 0.02945s, Loss: 1.15575\n",
      "update best: 0.25677\n",
      "Epoch: 68, Time: 0.03464s, Loss: 1.15585\n",
      "update best: 0.25803\n",
      "Epoch: 69, Time: 0.03152s, Loss: 1.15595\n",
      "update best: 0.25835\n",
      "Epoch: 70, Time: 0.03722s, Loss: 1.15586\n",
      "update best: 0.25866\n",
      "Epoch: 71, Time: 0.02826s, Loss: 1.15621\n",
      "Epoch: 72, Time: 0.03272s, Loss: 1.15589\n",
      "Epoch: 73, Time: 0.03118s, Loss: 1.15693\n",
      "Epoch: 74, Time: 0.02265s, Loss: 1.15596\n",
      "update best: 0.25961\n",
      "Epoch: 75, Time: 0.03024s, Loss: 1.15583\n",
      "Epoch: 76, Time: 0.02835s, Loss: 1.15777\n",
      "Epoch: 77, Time: 0.03109s, Loss: 1.15602\n",
      "Epoch: 78, Time: 0.02700s, Loss: 1.15594\n",
      "Epoch: 79, Time: 0.03067s, Loss: 1.15586\n",
      "update best: 0.25992\n",
      "Epoch: 80, Time: 0.02818s, Loss: 1.15567\n",
      "Epoch: 81, Time: 0.03168s, Loss: 1.15586\n",
      "Epoch: 82, Time: 0.03077s, Loss: 1.15606\n",
      "update best: 0.26024\n",
      "Epoch: 83, Time: 0.02648s, Loss: 1.15578\n",
      "update best: 0.26055\n",
      "Epoch: 84, Time: 0.03257s, Loss: 1.15599\n",
      "update best: 0.26087\n",
      "Epoch: 85, Time: 0.03028s, Loss: 1.15595\n",
      "Epoch: 86, Time: 0.03238s, Loss: 1.15612\n",
      "Epoch: 87, Time: 0.02833s, Loss: 1.15617\n",
      "Epoch: 88, Time: 0.02255s, Loss: 1.15612\n",
      "Epoch: 89, Time: 0.03143s, Loss: 1.15581\n",
      "Epoch: 90, Time: 0.03042s, Loss: 1.15598\n",
      "update best: 0.26150\n",
      "Epoch: 91, Time: 0.03111s, Loss: 1.15649\n",
      "Epoch: 92, Time: 0.02915s, Loss: 1.15632\n",
      "Epoch: 93, Time: 0.03287s, Loss: 1.15573\n",
      "update best: 0.26181\n",
      "Epoch: 94, Time: 0.03230s, Loss: 1.15726\n",
      "update best: 0.26213\n",
      "Epoch: 95, Time: 0.02853s, Loss: 1.15721\n",
      "Epoch: 96, Time: 0.03244s, Loss: 1.15582\n",
      "Epoch: 97, Time: 0.03126s, Loss: 1.15621\n",
      "update best: 0.26244\n",
      "Epoch: 98, Time: 0.02367s, Loss: 1.15600\n",
      "update best: 0.26307\n",
      "Epoch: 99, Time: 0.03417s, Loss: 1.15601\n",
      "Epoch: 100, Time: 0.02501s, Loss: 1.15614\n",
      "Epoch: 101, Time: 0.03208s, Loss: 1.15612\n",
      "Epoch: 102, Time: 0.02693s, Loss: 1.15597\n",
      "Epoch: 103, Time: 0.02612s, Loss: 1.15600\n",
      "Epoch: 104, Time: 0.02501s, Loss: 1.15746\n",
      "update best: 0.26402\n",
      "Epoch: 105, Time: 0.02798s, Loss: 1.15580\n",
      "update best: 0.26560\n",
      "Epoch: 106, Time: 0.02560s, Loss: 1.15584\n",
      "update best: 0.26654\n",
      "Epoch: 107, Time: 0.02655s, Loss: 1.15595\n",
      "update best: 0.26717\n",
      "Epoch: 108, Time: 0.03282s, Loss: 1.15584\n",
      "Epoch: 109, Time: 0.02493s, Loss: 1.15637\n",
      "update best: 0.26875\n",
      "Epoch: 110, Time: 0.02819s, Loss: 1.15628\n",
      "Epoch: 111, Time: 0.03212s, Loss: 1.15633\n",
      "Epoch: 112, Time: 0.02784s, Loss: 1.15634\n",
      "update best: 0.26969\n",
      "Epoch: 113, Time: 0.03127s, Loss: 1.15574\n",
      "Epoch: 114, Time: 0.02956s, Loss: 1.15815\n",
      "Epoch: 115, Time: 0.03216s, Loss: 1.15664\n",
      "Epoch: 116, Time: 0.02278s, Loss: 1.15611\n",
      "Epoch: 117, Time: 0.03280s, Loss: 1.15593\n",
      "Epoch: 118, Time: 0.02437s, Loss: 1.15600\n",
      "Epoch: 119, Time: 0.02233s, Loss: 1.15648\n",
      "Epoch: 120, Time: 0.03344s, Loss: 1.15758\n",
      "Epoch: 121, Time: 0.03560s, Loss: 1.15583\n",
      "Epoch: 122, Time: 0.03149s, Loss: 1.15573\n",
      "Epoch: 123, Time: 0.03710s, Loss: 1.15598\n",
      "Epoch: 124, Time: 0.02458s, Loss: 1.15600\n",
      "Epoch: 125, Time: 0.02412s, Loss: 1.15620\n",
      "Epoch: 126, Time: 0.03206s, Loss: 1.15640\n",
      "Epoch: 127, Time: 0.02860s, Loss: 1.15602\n",
      "Epoch: 128, Time: 0.02843s, Loss: 1.15700\n",
      "Epoch: 129, Time: 0.02828s, Loss: 1.15622\n",
      "Epoch: 130, Time: 0.02049s, Loss: 1.15669\n",
      "Epoch: 131, Time: 0.02248s, Loss: 1.15658\n",
      "Epoch: 132, Time: 0.02196s, Loss: 1.15610\n",
      "Epoch: 133, Time: 0.02219s, Loss: 1.15620\n",
      "Epoch: 134, Time: 0.02394s, Loss: 1.15636\n",
      "Epoch: 135, Time: 0.03258s, Loss: 1.15674\n",
      "Epoch: 136, Time: 0.02634s, Loss: 1.15605\n",
      "Epoch: 137, Time: 0.03094s, Loss: 1.15939\n",
      "update best: 0.27064\n",
      "Epoch: 138, Time: 0.02247s, Loss: 1.15665\n",
      "update best: 0.27127\n",
      "Epoch: 139, Time: 0.02647s, Loss: 1.15624\n",
      "update best: 0.27190\n",
      "Epoch: 140, Time: 0.02678s, Loss: 1.15860\n",
      "update best: 0.27505\n",
      "Epoch: 141, Time: 0.03109s, Loss: 1.15604\n",
      "Epoch: 142, Time: 0.02169s, Loss: 1.15715\n",
      "Epoch: 143, Time: 0.03042s, Loss: 1.15603\n",
      "Epoch: 144, Time: 0.02342s, Loss: 1.15832\n",
      "Epoch: 145, Time: 0.03094s, Loss: 1.15732\n",
      "Epoch: 146, Time: 0.03019s, Loss: 1.15621\n",
      "Epoch: 147, Time: 0.02492s, Loss: 1.15594\n",
      "Epoch: 148, Time: 0.02848s, Loss: 1.15635\n",
      "Epoch: 149, Time: 0.03001s, Loss: 1.15624\n",
      "Epoch: 150, Time: 0.03320s, Loss: 1.15611\n",
      "Epoch: 151, Time: 0.03006s, Loss: 1.15612\n",
      "Epoch: 152, Time: 0.03538s, Loss: 1.15713\n",
      "Epoch: 153, Time: 0.03224s, Loss: 1.15649\n",
      "Epoch: 154, Time: 0.03126s, Loss: 1.15601\n",
      "Epoch: 155, Time: 0.03383s, Loss: 1.15616\n",
      "Epoch: 156, Time: 0.02578s, Loss: 1.15601\n",
      "Epoch: 157, Time: 0.02708s, Loss: 1.15609\n",
      "Epoch: 158, Time: 0.03211s, Loss: 1.15645\n",
      "Epoch: 159, Time: 0.02848s, Loss: 1.15666\n",
      "Epoch: 160, Time: 0.02643s, Loss: 1.15719\n",
      "Epoch: 161, Time: 0.03038s, Loss: 1.15599\n",
      "Epoch: 162, Time: 0.03179s, Loss: 1.15762\n",
      "Epoch: 163, Time: 0.03095s, Loss: 1.15674\n",
      "Epoch: 164, Time: 0.02292s, Loss: 1.15618\n",
      "Epoch: 165, Time: 0.02855s, Loss: 1.15583\n",
      "Epoch: 166, Time: 0.02436s, Loss: 1.15596\n",
      "Epoch: 167, Time: 0.03066s, Loss: 1.15606\n",
      "Epoch: 168, Time: 0.03028s, Loss: 1.15703\n",
      "Epoch: 169, Time: 0.03328s, Loss: 1.15598\n",
      "Epoch: 170, Time: 0.03524s, Loss: 1.15600\n",
      "Epoch: 171, Time: 0.02893s, Loss: 1.15637\n",
      "Epoch: 172, Time: 0.02216s, Loss: 1.15700\n",
      "Epoch: 173, Time: 0.03289s, Loss: 1.15589\n",
      "Epoch: 174, Time: 0.02688s, Loss: 1.15718\n",
      "Epoch: 175, Time: 0.02044s, Loss: 1.15571\n",
      "Epoch: 176, Time: 0.02914s, Loss: 1.15585\n",
      "Epoch: 177, Time: 0.02653s, Loss: 1.15718\n",
      "Epoch: 178, Time: 0.02829s, Loss: 1.15691\n",
      "Epoch: 179, Time: 0.03039s, Loss: 1.15713\n",
      "Epoch: 180, Time: 0.03574s, Loss: 1.15608\n",
      "Epoch: 181, Time: 0.02998s, Loss: 1.15629\n",
      "Epoch: 182, Time: 0.03262s, Loss: 1.15587\n",
      "Epoch: 183, Time: 0.02428s, Loss: 1.15661\n",
      "Epoch: 184, Time: 0.03045s, Loss: 1.15654\n",
      "Epoch: 185, Time: 0.02416s, Loss: 1.15611\n",
      "Epoch: 186, Time: 0.03054s, Loss: 1.15641\n",
      "Epoch: 187, Time: 0.02843s, Loss: 1.15752\n",
      "Epoch: 188, Time: 0.02668s, Loss: 1.15607\n",
      "Epoch: 189, Time: 0.03046s, Loss: 1.15627\n",
      "Epoch: 190, Time: 0.02530s, Loss: 1.15581\n",
      "Epoch: 191, Time: 0.03288s, Loss: 1.15662\n",
      "Epoch: 192, Time: 0.03075s, Loss: 1.15663\n",
      "Epoch: 193, Time: 0.03123s, Loss: 1.15609\n",
      "Epoch: 194, Time: 0.02127s, Loss: 1.15605\n",
      "Epoch: 195, Time: 0.02872s, Loss: 1.15676\n",
      "Epoch: 196, Time: 0.02444s, Loss: 1.15607\n",
      "Epoch: 197, Time: 0.02503s, Loss: 1.15975\n",
      "Epoch: 198, Time: 0.03029s, Loss: 1.15604\n",
      "Epoch: 199, Time: 0.03272s, Loss: 1.15623\n",
      "\n",
      "train finished!\n",
      "best val: 0.27505\n",
      "test...\n",
      "final result: epoch: 140\n",
      "{'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n",
      "\n",
      "=== Comparison of Results ===\n",
      "Baseline performance: {'accuracy': 0.31411468982696533, 'f1_score': 0.2344598491491492, 'f1_score -> average@micro': 0.31411468178954}\n",
      "Top-k performance: {'accuracy': 0.27504727244377136, 'f1_score': 0.184430126663067, 'f1_score -> average@micro': 0.27504725897920607}\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# import os\n",
    "# from copy import deepcopy\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import dhg\n",
    "# from dhg import Graph, Hypergraph\n",
    "# from dhg.data import Cooking200, News20\n",
    "# from dhg.models import GCN, HGNN, HGNNP, HNHN\n",
    "# from dhg.random import set_seed\n",
    "# from dhg.metrics import HypergraphVertexClassificationEvaluator as Evaluator\n",
    "# from dhg.utils import split_by_ratio\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def get_top_k_densest_subgraphs(hg: Hypergraph, k: int = 3) -> list:\n",
    "    \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "    nodes = set(range(hg.num_v))\n",
    "    subgraphs = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        if len(nodes) < 3:  # min_size\n",
    "            break\n",
    "            \n",
    "        current_nodes = set(nodes)\n",
    "        best_subset = None\n",
    "        best_density = -1\n",
    "        \n",
    "        while len(current_nodes) >= 3:\n",
    "            edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "            density = edge_count / len(current_nodes)\n",
    "            \n",
    "            if density > best_density:\n",
    "                best_density = density\n",
    "                best_subset = set(current_nodes)\n",
    "            \n",
    "            # Remove node with lowest degree\n",
    "            degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "            node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "            current_nodes.remove(node_to_remove)\n",
    "        \n",
    "        if best_subset:\n",
    "            subgraphs.append((best_subset, best_density))\n",
    "            nodes -= best_subset\n",
    "    \n",
    "    return subgraphs\n",
    "\n",
    "def preprocess_hypergraph_with_topk(hg: Hypergraph, k: int = 3) -> Hypergraph:\n",
    "    \"\"\"Preprocess hypergraph by focusing on top-k densest subgraphs\"\"\"\n",
    "    subgraphs = get_top_k_densest_subgraphs(hg, k)\n",
    "    if not subgraphs:\n",
    "        return hg\n",
    "    \n",
    "    # Combine all nodes from top-k subgraphs\n",
    "    important_nodes = set()\n",
    "    for subset, _ in subgraphs:\n",
    "        important_nodes.update(subset)\n",
    "    \n",
    "    # Create new hypergraph with only important edges\n",
    "    new_edges = []\n",
    "    for e in hg.e[0]:\n",
    "        if set(e).issubset(important_nodes):\n",
    "            new_edges.append(e)\n",
    "    \n",
    "    return Hypergraph(hg.num_v, new_edges)\n",
    "\n",
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "\n",
    "    st = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Time: {time.time()-st:.5f}s, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    return res\n",
    "\n",
    "def run_experiment(use_topk=False, k=3):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "    X, lbl = torch.eye(data[\"num_vertices\"]), data[\"labels\"]\n",
    "    G = Hypergraph(data[\"num_vertices\"], data[\"edge_list\"])\n",
    "    \n",
    "    if use_topk:\n",
    "        print(f\"Using Top-{k} Densest Subgraphs preprocessing\")\n",
    "        G = preprocess_hypergraph_with_topk(G, k)\n",
    "    else:\n",
    "        print(\"Using original hypergraph (no Top-k preprocessing)\")\n",
    "    \n",
    "    train_mask = data[\"train_mask\"]\n",
    "    val_mask = data[\"val_mask\"]\n",
    "    test_mask = data[\"test_mask\"]\n",
    "\n",
    "    net = HGNN(X.shape[1], 32, data[\"num_classes\"], use_bn=True)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "    X, lbl = X.to(device), lbl.to(device)\n",
    "    G = G.to(device)\n",
    "    net = net.to(device)\n",
    "\n",
    "    best_state = None\n",
    "    best_epoch, best_val = 0, 0\n",
    "    for epoch in range(200):\n",
    "        # train\n",
    "        train(net, X, G, lbl, train_mask, optimizer, epoch)\n",
    "        # validation\n",
    "        if epoch % 1 == 0:\n",
    "            with torch.no_grad():\n",
    "                val_res = infer(net, X, G, lbl, val_mask)\n",
    "            if val_res > best_val:\n",
    "                print(f\"update best: {val_res:.5f}\")\n",
    "                best_epoch = epoch\n",
    "                best_val = val_res\n",
    "                best_state = deepcopy(net.state_dict())\n",
    "    print(\"\\ntrain finished!\")\n",
    "    print(f\"best val: {best_val:.5f}\")\n",
    "    # test\n",
    "    print(\"test...\")\n",
    "    net.load_state_dict(best_state)\n",
    "    res = infer(net, X, G, lbl, test_mask, test=True)\n",
    "    print(f\"final result: epoch: {best_epoch}\")\n",
    "    print(res)\n",
    "    return res\n",
    "\n",
    "# Run experiments\n",
    "print(\"=== Baseline Experiment (No Top-k) ===\")\n",
    "baseline_results = run_experiment(use_topk=False)\n",
    "\n",
    "print(\"\\n=== Experiment with Top-k Densest Subgraphs ===\")\n",
    "topk_results = run_experiment(use_topk=True, k=3)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Comparison of Results ===\")\n",
    "print(f\"Baseline performance: {baseline_results}\")\n",
    "print(f\"Top-k performance: {topk_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.11)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/utkarshx27/movies-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5.13M/5.13M [00:00<00:00, 23.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n",
      "Path to dataset files: C:\\Users\\rustem_izmailov\\.cache\\kagglehub\\datasets\\utkarshx27\\movies-dataset\\versions\\1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"utkarshx27/movies-dataset\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(movie_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4803 entries, 0 to 4802\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   index                 4803 non-null   int64  \n",
      " 1   budget                4803 non-null   int64  \n",
      " 2   genres                4775 non-null   object \n",
      " 3   homepage              1712 non-null   object \n",
      " 4   id                    4803 non-null   int64  \n",
      " 5   keywords              4391 non-null   object \n",
      " 6   original_language     4803 non-null   object \n",
      " 7   original_title        4803 non-null   object \n",
      " 8   overview              4800 non-null   object \n",
      " 9   popularity            4803 non-null   float64\n",
      " 10  production_companies  4803 non-null   object \n",
      " 11  production_countries  4803 non-null   object \n",
      " 12  release_date          4802 non-null   object \n",
      " 13  revenue               4803 non-null   int64  \n",
      " 14  runtime               4801 non-null   float64\n",
      " 15  spoken_languages      4803 non-null   object \n",
      " 16  status                4803 non-null   object \n",
      " 17  tagline               3959 non-null   object \n",
      " 18  title                 4803 non-null   object \n",
      " 19  vote_average          4803 non-null   float64\n",
      " 20  vote_count            4803 non-null   int64  \n",
      " 21  cast                  4760 non-null   object \n",
      " 22  crew                  4803 non-null   object \n",
      " 23  director              4773 non-null   object \n",
      "dtypes: float64(3), int64(5), object(16)\n",
      "memory usage: 900.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Action Adventure Fantasy Science Fiction\n",
       "1                    Adventure Fantasy Action\n",
       "2                      Action Adventure Crime\n",
       "3                 Action Crime Drama Thriller\n",
       "4            Action Adventure Science Fiction\n",
       "Name: genres, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genres.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypergraphExperiment:\n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = data_path\n",
    "        self.results = []\n",
    "        self.losses = {}\n",
    "        \n",
    "    def run_experiments(self, n_samples=500):\n",
    "        \"\"\"   \"\"\"\n",
    "        # 1.   (  )\n",
    "        print(\"\\n=== Experiment 1: Base Hypergraph ===\")\n",
    "        base_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=False\n",
    "        )\n",
    "        base_results = self._train_and_evaluate(base_data)\n",
    "        self.results.append((\"Base\", base_results))\n",
    "        \n",
    "        # 2.  -k  \n",
    "        print(\"\\n=== Experiment 2: With Densest Subgraphs ===\")\n",
    "        dense_data = MovieHypergraphDataset(\n",
    "            data_root=self.data_path,\n",
    "            n_samples=n_samples,\n",
    "            use_densest_subgraphs=True,\n",
    "            k=5\n",
    "        )\n",
    "        dense_results = self._train_and_evaluate(dense_data)\n",
    "        self.results.append((\"With Densest Subgraphs\", dense_results))\n",
    "        \n",
    "        # # 3.   \n",
    "        # print(\"\\n=== Experiment 3: Genre-Only Hypergraph ===\")\n",
    "        # genre_data = MovieHypergraphDataset(\n",
    "        #     data_root=self.data_path,\n",
    "        #     n_samples=n_samples,\n",
    "        #     hyperedge_types=[\"genre\"]\n",
    "        # )\n",
    "        # genre_results = self._train_and_evaluate(genre_data)\n",
    "        # self.results.append((\"Genre-Only\", genre_results))\n",
    "        \n",
    "        self._visualize_loss()\n",
    "        \n",
    "        \n",
    "        #  \n",
    "        self._visualize_results()\n",
    "\n",
    "    def _train_and_evaluate(self, data) -> Dict[str, float]:\n",
    "        \"\"\"    \"\"\"\n",
    "        # Get all the data we need\n",
    "        X = data[\"features\"]\n",
    "        lbl = data[\"labels\"]\n",
    "        edge_list = data[\"edge_list\"]\n",
    "        num_vertices = data[\"num_vertices\"]\n",
    "        train_mask = data[\"train_mask\"]\n",
    "        val_mask = data[\"val_mask\"]\n",
    "        test_mask = data[\"test_mask\"]\n",
    "        num_classes = data[\"num_classes\"]\n",
    "        \n",
    "        # Check if edge_list is empty\n",
    "        if not edge_list:\n",
    "            print(\"Warning: Empty edge list! Creating a fallback edge list based on nearest neighbors.\")\n",
    "            # Create a fallback edge list using k-nearest neighbors\n",
    "            from sklearn.neighbors import NearestNeighbors\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X.numpy())\n",
    "            _, indices = knn.kneighbors(X.numpy())\n",
    "            edge_list = [list(idx) for idx in indices]\n",
    "        \n",
    "        # Now use the potentially updated edge_list\n",
    "        hg = Hypergraph(num_vertices, edge_list)\n",
    "        masks = {\n",
    "            \"train\": train_mask,\n",
    "            \"val\": val_mask,\n",
    "            \"test\": test_mask\n",
    "        }\n",
    "        \n",
    "        models = {\n",
    "            \"HGNN\": HGNN(X.shape[1], 64, num_classes),\n",
    "            \"HGNNP\": HGNNP(X.shape[1], 64, num_classes, use_bn=True),\n",
    "            \"UniGCN\": UniGCN(X.shape[1], 64, num_classes, use_bn=True),\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            model = model.to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            \n",
    "            best_val_accuracy = 0\n",
    "            best_val_f1 = 0\n",
    "            losses = []\n",
    "            for epoch in range(100):\n",
    "                loss = train(model, X, hg, lbl, masks[\"train\"], optimizer, epoch)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                if epoch % 5 == 0:\n",
    "                    val_res = infer(model, X, hg, lbl, masks[\"val\"])\n",
    "                    # Extract the accuracy value from the dictionary\n",
    "                    val_accuracy = val_res.get('accuracy', 0)\n",
    "                    val_f1 = val_res.get('f1_score', 0)\n",
    "                    if val_accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = val_accuracy\n",
    "                    if val_f1 > best_val_f1:\n",
    "                        best_val_f1 = val_f1\n",
    "                        best_state = deepcopy(model.state_dict())\n",
    "            self.losses[name] = losses\n",
    "            \n",
    "            test_res = infer(model, X, hg, lbl, masks[\"test\"], test=True)\n",
    "            results[name] = {\n",
    "                \"val_accuracy\": best_val_accuracy,\n",
    "                \"val_f1\": best_val_f1,\n",
    "                \"test_accuracy\": test_res.get('accuracy', 0),\n",
    "                \"test_f1\": test_res.get('f1_score', 0)\n",
    "            }\n",
    "                  \n",
    "        return results\n",
    "\n",
    "\n",
    "    def _visualize_loss(self):\n",
    "        for name, losses in self.losses.items():\n",
    "            plt.plot(losses, label=name)\n",
    "        \n",
    "        plt.title(\"Losses\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"./images/losses.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _visualize_results(self):\n",
    "        print(self.results)\n",
    "        \"\"\"  \"\"\"\n",
    "        df_val = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        df_test = pd.DataFrame({\n",
    "            \"Experiment\": [exp[0] for exp in self.results],\n",
    "            # \"HGNN Val\": [exp[1][\"HGNN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNN Test\": [exp[1][\"HGNN\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"HGNNP Val\": [exp[1][\"HGNNP\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"HGNNP Test\": [exp[1][\"HGNNP\"][\"test_accuracy\"] for exp in self.results],\n",
    "            # \"UniGCN Val\": [exp[1][\"UniGCN\"][\"val_accuracy\"] for exp in self.results],\n",
    "            \"UniGCN Test\": [exp[1][\"UniGCN\"][\"test_accuracy\"] for exp in self.results]\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_val.plot(\n",
    "            x=\"Experiment\",\n",
    "            # y=[\"HGNN Val\", \"HGNN Test\", \"HGNNP Val\", \"HGNNP Test\", \"UniGCN Val\", \"UniGCN Test\"],\n",
    "            y=[\"HGNN Val\", \"HGNNP Val\", \"UniGCN Val\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Val)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./images/hypergraph_comparison_val.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = df_test.plot(\n",
    "            x=\"Experiment\",\n",
    "            y=[\"HGNN Test\", \"HGNNP Test\", \"UniGCN Test\"],\n",
    "            kind=\"bar\",\n",
    "            rot=45\n",
    "        )\n",
    "        plt.title(\"Comparison of Hypergraph Construction Methods (Test)\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./images/hypergraph_comparison_test.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n=== Results Summary Val ===\")\n",
    "        print(df_val.to_string(index=False))\n",
    "        print()\n",
    "        print(\"\\n=== Results Summary Test ===\")\n",
    "        print(df_test.to_string(index=False))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieHypergraphDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        n_samples: Optional[int] = None,\n",
    "        use_densest_subgraphs: bool = False,\n",
    "        k: int = 3,\n",
    "        hyperedge_types: List[str] = [\"genre\", \"director\", \"numerical\"]\n",
    "    ):\n",
    "        self.data_root = data_root\n",
    "        self.n_samples = n_samples\n",
    "        self.use_densest_subgraphs = use_densest_subgraphs\n",
    "        self.k = k\n",
    "        self.hyperedge_types = hyperedge_types\n",
    "        self._content = self._build_dataset()\n",
    "\n",
    "    def _build_dataset(self) -> Dict[str, Any]:\n",
    "        \"\"\"Build the complete dataset dictionary\"\"\"\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(self.data_root)\n",
    "        if self.n_samples:\n",
    "            df = df.sample(min(self.n_samples, len(df)))\n",
    "\n",
    "        df = self._preprocess_data(df)\n",
    "        \n",
    "        # Create features and labels\n",
    "        features = self._create_features(df)\n",
    "        labels = (df['revenue'] > df['budget']).astype(int).values\n",
    "        \n",
    "        # Create splits\n",
    "        train_mask, val_mask, test_mask = self._create_splits(labels)\n",
    "        \n",
    "        # Create hyperedges\n",
    "        edge_list = self._create_hyperedges(df)\n",
    "        \n",
    "        # Add densest subgraphs if enabled\n",
    "        if self.use_densest_subgraphs and len(edge_list) > 0:\n",
    "            temp_hg = Hypergraph(len(df), edge_list)\n",
    "            top_k_subgraphs = self._get_top_k_densest_subgraphs(temp_hg, k=self.k)\n",
    "            edge_list.extend([list(subset) for subset, _ in top_k_subgraphs])\n",
    "        \n",
    "        return {\n",
    "            \"num_classes\": 2,\n",
    "            \"num_vertices\": len(df),\n",
    "            \"num_edges\": len(edge_list),\n",
    "            \"features\": torch.FloatTensor(features),\n",
    "            \"labels\": torch.LongTensor(labels),\n",
    "            \"edge_list\": edge_list,\n",
    "            \"train_mask\": train_mask,\n",
    "            \"val_mask\": val_mask,\n",
    "            \"test_mask\": test_mask,\n",
    "        }\n",
    "\n",
    "    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Clean and preprocess the raw data\"\"\"\n",
    "        # Convert stringified lists to actual lists\n",
    "        for col in ['genres', 'keywords', 'production_companies', 'production_countries', 'spoken_languages']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: literal_eval(x) \n",
    "                    if pd.notna(x) and isinstance(x, str) and x.startswith('[') \n",
    "                    else []\n",
    "                )\n",
    "        \n",
    "        # Handle genres specially if they're space-separated strings\n",
    "        if 'genres' in df.columns:\n",
    "            df['genres'] = df['genres'].apply(\n",
    "                lambda x: [{'name': g.strip()} for g in x.split()] \n",
    "                if pd.notna(x) and isinstance(x, str) and not x.startswith('[')\n",
    "                else x\n",
    "            )\n",
    "        \n",
    "        # Fill missing values\n",
    "        text_cols = ['overview', 'tagline', 'director']\n",
    "        for col in text_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('')\n",
    "        \n",
    "        num_cols = ['runtime', 'budget', 'revenue', 'popularity', 'vote_average', 'vote_count']\n",
    "        for col in num_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _create_features(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create feature matrix combining numerical and text features\"\"\"\n",
    "        # Numerical features\n",
    "        num_features = ['budget', 'popularity', 'runtime', 'vote_average', 'vote_count']\n",
    "        X_num = StandardScaler().fit_transform(df[num_features].values) if num_features else np.zeros((len(df), 0))\n",
    "        \n",
    "        # Text features from overview\n",
    "        tfidf = TfidfVectorizer(max_features=200, stop_words='english')\n",
    "        X_text = tfidf.fit_transform(df['overview']).toarray() if 'overview' in df.columns else np.zeros((len(df), 0))\n",
    "        \n",
    "        return np.concatenate([X_num, X_text], axis=1)\n",
    "\n",
    "    def _create_splits(self, labels: np.ndarray) -> tuple:\n",
    "        \"\"\"Create train/val/test splits with stratification\"\"\"\n",
    "        indices = np.arange(len(labels))\n",
    "        train_idx, test_idx = train_test_split(indices, test_size=0.3, stratify=labels)\n",
    "        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=labels[test_idx])\n",
    "        \n",
    "        train_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        val_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(len(labels), dtype=torch.bool)\n",
    "        \n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "        \n",
    "        return train_mask, val_mask, test_mask\n",
    "\n",
    "    def _create_hyperedges(self, df: pd.DataFrame) -> list:\n",
    "        \"\"\"Create hyperedges based on specified types\"\"\"\n",
    "        edge_list = []\n",
    "        \n",
    "        if \"genre\" in self.hyperedge_types and 'genres' in df.columns:\n",
    "            genre_to_movies = {}\n",
    "            for idx, genres in enumerate(df['genres']):\n",
    "                if isinstance(genres, list):\n",
    "                    for genre in genres:\n",
    "                        name = genre['name'] if isinstance(genre, dict) else genre\n",
    "                        if name not in genre_to_movies:\n",
    "                            genre_to_movies[name] = []\n",
    "                        genre_to_movies[name].append(idx)\n",
    "            edge_list.extend(list(genre_to_movies.values()))\n",
    "        \n",
    "        if \"director\" in self.hyperedge_types and 'director' in df.columns:\n",
    "            director_to_movies = {}\n",
    "            for idx, director in enumerate(df['director']):\n",
    "                if pd.notna(director):\n",
    "                    if director not in director_to_movies:\n",
    "                        director_to_movies[director] = []\n",
    "                    director_to_movies[director].append(idx)\n",
    "            edge_list.extend(list(director_to_movies.values()))\n",
    "        \n",
    "        if \"numerical\" in self.hyperedge_types:\n",
    "            numerical_features = ['budget', 'popularity', 'runtime', 'vote_average']\n",
    "            X_num = df[numerical_features].values\n",
    "            knn = NearestNeighbors(n_neighbors=5).fit(X_num)\n",
    "            _, indices = knn.kneighbors(X_num)\n",
    "            edge_list.extend([list(idx) for idx in indices])\n",
    "        \n",
    "        return edge_list\n",
    "\n",
    "    def _get_top_k_densest_subgraphs(self, hg: Hypergraph, k: int = 3) -> list:\n",
    "        \"\"\"Find top-k densest subgraphs using greedy approximation\"\"\"\n",
    "        nodes = set(range(hg.num_v))\n",
    "        subgraphs = []\n",
    "        \n",
    "        for _ in range(k):\n",
    "            if len(nodes) < 3:  # min_size\n",
    "                break\n",
    "                \n",
    "            current_nodes = set(nodes)\n",
    "            best_subset = None\n",
    "            best_density = -1\n",
    "            \n",
    "            while len(current_nodes) >= 3:\n",
    "                edge_count = sum(1 for e in hg.e[0] if set(e).issubset(current_nodes))\n",
    "                density = edge_count / len(current_nodes)\n",
    "                \n",
    "                if density > best_density:\n",
    "                    best_density = density\n",
    "                    best_subset = set(current_nodes)\n",
    "                \n",
    "                # Remove node with lowest degree\n",
    "                degrees = {v: sum(v in e for e in hg.e[0]) for v in current_nodes}\n",
    "                node_to_remove = min(degrees.items(), key=lambda x: x[1])[0]\n",
    "                current_nodes.remove(node_to_remove)\n",
    "            \n",
    "            if best_subset:\n",
    "                subgraphs.append((best_subset, best_density))\n",
    "                nodes -= best_subset\n",
    "        \n",
    "        return subgraphs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._content[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, X, A, lbls, train_idx, optimizer, epoch):\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[train_idx], lbls[train_idx]\n",
    "    loss = F.cross_entropy(outs, lbls)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item():.5f}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer(net, X, A, lbls, idx, test=False):\n",
    "    net.eval()\n",
    "    outs = net(X, A)\n",
    "    outs, lbls = outs[idx], lbls[idx]\n",
    "    if not test:\n",
    "        res = evaluator.validate(lbls, outs)\n",
    "    else:\n",
    "        res = evaluator.test(lbls, outs)\n",
    "    \n",
    "    # Handle both cases: when evaluator returns dict or float\n",
    "    if isinstance(res, dict):\n",
    "        return res\n",
    "    else:\n",
    "        return {'accuracy': res, 'f1_score': res}  \n",
    "\n",
    "\n",
    "def evaluate_model(model, hypergraph_type, X, hg, labels, masks):\n",
    "    results = {}\n",
    "    for name, mask in masks.items():\n",
    "        res = infer(model, X, hg, labels, mask, test=(name == \"test\"))\n",
    "        results[f\"{hypergraph_type}_{name}\"] = res\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experiment 1: Base Hypergraph ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training HGNN...\n",
      "Epoch: 0, Loss: 0.69275\n",
      "Epoch: 1, Loss: 0.66751\n",
      "Epoch: 2, Loss: 0.64399\n",
      "Epoch: 3, Loss: 0.62005\n",
      "Epoch: 4, Loss: 0.59748\n",
      "Epoch: 5, Loss: 0.57722\n",
      "Epoch: 6, Loss: 0.56126\n",
      "Epoch: 7, Loss: 0.55088\n",
      "Epoch: 8, Loss: 0.54155\n",
      "Epoch: 9, Loss: 0.53810\n",
      "Epoch: 10, Loss: 0.53098\n",
      "Epoch: 11, Loss: 0.52912\n",
      "Epoch: 12, Loss: 0.52214\n",
      "Epoch: 13, Loss: 0.51526\n",
      "Epoch: 14, Loss: 0.51033\n",
      "Epoch: 15, Loss: 0.50401\n",
      "Epoch: 16, Loss: 0.49680\n",
      "Epoch: 17, Loss: 0.49455\n",
      "Epoch: 18, Loss: 0.49382\n",
      "Epoch: 19, Loss: 0.48841\n",
      "Epoch: 20, Loss: 0.48594\n",
      "Epoch: 21, Loss: 0.48648\n",
      "Epoch: 22, Loss: 0.48898\n",
      "Epoch: 23, Loss: 0.48500\n",
      "Epoch: 24, Loss: 0.48525\n",
      "Epoch: 25, Loss: 0.48145\n",
      "Epoch: 26, Loss: 0.48317\n",
      "Epoch: 27, Loss: 0.47578\n",
      "Epoch: 28, Loss: 0.47731\n",
      "Epoch: 29, Loss: 0.47532\n",
      "Epoch: 30, Loss: 0.47534\n",
      "Epoch: 31, Loss: 0.47260\n",
      "Epoch: 32, Loss: 0.47090\n",
      "Epoch: 33, Loss: 0.47327\n",
      "Epoch: 34, Loss: 0.46989\n",
      "Epoch: 35, Loss: 0.46892\n",
      "Epoch: 36, Loss: 0.46661\n",
      "Epoch: 37, Loss: 0.46509\n",
      "Epoch: 38, Loss: 0.46514\n",
      "Epoch: 39, Loss: 0.46445\n",
      "Epoch: 40, Loss: 0.46416\n",
      "Epoch: 41, Loss: 0.46074\n",
      "Epoch: 42, Loss: 0.46219\n",
      "Epoch: 43, Loss: 0.46195\n",
      "Epoch: 44, Loss: 0.46066\n",
      "Epoch: 45, Loss: 0.45743\n",
      "Epoch: 46, Loss: 0.45811\n",
      "Epoch: 47, Loss: 0.45799\n",
      "Epoch: 48, Loss: 0.45602\n",
      "Epoch: 49, Loss: 0.45415\n",
      "Epoch: 50, Loss: 0.45412\n",
      "Epoch: 51, Loss: 0.45352\n",
      "Epoch: 52, Loss: 0.45250\n",
      "Epoch: 53, Loss: 0.45074\n",
      "Epoch: 54, Loss: 0.45281\n",
      "Epoch: 55, Loss: 0.44824\n",
      "Epoch: 56, Loss: 0.44892\n",
      "Epoch: 57, Loss: 0.44966\n",
      "Epoch: 58, Loss: 0.44648\n",
      "Epoch: 59, Loss: 0.44867\n",
      "Epoch: 60, Loss: 0.44496\n",
      "Epoch: 61, Loss: 0.44345\n",
      "Epoch: 62, Loss: 0.44587\n",
      "Epoch: 63, Loss: 0.44373\n",
      "Epoch: 64, Loss: 0.44034\n",
      "Epoch: 65, Loss: 0.44206\n",
      "Epoch: 66, Loss: 0.44259\n",
      "Epoch: 67, Loss: 0.44357\n",
      "Epoch: 68, Loss: 0.44092\n",
      "Epoch: 69, Loss: 0.44260\n",
      "Epoch: 70, Loss: 0.44011\n",
      "Epoch: 71, Loss: 0.43777\n",
      "Epoch: 72, Loss: 0.43630\n",
      "Epoch: 73, Loss: 0.43591\n",
      "Epoch: 74, Loss: 0.43466\n",
      "Epoch: 75, Loss: 0.43439\n",
      "Epoch: 76, Loss: 0.43969\n",
      "Epoch: 77, Loss: 0.43926\n",
      "Epoch: 78, Loss: 0.43069\n",
      "Epoch: 79, Loss: 0.43515\n",
      "Epoch: 80, Loss: 0.42936\n",
      "Epoch: 81, Loss: 0.43134\n",
      "Epoch: 82, Loss: 0.42951\n",
      "Epoch: 83, Loss: 0.43099\n",
      "Epoch: 84, Loss: 0.42979\n",
      "Epoch: 85, Loss: 0.42477\n",
      "Epoch: 86, Loss: 0.42727\n",
      "Epoch: 87, Loss: 0.42785\n",
      "Epoch: 88, Loss: 0.42589\n",
      "Epoch: 89, Loss: 0.42931\n",
      "Epoch: 90, Loss: 0.42743\n",
      "Epoch: 91, Loss: 0.42040\n",
      "Epoch: 92, Loss: 0.42554\n",
      "Epoch: 93, Loss: 0.42511\n",
      "Epoch: 94, Loss: 0.42653\n",
      "Epoch: 95, Loss: 0.42301\n",
      "Epoch: 96, Loss: 0.42423\n",
      "Epoch: 97, Loss: 0.42115\n",
      "Epoch: 98, Loss: 0.42043\n",
      "Epoch: 99, Loss: 0.42081\n",
      "\n",
      "Training HGNNP...\n",
      "Epoch: 0, Loss: 0.77593\n",
      "Epoch: 1, Loss: 0.54626\n",
      "Epoch: 2, Loss: 0.53010\n",
      "Epoch: 3, Loss: 0.51273\n",
      "Epoch: 4, Loss: 0.49577\n",
      "Epoch: 5, Loss: 0.48369\n",
      "Epoch: 6, Loss: 0.47760\n",
      "Epoch: 7, Loss: 0.47689\n",
      "Epoch: 8, Loss: 0.47129\n",
      "Epoch: 9, Loss: 0.46308\n",
      "Epoch: 10, Loss: 0.45997\n",
      "Epoch: 11, Loss: 0.45179\n",
      "Epoch: 12, Loss: 0.44965\n",
      "Epoch: 13, Loss: 0.44638\n",
      "Epoch: 14, Loss: 0.44659\n",
      "Epoch: 15, Loss: 0.43888\n",
      "Epoch: 16, Loss: 0.43881\n",
      "Epoch: 17, Loss: 0.43651\n",
      "Epoch: 18, Loss: 0.42904\n",
      "Epoch: 19, Loss: 0.42516\n",
      "Epoch: 20, Loss: 0.42388\n",
      "Epoch: 21, Loss: 0.42166\n",
      "Epoch: 22, Loss: 0.41792\n",
      "Epoch: 23, Loss: 0.41549\n",
      "Epoch: 24, Loss: 0.41303\n",
      "Epoch: 25, Loss: 0.41139\n",
      "Epoch: 26, Loss: 0.40291\n",
      "Epoch: 27, Loss: 0.40190\n",
      "Epoch: 28, Loss: 0.39669\n",
      "Epoch: 29, Loss: 0.39806\n",
      "Epoch: 30, Loss: 0.39179\n",
      "Epoch: 31, Loss: 0.38685\n",
      "Epoch: 32, Loss: 0.38570\n",
      "Epoch: 33, Loss: 0.38610\n",
      "Epoch: 34, Loss: 0.38129\n",
      "Epoch: 35, Loss: 0.37927\n",
      "Epoch: 36, Loss: 0.37303\n",
      "Epoch: 37, Loss: 0.37624\n",
      "Epoch: 38, Loss: 0.36491\n",
      "Epoch: 39, Loss: 0.36688\n",
      "Epoch: 40, Loss: 0.35823\n",
      "Epoch: 41, Loss: 0.35776\n",
      "Epoch: 42, Loss: 0.35323\n",
      "Epoch: 43, Loss: 0.35156\n",
      "Epoch: 44, Loss: 0.35066\n",
      "Epoch: 45, Loss: 0.34579\n",
      "Epoch: 46, Loss: 0.34292\n",
      "Epoch: 47, Loss: 0.33437\n",
      "Epoch: 48, Loss: 0.33539\n",
      "Epoch: 49, Loss: 0.33279\n",
      "Epoch: 50, Loss: 0.32982\n",
      "Epoch: 51, Loss: 0.32197\n",
      "Epoch: 52, Loss: 0.33249\n",
      "Epoch: 53, Loss: 0.32492\n",
      "Epoch: 54, Loss: 0.31782\n",
      "Epoch: 55, Loss: 0.31889\n",
      "Epoch: 56, Loss: 0.31668\n",
      "Epoch: 57, Loss: 0.30872\n",
      "Epoch: 58, Loss: 0.30653\n",
      "Epoch: 59, Loss: 0.31057\n",
      "Epoch: 60, Loss: 0.31115\n",
      "Epoch: 61, Loss: 0.29970\n",
      "Epoch: 62, Loss: 0.30851\n",
      "Epoch: 63, Loss: 0.30161\n",
      "Epoch: 64, Loss: 0.30142\n",
      "Epoch: 65, Loss: 0.29599\n",
      "Epoch: 66, Loss: 0.29206\n",
      "Epoch: 67, Loss: 0.29337\n",
      "Epoch: 68, Loss: 0.30119\n",
      "Epoch: 69, Loss: 0.29898\n",
      "Epoch: 70, Loss: 0.28486\n",
      "Epoch: 71, Loss: 0.29513\n",
      "Epoch: 72, Loss: 0.28461\n",
      "Epoch: 73, Loss: 0.27863\n",
      "Epoch: 74, Loss: 0.28640\n",
      "Epoch: 75, Loss: 0.28297\n",
      "Epoch: 76, Loss: 0.29273\n",
      "Epoch: 77, Loss: 0.27982\n",
      "Epoch: 78, Loss: 0.28075\n",
      "Epoch: 79, Loss: 0.28065\n",
      "Epoch: 80, Loss: 0.28025\n",
      "Epoch: 81, Loss: 0.26988\n",
      "Epoch: 82, Loss: 0.27705\n",
      "Epoch: 83, Loss: 0.27636\n",
      "Epoch: 84, Loss: 0.27104\n",
      "Epoch: 85, Loss: 0.26795\n",
      "Epoch: 86, Loss: 0.27578\n",
      "Epoch: 87, Loss: 0.26968\n",
      "Epoch: 88, Loss: 0.26599\n",
      "Epoch: 89, Loss: 0.26709\n",
      "Epoch: 90, Loss: 0.26729\n",
      "Epoch: 91, Loss: 0.26948\n",
      "Epoch: 92, Loss: 0.27705\n",
      "Epoch: 93, Loss: 0.26136\n",
      "Epoch: 94, Loss: 0.26204\n",
      "Epoch: 95, Loss: 0.26939\n",
      "Epoch: 96, Loss: 0.26389\n",
      "Epoch: 97, Loss: 0.26466\n",
      "Epoch: 98, Loss: 0.25619\n",
      "Epoch: 99, Loss: 0.25468\n",
      "\n",
      "Training UniGCN...\n",
      "Epoch: 0, Loss: 0.87361\n",
      "Epoch: 1, Loss: 0.55460\n",
      "Epoch: 2, Loss: 0.52932\n",
      "Epoch: 3, Loss: 0.50185\n",
      "Epoch: 4, Loss: 0.48618\n",
      "Epoch: 5, Loss: 0.48152\n",
      "Epoch: 6, Loss: 0.47448\n",
      "Epoch: 7, Loss: 0.47205\n",
      "Epoch: 8, Loss: 0.46349\n",
      "Epoch: 9, Loss: 0.45203\n",
      "Epoch: 10, Loss: 0.45348\n",
      "Epoch: 11, Loss: 0.44363\n",
      "Epoch: 12, Loss: 0.44014\n",
      "Epoch: 13, Loss: 0.44013\n",
      "Epoch: 14, Loss: 0.43754\n",
      "Epoch: 15, Loss: 0.43594\n",
      "Epoch: 16, Loss: 0.42976\n",
      "Epoch: 17, Loss: 0.42355\n",
      "Epoch: 18, Loss: 0.41828\n",
      "Epoch: 19, Loss: 0.41597\n",
      "Epoch: 20, Loss: 0.41544\n",
      "Epoch: 21, Loss: 0.40776\n",
      "Epoch: 22, Loss: 0.40770\n",
      "Epoch: 23, Loss: 0.39922\n",
      "Epoch: 24, Loss: 0.39903\n",
      "Epoch: 25, Loss: 0.39756\n",
      "Epoch: 26, Loss: 0.39307\n",
      "Epoch: 27, Loss: 0.38976\n",
      "Epoch: 28, Loss: 0.38254\n",
      "Epoch: 29, Loss: 0.37883\n",
      "Epoch: 30, Loss: 0.37601\n",
      "Epoch: 31, Loss: 0.37324\n",
      "Epoch: 32, Loss: 0.37644\n",
      "Epoch: 33, Loss: 0.36725\n",
      "Epoch: 34, Loss: 0.36471\n",
      "Epoch: 35, Loss: 0.35826\n",
      "Epoch: 36, Loss: 0.35596\n",
      "Epoch: 37, Loss: 0.35122\n",
      "Epoch: 38, Loss: 0.35162\n",
      "Epoch: 39, Loss: 0.34917\n",
      "Epoch: 40, Loss: 0.34001\n",
      "Epoch: 41, Loss: 0.34370\n",
      "Epoch: 42, Loss: 0.33658\n",
      "Epoch: 43, Loss: 0.33254\n",
      "Epoch: 44, Loss: 0.33267\n",
      "Epoch: 45, Loss: 0.32898\n",
      "Epoch: 46, Loss: 0.32405\n",
      "Epoch: 47, Loss: 0.32216\n",
      "Epoch: 48, Loss: 0.31701\n",
      "Epoch: 49, Loss: 0.31136\n",
      "Epoch: 50, Loss: 0.32024\n",
      "Epoch: 51, Loss: 0.32043\n",
      "Epoch: 52, Loss: 0.31340\n",
      "Epoch: 53, Loss: 0.30724\n",
      "Epoch: 54, Loss: 0.31035\n",
      "Epoch: 55, Loss: 0.30884\n",
      "Epoch: 56, Loss: 0.30704\n",
      "Epoch: 57, Loss: 0.29378\n",
      "Epoch: 58, Loss: 0.29450\n",
      "Epoch: 59, Loss: 0.29468\n",
      "Epoch: 60, Loss: 0.29437\n",
      "Epoch: 61, Loss: 0.29425\n",
      "Epoch: 62, Loss: 0.29893\n",
      "Epoch: 63, Loss: 0.28973\n",
      "Epoch: 64, Loss: 0.28572\n",
      "Epoch: 65, Loss: 0.28711\n",
      "Epoch: 66, Loss: 0.29316\n",
      "Epoch: 67, Loss: 0.28117\n",
      "Epoch: 68, Loss: 0.28749\n",
      "Epoch: 69, Loss: 0.28553\n",
      "Epoch: 70, Loss: 0.28448\n",
      "Epoch: 71, Loss: 0.28617\n",
      "Epoch: 72, Loss: 0.28147\n",
      "Epoch: 73, Loss: 0.28613\n",
      "Epoch: 74, Loss: 0.27334\n",
      "Epoch: 75, Loss: 0.28171\n",
      "Epoch: 76, Loss: 0.28077\n",
      "Epoch: 77, Loss: 0.27724\n",
      "Epoch: 78, Loss: 0.27138\n",
      "Epoch: 79, Loss: 0.27402\n",
      "Epoch: 80, Loss: 0.27417\n",
      "Epoch: 81, Loss: 0.26695\n",
      "Epoch: 82, Loss: 0.26503\n",
      "Epoch: 83, Loss: 0.26273\n",
      "Epoch: 84, Loss: 0.26329\n",
      "Epoch: 85, Loss: 0.26976\n",
      "Epoch: 86, Loss: 0.26135\n",
      "Epoch: 87, Loss: 0.25190\n",
      "Epoch: 88, Loss: 0.25407\n",
      "Epoch: 89, Loss: 0.25985\n",
      "Epoch: 90, Loss: 0.26440\n",
      "Epoch: 91, Loss: 0.26333\n",
      "Epoch: 92, Loss: 0.25466\n",
      "Epoch: 93, Loss: 0.25587\n",
      "Epoch: 94, Loss: 0.24817\n",
      "Epoch: 95, Loss: 0.26755\n",
      "Epoch: 96, Loss: 0.25613\n",
      "Epoch: 97, Loss: 0.26318\n",
      "Epoch: 98, Loss: 0.25815\n",
      "Epoch: 99, Loss: 0.24713\n",
      "\n",
      "=== Experiment 2: With Densest Subgraphs ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rustem_izmailov\\AppData\\Local\\Temp\\ipykernel_1216\\2834835554.py:68: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  lambda x: [{'name': g.strip()} for g in x.split()]\n"
     ]
    }
   ],
   "source": [
    "movie_path = r'.\\datasets\\movie\\movie_dataset.csv'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "evaluator = Evaluator([\"accuracy\", \"f1_score\", {\"f1_score\": {\"average\": \"micro\"}}])\n",
    "\n",
    "experiment = HypergraphExperiment(\n",
    "    data_path=movie_path\n",
    ")\n",
    "experiment.run_experiments(n_samples=3000)\n",
    "\n",
    "experiment.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
